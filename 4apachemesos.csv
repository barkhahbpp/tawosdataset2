Name,Title,Description,Type,Priority,Status,Creation_Date,Estimation_Date,Story_Point
"Apache Mesos","Improve CSI service manager to set node ID for managed CSI plugins","For some CSI Plugins (like NFS CSI plugin), their node service need a node ID specified by container orchestrator, see [here|https://github.com/kubernetes-csi/csi-driver-nfs/blob/d94b64bbb3171a45dd91f8686611a062c0dd6219/deploy/kubernetes/csi-nodeplugin-nfsplugin.yaml#L49] for an example, so we need to improve our CSI service manager to set it when launching managed CSI plugins.",Task,Major,Resolved,"2020-08-12 10:24:47","2020-08-12 09:24:47",2
"Apache Mesos","Add secrets support to the CSI volume managers","We must update our CSI code to pass secrets to CSI drivers when staging/unstaging and publishing/unpublishing volumes. We must ensure that we avoid writing any secrets to disk by holding a secret resolver in the appropriate component to resolve secrets associated with already-attached volumes during/after recovery.",Task,Major,Resolved,"2020-07-28 22:22:38","2020-07-28 21:22:38",5
"Apache Mesos","Add documentation for the `volume/csi` isolator",,Task,Major,Resolved,"2020-07-07 09:17:26","2020-07-07 08:17:26",2
"Apache Mesos","Enable the `volume/csi` isolator in UCR",,Task,Major,Resolved,"2020-07-07 09:14:31","2020-07-07 08:14:31",2
"Apache Mesos","Implement the `recover` method of the `volume/csi` isolator",,Task,Major,Resolved,"2020-07-07 09:13:52","2020-07-07 08:13:52",5
"Apache Mesos","Implement the `cleanup` method of the `volume/csi` isolator",,Task,Major,Resolved,"2020-07-07 09:13:09","2020-07-07 08:13:09",3
"Apache Mesos","Implement the `prepare` method of the `volume/csi` isolator",,Task,Major,Resolved,"2020-07-07 09:12:43","2020-07-07 08:12:43",5
"Apache Mesos","Implement the `create` method of the `volume/csi` isolator",,Task,Major,Resolved,"2020-07-07 09:12:00","2020-07-07 08:12:00",3
"Apache Mesos","Introduce a new agent flag `--csi_plugin_config_dir`",,Task,Major,Resolved,"2020-07-07 09:09:12","2020-07-07 08:09:12",3
"Apache Mesos","Refactor CSI volume manager to support pre-provisioned CSI volumes","The existing [VolumeManager|https://github.com/apache/mesos/blob/1.10.0/src/csi/volume_manager.hpp#L55:L138] is like a wrapper for various CSI gRPC calls, we could consider leveraging it to call CSI plugins rather than making raw CSI gRPC calls in `volume/csi` isolator. But there is a problem, the lifecycle of the volumes managed by VolumeManager starts from the `[createVolume|https://github.com/apache/mesos/blob/1.10.0/src/csi/v1_volume_manager.cpp#L281:L329]` CSI call, but what we plan to support in MVP is pre-provisioned volumes, so we need to refactor VolumeManager by making it support pre-provisioned volumes.",Task,Major,Resolved,"2020-07-07 08:45:09","2020-07-07 07:45:09",5
"Apache Mesos","Improve CSI service manager to support unmanaged CSI plugins","Refactor [CSI service manager|https://github.com/apache/mesos/blob/1.10.0/src/csi/service_manager.hpp#L50:L81] by making it support unmanaged plugins (i.e. the plugin deployed out of Mesos) and make it’s `getServiceEndpoint` method can also return unmanaged plugins's endpoint.",Task,Major,Resolved,"2020-07-07 08:41:49","2020-07-07 07:41:49",3
"Apache Mesos","Update the `CSIPluginInfo` protobuf message for supporting 3rd party CSI plugins","See [here|https://docs.google.com/document/d/1NfWLS2OdiYjXZa2dpd_DOWOK4eou-SedY396Jl68s9Y/edit#bookmark=id.x6m8mytigrg7] for the detailed design.",Task,Major,Resolved,"2020-07-07 04:00:34","2020-07-07 03:00:34",1
"Apache Mesos","Introduce a new volume type `CSI` into the `Volume` protobuf message","See [here|https://docs.google.com/document/d/1NfWLS2OdiYjXZa2dpd_DOWOK4eou-SedY396Jl68s9Y/edit#heading=h.l7wa1w8789pg] for the detailed design.",Task,Major,Resolved,"2020-07-07 03:55:20","2020-07-07 02:55:20",1
"Apache Mesos","MasterQuotaTest.ValidateLimitAgainstConsumed is flaky","Observed in internal CI. Log attached.",Bug,Major,Open,"2020-06-23 01:25:34","2020-06-23 00:25:34",3
"Apache Mesos","CSI External Volumes MVP Design Doc","This ticket tracks the design doc for our initial implementation of external volume support in Mesos using the CSI standard.",Task,Major,Resolved,"2020-06-17 20:47:17","2020-06-17 19:47:17",8
"Apache Mesos","CSI External Volume Support","This epic tracks work for our MVP of external volume support in Mesos using the CSI standard.",Epic,Major,Resolved,"2020-06-17 20:45:51","2020-06-17 19:45:51",13
"Apache Mesos","Docker volume isolator needs to clean up the `info` struct regardless the result of unmount operation","Currently when [DockerVolumeIsolatorProcess::cleanup()|https://github.com/apache/mesos/blob/1.9.0/src/slave/containerizer/mesos/isolators/docker/volume/isolator.cpp#L610] is called, we will unmount the volume first, but if the unmount operation fails we will not remove the container's checkpoint directory and NOT erase the container's `info` struct from `infos`. This is problematic, because the remaining `info` in the `infos` will cause the reference count of the volume is larger than 0, but actually the volume is not being used by any containers. And next time when another container using this volume is destroyed, we will NOT unmount the volume since its reference count will be larger than 1 (see [here|https://github.com/apache/mesos/blob/1.9.0/src/slave/containerizer/mesos/isolators/docker/volume/isolator.cpp#L631:L651] for details) which should be 2, so we will never have chance to unmount this volume.    We have this issue since Mesos 1.0.0 release when Docker volume isolator was introduced.",Bug,Critical,Resolved,"2020-05-13 04:02:40","2020-05-13 03:02:40",3
"Apache Mesos","Authorization for /logging/toggle and  /metrics/snapshot is skipped on Windows.","Due to path::join without specifying a separator being used to join an URI when looking for the authorization callback:  https://github.com/apache/mesos/blob/5e5783d748af17dfb1502df5870a5397879c82f1/3rdparty/libprocess/src/process.cpp#L3845",Bug,Major,Resolved,"2020-04-20 11:24:08","2020-04-20 10:24:08",1
"Apache Mesos","Update the `usage()` method of containerizer to set resource limits in the `ResourceStatistics` protobuf message","In the `ResourceStatistics` protobuf message, there are a couple of issues:   # There are already `cpu_limit` and `mem_limit_bytes` fields, but they are actually CPU & memory requests when resources limits are specified for a task.   # There is already `mem_soft_limit_bytes` field, but this field seems not set anywhere.    So we need to update this protobuf message and also the related containerizer code which set the fields of this protobuf message.",Task,Major,Resolved,"2020-04-15 13:56:52","2020-04-15 12:56:52",3
"Apache Mesos","Add documentation for task resource limits",,Task,Major,Resolved,"2020-04-14 07:41:34","2020-04-14 06:41:34",2
"Apache Mesos","After failover, master crashes on re-adding an agent with maintenance schedule set.","Stacktrace:      This immediately follows re-adding an agent after master failover.    The issue was introduced by this patch:  https://reviews.apache.org/r/71428  which didn't account for the fact that `addSlave()` takes as an argument per-framework used resources that potentially can contain frameworks that were not added to allocator yet.    (Note that when master re-registers an agent, it first calls addSlave(), and only then calls addFramework() for the frameworks recovered from the agent.)",Bug,Blocker,Resolved,"2020-04-03 13:31:31","2020-04-03 12:31:31",1
"Apache Mesos","SSL Improvements",,Epic,Major,Open,"2020-04-02 17:51:30","2020-04-02 16:51:30",13
"Apache Mesos","Mesos agent fails to start on outdated systemd.","Mesos agent refuses to start due to a failure caused by the systemd-specific code:      It turns out that some versions of systemd do not set environment variables `LISTEN_PID`, `LISTEN_FDS` and `LISTEN_FDNAMES` to the Mesos agent process, if its systemd unit is [ill-formed|https://github.com/dcos/dcos/pull/6886/files]. If this happens, `listenFdsWithName` returns an empty list, therefore leading to the error above.    After fixing the problem with the systemd unit, systemd sets the value for `LISTEN_FDNAMES` taken from the `FileDescriptorName` field. In our case, the env variable is set to `systemd:dcos-mesos-slave`. Since the value is expected to be equal to systemd:unknown (for the compatibility with older systemd versions), the mismatch of values happens and we see the same error message.     ",Bug,Major,Resolved,"2020-02-24 15:23:49","2020-02-24 15:23:49",3
"Apache Mesos","After HTTP framework disconnects, heartbeater idle-loops instead of being deleted.","In some cases, Master closes connection of HTTP framework without deleting the heartbeater:  https://github.com/apache/mesos/blob/65e18bef2c5ff356ef74bac9aa79b128c5b186d9/src/master/master.cpp#L3323  https://github.com/apache/mesos/blob/65e18bef2c5ff356ef74bac9aa79b128c5b186d9/src/master/master.cpp#L10910    It can be argued that this does not constitute a leak, because old heartbeaters are deleted on reconnection/removal.    However, this means that for each disconnected framework there is a ResponseHeartbeaterProcess that performs an idle loop.",Bug,Minor,Resolved,"2020-02-13 11:19:52","2020-02-13 11:19:52",1
"Apache Mesos","Reactivating a draining agent leaves the agent in draining state.","When reactivating an agent that's in the draining state, the master erases it from its draining maps, and erases its estimated drain time.    However, it doesn't send any message to the agent, so if the agent is still draining and waiting for tasks to terminate, it will stay in that state, ultimately making any tasks that then get launched get DROPPED due to the agent still being in a draining state.    Seems like we should either:    * Disallow the user from reactivating if still in draining, or  * Send a message to the agent, and have the agent move itself out of draining.",Bug,Major,Resolved,"2020-02-11 22:23:05","2020-02-11 22:23:05",3
"Apache Mesos","Agent draining logging makes it hard to tell which tasks did not terminate.","When draining an agent, it's hard to tell which tasks failed to terminate.    The master prints a count of the tasks remaining (only as VLOG(1) however), but not the IDs:        The agent does not print how many or which ones.    It would be helpful to at least see which tasks need to be drained when it begins, and possibly, upon each check, which ones remain.",Improvement,Major,Resolved,"2020-02-11 22:19:27","2020-02-11 22:19:27",1
"Apache Mesos","Master's agent draining VLOG prints incorrect task counts.","This logic is printing the framework counts of these maps rather than the task counts:    https://github.com/apache/mesos/blob/4575c9b452c25f64e6c6cc3eddc12ed3b1f8538b/src/master/master.cpp#L6318-L6319        Since these are {{hashmap<FrameworkID, hashmap<TaskID, Task>>}}.",Bug,Major,Resolved,"2020-02-11 20:58:08","2020-02-11 20:58:08",1
"Apache Mesos","Libprocess does not properly escape subprocess argument strings on Windows","When running some tests of Mesos on Windows, I discovered that the following command would not execute successfully when passed to the Docker containerizer in {{TaskInfo.command}}:      The following error is found in the task sandbox:  ",Bug,Major,Accepted,"2020-02-05 22:48:02","2020-02-05 22:48:02",2
"Apache Mesos","Detecting whether executor is generated for command task should work when the launcher_dir changes","As currently implemented, on recovery Mesos agent determines that the executor is generated for command task by comparing the executor command with a current path to Mesos executor:    https://github.com/apache/mesos/blob/1.7.x/src/slave/slave.cpp#L9635    During upgrade of production cluster we observed this check to break due to the new launcher_dir being different from the one of checkpointed executor.    This can cause problems of various kind: for example, after such upgrade, Mesos master can begin to treat the checkpointed command executors as subject to resource quota.    Design considerations:   - proper solution is to checkpoint the flag indicating whether the executor is a command/docker one.   - for correct upgrade from older Mesos versions, we will need some kind of workaround to detect command executors after upgrade; the workaround logic should be skipped if there is a checkpointed flag.",Bug,Critical,Resolved,"2019-12-31 17:48:05","2019-12-31 17:48:05",3
"Apache Mesos","Cgroups isolator: update cleanup logic to support nested cgroups","Update Cgroups isolator to cleanup a nested cgroup for a nested container taking into account hierarchical layout of cgroups. Lowest nested cgroups should be destroyed first.",Task,Major,Resolved,"2019-12-23 13:18:43","2019-12-23 13:18:43",5
"Apache Mesos","Cgroups isolator: recover nested cgroups","Update recovery of Cgroups isolator to recover nested cgroups for those nested containers, which were launched in nested cgroups.",Task,Major,Resolved,"2019-12-23 13:16:54","2019-12-23 13:16:54",5
"Apache Mesos","Cgroups isolator: allow updating and isolating resources for nested cgroups","Allow Cgroups isolator to update and isolate resources for nested cgroups.",Task,Major,Resolved,"2019-12-23 13:09:19","2019-12-23 13:09:19",3
"Apache Mesos","Cgroups isolator: create nested cgroups","Update Cgroups isolator to create nested cgroups for a nested container, which supports nested cgroups, during container launch preparation.",Task,Major,Resolved,"2019-12-23 13:06:04","2019-12-23 13:06:04",3
"Apache Mesos","Add the `shared_cgroups` field into  the protobuf message `LinuxInfo`",,Task,Major,Resolved,"2019-12-20 02:07:56","2019-12-20 02:07:56",1
"Apache Mesos","Adapt design for executor domain sockets for agent restarts","During testing, it was found that the proposed design for executor domain sockets does not correctly handle agent restarts; in particular on a domain socket-enabled agent all tasks running in containers with a separate root filesystem would not have survived an agent reboot.    We should change the design to fix that. (and implement the change in a follow-up ticket)",Task,Major,Resolved,"2019-12-18 13:47:16","2019-12-18 13:47:16",5
"Apache Mesos","Implement SSL downgrade on the native SSL socket","The new SSL socket implementation (the non-libevent one) does not currently implement the SSL downgrade hack.  We could probably use {{peek}} to achieve the same result, or modify our socket BIO to look at the first few bytes.",Task,Minor,Resolved,"2019-12-17 23:10:06","2019-12-17 23:10:06",3
"Apache Mesos","Windows: Curl requires zlib when built with SSL support on Windows","After building Windows with --enable-ssl, some curl-related tests, like health check tests, start failing with the odd exit code {{-1073741515}}.    Running curl directly with the Visual Studio debugger yields this error:   !Screen Shot 2019-12-17 at 1.38.43 PM.png|width=343,height=164!    Some documentation online seems to support this additional requirement:   [https://wiki.dlang.org/Curl_on_Windows]",Task,Major,Open,"2019-12-17 21:46:18","2019-12-17 21:46:18",3
"Apache Mesos","Add a timeout on SSL listening sockets for sockets that never complete handshaking","Right now, if a plain socket makes a connection to an SSL server socket, but the plain socket never transmits any data, the server side will keep the connection open indefinitely.  We should consider adding a timeout (or other limit) to prevent a build-up of invalid sockets.",Task,Minor,Open,"2019-12-16 22:02:12","2019-12-16 22:02:12",2
"Apache Mesos","Add a unit test for client-initiated SSL renegotiation","https://www.openssl.org/docs/man1.1.1/man3/SSL_renegotiate.html    On certain versions of TLS, the client can attempt to renegotiate an existing SSL connection at any time.  This basically means performing an SSL handshake again on the same connection.    To ensure our sockets don't break when this happens, we should add a unit test exercising the case.",Task,Minor,Open,"2019-12-16 19:06:05","2019-12-16 19:06:05",2
"Apache Mesos","Consider modifying Process_BENCHMARK_ClientServer to compare socket performance on the new OpenSSL socket implementation","There is a pre-existing benchmark in the libprocess benchmarks.cpp file called {{Process_BENCHMARK_ClientServer}}.  We could have this benchmark make HTTPS connections en-masse as well, to check performance differences between the different implementations of our sockets.    We will have the following implementations:  * Libevent + OpenSSL (Posix & Windows)  * Libev + OpenSSL (Posix)  * Native Windows event loop + OpenSSL    Since the new OpenSSL socket defers work off onto libprocess worker threads, it will be interesting to see if performance improves or not.",Task,Major,Open,"2019-12-16 19:03:29","2019-12-16 19:03:29",3
"Apache Mesos","Update the `update()` method of cgroups subsystem interface to handle container resource limits",,Task,Major,Resolved,"2019-12-06 07:55:32","2019-12-06 07:55:32",1
"Apache Mesos","Update the `update()` method of isolator interface to handle container resource limits",,Task,Major,Resolved,"2019-12-05 03:20:49","2019-12-05 03:20:49",2
"Apache Mesos","Accommodate the Infinity value in JSON","See [here|https://docs.google.com/document/d/1iEXn2dBg07HehbNZunJWsIY6iaFezXiRsvpNw4dVQII/edit?ts=5de78977#heading=h.ejuvxat6x3eb] for what need to be done for this ticket.",Task,Major,Resolved,"2019-12-04 13:50:57","2019-12-04 13:50:57",3
"Apache Mesos","Update default executor to call `LAUNCH_CONTAINER` to launch nested containers","The default executor will be updated to use the LAUNCH_CONTAINER call instead of the LAUNCH_NESTED_CONTAINER call when launching nested containers. This will allow the default executor to set task limits when launching its task containers.",Task,Major,Resolved,"2019-12-03 02:25:02","2019-12-03 02:25:02",2
"Apache Mesos","Implement relative path computation for stout","When using executor domain sockets, we might need to specify relative paths in order to stay below the path length limit of 108 characters.    To do so, we should implement a `path::relative_path()` function in stout that can compute the relative path between two directories.",Task,Major,Resolved,"2019-12-02 11:58:44","2019-12-02 11:58:44",5
"Apache Mesos","Implement chmod() support for stout","When using executor domain sockets, we need to be able to change permissions on the domain socket to 0600. To do that, we should implement a new function `os::chmod()` in stout.",Task,Major,Resolved,"2019-12-02 11:56:29","2019-12-02 11:56:29",1
"Apache Mesos","Let the command executor connect through a domain socket when available","If the command executor is using the v1 API (--http_command_executors agent flag) and the MESOS_DOMAIN_SOCKET environment variable is set, the command executor should use the domain socket to communicate with the agent or die trying.",Task,Major,Resolved,"2019-12-02 11:51:46","2019-12-02 11:51:46",3
"Apache Mesos","Update Docker containerizer to set Docker container's resource limits and `oom_score_adj`","This is to set resource limits for executor which will run as a Docker container.",Task,Major,Resolved,"2019-11-27 13:28:52","2019-11-27 13:28:52",2
"Apache Mesos","Update Docker executor to set Docker container's resource limits and `oom_score_adj`","This is to set resource limits for command task which will run as a Docker container.",Task,Major,Resolved,"2019-11-27 13:28:20","2019-11-27 13:28:20",3
"Apache Mesos","Update the `LaunchContainer` agent API to support container resource limits",,Task,Major,Resolved,"2019-11-27 13:26:58","2019-11-27 13:26:58",2
"Apache Mesos","Update the `update()` method of containerizer to handle container resource limits",,Task,Major,Resolved,"2019-11-27 13:26:39","2019-11-27 13:26:39",5
"Apache Mesos","Add a new reason in `TaskStatus::Reason` for the case that a task is OOM-killed due to exceeding its memory request",,Task,Major,Resolved,"2019-11-27 13:26:12","2019-11-27 13:26:12",1
"Apache Mesos","Update the memory subsystem in the cgroup isolator to set container's memory resource limits and `oom_score_adj`","Update the memory subsystem in the cgroup isolator to set container’s memory resource limits and `oom_score_adj`",Task,Major,Resolved,"2019-11-27 13:24:35","2019-11-27 13:24:35",5
"Apache Mesos","Launch executor container with resource limits","We need to add resource limits into `ContainerConfig` first, and then set the resources limits in it according to the executor/task resource limits when launching executor container.",Task,Major,Resolved,"2019-11-27 13:23:40","2019-11-27 13:23:40",3
"Apache Mesos","Validate task's resources limits and the `share_cgroups` field","When launching a task, we need to validate:   # Only CPU and memory are supported as resource limits.   # Resource limit must be larger than resource request.   # `TaskInfo` can only include resource limits when the relevant agent possesses the TASK_RESOURCE_LIMITS capability.   # The value of the field `share_cgroups` should be same for all the tasks launched by a single default executor.   # It is not allowed to set resource limits for the task which has the field `share_cgroups` set as true.    We also need to add validation to the agent which will ensure that non-debug 2nd-or-lower-level nested containers cannot be launched via the {{LaunchContainer}} call.",Task,Major,Resolved,"2019-11-27 13:19:02","2019-11-27 13:19:02",3
"Apache Mesos","Add a new capability `TASK_RESOURCE_LIMITS` into Mesos agent",,Task,Major,Resolved,"2019-11-27 13:16:18","2019-11-27 13:16:18",1
"Apache Mesos","Add resource limits into the protobuf message `TaskInfo`",,Task,Major,Resolved,"2019-11-27 13:15:46","2019-11-27 13:15:46",1
"Apache Mesos","Libprocess SSL verification can leak memory","In {{process::network::openssl::verify()}}, when the SSL hostname validation scheme is set to openssl, the function can return without freeing an {{X509}} object, leading to a memory leak.",Bug,Major,Resolved,"2019-11-22 20:22:52","2019-11-22 20:22:52",1
"Apache Mesos","Implement agent code to listen on a domain socket","On an agent with executor domain sockets enabled, we need to implement code such that the agent listens for incoming connections on its domain sockets, and creates `Connection` objects through which executor <-> agent v1 communication can happen.    The existing implementation of the I/O switchboard might give some inspiration on how this can be implemented.",Task,Major,Resolved,"2019-11-21 17:36:38","2019-11-21 17:36:38",5
"Apache Mesos","Implement `enable_http_executor_domain_sockets` agent flag","Based on the design in https://docs.google.com/document/d/1RUvjoBvM3UX_lLcq_J_crWpMMn3nO8CY0KWc655ELsM/edit we need a `--enable_http_executor_domain_sockets[=true|false]` flag for the mesos agent.    The basic functionality we'd like for this task is, in pseudocode:       Setting the environment variable can be done in the `slave.cpp:executorEnvironment()` function.    The code that actually creates the socket and puts it into the location pointed to by `MESOS_DOMAIN_SOCKET` will be implemented in a separate ticket.",Task,Major,Resolved,"2019-11-21 17:17:05","2019-11-21 17:17:05",5
"Apache Mesos","Design per-task cgroup isolation","To provide container resource isolation which more closely matches the isolation implied by the Mesos nested container API, we should limit CPU and memory on a per-task basis. The current Mesos containerizer implementation limits CPU and memory at the level of the  executor only, which means that tasks within a task group can burst above their CPU or memory resources. Instead, we should apply these limits using per-task cgroups.",Task,Major,Resolved,"2019-11-20 15:57:59","2019-11-20 15:57:59",5
"Apache Mesos","Agent's 'executorTerminated()' can cause double task status update","When the agent first receives a task status update from an executor, it executes {{Slave::statusUpdate()}}, which adds the task ID to the {{Executor::pendingStatusUpdates}} map, but leaves the ID in {{Executor::launchedTasks}}.    Meanwhile, the code in {{Slave::executorTerminated()}} is not capable of handling the intermediate task state which exists in between the execution of {{Slave::statusUpdate()}} and {{Slave::_statusUpdate()}}. If {{Slave::executorTerminated()}} executes at that point in time, it's possible that the task will be transitioned to a terminal state twice (for example, it could be transitioned to TASK_FINISHED by the executor, then to TASK_FAILED by the agent if the executor suddenly terminates).    If the agent has already received a status update from an executor, that state transition should be honored even if the executor terminates immediately after it's sent. We should ensure that {{Slave::executorTerminated()}} cannot cause a valid update received from an executor to be ignored.",Bug,Major,Resolved,"2019-11-06 20:32:00","2019-11-06 20:32:00",3
"Apache Mesos","Improve v1 operator API read performance.","Currently, the v1 operator API has poor performance relative to the v0 json API. The following initial numbers were provided by [~<USER> from our state serving benchmark:       |OPTIMIZED - Master (baseline)| | | | |  |Test setup|1000 agents with a total of 10000 running tasks and 10000 completed tasks|10000 agents with a total of 100000 running tasks and 100000 completed tasks|20000 agents with a total of 200000 running tasks and 200000 completed tasks|40000 agents with a total of 400000 running tasks and 400000 completed tasks|  |v0 'state' response|0.17|1.66|8.96|12.42|  |v1 x-protobuf|0.35|3.21|9.47|19.09|  |v1 json|0.45|4.72|10.81|31.43|      There is quite a lot of variance, but v1 protobuf consistently slower than v0 (sometimes significantly so) and v1 json is consistently slower than v1 protobuf (sometimes significantly so).    The reason that the v1 operator API is slower is that it does the following:    (1) Construct temporary unversioned state response object by copying in-memory un-versioned state into overall response object. (expensive!)  (2) Evolve it to v1: serialize, de-serialize into v1 overall state object. (expensive!)  (3) Serialize the overall v1 state object to protobuf or json.  (4) Destruct the temporaries (expensive! but is done after response starts serving)    On the other hand, the v0 jsonify approach does the following:    (1) Serialize the in-memory unversioned state into json, by traversing state and accumulating the overall serialized json.    This means that v1 has substantial overhead vs v0, and we need to remove it to bring v1 on-par or better than v0. v1 should serialize directly to json (straightforward with jsonify) or protobuf (this can be done via a io::CodedOutputStream).",Improvement,Major,Resolved,"2019-11-02 23:34:10","2019-11-02 23:34:10",13
"Apache Mesos","Allocator method dispatches can be reordered (relative to scheduler API calls which triggered them).","Observed an example of such reordering on a testing cluster with a V1 framework.  Framework side:   - framework issues ACCEPT for a slave with no operations and a 365+ days filter    - framework issues REVIVE call for all roles (which should clear all filters)   - framework waits for an offer for that slave and never receives it    Master side:   - master receives ACCEPT, processes the first part and starts authorization   - master receives REVIVE and dispatches reviveOffers() to the allocator   - master receives a response from authorizer (for ACCEPT) and dispatches recoverResources() with a 365-day filter to the allocator    *We need to provide an ability for the framework to avoid such kind of reorderings.*    Things to consider:   - v1 framework are not required to use a single connection for API requests; even if they were, there still is a reconnection case, during which the views of the framework and the master on the state of connection might differ. This means that we cannot completely avoid this problem by sequencing processing of requests from the same connection.    - Currently, all calls directly influencing allocator (except for UPDATE_FRAMEWORK) return '202 ACCEPTED` at an early stage of processing. _Unconditionally_ changing this might break compatibility with some existing frameworks.",Bug,Major,Resolved,"2019-10-30 18:05:08","2019-10-30 18:05:08",8
"Apache Mesos","Duplicate tasks if agent partitioned during maintenance down","When the master starts maintenance for a node it    (1) sends a {{ShutdownMessage}} message to agent, and  (2) removes the slave which transitions all tasks to {{TASK_LOST}} and moves them  to the completed task set.    If the {{ShutdownMessage}} isn't fully processed on the agent (e.g., message dropped between (1) and (2), or agent process killed before the executor has shut down), the agent could come back with the lost task running. It would report the task on registration with the master, which would add it to the list of active tasks. With that the same task could be both completed and active.  ",Bug,Major,Resolved,"2019-10-23 14:13:04","2019-10-23 13:13:04",5
"Apache Mesos","Log all reverse DNS lookup failures in 'legacy' TLS (SSL) hostname validation scheme.","There were being logged at VLOG(2):    https://github.com/apache/mesos/blob/1.9.0/3rdparty/libprocess/src/openssl.cpp#L859-L860    In the same spirit as MESOS-9340, we'd like to log all networking related errors as warnings and include any relevant information (IP address, etc).",Improvement,Major,Resolved,"2019-10-22 01:04:45","2019-10-22 00:04:45",1
"Apache Mesos","Add a benchmark for HierarchicalAllocatorProcess::updateAllocation() ","We need a benchmark to capture regressions in the `updateAllocation()` method to avoid problems like described in https://issues.apache.org/jira/browse/MESOS-10015    Initial implementation (which led to filing MESOS-10015): https://reviews.apache.org/r/71639/",Improvement,Major,Resolved,"2019-10-21 19:03:09","2019-10-21 18:03:09",5
"Apache Mesos","updateAllocation() can stall the allocator with a huge number of reservations on an agent.","Currently, updateAllocation() called for a single-object Resources for a single framework on a single slave requires `(total number of frameworks) * (number of resource objects per this slave)^2` calls of `Resource::addable()`    In a cluster with a large number of frameworks this results in severe degradation of allocator performance  when a bunch of RESERVE/UNRESERVE operations occurs for an agent with hundreds of unique resources.     On our testing cluster task we observed task scheduling delays up to 30 minutes due to allocator being occupied with processing UNRESERVE operations.",Bug,Critical,Resolved,"2019-10-21 18:57:23","2019-10-21 17:57:23",5
"Apache Mesos","Have fine grained control over OpenSSL dependency.","We currently decide during the build configuration phase whether to link against OpenSSL or not. Whenever the developer enabled both  libevent+openssl, we internally signal via {{USE_SSL_SOCKET}}.    OpenSSL already provides more to us than only code used for TLS – our JWT handling also relies on OpenSSL.    We should consider cleaning this up. We could introduce another internal configuration signal, say {{HAVE_SSL}}. When configuring, the user could enable SSL without  libevent and get our JWT specific code but no TLS.    In our CMake code, we could replace  [https://github.com/apache/mesos/blob/558829eb24f4ad636348497075bbc0428a4794a4/cmake/CompilationConfigure.cmake#L583-L586] with    When {{-DENABLE_SSL}} gets supplied, the preprocessor would see {{HAVE_SSL}}. When the user supplies {{-DENABLE_SSL}} and {{-DENABLE_LIBVENT}}, the resulting set preprocessor defines would be {{USE_SSL_SOCKET}} and {{HAVE_SSL}}.",Improvement,Minor,Open,"2019-10-18 02:06:54","2019-10-18 01:06:54",3
"Apache Mesos","Implement an SSL socket for Windows, using OpenSSL directly",,Task,Major,Resolved,"2019-10-10 00:19:04","2019-10-09 23:19:04",5
"Apache Mesos","Implement glue code for the Windows event loop and OpenSSL's basic I/O abstraction","In order for the Windows event loop to pass data to the OpenSSL library, we will need some glue code in the form of a BIO:  https://www.openssl.org/docs/man1.1.1/man7/bio.html    This will basically need to wrap the two {{windows::read}} and {{windows::write}} async I/O functions in the appropriate callbacks necessary for OpenSSL.  There are also a few other callbacks necessary.  This page contains the set of functions used to build up a new BIO type:  https://www.openssl.org/docs/man1.1.1/man3/BIO_meth_new.html",Task,Major,Resolved,"2019-10-10 00:16:29","2019-10-09 23:16:29",3
"Apache Mesos","Very large quota values can crash master.","We are observing the following crash on the 1.9.1 master:        Note that the value of disk quota limit is *logged* as negative.    Update: we figured out that in reality the quota limit on that master has been set to an insanely large value.    The situation is exacerbated by the fact that the crash is not guaranteed to occur immediately, i.e. these values might become persisted in the registry.",Bug,Blocker,Resolved,"2019-10-08 15:23:53","2019-10-08 14:23:53",3
"Apache Mesos","Command executor can miss exit status for short-lived commands due to double-reaping.","Hi,    While testing Mesos to see if we could use it at work, I encountered a random bug which I believe happens when a command exits really quickly, when run via the command executor.    See the attached test case, but basically all it does is constantly start exit 0 tasks.    At some point, a task randomly fails with the error Failed to get exit status for Command:               I've had a look at the code, and I found something which could potentially explain it - it's the first time I look at the code so apologies if I'm missing something.     We can see the error originates from `reaped`:    [https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L1017]         Looking at the code, we can see that the `status_` future can be set to `None` in `ReaperProcess::reap`:    [https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/reap.cpp#L69]                        So we could have this if the process has already been reaped (`kill -0` will fail).         Now, looking at the code path which spawns the process:    `launchTaskSubprocess`    [https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L724]         calls `subprocess`:    [https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L315]         If we look at the bottom of the function we can see the following:    [https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462]                        So at this point we've already called `process::reap`.         And after that, the executor also calls `process::reap`:    [https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L801]                        But if we look at the implementation of `process::reap`:    [https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/reap.cpp#L152]              We can see that `ReaperProcess::reap` is going to get called asynchronously.         Doesn't this mean that it's possible that the first call to `reap` set up by `subprocess` ([https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462)|https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462]    will get executed first, and if the task has already exited by that time, the child will get reaped before the call to `reap` set up by the executor ([https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L801]) gets a chance to run?         In that case, when it runs         would return false, `reap` would set the future to None which would result in this error.     ",Bug,Major,Resolved,"2019-10-04 17:34:15","2019-10-04 16:34:15",1
"Apache Mesos","Enable SSL on Windows",,Epic,Major,Resolved,"2019-10-02 19:41:46","2019-10-02 18:41:46",13
"Apache Mesos","Design doc for SSL on Windows",,Task,Major,Resolved,"2019-10-02 19:41:03","2019-10-02 18:41:03",3
"Apache Mesos","Design doc for container bursting",,Task,Major,Resolved,"2019-10-02 19:27:56","2019-10-02 18:27:56",5
"Apache Mesos","Resource Limits and Requests",,Epic,Major,Resolved,"2019-10-02 19:21:36","2019-10-02 18:21:36",13
"Apache Mesos","Update scheduler API documentation for re-reservations",,Task,Major,Open,"2019-10-01 14:18:31","2019-10-01 13:18:31",2
"Apache Mesos","End to end test of framework exercising re-reservations",,Task,Major,Open,"2019-10-01 14:17:55","2019-10-01 13:17:55",3
"Apache Mesos","Remove 'contains' CHECKS in 'update' in the 'Sorter' impls","We need to be able to update allocations so they can change a {{Resource}}'s reservation role. For that we need to remove existing {{contains}} checks in {{Sorter}} implementations.",Task,Major,Open,"2019-10-01 14:17:29","2019-10-01 13:17:29",3
"Apache Mesos","Update 'internal::protobuf::stripAllocationInfo' for 'source' in RESERVE operations",,Task,Major,Open,"2019-10-01 14:07:32","2019-10-01 13:07:32",1
"Apache Mesos","Update 'injectAllocationInfo' to also update a reservation's 'source' field",,Task,Major,Open,"2019-10-01 14:07:01","2019-10-01 13:07:01",1
"Apache Mesos","Update master scheduler call validation for 'source' in RESERVE operations",,Task,Major,Open,"2019-10-01 14:06:18","2019-10-01 13:06:18",1
"Apache Mesos","Update operator API documentation for re-reservations",,Task,Major,Resolved,"2019-10-01 12:30:29","2019-10-01 11:30:29",2
"Apache Mesos","Add end-to-end test excercising re-reservation operator API",,Task,Major,Resolved,"2019-10-01 12:30:08","2019-10-01 11:30:08",3
"Apache Mesos","Update 'Master::authorizeReserveResources' for re-reservations","We need to authorize all modifications to bring {{source}} to common ancestor, and from common ancestor to {{resources}}.   * each removed authorizations needs to be authorized as an {{unreserve}} operation   * each added reservation needs to be authorized as a {{reserve}} operation",Task,Major,Resolved,"2019-10-01 12:29:31","2019-10-01 11:29:31",5
"Apache Mesos","Consolidate 'Master::authorizeReserveResources' overloads","We should remove {{Master::authorizeReserveResources(Resources, Option<Principal>}} in favor of {{Master::authorizeReserveResources(Reserve, Option<Principal>)}}.",Task,Major,Resolved,"2019-10-01 12:24:38","2019-10-01 11:24:38",1
"Apache Mesos","Update 'Master::Http::_reserve' to pass 'source' into generated operation",,Task,Major,Resolved,"2019-10-01 12:23:18","2019-10-01 11:23:18",1
"Apache Mesos","Add 'source' field to scheduler reservation API",,Task,Major,Resolved,"2019-10-01 12:22:39","2019-10-01 11:22:39",1
"Apache Mesos","Update 'Master::Http::_reserve' to also require 'source' resources","We need to always pass {{source}} into {{Master::Http::_reserve}}.",Task,Major,Resolved,"2019-10-01 12:20:54","2019-10-01 11:20:54",2
"Apache Mesos","Update 'getConsumedResources' and 'getResourceConversions' for 'source' in reservations",,Task,Major,Resolved,"2019-10-01 12:18:11","2019-10-01 11:18:11",1
"Apache Mesos","Update validation of 'ReserveResources' for 'source'","We need to update {{master::validation::master::call}} for {{source}}. In particular we need to require that {{source}} and {{resources}} have a common ancestor.",Task,Major,Resolved,"2019-10-01 12:17:28","2019-10-01 11:17:28",1
"Apache Mesos","Provide a function to compute a common reservation ancestor between two 'Resources'","We need to provide a function to compute a common reservation ancestor between two resources, {{Try<Resources> getReservationAncestor(const Resources&, const Resources&)}}.    The common ancestor can be found by repeatedly popping dynamic reservations from the full {{Resources}}.    We should test the following cases:   * either LHS or RHS empty   * both empty -> empty ancestor   * {{STATIC}} reservations on path   * partially reserved LHS/RHS (partially reserved: not all {{Resource}} have the same reservation).",Task,Major,Resolved,"2019-10-01 12:16:27","2019-10-01 11:16:27",2
"Apache Mesos","Intermediate rejection of Reserve operations with source set","We need to update {{Master::authorizeReserveResources}} to reject any {{Reserve}} operation whenever {{source}} is set until we have a proper implementation in place.",Task,Major,Resolved,"2019-10-01 12:13:12","2019-10-01 11:13:12",1
"Apache Mesos","Add a 'source' field to operator API ReserveResources protobuf",,Task,Major,Resolved,"2019-10-01 12:08:56","2019-10-01 11:08:56",1
"Apache Mesos","HierarchicalAllocatorTest.MaintenanceInverseOffers is flaky","This test seems to flake pretty quickly for me under system stress with {{aed0b871479}},      When saturating my system with {{stress-ng}} this test only succeeds ~30% of the time.",Bug,Major,Resolved,"2019-09-30 11:42:53","2019-09-30 10:42:53",1
"Apache Mesos","Add docs for FrameworkInfo updates and the UPDATE_FRAMEWORK call.",,Documentation,Major,Resolved,"2019-09-26 13:23:00","2019-09-26 12:23:00",1
"Apache Mesos","Nvml isolator cannot be disabled which makes it impossible to exclude non-free code","We currently do not allow disabling of the link against {{libnvml}} which is probably not under a free license. This makes it hard to include Mesos at all in distributions requiring only free licenses, see e.g., https://bugzilla.redhat.com/show_bug.cgi?id=1749383.    We should add a configuration time flag to disable this feature completely until we can provide a free replacement.",Bug,Major,Resolved,"2019-09-24 19:29:08","2019-09-24 18:29:08",1
"Apache Mesos","Agent does not check for immutable files while removing persistent volumes (and possibly in other GC operations)","We observed an exit/crash loop on an agent originating from deleting a persistent volume:      This persistent volume happened to have one (or more) files within marked as {{immutable}}.    When the agent went to delete this persistent volume, via {{os::rmdir(...)}}, it encountered these immutable file(s) and exits like:      The agent would then be unable to start up again, because during recovery, the agent would attempt to delete the same persistent volume and fail to do so.    Manually removing the immutable attribute from files within the persistent volume allows the agent to recover:      Immutable attributes can be easily introduced by any tasks running on the agent.  As long as the task has sufficient permissions, it could easily call {{chattr +i ...}}.  This attribute could also affect sandbox GC, which also uses {{os::rmdir}} to clean up.  However, sandbox GC tends to warn rather than exit on failure.",Bug,Major,Accepted,"2019-09-21 20:14:45","2019-09-21 19:14:45",3
"Apache Mesos","Sorter may leak clients allocations.","In MESOS-9015, we allowed resource quantities to change when updating an existing allocation. When the allocation is updated to empty, however, we forget to remove the client in the map in the `sorter::update()` if the `newAllocation` is `empty()`.    https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/sorter/drf/sorter.hpp#L382-L384    The above case could happen, for example, when a CSI volume with a stale profile is destroyed, it would be better to convert it into an empty resource since the disk space is no longer available. ",Bug,Major,Resolved,"2019-09-19 00:09:06","2019-09-18 23:09:06",2
"Apache Mesos","Update Names for TLS-related environment variables in libprocess.","The environment variables `LIBPROCESS_SSL_VERIFY_CERT` and `LIBPROCESS_SSL_REQUIRE_CERT` regularly cause confusion because they do not precisely describe their function.    In particular, one might mistakenly assume that certificates are not required when setting `LIBPROCESS_SSL_REQUIRE_CERT=false`, or that all certificates are verified when `LIBPROCESS_SSL_VERIFY_CERT=true`.    We should rename the options to `LIBPROCESS_SSL_VERIFY_SERVER_CERT` and `LIBPROCESS_SSL_REQUIRE_CLIENT_CERT` to make the semantics more clear.",Improvement,Major,Resolved,"2019-09-18 11:31:11","2019-09-18 10:31:11",5
"Apache Mesos","'dist' and 'distcheck' cmake targets are implemented as shell scripts, so fail on Windows/MSVC.","Mesos failed to build due to error MSB6006: cmd.exe exited with code 1 on Windows using MSVC. It can be first reproduced on {color:#24292e}e0f7e2d{color} reversion on master branch. Could you please take a look at this isssue? Thanks a lot!    Reproduce steps:    1. git clone -c core.autocrlf=true [https://github.com/apache/mesos] D:\mesos\src   2. Open a VS 2017 x64 command prompt as admin and browse to D:\mesos   3. cd src   4. .\bootstrap.bat   5. cd ..   6. mkdir build_x64 && pushd build_x64   7. cmake ..\src -G Visual Studio 15 2017 Win64 -DCMAKE_SYSTEM_VERSION=10.0.17134.0 -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -DPATCHEXE_PATH=C:\gnuwin32\bin -T host=x64   8. msbuild Mesos.sln /p:Configuration=Debug /p:Platform=x64 /maxcpucount:4 /t:Rebuild         ErrorMessage:    67>PrepareForBuild:           Creating directory x64\Debug\dist\dist.tlog\.         InitializeBuildStatus:           Creating x64\Debug\dist\dist.tlog\unsuccessfulbuild because AlwaysCreate was specified.    67>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(209,5): error MSB6006: cmd.exe exited with code 1. [D:\Mesos\build_x64\dist.vcxproj]  67>Done Building Project D:\Mesos\build_x64\dist.vcxproj (Rebuild target(s)) -- FAILED.     ",Bug,Trivial,Resolved,"2019-09-18 09:46:05","2019-09-18 08:46:05",1
"Apache Mesos","WWWAuthenticate header parsing fails when commas are in (quoted) realm","This was discovered when trying to launch the {{[nvcr.io/nvidia/tensorflow:19.08-py3|http://nvcr.io/nvidia/tensorflow:19.08-py3]}} image using the Mesos containerizer. This launch fails with    This is because the [header tokenization in libprocess|https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/http.cpp#L640] can't handle commas in quoted realm values.",Bug,Major,Resolved,"2019-09-17 09:01:15","2019-09-17 08:01:15",1
"Apache Mesos","Agent crashes when trying to destroy orphaned nested container if root container is orphaned as well","Noticed an agent crash-looping when trying to recover. It recognized a container and its nested container as orphaned. When trying to destroy the nested container, the agent crashes. Probably when trying to [get the sandbox path of the root container|https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/containerizer.cpp#L2966].    ",Bug,Critical,Resolved,"2019-09-13 09:54:00","2019-09-13 08:54:00",3
"Apache Mesos","agent should not send `TASK_GONE_BY_OPERATOR` if the framework is not partition aware.","The Mesos agent should not send `TASK_GONE_BY_OPERATOR` if the framework is not partition-aware. We should distinguish the framework capability and send different updates to legacy frameworks.    The issue is exposed from here:  https://github.com/apache/mesos/blob/f0be23765531b05661ed7f1b124faf96744aa80b/src/slave/slave.cpp#L5803    An example to follow:  https://github.com/apache/mesos/blob/f0be23765531b05661ed7f1b124faf96744aa80b/src/master/master.cpp#L9921",Bug,Major,Resolved,"2019-09-12 23:02:22","2019-09-12 22:02:22",1
"Apache Mesos","Support destroying UCR containers in provisioning state","Currently when destroying a UCR container, if the container is in provisioning state, we will wait for the provisioner to finish provisioning before we start destroying the container, see [here|https://github.com/apache/mesos/blob/1.9.0/src/slave/containerizer/mesos/containerizer.cpp#L2685:L2693] for details. This may cause the container stuck at destroying, and more seriously it may cause the subsequent containers created from the same image stuck at provisioning state, because if the first container was stuck at pulling the image somehow, the subsequent containers have to wait for the puller to finish the pulling, see [here|https://github.com/apache/mesos/blob/1.9.0/src/slave/containerizer/mesos/provisioner/docker/store.cpp#L341:L345] for details.    So we'd better to support destroying the container in provisioning state so that the subsequent containers created from the same image will not be affected.",Improvement,Major,Resolved,"2019-09-10 14:01:21","2019-09-10 13:01:21",8
"Apache Mesos","URI stringification constructs malformed URIs.","Setting {{docker_registry=https://docker-cache.example.com/}} and then pulling an image named {{org/image-name:latest, }}the Docker image puller ends up constructing a malformed URL for the manifest:  ",Improvement,Major,Reviewable,"2019-09-06 06:43:09","2019-09-06 05:43:09",3
"Apache Mesos","Mesos may report completed task as running in the state.","When the following steps occur:  1) A graceful shutdown is initiated on the agent (i.e. SIGUSR1 or /master/machine/down).  2) The executor is sent a kill, and the agent counts down on executor_shutdown_grace_period.  3) The executor exits, before all terminal status updates reach the agent. This is more likely if executor_shutdown_grace_period passes.    This results in a completed executor, with non-terminal tasks (according to status updates).    This would produce a confusing report where completed tasks are still TASK_RUNNING.",Bug,Major,Open,"2019-09-04 22:27:40","2019-09-04 21:27:40",5
"Apache Mesos","Agent could fail to report completed tasks.","When agent reregisters with a master, we don't report completed executors for active frameworks. We only report completed executors if the framework is also completed on the agent:    https://github.com/apache/mesos/blob/1.7.x/src/slave/slave.cpp#L1785-L1832",Bug,Major,Resolved,"2019-09-04 22:08:16","2019-09-04 21:08:16",2
"Apache Mesos","New CLI is not included in distribution tarball","The files needed to build the new CLI are not included in distribution tarballs. This makes it impossible to build the CLI from released tarballs, and users have instead build directly from the git sources.",Bug,Major,Resolved,"2019-09-02 20:24:41","2019-09-02 19:24:41",1
"Apache Mesos","Sequence all operations on the agent","The resolution of MESOS-8582 requires that an asynchronous step be added to the code path which applies speculative operations like RESERVE and CREATE on the agent. In order to ensure that the {{FrameworkInfo}} associated with an incoming operation will be successfully retained, we must first unschedule GC on the framework meta directory if the framework struct does not exist but that directory does. By introducing this asynchronous step, we allow the possibility that an operation may be executed out-of-order with respect to an incoming dependent LAUNCH or LAUNCH_GROUP.    For example, if a scheduler issues an ACCEPT call containing both a RESERVE operation  as well as a LAUNCH operation containing a task which consumes the new reserved resources, it's possible that this task will be launched on the agent before the reserved resources exist.    While we already [sequence task launches on a per-executor basis|https://github.com/apache/mesos/blob/9297e2d3b0d44b553fc89bcf5f6109c76cc53668/src/slave/slave.cpp#L2337-L2408], the aforementioned corner case requires that we sequence _all_ offer operations on a per-framework basis.",Task,Major,Open,"2019-08-30 21:50:10","2019-08-30 20:50:10",5
"Apache Mesos","CSI plugins reporting duplicated volumes will crash the agent.","The CSI spec requires volumes to be uniquely identifiable by ID, and thus SLRP currently assumes that a {{ListVolumes}} call does not return duplicated volumes. However, if a SLRP uses a non-conforming CSI plugin that reports duplicated volumes, these volumes would corrupt the SLRP checkpoint and cause the agent to crash at the next reconciliation:    MESOS-9254 introduces periodic reconciliation which make this problem much easier to manifest.",Bug,Blocker,Resolved,"2019-08-29 22:52:55","2019-08-29 21:52:55",2
"Apache Mesos","Flapping tasks with large sandboxes can fill agent disk","If a task on an agent is repeatedly re-launched after failing and pulls a large artifact into its sandbox, it can quickly fill the agent disk. This may happen on a time scale shorter than the disk watch interval, leading to the agent disk filling up.    We should evaluate solutions to this issue. A couple options:  * Perhaps an aggressive (short) disk watch interval is sufficient? We should investigate the performance impact of this approach.  * If the former doesn't work, then maybe polling free disk space whenever a task is launched makes sense? (Rate-limiting this might be necessary)  * Perhaps we can come up with some fundamentally different approach for detecting free disk space which would solve this issue?",Bug,Major,Open,"2019-08-26 23:17:13","2019-08-26 22:17:13",5
"Apache Mesos","Expose quota config in the GET_ROLES response.","Currently, GET_ROLES does not expose the quota configuration, unlike /roles.",Task,Major,Open,"2019-08-24 22:52:39","2019-08-24 21:52:39",1
"Apache Mesos","ExampleTest.DiskFullFramework is slow","Executing {{ExampleTest.DiskFullFramework}} on my setup takes almost 18s in a not optimized build. This is way too long for a default-enabled test.",Bug,Major,Resolved,"2019-08-22 14:15:49","2019-08-22 13:15:49",1
"Apache Mesos","Track allocated/offered in the allocator's role tree.","Currently the allocator's role tree only tracks the reserved resources for each role subtree. For metrics purposes, it would be ideal to track offered / allocated as well.    This requires augmenting the allocator's structs and recoverResources to hold the two categories independently and transition from offered -> allocated as applicable when recovering resources. This might require a slight change to the recoverResources interface.",Task,Major,Resolved,"2019-08-21 19:50:45","2019-08-21 18:50:45",5
"Apache Mesos","master::Slave::hasExecutor occupies 37% of a 150 second perf sample.","If you drop the attached perf stacks into flamescope, you can see that mesos::internal::master::Slave::hasExecutor occupies 37% of the overall samples!    This function does 3 hashmap lookups, 1 can be eliminated for a quick win. However, the larger improvement here will come from eliminating many of the calls to this function.    This was reported by [~<USER>.",Improvement,Major,Resolved,"2019-08-21 17:37:29","2019-08-21 16:37:29",3
"Apache Mesos","Java bindings sporadically fail to build","We sporadically (maybe once a month?) observe build failures in the java bindings in our internal CI. They look like this:    ",Bug,Major,Resolved,"2019-08-21 13:57:32","2019-08-21 12:57:32",3
"Apache Mesos","Dedicate sorter for roles.","Once MESOS-9942 has landed, we can clean up and optimize the sorter for roles. Specifically, each node in the tree (except the root and virtual leaf node) will carry a back pointer to the role tree structure in the allocator. This will eliminate all the state duplications and unnecessary trackings that currently done inside the sorter.",Improvement,Major,Open,"2019-08-20 01:22:12","2019-08-20 00:22:12",5
"Apache Mesos","Deprecate framework sorter.","Given the flat structure of the framework, there is no need to store and sort frameworks in the sorter tree structure. We should deprecate framework sorter. This would dedicate the sorter for roles, opening up room for optimization and cleanup. ",Improvement,Major,Open,"2019-08-20 01:19:31","2019-08-20 00:19:31",8
"Apache Mesos","PersistentVolumeEndpointsTest.DynamicReservation is flaky.",,Bug,Major,Accepted,"2019-08-14 03:00:01","2019-08-14 02:00:01",1
"Apache Mesos","Standalone container documentation","We should add documentation for standalone containers.",Documentation,Major,Resolved,"2019-08-13 21:14:23","2019-08-13 20:14:23",3
"Apache Mesos","53598228fe should be backported to 1.7.x","Commit 53598228fe on the master branch should be backported to 1.7.x.      ",Bug,Blocker,Resolved,"2019-08-13 14:17:32","2019-08-13 13:17:32",1
"Apache Mesos","The agent crashes after the disk du isolator supporting rootfs checks.","This issue was broken by this patch:  https://github.com/apache/mesos/commit/8ba0682521c6051b42f33b3dd96a37f4d46a290d#diff-33089e53bdf9f646cdb9317c212eda02    A task can be launched without disk resource. However, after this patch, if the disk resource does not exist, the agent crashes - because the info->paths only add an entry 'path' when there is a quota and the quota comes from the disk resource.    ",Bug,Blocker,Resolved,"2019-08-12 22:23:11","2019-08-12 21:23:11",2
"Apache Mesos","Master does not handle returning unreachable agents as draining/deactivated","The master has two code paths for handling agent reregistration messages, one culminating in {{Master::___reregisterSlave}} and the other in {{Master::}}{{__reregisterSlave}}. The two paths are not continuations of each other.  Looks like we missed the double-underscore case in the initial implementation.  This is the path that unreachable agents take, when/if they come back to the cluster.  The result is that when unreachable agents are marked for draining, they do not get sent the appropriate message unless they are forced to reregister again (i.e. restarted manually).",Bug,Critical,Resolved,"2019-08-12 19:11:56","2019-08-12 18:11:56",3
"Apache Mesos","Removal of a role from the suppression list should be equivalent to REVIVE.","[~<USER> and [~<USER> pointed out that removal of a role from the suppression list (e.g. via UPDATE_FRAMEWORK) does not clear filters. This means that schedulers have to issue a separate explicit REVIVE for the roles they want to remove.    It seems like these are not the semantics we want, and we should instead be clearing filters upon removing a role from the suppression list.",Improvement,Major,Resolved,"2019-08-09 20:31:02","2019-08-09 19:31:02",3
"Apache Mesos","DRF sorter may omit clients in sorting after removing an inactive leaf node.","The sorter assumes inactive leaf nodes are placed in the tail in the children list of a node.  However, when collapsing a parent node with a single . virtual child node, its position may fail to be updated due to a bug in `Sorter::remove()`:        This bug would manifest, if  (1) we have a/b and a/.  (2) deactivate(a),  i.e. a/. becomes inactive_leaf  (3) remove(a/b)  When these happens, a/. will collapse to `a` as an inactive_leaf, due to the bug above, however, it will not be placed at the end, resulting in all the clients after `a` not included in the sort().    Luckily, this should never happen in practice, because only frameworks will get deactivated, and frameworks don’t have sub clients.  ",Bug,Minor,Resolved,"2019-08-08 19:10:52","2019-08-08 18:10:52",3
"Apache Mesos","OperationReconciliationTest.FrameworkReconciliationRaceWithUpdateSlaveMessage is severely flaky","Flakes are frequently observed in the internal CI.    Example:  ",Bug,Major,Resolved,"2019-08-07 12:48:36","2019-08-07 11:48:36",2
"Apache Mesos","Assertion failed in Master for `Slave::apply` while running `UnreserveVolumeResources` test.","`PersistentVolumeEndpointsTest.UnreserveVolumeResources` test failed:  ",Bug,Major,Accepted,"2019-08-06 13:37:44","2019-08-06 12:37:44",3
"Apache Mesos","Default executor takes a couple of seconds to start and subscribe Mesos agent","When launching a task group, it may take 6 seconds for default executor to start and subscribe Mesos agent:    This is obviously too long which may affect the performance of launching task groups.",Bug,Major,Resolved,"2019-08-06 04:08:29","2019-08-06 03:08:29",2
"Apache Mesos","MasterQuotaTest.RescindOffersEnforcingLimits is flaky","Showed up on ASF CI:  https://builds.apache.org/view/M-R/view/Mesos/job/Mesos-Buildbot/6657/BUILDTOOL=cmake,COMPILER=clang,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--enable-parallel-test-execution=no,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1%20MESOS_TEST_AWAIT_TIMEOUT=60secs,OS=centos:7,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!ubuntu-4)&&(!H21)&&(!H23)&&(!H26)&&(!H27)/consoleFull        If I understand correctly, the offer with resources on a second slave went to the fiirst framework, because the allocator wasn't aware about the second framework when the second slave was added.  ",Bug,Major,Resolved,"2019-08-02 14:04:58","2019-08-02 13:04:58",1
"Apache Mesos","Health check performance decreases on large machines","In recent testing, it appears that the performance of Mesos command health checks decreases dramatically on nodes with large numbers of cores and lots of memory. This may be due to the changes in the cost of forking the agent process on such nodes. We need to investigate this issue to understand the root cause.",Task,Major,Resolved,"2019-07-31 18:04:46","2019-07-31 17:04:46",5
"Apache Mesos","Store a role tree in the allocator.","Currently, the client (role and framework) tree for the allocator is stored in the sorter abstraction. This is not ideal. The role/framework tree is generic information that is needed regardless of the sorter used. The current sorter interface and its associated states are tech debts that contribute to performance slowdown and code convolution.     We should store a role/framework tree in the allocator.",Improvement,Major,Resolved,"2019-07-31 02:36:44","2019-07-31 01:36:44",8
"Apache Mesos","Store a role tree in the master.","Currently, both the master and allocator track known roles in maps (note however that the master does not currently have complete tracking of known roles).    These Role structs track some information about roles, but currently do not track information hierarchically. As a result, when per-role resource quantities were exposed in the API, we had to add code outside of the master's Role struct to perform the hierarchical aggregation.    It would be nice if the master (and allocator) had a complete Role tree stored and updated in an event driven manner to obtain information cheaply at any point in time. Ideally this role tree abstraction can be shared (e.g. with the allocator) which may not be trivial since the information tracked might differ.",Improvement,Major,Open,"2019-07-30 23:09:48","2019-07-30 22:09:48",8
"Apache Mesos","SSL socket error logging can be improved.","While debugging some unrelated linkage problem, I noticed the following error output;      The error message appears not very helpful and that we can improve on.    When receiving a libevent openssl-error, we do not check the error code but pass it on to openssl for retrieving an error string -- this is not ideal considering that openssl does signal more;   The error code 5, which actually means {{SSL_ERROR_SYSCALL}} does hint that we should now check {{errno}} for more information on the problem.  We should only ever invoke openssl's error string generator when we did receive a {{SSL_ERROR_SSL}}.    Also see http://openssl.6102.n7.nabble.com/SSL-read-return-1-error-00000005-lib-0-func-0-DH-lib-tp27612p27613.html",Bug,Minor,Resolved,"2019-07-29 14:44:22","2019-07-29 13:44:22",1
"Apache Mesos","Mesos agent crashes after recovery when there is nested container joins a CNI network","Reproduce steps:    1. Use `mesos-execute` to launch a task group with checkpoint enabled. The task in the task group joins a CNI network `net1` and has health check enabled, and the health check will succeed for the first time, fail for the second time, and succeed for the third time, ... The reason that we do health check in this way is that we want to keep generating status updates for this task after recovery.     2. Restart Mesos agent, and then we will see Mesos agent crashes when it handles `TASK_RUNNING` status update triggered by the health check.     ",Bug,Major,Resolved,"2019-07-28 10:33:36","2019-07-28 09:33:36",2
"Apache Mesos","Introduce a new agent flag and support docker volume chown to task user.","Currently, docker volume is always mounted as root, which is not accessible by non-root task users. For security concerns, there are use cases that operator may only allow non-root users to run as container user and docker volume needs to be supported for those non-root users.    A new agent flag is needed to make this support configurable, because chown-ing a docker volume may be limited to some use case - e.g., multiple non-root users on different hosts sharing the same docker volume simultaneously. Operators are expected to turn on this flag if their cluster's docker volume is not shared by multiple non-root users.",Improvement,Major,Resolved,"2019-07-26 00:30:11","2019-07-25 23:30:11",5
"Apache Mesos","Retain agent draining start time in master","The master should store in memory the last time that a {{DrainSlaveMessage}} was sent to the agent so that this time can be displayed in the web UI. This would help operators determine the expected time at which the agent should transition to DRAINED.    We should update the webui to use that time as a starting point and the {{DrainConfig}}'s {{max_grace_period}} to calculate the expected maximum time until the agent is drained.",Task,Major,Resolved,"2019-07-25 17:01:56","2019-07-25 16:01:56",1
"Apache Mesos","Libprocess tests hangs on arm","  https://builds.apache.org/job/Mesos-Buildbot-ARM/BUILDTOOL=cmake,COMPILER=clang,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--disable-java%20--disable-python%20--disable-parallel-test-execution,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1%20%20MESOS_TEST_AWAIT_TIMEOUT=60secs%20JOBS=16%20GTEST_FILTER=-DiskQuotaTest.SlaveRecovery,label_exp=arm/lastBuild/console",Bug,Major,Resolved,"2019-07-24 10:53:11","2019-07-24 09:53:11",1
"Apache Mesos","jsonify uses non-standard mapping for protobuf map fields.","Jsonify current treats protobuf as a regular repeated field. For example, for the schema         it will produce:        This output cannot be parsed back to proto messages. We need to specialize jsonify for Maps type to get the standard output:    ",Bug,Major,Resolved,"2019-07-23 02:57:41","2019-07-23 01:57:41",3
"Apache Mesos","Using a symlink as the agent's work directory results in non-removal of persistent volume mounts.","The directory layout of the agent's information places created persistent volumes under the agent's {{--work_dir}}:      When these persistent volumes are used, they will (on Linux) generally be mounted underneath the sandbox directory (also located under {{--work_dir}}).  Upon termination of use, persistent volumes are unmounted by reading the mount table, and checking if any mount targets are under the sandbox:      ---    However, when an agent's work directory is placed under a symlink, the same code above might not find any persistent volumes to remove.  This is because the mount table shows the real location on disk, but the sandbox expects the symlinked location.    For example, suppose:  * The {{--work_dir}} is {{/var/run/mesos}}.  * {{/var/run/mesos}} is a symlink pointing to {{/tmp/link}}.    The agent will create sandboxes under paths like {{/var/run/mesos/slave/.../framework/.../...}}.  The mount table however, will show mount targets like {{/tmp/link/slave/.../framework/.../...}}.  Since the mount table target does not start with the sandbox path, the {{filesystem/linux}} isolator will not find any persistent volumes to clean up.  The agent's garbage collector will also fail here, because it tries to unmount any persistent volumes under the agent's work directory.  ",Bug,Major,Open,"2019-07-19 18:59:00","2019-07-19 17:59:00",2
"Apache Mesos","SlaveTest.DrainingAgentRejectLaunch is flaky","We saw {{SlaveTest.DrainingAgentRejectLaunch}} fail repeatedly on ASF Jenkins CI.  ",Bug,Major,Resolved,"2019-07-16 15:18:00","2019-07-16 14:18:00",1
"Apache Mesos","Mesos failed to build due to fatal error C1083 on Windows using MSVC.","Mesos failed to build due to fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory on Windows using MSVC. It can be first reproduced on 6a026e3 reversion on master branch. Could you please take a look at this isssue? Thanks a lot!    Reproduce steps:    1. git clone -c core.autocrlf=true https://github.com/apache/mesos D:\mesos\src  2. Open a VS 2017 x64 command prompt as admin and browse to D:\mesos  3. cd src  4. .\bootstrap.bat  5. cd ..  6. mkdir build_x64 && pushd build_x64  7. cmake ..\src -G Visual Studio 15 2017 Win64 -DCMAKE_SYSTEM_VERSION=10.0.17134.0 -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -DPATCHEXE_PATH=C:\gnuwin32\bin -T host=x64  8. msbuild Mesos.sln /p:Configuration=Debug /p:Platform=x64 /maxcpucount:4 /t:Rebuild         ErrorMessage:    D:\Mesos\src\include\mesos/docker/spec.hpp(29): fatal error C1083: Cannot open include file: 'mesos/docker/spec.pb.h': No such file or directory    D:\Mesos\src\src\slave/volume_gid_manager/state.hpp(21): fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory    D:\Mesos\src\src\slave/volume_gid_manager/state.hpp(21): fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory          ",Bug,Major,Resolved,"2019-07-16 09:29:26","2019-07-16 08:29:26",1
"Apache Mesos","`volume/secret` isolator should cleanup the stored secret from runtime directory when the container is destroyed","`volume/secret` isolator writes secret into a file (its filename is a UUID) under `/run/mesos/.secret` when launching container, but it does not clean up that file when the container is destroyed. Over time, the `/run/mesos/.secret` directory may take up all disk space on the partition.",Bug,Major,Resolved,"2019-07-16 09:05:39","2019-07-16 08:05:39",3
"Apache Mesos","Test various agent state transitions involving agent draining","We should add tests which verify correct behavior in the various cases of transitions between different agent states and the DRAINING or DRAINED states.",Task,Major,Resolved,"2019-07-15 23:21:48","2019-07-15 22:21:48",5
"Apache Mesos","Add AGENT_UPDATED event with drain state.","The master's event stream for schedulers and operators should get a new event whenever an agent's drain or deactivation state changes:  ",Task,Major,Open,"2019-07-13 00:58:37","2019-07-12 23:58:37",3
"Apache Mesos","/roles and GET_ROLES does not always expose parent roles.","If some descendant roles are present in frameworks, then the parent roles will not be exposed in the /roles and GET_ROLES endpoints.    This is because the tracking is currently based on frameworks being subscribed to the role.",Bug,Major,Resolved,"2019-07-12 19:51:02","2019-07-12 18:51:02",3
"Apache Mesos","/roles and GET_ROLES do not expose roles with only static reservations","If a role is only known to the master because of an agent with static reservations to that role, it will not be shown in the /roles and GET_ROLES APIs.    This is because the roles are tracked based on frameworks primarily. We'll need to update the tracking to include when there are agents with reservations.",Bug,Major,Resolved,"2019-07-11 19:31:46","2019-07-11 18:31:46",3
"Apache Mesos","Race condition between two terminal task status updates for Docker/Command executor.","h2. Overview    Expected behavior:   Task successfully finishes and sends TASK_FINISHED status update.    Observed behavior:   Task successfully finishes, but the agent sends TASK_FAILED with the reason REASON_EXECUTOR_TERMINATED.    In normal circumstances, Docker executor [sends|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/docker/executor.cpp#L758] final status update TASK_FINISHED to the agent, which then [gets processed|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5543] by the agent before termination of the executor's process.    However, if the processing of the initial TASK_FINISHED gets delayed, then there is a chance that Docker executor terminates and the agent [triggers|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L6662] TASK_FAILED which will [be handled|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5816-L5826] prior to the TASK_FINISHED status update.    See attached logs which contain an example of the race condition.  h2. Reproducing bug    1. Add the following code:    to the [`ComposingContainerizerProcess::status`|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/composing.cpp#L578]   and to the [`DockerContainerizerProcess::status`|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/docker.cpp#L2167].    2. Recompile mesos    3. Launch mesos master and agent locally    4. Launch a simple Docker task via `mesos-execute`:    h2. Race condition - description    1. Mesos agent receives TASK_FINISHED status update and then subscribes on [`containerizer->status()`|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5754-L5761].    2. `containerizer->status()` operation for TASK_FINISHED status update gets delayed in the composing containerizer (e.g. due to switch of the worker thread that executes `status` method).    3. Docker executor terminates and the agent [triggers|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L6662] TASK_FAILED.    4. Docker containerizer destroys the container. A registered callback for the `containerizer->wait` call in the composing containerizer dispatches [lambda function|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/composing.cpp#L368-L373] that will clean up `containers_` map.    5. Composing c'zer resumes and dispatches `[status()|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/composing.cpp#L579]` method to the Docker containerizer for TASK_FINISHED, which in turn hangs for a few seconds.    6. Corresponding `containerId` gets removed from the `containers_` map of the composing c'zer.    7. Mesos agent subscribes on [`containerizer->status()`|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5754-L5761] for the TASK_FAILED status update.    8. Composing c'zer returns [Container not found|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/composing.cpp#L576] for TASK_FAILED.    9. `[Slave::_statusUpdate|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5826]` stores TASK_FAILED terminal status update in the executor's data structure.    10. Docker containerizer resumes and finishes processing of `status()` method for TASK_FINISHED. Finally, it returns control to the `Slave::_statusUpdate` continuation. This method [discovers|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5808-L5814] that the executor has already been destroyed.",Bug,Blocker,Resolved,"2019-07-10 18:46:27","2019-07-10 17:46:27",8
"Apache Mesos","RoleTest.RolesEndpointContainsConsumedQuota is flaky.",,Bug,Major,Resolved,"2019-07-10 17:08:20","2019-07-10 16:08:20",1
"Apache Mesos","Mesos.UpdateFrameworkV0Test.SuppressedRoles is flaky.","Observed in CI, log attached.      ",Bug,Major,Resolved,"2019-07-04 01:13:19","2019-07-04 00:13:19",1
"Apache Mesos","StorageLocalResourceProviderTest.RetryOperationStatusUpdateAfterRecovery is flaky.","This failed in CI:        Full test output:    ",Bug,Major,Resolved,"2019-07-03 21:18:13","2019-07-03 20:18:13",1
"Apache Mesos","Update SUPPRESS/REVIVE calls to return error codes / 200 OK.","Currently, the SUPPRESS/REVIVE calls always return '202 Accepted' even if the call is invalid.    Instead, to be aligned with UPDATE_FRAMEWORK, these calls should:    -Return 200 OK if successful.  -Return appropriate error response if invalid or erroneous.    For the v0 driver, this means:    -Send back a FrameworkErrorMessage if invalid or erroneous.",Improvement,Major,Accepted,"2019-07-03 20:02:11","2019-07-03 19:02:11",3
"Apache Mesos","Enable libprocess users to pass a custom SSL context when using Socket","Connections made through the `Socket::connect()` API will always use the libprocess-global SSL configuration made through the `LIBPROCESS_SSL_*` environment variables.    Libprocess users might want to override these options while still using the generic socket class.    Therefore we should provide a way to pass custom configuration to the `Socket::connect()` function.",Improvement,Minor,Resolved,"2019-07-02 18:56:34","2019-07-02 17:56:34",5
"Apache Mesos","Mesos did not respond correctly when operations should fail","For testing persistent volumes with {{OPERATION_FAILED/ERROR}} feedbacks, we sshed into the mesos-agent and made it unable to create subdirectories in {{/srv/mesos/work/volumes}}, however, mesos did not respond any operation failed response. Instead, we received {{OPERATION_FINISHED}} feedback.    Steps to recreate the issue:    1. Ssh into a magent.   2. Make it impossible to create a persistent volume (we expect the agent to crash and reregister, and the master to release that the operation is {{OPERATION_DROPPED}}):   * cd /srv/mesos/work (if it doesn't exist mkdir /srv/mesos/work/volumes)   * chattr -RV +i volumes (then no subdirectories can be created)    3. Launch a service with persistent volumes with the constraint of only using the magent modified above.              Logs for the scheduler for receiving `OPERATION_FINISHED`:    (Also see screenshot)         2019-06-27 21:57:11.879 [12768651|rdar://12768651] [Jarvis-mesos-dispatcher-105] INFO c.a.j.s.ServicePodInstance - Stored operation=4g3k02s1gjb0q_5f912b59-a32d-462c-9c46-8401eba4d2c1 and feedback=OPERATION_FINISHED in podInstanceID=4g3k02s1gjb0q on serviceID=yifan-badagents-1         * 2019-06-27 21:55:23: task reached state TASK_FAILED for mesos reason: REASON_CONTAINER_LAUNCH_FAILED with mesos message: Failed to launch container: Failed to change the ownership of the persistent volume at '/srv/mesos/work/volumes/roles/test-2/19b564e8-3a90-4f2f-981d-b3dd2a5d9f90' with uid 264 and gid 264: No such file or directory",Bug,Blocker,Resolved,"2019-07-02 00:29:16","2019-07-01 23:29:16",8
"Apache Mesos","Add environment variable `MESOS_ALLOCATION_ROLE` to the task/container.","Set this env var as the role from the task resource. Here is an example:  https://github.com/apache/mesos/blob/master/src/master/readonly_handler.cpp#L197    We probably want to set this env from executors, by adding this env to CommandInfo.    Mesos and docker containerizers should be supported.",Task,Major,Resolved,"2019-07-01 22:16:35","2019-07-01 21:16:35",3
"Apache Mesos","Document the container environment variables semantics on mesos.","Environment variables might be overwritten on mesos (task env, image env, agent env, etc). We should document how env var could be overwritten.    Also, we need to document the mesos-injected environment variables (like `MESOS_SANDBOX`), and how are they different in different executors:  - What are the mesos-injected environment variables for executor containers  - What are the mesos-injected environment variables for container tasks via default executor  - What are the mesos-injected environment variables for old-style command tasks",Documentation,Major,Open,"2019-07-01 18:47:38","2019-07-01 17:47:38",5
"Apache Mesos","Expose quota consumption in /roles endpoint.","As part of exposing quota consumption to users and displaying quota consumption in the ui, we will need to add it to the /roles endpoint (which is currently what the ui uses for the roles table).",Task,Major,Resolved,"2019-06-28 22:25:00","2019-06-28 21:25:00",3
"Apache Mesos","Simultaneous adding/removal of a role from framework's roles and its suppressed roles crashes the master.","Calling UPDATE_FRAMEWORK with a new role added both to 'FrameworkInfo.roles` and `suppressed_roles` crashes the master.    The first place which doesn't expect this is increasing a `suppressed` allocator metric:  [https://github.com/apache/mesos/blob/fe7be9701e92d863734621ae1a3d339bb8598044/src/master/allocator/mesos/hierarchical.cpp#L507]  [  https://github.com/apache/mesos/blob/fe7be9701e92d863734621ae1a3d339bb8598044/src/master/allocator/mesos/metrics.cpp#L255]    Probably there are other similar places.  Adding a new role in a suppressed state via re-subscribing  should also trigger this bug - haven't checked it",Bug,Blocker,Resolved,"2019-06-27 17:17:19","2019-06-27 16:17:19",5
"Apache Mesos","NetworkInfo from the agent /state endpoint is not correct.","NetworkInfo from the agent /state endpoint is not correct, which is also different from the networkInfo of /containers endpoint. Some frameworks rely on the state endpoint to get the ip address for other containers to run.    agent's state endpoint    agent's /containers endpoint    The ip addresses are different^^.    The container is in RUNNING state and is running correctly. Just the state endpoint is not correct. One thing to notice is that the state endpoint used to show the correct IP. After there was an agent restart and master leader re-election, the IP address in the state endpoint was changed.    Here is the checkpoint CNI network information    ",Bug,Blocker,Resolved,"2019-06-27 02:31:03","2019-06-27 01:31:03",8
"Apache Mesos","Libevent fd cleanup failure may cause hangs in combination with client certificate validation","A listening LibeventSSLSocket will check cryptographic certificate validity during the OpenSSL handshake and afterwards call the `openssl::verify()` function to perform hostname validation and other checks on the client certificate. If these checks fail, the bufferevent is deleted and the connection closed:      However, when we close the socket fd in the above code, libevent had already registered that file descriptor with epoll() to watch for read and write events on that socket. Since the socket is closed, attempts to remove the corresponding fd from the epoll() structs will fail: (See also: https://idea.popcount.org/2017-03-20-epoll-is-fundamentally-broken-22/)      However, that in itself is harmless since the kernel will remove the kernel object that was associated with fd 9 from the data structure associated with that epoll instance in the kernel. So while we get an error attempting to remove fd 9, there is actually nothing left to remove. However, in a case of epoll failure, libprocess does not adjust the number of readers and writers on that file descriptor:      In the above, ctx is part of an array collecting information for each file descriptor. That still wouldn't be so bad, however libevent also only adds file descriptors to `epoll()` struct the *first* time we attempt to create a read or write event on that file descriptor:        So when the same file descriptor is attempted to be used again by libevent for epoll() polling, the process will hang because reads or writes to that file descriptor are never noticed.    This can be reproduced for example by running a test where the `verify()`-callback fails on the server side twice in a row: (note, the LIBPROCESS_IP below is set in order to induce a test failure, result may vary on your local network and ssl configuration)      There is a chance that the issue described here is the same as the ominous issues described in MESOS-3008, ",Bug,Major,Accepted,"2019-06-27 00:53:45","2019-06-26 23:53:45",3
"Apache Mesos","Add enchanced multi-role capability support to python bindings.","Methods of the V0 SchedulerDriver currently not supported by python bindings:   * updateFramework()   * reviveOffers(roles)   * subscribing with non-empty suppressed roles on construction   * suppressOffers(roles), if added to the driver",Task,Major,Resolved,"2019-06-25 18:58:49","2019-06-25 17:58:49",5
"Apache Mesos","Agent should fail task launches while draining",,Task,Major,Resolved,"2019-06-25 12:54:08","2019-06-25 11:54:08",2
"Apache Mesos","Make PushGauges support floating point stats.","Currently, PushGauges are modeled against counters. Thus it does not support floating point stats. This prevents many existing PullGauges to use it. We need to add support for floating point stat.",Bug,Major,Resolved,"2019-06-24 23:50:39","2019-06-24 22:50:39",1
"Apache Mesos","Agent should erase DrainInfo when draining complete","When the agent is in the DRAINING state and it sees that all terminal acknowledgements for completed operations and tasks have been received, it should clear the checkpointed {{DrainInfo}} from disk and from memory so that it no longer believes it is DRAINING. It will then be ready to receive new tasks/operations if it is reactivated.",Task,Major,Resolved,"2019-06-24 17:07:39","2019-06-24 16:07:39",2
"Apache Mesos","REVIVE call with specified role(s) clears filters for all roles of a framework.","As pointed out by [~<USER>, the REVIVE implementation in the allocator incorrectly clears decline filters for all of the framework's roles, rather than only those that were specified in the REVIVE call:    https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1392    This should only clear filters for the roles specified in the REVIVE call.",Bug,Major,Resolved,"2019-06-21 17:49:44","2019-06-21 16:49:44",3
"Apache Mesos","/roles endpoint should return both guarantees and limits. ",,Bug,Major,Resolved,"2019-06-21 01:16:41","2019-06-21 00:16:41",2
"Apache Mesos","Update Docker executor to allow kill policy overrides","In order for the agent to successfully override the task kill policy of Docker tasks when the agent is being drained, the Docker executor must be able to receive kill policy overrides and must be updated to honor them. Since the Docker executor runs using the executor driver, this is currently not possible. We could, for example, update the executor driver interface, or move the Docker executor off of the executor driver.",Task,Major,Resolved,"2019-06-19 20:49:28","2019-06-19 19:49:28",3
"Apache Mesos","Slow memory growth in master due to deferred deletion of offer filters and timers.","The allocator does not keep a handle to the offer filter timer, which means it cannot remove the timer overhead (in this case memory) when removing the offer filter earlier (e.g. due to revive):    https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1338-L1352    In addition, the offer filter is allocated on the heap but not deleted until the timer fires (which might take forever!):    https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1321  https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1408-L1413  https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L2249    We'll need to try to backport this to all active release branches.",Bug,Critical,Resolved,"2019-06-19 17:47:15","2019-06-19 16:47:15",3
"Apache Mesos","Migrate allocator metrics to PushGauge.","We should migrate all metrics in the master actor to use PushGauges instead of PullGauges for better performance.",Bug,Major,Open,"2019-06-19 02:22:51","2019-06-19 01:22:51",5
"Apache Mesos","Add support for per-role REVIVE / SUPPRESS to V0 scheduler driver.","Unfortunately, there are still schedulers that are using the v0 bindings and are unable to move to v1 before wanting to use the per-role REVIVE / SUPPRESS calls.    We'll need to add per-role REVIVE / SUPPRESS into the v1 scheduler driver.",Task,Major,Resolved,"2019-06-17 17:44:16","2019-06-17 16:44:16",5
"Apache Mesos","Docker executor doesn't wait for status updates to be ack'd before shutting down.","The docker executor doesn't wait for pending status updates to be acknowledged before shutting down, instead it sleeps for one second and then terminates:        This would result in racing between task status update (e.g. TASK_FINISHED) and executor exit. The latter would lead agent generating a `TASK_FAILED` status update by itself, leading to the confusing case where the agent handles two different terminal status updates.",Bug,Major,Resolved,"2019-06-14 00:45:41","2019-06-13 23:45:41",5
"Apache Mesos","Update UI for agent draining","We should expose the new agent metadata in the web UI:  * Drain info  * Deactivation state    It may also be worth exposing unreachable and gone agents in some way, so that agents do not simply disappear from the UI when they transition to unreachable and/or gone, during or after maintenance.",Task,Major,Resolved,"2019-06-13 18:30:15","2019-06-13 17:30:15",3
"Apache Mesos","Add docs for automatic agent draining","Will probably require:  * A separate page describing the feature (in lieu or superceding the maintenance doc)  * Updates to the API docs, for master and agent APIs.  Any GET_STATE or similar call changes will also be included.",Task,Major,Resolved,"2019-06-13 18:28:46","2019-06-13 17:28:46",5
"Apache Mesos","Update documentation describing `containerizer/debug` endpoint.",,Documentation,Major,Resolved,"2019-06-12 15:36:47","2019-06-12 14:36:47",3
"Apache Mesos","Implement tests for the `containerizer/debug` endpoint.","Implement tests for container stuck issues and check that the agent's `containerizer/debug` endpoint returns a JSON object containing information about pending operations.",Task,Major,Resolved,"2019-06-12 15:35:26","2019-06-12 14:35:26",3
"Apache Mesos","Implement tests for the `FutureTracker` class and for its helper functions.",,Task,Major,Resolved,"2019-06-12 15:32:32","2019-06-12 14:32:32",3
"Apache Mesos","Integrate `IsolatorTracker` and `LinuxLauncher` with Mesos containerizer.",,Task,Major,Resolved,"2019-06-12 15:29:23","2019-06-12 14:29:23",3
"Apache Mesos","Implement `LauncherTracker` class.",,Task,Major,Resolved,"2019-06-12 15:28:24","2019-06-12 14:28:24",3
"Apache Mesos","Implement `IsolatorTracker` class.",,Task,Major,Resolved,"2019-06-12 15:26:21","2019-06-12 14:26:21",3
"Apache Mesos","Implement `FutureTracker` class along with helper functions.","Both `track()` and `pending_futures()` helper functions depend on the `FutureTracker` actor.  `FutureTracker` actor must be available globally and there must be only one instance of this actor.",Task,Major,Resolved,"2019-06-12 11:53:42","2019-06-12 10:53:42",5
"Apache Mesos","Docker containerizer overwrites `/mesos/slave` cgroups.","The following bug was observed on our internal testing cluster.    The docker containerizer launched a container on an agent:    After the container was launched, the docker containerizer did a {{docker inspect}} on the container and cached the pid:   [https://github.com/apache/mesos/blob/0c431dd60ae39138cc7e8b099d41ad794c02c9a9/src/slave/containerizer/docker.cpp#L1764]   The pid should be slightly greater than 13716.    The docker executor sent a {{TASK_FINISHED}} status update around 16 minutes later:    After receiving the terminal status update, the agent asked the docker containerizer to update {{cpu.cfs_period_us}}, {{cpu.cfs_quota_us}} and {{memory.soft_limit_in_bytes}} of the container through the cached pid:   [https://github.com/apache/mesos/blob/0c431dd60ae39138cc7e8b099d41ad794c02c9a9/src/slave/containerizer/docker.cpp#L1696]    Note that the cgroup of {{cpu.shares}} was {{/mesos/slave}}. This was possibly because that over the 16 minutes the pid got reused:    It was highly likely that the container itself exited around 06:09:35, way before the docker executor detected and reported the terminal status update, and then its pid was reused by another forked child of the agent, and thus {{cpu.cfs_period_us}}, {{cpu.quota_us}} and {{memory.soft_limit_in_bytes}} of the {{/mesos/slave}} cgroup was mistakenly overwritten.",Bug,Critical,Resolved,"2019-06-11 04:27:56","2019-06-11 03:27:56",5
"Apache Mesos","`QuotaRoleAllocateNonQuotaResource` is failing.","    The test is failing because:    After agent3 is added, it misses a settle call where the allocation of agent3 is racy.  In addition, after https://github.com/apache/mesos/commit/7df8cc6b79e294c075de09f1de4b31a2b88423c8  we now offer nonquota resources on an agent (even that means chopping) on top of role's satisfied guarantees, the test needs to be updated in accordance with the behavior change.",Bug,Major,Resolved,"2019-06-11 01:42:13","2019-06-11 00:42:13",2
"Apache Mesos","Introduce an agent flag for the default `/dev/shm` size",,Improvement,Major,Resolved,"2019-06-07 00:17:24","2019-06-06 23:17:24",2
"Apache Mesos","Master should not report disconnected resource providers.","MESOS-9384 attempted to make the master to garbage-collect disconnected resource providers. However, if there are disconnected resource providers but none of the connected ones changes, the following code snippet would make the master ignore the agent update and skip the garbage collection:  https://github.com/apache/mesos/blob/2ae1296c668686d234be92b00bd7abbc0a6194b0/src/master/master.cpp#L8186-L8234  The condition to ignore the agent update will be triggered in one of the following conditions:  1. The resource provider has no resource, so the agent's total resource remains the same.  2. When the agent restarts and reregisters, its resource provider resources will be reset.    As a result, the master will still keep records for the disconnected resource providers and report them.",Bug,Blocker,Resolved,"2019-06-05 22:36:33","2019-06-05 21:36:33",2
"Apache Mesos","Implement the container debug endpoint on slave/http.cpp",,Task,Major,Resolved,"2019-06-05 19:46:18","2019-06-05 18:46:18",5
"Apache Mesos","Document the IPC namespace and shm on UCR.",,Task,Major,Resolved,"2019-06-05 19:32:47","2019-06-05 18:32:47",3
"Apache Mesos","Introduce the configurable shm protobuf API.",,Task,Major,Resolved,"2019-06-05 19:31:09","2019-06-05 18:31:09",3
"Apache Mesos","Set up `/dev/shm` in `filesystem/linux` isolator only when `namespaces/ipc` isolator is not enabled",,Task,Major,Resolved,"2019-06-05 19:30:20","2019-06-05 18:30:20",2
"Apache Mesos","Introduce an agent flag to disallow sharing the IPC namespace from the host.",,Task,Major,Resolved,"2019-06-05 19:29:06","2019-06-05 18:29:06",2
"Apache Mesos","Support rdma cgroup subsystem.","Support rdma cgroup subsystem.    This subsystem is supported by some latest kernel versions, e.g., kernel 5.1.15.",Task,Major,Open,"2019-06-05 18:27:36","2019-06-05 17:27:36",8
"Apache Mesos","Agent should modify status updates while draining","While it's draining, the agent should decorate TASK_KILLING and TASK_KILLED status updates with REASON_AGENT_DRAINING. It should also convert TASK_KILLED to TASK_GONE_BY_OPERATOR in the {{mark_gone}} case, ensuring that TASK_GONE_BY_OPERATOR is the state checkpointed to disk.",Task,Major,Resolved,"2019-06-05 17:56:59","2019-06-05 16:56:59",3
"Apache Mesos","Agent recovery code for task draining","In the case where the agent crashes while it's in the process of killing tasks due to agent draining, it must recover the checkpointed {{DrainInfo}} and kill any tasks which did not have KILL events sent to them already.",Task,Major,Resolved,"2019-06-05 17:55:20","2019-06-05 16:55:20",2
"Apache Mesos","Agent kills all tasks when draining","The agent's {{DrainSlaveMessage}} handler should kill all tasks when draining is initiated, specifying a kill policy with a grace period equal to the minimum of the task's grace period and the min_grace_period specified in the drain message.",Task,Major,Resolved,"2019-06-05 17:54:00","2019-06-05 16:54:00",2
"Apache Mesos","Add `updateQuota()` method to the allocator.","This is the method that underlies the `UPDATE_QUOTA` operator call. This will allow the allocator to set different values for guarantees and limits.    The existing `setQuota` and `removeQuota` methods in the allocator will be deprecated. This will likely break many existing allocator tests. We should fix and refactor tests to verify the bursting up to limits feature.",Improvement,Major,Resolved,"2019-06-05 04:46:43","2019-06-05 03:46:43",5
"Apache Mesos","Update the agent's behavior when marked GONE","Currently, when an agent is marked GONE, the master sends a {{ShutdownMessage}} to the agent, which causes it to shutdown all frameworks and then terminate.    As part of the agent draining work, we would like to change this behavior so that instead of terminating, the agent will sleep indefinitely once all frameworks are shut down. This will avoid the issue of a flapping agent process when the agent is managed by an init service like systemd.",Task,Major,Resolved,"2019-06-04 23:09:23","2019-06-04 22:09:23",5
"Apache Mesos","Implement minimal agent-side draining handler","To unblock other work that can be done in parallel, this ticket captures the implementation of a handler for the {{DrainSlaveMessage}} in the agent which will:  * Checkpoint the {{DrainInfo}}  * Populate a new data member in the agent with the {{DrainInfo}}  ",Task,Major,Resolved,"2019-06-04 23:06:53","2019-06-04 22:06:53",2
"Apache Mesos","Add minimum master capability for draining and deactivation states","Since we are adding new fields to the registry to represent agent draining/deactivation, we cannot allow downgrades of masters while such features are in use.      A new minimum capability should be added to the registry with the appropriate documentation:  https://github.com/apache/mesos/blob/663bfa68b6ab68f4c28ed6a01ac42ac2ad23ac07/src/master/master.cpp#L1681-L1688  http://mesos.apache.org/documentation/latest/downgrades/",Task,Major,Resolved,"2019-06-04 22:52:50","2019-06-04 21:52:50",3
"Apache Mesos","Add draining state information to master state endpoints","The response for {{GET_STATE}} and {{GET_AGENTS}} should include the new fields indicating deactivation or draining states:      The {{/state}} and {{/state-summary}} handlers should also expose this information.",Task,Major,Resolved,"2019-06-04 22:47:17","2019-06-04 21:47:17",3
"Apache Mesos","Deprecate maintenance primitives","The existing maintenance primitives should be marked deprecated in the protobuf definitions, and the documentation should be mostly removed with links which redirect to the new agent draining feature.    The {{updateMaintenanceSchedule()}} handler code path should also be updated to verify that the new agent draining feature is not in use before allowing maintenance schedules to be created.",Task,Major,Resolved,"2019-06-04 22:46:02","2019-06-04 21:46:02",5
"Apache Mesos","Implement DrainAgent master/operator call with associated registry actions","We want to add several calls associated with agent draining:      Each field will be persisted in the registry:  ",Task,Major,Resolved,"2019-06-04 22:44:19","2019-06-04 21:44:19",8
"Apache Mesos","Track role consumed quota for all roles in the allocator.","We are already tracking role consumed quota for roles with non-default quota in the allocator. We should expand that to track all roles' consumptions which will then be exposed through metrics later.",Improvement,Major,Resolved,"2019-06-04 22:26:42","2019-06-04 21:26:42",3
"Apache Mesos","Add achievability validation for update quota call.","Add overcommit check, hierarchical quota validation and force flag override for update quota call.    Right now, we only have validation for per quota config. We need to add further validation for the update quota call regarding:    1. Check if the role's resource limits are already breached. To achieve this, we need to first rescind offers until its allocated resources are below limits. If after all rescinds, allocated resources are still above the requested limits, we will return an error unless the `force` flag is used.    2. If the aggregated quota guarantees of all roles are less than the cluster capacity. If so we will return an error unless the `force` flag is used.    3. hierarchical limits validation    a. Check a role's limit is less than its parent's limit.    b. Check the sum of children's guarantees is less than its parent's guarantees.",Improvement,Major,Resolved,"2019-06-04 19:20:20","2019-06-04 18:20:20",5
"Apache Mesos","Don't use reverse DNS for hostname validation","Upon connection we first resolve the hostname and forget about it    https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/http.cpp#L1462-L1504    then later use reverse DNS on the remote address to get back a hostname    https://github.com/apache/mesos/blob/4708c2a368e12a89669135f47777d0dd05d9b0b2/3rdparty/libprocess/src/posix/libevent/libevent_ssl_socket.cpp#L548-L556    and verify the server certificate against *that*.    Instead, we should verify the server certificate against the hostname that was used by t he client to initiate the connection.",Bug,Major,Resolved,"2019-06-04 00:01:00","2019-06-03 23:01:00",5
"Apache Mesos","Reject certificate-less ciphers when certificate verification is enabled","A TLS server is required by the spec to always send a server certificate, unless an anonymous cipher is used.    In libprocess, this certificate is verified to be valid and trusted when the flag LIBPROCESS_VERIFY_CERT is set to true.    However, when an anonymous cipher is used, the server does not present a certificate, meaning the verification step will not happen. If a TLS server would be allowed to use such a cipher, it could trivially sidestep the security provided by certificate verification.    Therefore, we should always reject connections using anonymous ciphers when certificate verification is enabled.",Task,Major,Resolved,"2019-06-03 23:55:36","2019-06-03 22:55:36",5
"Apache Mesos","Use OpenSSL built-in functions for hostname validation","We traditionally use a hand-written hostname validation algorithm in libprocess that is based on the example code in https://wiki.openssl.org/index.php/Hostname_validation    However, since OpenSSL 1.1.0, there is a new built-in function API `SSL_set1_host()` that can be used to let OpenSSL handle hostname validation during the TLS handshake in a standardized manner.    We should take advantage of this when possible.",Task,Major,Resolved,"2019-06-03 23:40:44","2019-06-03 22:40:44",5
"Apache Mesos","Introduce a `struct Quota` wrapper.","We should introduce:    struct Qutota {    ResourceQuantities guarantees;    ResourceLimits limits;  }    There are a couple of small hurdles. First, there is already a struct Quota wrapper in include/mesos/quota/quota.hpp, we need to deprecate that first. Second, `ResourceQuantities` and `ResourceLimits` are right now only used in internal headers. We probably want to move them into public header, since this struct will also be used in allocator interface which is also in the public header. (Looking at this line, the boundary is alreayd breached: https://github.com/apache/mesos/blob/master/include/mesos/allocator/allocator.hpp#L41)",Improvement,Major,Resolved,"2019-06-03 17:15:15","2019-06-03 16:15:15",5
"Apache Mesos","Address allocator performance regression due to the addition of quota limits.","In MESOS-9802, we removed the quota role sorter which is tech debt.    However, this slows down the allocator. The problem is that in the first stage, even though a cluster might have no active roles with non-default quota, the allocator will now have to sort and go through each and every role in the cluster. Benchmark result shows that for 1k roles with 2k frameworks, the allocator could experience ~50% performance degradation.    There are a couple of ways to address this issue. For example, we could make the sorter aware of quota. And add a method, say `sortQuotaRoles`, to return all the roles with non-default quota. Alternatively, an even better approach would be to deprecate the sorter concept and just have two standalone functions e.g. sortRoles() and sortQuotaRoles() that takes in the role tree structure (not yet exist in the allocator) and return the sorted roles.    In addition, when implementing MESOS-8068, we need to do more during the allocation cycle. In particular, we need to call shrink many more times than before. These all contribute to the performance slowdown. Specifically, for the quota oriented benchmark `HierarchicalAllocator_WithQuotaParam.LargeAndSmallQuota/2` we can observe 2-3x slowdown compared to the previous release (1.8.1):    Current master:    QuotaParam/BENCHMARK_HierarchicalAllocator_WithQuotaParam.LargeAndSmallQuota/2  Benchmark setup: 3000 agents, 3000 roles, 3000 frameworks, with drf sorter  Made 3500 allocations in 32.051382735secs  Made 0 allocation in 27.976022773secs    1.8.1:  HierarchicalAllocator_WithQuotaParam.LargeAndSmallQuota/2  Made 3500 allocations in 13.810811063secs  Made 0 allocation in 9.885972984secs",Improvement,Critical,Resolved,"2019-06-03 01:54:06","2019-06-03 00:54:06",5
"Apache Mesos","Memory leak caused by an infinite chain of futures in `UriDiskProfileAdaptor`.","Before MESOS-8906, {{UriDiskProfileAdaptor}} only update its promise for watchers if the polled profile matrix becomes larger in size, and this prevents the following code in the {{watch}} function from creating an infinite chain of futures when the profile matrix keeps the same:  https://github.com/apache/mesos/blob/fa410f2fb8efb988590f4da2d4cfffbb2ce70637/src/resource_provider/storage/uri_disk_profile_adaptor.cpp#L159-L160    However, the patch of MESOS-8906 removes the size check in the {{notify}} function to allow profile selectors to be updated. As a result, once the watch function is called, the returned future will be chained with a new promise every time a poll is made, hence creating a memory leak.    A jemalloc call graph for a 2hr trace is attached.",Bug,Blocker,Resolved,"2019-05-30 04:06:51","2019-05-30 03:06:51",2
"Apache Mesos","Remove quota role sorter in the allocator.","Remove the dedicated quota role sorter in favor of using the same sorting between satisfying guarantees and bursting above guarantees up to limits. This is tech debt from when a quota role was considered different from a non-quota role. However, they are the same, one just has a default quota.    The only practical difference between quota role sorter and role sorter now is that quota role sorter ignores the revocable resources both in its total resource pool as well as role allocations. Thus when using DRF, it does not count revocable resources which is arguably the right behavior.    By removing the quota sorter, we will have all roles sorted together. When using DRF, in the 1st quota guarantee allocation stage, its share calculation will also include revocable resources.",Improvement,Major,Resolved,"2019-05-29 11:31:12","2019-05-29 10:31:12",2
"Apache Mesos","Design doc for container debug endpoint.","Design doc for container debug endpoint.",Task,Major,Resolved,"2019-05-22 18:30:18","2019-05-22 17:30:18",8
"Apache Mesos","Implement UPDATE_FRAMEWORK call in V0 API for C++/Java",,Task,Major,Resolved,"2019-05-22 17:37:29","2019-05-22 16:37:29",8
"Apache Mesos","Configurable IPC namespace and shared memory in `namespaces/ipc` isolator","See [design doc|https://docs.google.com/document/d/10t1jf97vrejUWEVSvxGtqw4vhzfPef41JMzb5jw7l1s/edit?usp=sharing] for the background of this improvement and how we are going to implement it.",Task,Major,Resolved,"2019-05-17 11:21:23","2019-05-17 10:21:23",8
"Apache Mesos","Race between two REMOVE_QUOTA calls crashes the master.","The existence of the quota in the master is validated here:  [https://github.com/apache/mesos/blob/a9a2acabd03181865055b77cf81e7bb310b236d6/src/master/quota_handler.cpp#L700]    Then the quota is removed from master in a deferred method call:  [https://github.com/apache/mesos/blob/a9a2acabd03181865055b77cf81e7bb310b236d6/src/master/quota_handler.cpp#L744]    And then removed from allocator in another deferred call:  [https://github.com/apache/mesos/blob/a9a2acabd03181865055b77cf81e7bb310b236d6/src/master/quota_handler.cpp#L753]    So, there is a race between two simultaneous REMOVE_QUOTA calls.    We observe this race on a heavily loaded cluster. Currently we suspect that the client retries the call (due to the call being not processed for a long time),  and this triggers the race.",Bug,Critical,Resolved,"2019-05-15 15:25:15","2019-05-15 14:25:15",2
"Apache Mesos","Frameworks recovered from reregistered agents are not reported to master `/api/v1` subscribers.","Currently when an operator subscribes to the {{/api/v1}} master endpoint, it would receive a {{SUBSCRIBED}} event carrying information about all known frameworks, including registered ones and unregistered ones. If an unregistered framework reregisters later, a {{FRAMEWORK_UPDATED}} event would be sent to the operator.    However, if an operator subscribes to the {{/api/v1}} master endpoint after a master failover but before any of the frameworks and agents reregisters, {{SUBSCRIBED}} would contain no recovered framework information. When a agent with running tasks reregisters later, unregistered frameworks of those tasks will be recovered, but no {{FRAMEWORK_ADDED}} will be sent to the operator, so the operator will receive {{TASK_ADDED}} for those tasks with unknown framework IDs.",Bug,Critical,Resolved,"2019-05-14 22:45:06","2019-05-14 21:45:06",2
"Apache Mesos","Centos 6 RPM build is broken on Apache CI","The centos 6 rpm build on the Apache CI on `build.apache.org` has been broken since April 16, as it fails on the following step:        The URL returns a 404 response because the package was removed from the upstream fileserver.",Improvement,Major,Resolved,"2019-05-14 11:02:48","2019-05-14 10:02:48",1
"Apache Mesos","Random sorter fails to clear removed clients.","In `RandomSorter::SortInfo::updateRelativeWeights()`, we do not clear the stale `clients` and `weights` vector if the state is dirty. This would result in an allocator crash due to including removed framework and roles in a sorted result e.g. check failure would occur here (https://github.com/apache/mesos/blob/62f0b6973b2268a3305fd631a914433a933c6757/src/master/allocator/mesos/hierarchical.cpp#L1849).",Bug,Critical,Resolved,"2019-05-13 13:32:19","2019-05-13 12:32:19",1
"Apache Mesos","`UPDATE_RESOURCE_PROVIDER_CONFIG` agent call returns 404 ambiguously.","The {{UPDATE_RESOURCE_PROVIDER_CONFIG}} API call returns 404 if the specified resource provider does not exist. However, libprocess also returns 404 when the `/api/v1` route is not set up. As a result, a client will get confused when receiving 404 and wouldn't know the actual state of the resource provider config. We should not overload 404 with different errors.    The other codes for client errors returned by this call are:  * 400 if the request is not well-formed.  * 403 if the call is not authorized.    To avoid ambiguity, we could keep 404 to represent that the requested URI does not exist, and use 409 to indicate that based on the current the current agent state, the update request cannot be done because the specified resource provider config does not exist, similar to what a PATCH command would return if certain elements do not exist in the requsted resource (https://www.ietf.org/rfc/rfc5789.txt):    Adapting 409 also makes {{UPDATE_RESOURCE_PROVIDER_CONFIG}} symmetric to {{ADD_RESOURCE_PROVIDER_CONFIG}}.",Bug,Major,Resolved,"2019-05-10 23:11:36","2019-05-10 22:11:36",1
"Apache Mesos","Randomized the agents in the second allocation stage.","Agents are currently randomized before the 1st  allocation stage (the quota allocation stage) but not in  the 2nd stage. One perceived issue is that resources on  the agents in the front of the queue are likely to be mostly  allocated in the 1st stage, leaving only slices of resources  available for the second stage. Thus we may see consistently  low quality offers for role/frameworks that get allocated first  in the 2nd stage.    Consider randomizing the agents in the second allocation stage.",Improvement,Major,Resolved,"2019-05-09 23:00:04","2019-05-09 22:00:04",1
"Apache Mesos","Design doc for UCR shared memory.",,Task,Major,Resolved,"2019-05-08 21:34:15","2019-05-08 20:34:15",8
"Apache Mesos","Design client side SSL certificate verification in Libprocess.","Notes from an offline discussion with [~<USER>, [~<USER>, [~<USER>, [~CarlDellar].  * Authentication can happen at the transport and/or at the application layer. There is no real benefit in doing it at both layers.  * Authentication at the application layer allows for subsequent authorization.  * We would like to have an option to mutually authenticate all components in a Mesos cluster, including external tooling, regardless at which layer, to secure communication channels.  * Mutual authentication at the transport layer everywhere can be hard because some components can't or don't want to provide certificates, e.g., a Lua HTTP client reading master's state.  * Theoretically, some components, e.g., Mesos masters and agents, can form an ensemble inside which all connections are authenticated on both sides at the transport layer (TLS certificate verification). Practically, it may then be hard to implement communication with the components outside such ensemble, e.g., frameworks, executors, since at least two types of connections/sockets should be distinguished: with and without client certificate verification (Libprocess can't do it now), or all the traffic between the ensemble and outside components should go via a proxy.  * An alternative is to combine server side TLS certificate verification with the client side application layer authentication. For that to be secure, we need to implement client authentication for Mesos components, e.g., master with agent, replica with other replica (see MESOS-9638). Plus relax certificate verification option in Libprocess for outgoing connections only. For non-streaming connections a secret connection identifier should be passed by the client to prove they are the entity that has been previously authenticated.  * Whatever path we choose, truly secure communication channels will become when separate certificates for Mesos components are used, either signed by a different root CA or using a specific CN/SAN, which can't be obtained by everyone.    What needs to be done:  * Introduce or adjust the Libprocess flag for verifying certificates for outgoing connections only.  * Verify how replicas in the master's replicated log discover other replicas and what harm a rogue replica can do if it tries to join the quorum. Estimate whether master's replicated log can use its own copy of Libprocess.  * Implement Mesos master authentication with Mesos agents, MESOS-9638.",Task,Major,Resolved,"2019-05-08 18:54:53","2019-05-08 17:54:53",8
"Apache Mesos","/__processes__ endpoint can hang.","A user reported that the {{/\_\_processes\_\_}} endpoint occasionally hangs.    Stack traces provided by [~<USER> revealed that all the threads appeared to be idle waiting for events. After investigating the code, the issue was found to be possible when a process gets terminated after the {{/\_\_processes\_\_}} route handler dispatches to it, thus dropping the dispatch and abandoning the future.",Bug,Major,Resolved,"2019-05-03 20:50:10","2019-05-03 19:50:10",3
"Apache Mesos","Test `ROOT_CreateDestroyPersistentMountVolumeWithReboot` is flaky.","Observed a failure on test {{CSIVersion/StorageLocalResourceProviderTest.ROOT_CreateDestroyPersistentMountVolumeWithReboot/v0}} with the following error:      The problem is that the task was OOM-killed:      This might happen to other SLRP root tests as well.",Bug,Major,Resolved,"2019-05-03 20:03:33","2019-05-03 19:03:33",1
"Apache Mesos","Encode framework-specified operation ID into CSI volume name.","Currently, SLRP uses the internally-generated operation UUID as CSI volume creation name, which brings extra difficulty for debugging because users have to go through logs to figure out the operation UUID for a given {{CREATE_DISK}} call.    Instead, we could encode the framework-specified operation ID into the volume name. To ensure the uniqueness of each volume name, we can use the following schema:    The prefix before the framework-specified operation ID would be 29 characters long.    We can consider not having the {{mesos-}} prefix to even shorten it, but it might not be a bad idea to have that prefix reserved for Mesos.",Improvement,Major,Open,"2019-05-02 19:33:59","2019-05-02 18:33:59",1
"Apache Mesos","Log required quota headroom and available quota headroom in the allocator.","This would ease the debugging of allocation issues.",Improvement,Major,Resolved,"2019-05-01 20:18:21","2019-05-01 19:18:21",2
"Apache Mesos","Take ports out of the GET_ROLES endpoints.","It does not make sense to combine ports across agents.",Improvement,Major,Resolved,"2019-05-01 20:11:18","2019-05-01 19:11:18",3
"Apache Mesos","Design doc for container debug endpoint.",,Task,Major,Resolved,"2019-05-01 19:36:20","2019-05-01 18:36:20",8
"Apache Mesos","Design doc for agent draining",,Task,Major,Resolved,"2019-05-01 18:23:31","2019-05-01 17:23:31",8
"Apache Mesos","Agent Draining","This epic holds tickets related to maintenance primitive improvements which facilitate draining of agent nodes.",Epic,Major,"In Progress","2019-05-01 18:22:52","2019-05-01 17:22:52",13
"Apache Mesos","Agent V1 GET_STATE response may report a complete executor's tasks as non-terminal after a graceful agent shutdown","When the following steps occur:  1) A graceful shutdown is initiated on the agent (i.e. SIGUSR1 or /master/machine/down).  2) The executor is sent a kill, and the agent counts down on {{executor_shutdown_grace_period}}.  3) The executor exits, before all terminal status updates reach the agent. This is more likely if {{executor_shutdown_grace_period}} passes.    This results in a completed executor, with non-terminal tasks (according to status updates).    When the agent starts back up, the completed executor will be recovered and shows up correctly  as a completed executor in {{/state}}.  However, if you fetch the V1 {{GET_STATE}} result, there will be an entry in {{launched_tasks}} even though nothing is running.      This happens because we combine executors and completed executors when constructing the response.  The terminal task(s) with non-terminal updates appear under completed executors.  https://github.com/apache/mesos/blob/89c3dd95a421e14044bc91ceb1998ff4ae3883b4/src/slave/http.cpp#L1734-L1756",Bug,Major,Resolved,"2019-05-01 02:54:19","2019-05-01 01:54:19",2
"Apache Mesos","Argument forwaring in CMake build result in glog 0.4.0 build as shared library","GLog versions >= 0.3.5 introduces a {{BUILD_SHARED_LIBS}} CMake option. The CMake configuration of Mesos also has such an option. Because these options are forwarded to third-party packages, GLog will be build as a shared library if Mesos is build with {{BUILD_SHARED_LIBS=OFF}}. This is not intended, as in that case the GLog shared library is not copied over, resulting in Mesos binaries failing to start.  ",Bug,Major,Open,"2019-04-25 08:54:10","2019-04-25 07:54:10",1
"Apache Mesos","Invalid protobuf unions in ExecutorInfo::ContainerInfo will prevent agents from reregistering with 1.8+ masters","As part of MESOS-6874, the master now validates protobuf unions passed as part of an {{ExecutorInfo::ContainerInfo}}.  This prevents a task from specifying, for example, a {{ContainerInfo::MESOS}}, but filling out the {{docker}} field (which is then ignored by the agent).    However, if a task was already launched with an invalid protobuf union, the same validation will happen when the agent tries to reregister with the master.  In this case, if the master is upgraded to validate protobuf unions, the agent reregistration will be rejected.        This bug was found when upgrading a 1.7.x test cluster to 1.8.0.  When MESOS-6874 was committed, I had assumed the invalid protobufs would be rare.  However, on the test cluster, 13/17 agents had at least one invalid ContainerInfo when reregistering.",Bug,Blocker,Resolved,"2019-04-24 05:04:04","2019-04-24 04:04:04",3
"Apache Mesos","Migrate master metrics to PushGauge","We should migrate all metrics in the master actor to use {{PushGauges}} instead of {{PullGauges}}. If there are any cases that would be very cumbersome to handle with the {{PushGauge}} (i.e. uptime), we should file JIRA tickets for the design/development of metric types that can handle those cases.",Task,Major,Open,"2019-04-22 23:40:50","2019-04-22 22:40:50",8
"Apache Mesos","Random sorter generates non-uniform result for hierarchical roles.","In the presence of hierarchical roles, the random sorter shuffles roles level by level and then pick the active leave nodes using DFS:    https://github.com/apache/mesos/blob/7e7cd8de1121589225049ea33df0624b2a1bd754/src/master/allocator/sorter/random/sorter.cpp#L513-L529    This makes the result less random because subtrees are always picked together. For example, random sorting result such as `[a/., c/d, a/b, …]` is impossible.   ",Bug,Major,Resolved,"2019-04-17 19:16:50","2019-04-17 18:16:50",3
"Apache Mesos","Unpublishing a volume that is failed to publish crashes the agent with CSI v1.","The CSI v1 volume manager recovers a failed `publishVolume` call through `unpublishVolume`, which mistakenly assume that the target path, which is supposed to be created by the CSI plugin after a successful publishing, always exists. If volume publishing fails, a subsequent unpublishing would crash the agent with the following message:     ",Bug,Critical,Resolved,"2019-04-12 21:57:43","2019-04-12 20:57:43",1
"Apache Mesos","Update bundled libevent to 2.1.8 for autotools builds - or unbundle it.","We should update the bundled libevent to 2.1.8-stable. Alternatively, we might even consider unbundling given that we have no strict version or patch constraints on that dependency. Anything installed by an up-to-date package manager should be fine.    Changes since from 2.0.22 to 2.1.8: https://github.com/libevent/libevent/compare/c51b159cff9f5e86696f5b9a4c6f517276056258...e7ff4ef2b4fc950a765008c18e74281cdb5e7668",Task,Minor,Open,"2019-04-12 01:56:05","2019-04-12 00:56:05",1
"Apache Mesos","Heartbeat calls from executor to agent are reported as errors","These HEARTBEAT calls and events were added in MESOS-7564.     HEARTBEAT calls are generated by the executor library, which does not have access to the executor's Framework/Executor IDs.  The library therefore uses some dummy values instead, because HEARTBEAT calls do not really require required fields.  When the agent receives these dummy values, it returns a 400 Bad Request.  It should return 202 Accepted instead.",Bug,Minor,Resolved,"2019-04-11 01:07:55","2019-04-11 00:07:55",1
"Apache Mesos","Perform incremental sorting in the random sorter.","By doing random sampling every time as the caller asks for the next client (See MESOS-9722) we could avoid the cost of full shuffling and only pay as we go.    While the hope is to do each random sampling with O(1) cost, the presence of weights complicates the matter. We will need to pay O(log( n )) for every sample even with fancy data structures like segment tree or binary index trees (naive ones will result in O( n ) since we need to look at every node's weights). And the current full node shuffling is already optimal (nlog( n )) if all nodes are picked.    However, since the number of *distinct* weights is usually much smaller comparing to the size of clients, we can minimize the sample cost by picking a client in two steps:    Step1: randomly pick a group of clients that has the same weight by generating a weighted random number.    Step2: Once a vector of clients is chosen, randomly sample a specific client within the group. Since all the clients in the chosen vector have the same weight, we do not need to consider any weights.     Since the size of distinct weights is usually much smaller comparing to the size of clients, this way, we minimize the cost of generating weighted random numbers which are linear with the size of weights.",Improvement,Major,Accepted,"2019-04-10 18:49:09","2019-04-10 17:49:09",3
"Apache Mesos","Flatten the weighted shuffling in the random sorter.","Due to the presence of hierarchical weights, the random sorter currently shuffles level-by-level. We should be able to shuffle all the active leaves only once by calculating (and caching) active leaves' relative weights. This should improve the performance in the presence of hierarchical roles. ",Improvement,Major,Resolved,"2019-04-10 18:32:10","2019-04-10 17:32:10",1
"Apache Mesos","Apply in place permutation to avoid copying when doing random shuffling.","This should improve the performance of random sorter. Se [~<USER>'s comment here:    https://github.com/apache/mesos/blob/master/src/master/allocator/sorter/random/utils.hpp#L69-L74",Improvement,Major,Resolved,"2019-04-10 18:28:40","2019-04-10 17:28:40",3
"Apache Mesos","Refactor the sorter interface to enable lazy sorting.","Currently, the only way for getting a sorted client from sorter is through:        This sorts all the active clients in the tree and returns all of them in a single vector. This is inefficient if the callers end up only needing a few of clients (e.g. when allocating one agent, only one or a few roles are allocated).    We could refactor the interface to return an iterator-like handle and then callers can query the next the client in the sorting order. This would pave the way for lazy sorting (i.e. only get the nth client) and improve performance.",Improvement,Major,Accepted,"2019-04-10 18:14:13","2019-04-10 17:14:13",3
"Apache Mesos","Support specifying file name in URI fetcher fetch() interface.",,Improvement,Major,Resolved,"2019-04-10 16:46:37","2019-04-10 15:46:37",3
"Apache Mesos","Test `AgentFailoverHTTPExecutorUsingResourceProviderResources` is flaky.","The test is flaky because:   # It assumes the mock RP never reregisters, which might not be true.   # It does not wait for the task and executor to be reaped, which would lead to a race between containerizer destroy and test teardown and cause cgroups cleanup to fail.   # It fast-forwards the clock, which might lead to containerizer destroy failures.   # It assumes that the framework only receives two status updates, which might not be true.    Example failure log:  ",Bug,Critical,Resolved,"2019-04-10 04:34:16","2019-04-10 03:34:16",2
"Apache Mesos","Support specifying output file name for curl fetcher plugin",,Task,Major,Resolved,"2019-04-10 03:27:03","2019-04-10 02:27:03",3
"Apache Mesos","StorageLocalResourceProviderTest.CsiPluginRpcMetrics is flaky.","From an internal CI run:  ",Bug,Critical,Resolved,"2019-04-09 15:38:20","2019-04-09 14:38:20",3
"Apache Mesos","Avoid shutting down executors registering before a required resource provider.","If an HTTP-based executor resubscribes after agent failover before a resource provider exposing some of its resources has subscribed itself the agent currently does not know how to inform the resource provider about the existing resource user and shuts the executor down.    This is not optimal as the resource provider might subscribe soon, but we fail the task nevertheless.    We should consider improving on that, e.g., by deferring executor subscription until all providers have resubscribed or their registration timeout is reached, see MESOS-7554.",Bug,Blocker,Resolved,"2019-04-09 12:48:45","2019-04-09 11:48:45",1
"Apache Mesos","Add tests to ensure random sorter performs correct weighted sorting.","We added tests for the weighted shuffle algorithm, but didn't test that the RandomSorter's sort() function behaves correctly.    We should also test that hierarchical weights in the random sorter behave correctly.",Task,Major,Resolved,"2019-04-08 21:22:55","2019-04-08 20:22:55",3
"Apache Mesos","Release Mesos 1.8.0.","Release Mesos 1.8.0.",Task,Major,Resolved,"2019-04-08 17:47:49","2019-04-08 16:47:49",5
"Apache Mesos","Operation Feedback Improvements","Since the main functionality of operation feedback for non-launch operations has been completed, this epic will track improvements and bug fixes related to this code.",Epic,Major,Open,"2019-04-05 17:57:48","2019-04-05 16:57:48",13
"Apache Mesos","Support docker manifest v2s2 config GC.","After docker manifest v2s2 support, layer GC is still properly supported.    However, the manifest config is not garbage collected. Need to add the config dir to the checkpointed LAYERS_FILE to support config GC.",Improvement,Blocker,Resolved,"2019-04-05 07:06:47","2019-04-05 06:06:47",3
"Apache Mesos","Support docker manifest v2s2 external urls.","docker manifest v2s2 spec define external URLs. Some windows image rely on those urls to download some private layers from microsoft server.    Some refactoring may be needed to get rid of the current external urls support because the uri fetcher has to parse the manifest when pulling every layer (see the patches of MESOS-9159 for details).",Improvement,Major,Open,"2019-04-05 07:04:48","2019-04-05 06:04:48",5
"Apache Mesos","Allocator's roles map should track reservations.","Currently, the allocator's {{roles}} map only tracks roles that have allocations or framework subscriptions:    https://github.com/apache/mesos/blob/1.7.2/src/master/allocator/mesos/hierarchical.hpp#L531-L535    And we separately track a map of total reservations for each role:    https://github.com/apache/mesos/blob/1.7.2/src/master/allocator/mesos/hierarchical.hpp#L541-L547    Confusingly, the {{roles}} map won't have an entry when there is a reservation for a role but no allocations or frameworks subscribed. We should ensure that the map has an entry when there are reservations. Also, we can consolidate the reservation information and framework ids into the same map, e.g.:    ",Improvement,Major,Resolved,"2019-04-04 22:24:11","2019-04-04 21:24:11",3
"Apache Mesos","Pull in glog 0.4.0","There are at the very least two reasons to update.  - We will be able to get rid of patches for Windows build https://issues.apache.org/jira/browse/MESOS-3394 and a huge number of other patches, and will switch cmake build to building glog with cmake on Linux/BSD (now it uses autotools under cmake!)  - The microseconds patch introduced in https://issues.apache.org/jira/browse/MESOS-9687 will still remain, but at least there will be no _backported_ microseconds patch.",Task,Major,Resolved,"2019-04-04 16:50:22","2019-04-04 15:50:22",3
"Apache Mesos","DroppedOperationStatusUpdate test is flaky","DroppedOperationStatusUpdate test failed with the following backtrace:  ",Bug,Major,Resolved,"2019-04-04 11:48:15","2019-04-04 10:48:15",5
"Apache Mesos","Release RPMs are not uploaded to bintray","While we currently build release RPMs, e.g., [https://builds.apache.org/view/M-R/view/Mesos/job/Packaging/job/CentOS/job/1.7.x/], these artifacts are not uploaded to bintray. Due to that RPM links on the downloads page [http://mesos.apache.org/downloads/] are broken.",Bug,Critical,Resolved,"2019-04-03 10:40:50","2019-04-03 09:40:50",3
"Apache Mesos","Test MasterQuotaTest.AvailableResourcesSingleDisconnectedAgent is flaky","The test {{MasterQuotaTest.AvailableResourcesSingleDisconnectedAgent}} is flaky, especially under additional system load.",Bug,Major,Resolved,"2019-04-03 09:36:09","2019-04-03 08:36:09",1
"Apache Mesos","Remove the duplicate pid check in Docker containerizer","In `DockerContainerizerProcess::_recover`, we check if there are two executors use duplicate pid, and error out if we find duplicate pid (see [here|https://github.com/apache/mesos/blob/1.7.2/src/slave/containerizer/docker.cpp#L1068:L1078] for details). However I do not see the value this check can give us but it will cause serious issue (agent crash loop when restarting) in rare case (a new executor reuse pid of an old executor), so I think we'd better to remove it from Docker containerizer.",Improvement,Major,Resolved,"2019-04-02 08:02:38","2019-04-02 07:02:38",2
"Apache Mesos","Refactor UCR docker store to construct 'Image' protobuf at Puller.",,Task,Major,Resolved,"2019-04-02 02:20:50","2019-04-02 01:20:50",3
"Apache Mesos","Add master validation for SeccompInfo.","1. if seccomp is not enabled, we should return failure if any fw specify seccompInfo and return appropriate status update.  2. at most one field of profile_name and unconfined should be set. better to validate in master",Task,Major,Resolved,"2019-03-29 23:25:02","2019-03-29 23:25:02",3
"Apache Mesos","Quota may be under allocated for disk resources.","Due to a bug in the resources chopping logic:  https://github.com/apache/mesos/blob/1915150c6a83cd95197e25a68a6adf9b3ef5fb11/src/master/allocator/mesos/hierarchical.cpp#L1665-L1668        When chopping different resources with the same name (e.g. vanilla disk and mount disk), we only include one of the resources. For example, if a role has a quota of 100disk, and an agent has 50 vanilla disk and 50 mount disk, the offer will only contain 50 disk (either vanilla or the mount type). The correct behavior should be that both disks should be offered.    Since today, only disk resources might have the same name but different meta-data (for unreserved/nonrevocable/nonshared resources -- we only chop this), this bug should only affect disk resources today.    The correct code should be:      ",Bug,Major,Resolved,"2019-03-29 18:29:44","2019-03-29 18:29:44",2
"Apache Mesos","Quota headroom calculation is off when subroles are involved.","Quota availableHeadroom calculation:    https://github.com/apache/mesos/blob/6276f7e73b0dbe7df49a7315cd1b83340d66f4ea/src/master/allocator/mesos/hierarchical.cpp#L1751-L1754    is off when subroles are involved.    Specifically, in the formula       -The allocated resources part is hierarchical-aware and aggregate that across all roles, thus allocations to subroles will be counted multiple times (in the case of a/b, once for a and once for a/b).- Looks like due to the presence of `INTERNAL` node, `roleSorter->allocationScalarQuantities(role)` is *not* hierarchical. Thus this is not an issue.    (If role `a/b` consumes 1cpu and `a` consumes 1cpu, if we query `roleSorter->allocationScalarQuantities(a);` It will return 1cpu, which is correct. In the sorter, there are four nodes, root, `a` (internal, 1cpu), `a/.` (leaf, 1cpu), `a/b` (leaf, 1cpu). Query `a` will return `a/.`)    The total reservations  is correct, since today it is flat (reservations made to a/b are not counted to a). Thus all reservations are only counted once -- which is the correct semantic here. However, once we fix MESOS-9688 (which likely requires reservation tracking to be hierarchical-aware), we need to ensure that the accounting is still correct.    -The allocated reservations is hierarchical-aware, thus overlap accounting would occur.- Similar to the `allocated resources` above, this is also not an issue at the moment.    Basically, when calculating the available headroom, we need to ensure single-counting. Ideally, we only need to look at the root's consumptions.",Bug,Critical,Resolved,"2019-03-28 22:53:02","2019-03-28 22:53:02",3
"Apache Mesos","Quota is not enforced properly when subroles have reservations.","Note: the discussion here concerns quota enforcement for top-level role, setting quota on sublevel role is not supported.    If a subrole directly makes a reservation, the accounting of `roleConsumedQuota` will be off:    https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/hierarchical.cpp#L1703-L1705    Specifically, in this formula:  `Consumed Quota = reservations + allocation - allocated reservations`    The `reservations` part does not account subrole's reservation to its ancestors. If a reservation is made directly for role a/b, its reservation is accounted only for a/b but not for a. Similarly, if a top role ( a) reservation is refined to a subrole (a/b), the current code first subtracts the reservation from a and then track that under a/b.    We should make it hierarchical-aware.    The allocation and allocated reservations are both tracked in the sorter where the hierarchical relationship is considered -- allocations are added hierarchically.",Bug,Critical,Resolved,"2019-03-28 22:31:06","2019-03-28 22:31:06",3
"Apache Mesos","Add the glog patch to pass microseconds via the LogSink interface.","Currently, custom LogSink implementations in the modules (for example, this one:   [https://github.com/dcos/dcos-mesos-modules/blob/master/logsink/logsink.hpp] )   are logging `000000` instead of microseconds in the timestamp - simply because the LogSink interface in glog has no place for microseconds.    The proposed glog fix is here: [https://github.com/google/glog/pull/441]    Getting this into glog release might take a long time (they released 0.4.0 recently, but the previous release 0.3.5 was two years ago), therefore it makes sense to add this patch into Mesos build.",Task,Major,Resolved,"2019-03-28 18:26:04","2019-03-28 18:26:04",3
"Apache Mesos","Backport docker manifest v2s2 support to 1.4.x",,Task,Major,Resolved,"2019-03-27 19:32:29","2019-03-27 19:32:29",3
"Apache Mesos","Backport docker manifest v2s2 support to 1.5.x",,Task,Major,Resolved,"2019-03-27 19:31:53","2019-03-27 19:31:53",5
"Apache Mesos","Backport docker manifest v2s2 support to 1.6.x",,Task,Major,Resolved,"2019-03-27 19:31:10","2019-03-27 19:31:10",3
"Apache Mesos","Backport docker manifest v2s2 support to 1.7.x.",,Task,Major,Resolved,"2019-03-27 19:30:29","2019-03-27 19:30:29",5
"Apache Mesos","RPM packages should be built with launcher sealing","We should consider enabling launcher sealing in the Mesos RPM packages. Since this feature is built conditionally, it is hard to write e.g., module code against Mesos packages since required functions might be missing (e.g., [https://github.com/dcos/dcos-mesos-modules/commit/8ce70e6cc789054831daa3058647e326b2b11bc9] cannot be linked against the default RPM package anymore). The RPM's target platform centos7 should include a recent enough kernel for this.",Task,Major,Resolved,"2019-03-25 15:20:23","2019-03-25 15:20:23",3
"Apache Mesos","Add prettyjws support for docker v2 s1 manifest.",,Task,Major,Resolved,"2019-03-24 04:29:14","2019-03-24 04:29:14",2
"Apache Mesos","Docker containerizer should ignore pids of executors that do not pass the connection check.","When recovering executors with a tracked pid we first try to establish a connection to its libprocess address to avoid reaping an irrelevant process:    https://github.com/apache/mesos/blob/4580834471fb3bc0b95e2b96e04a63d34faef724/src/slave/containerizer/docker.cpp#L1019-L1054    If the connection fails to establish, we should not track its pid: https://github.com/apache/mesos/blob/4580834471fb3bc0b95e2b96e04a63d34faef724/src/slave/containerizer/docker.cpp#L1071    One trouble this might cause is that if the pid is being used by another executor, this could lead to duplicate pid error and lead the agent into a crash loop:    https://github.com/apache/mesos/blob/4580834471fb3bc0b95e2b96e04a63d34faef724/src/slave/containerizer/docker.cpp#L1066-L1068",Bug,Critical,Resolved,"2019-03-22 20:04:31","2019-03-22 20:04:31",2
"Apache Mesos","Deprecate v0 quota calls.","Once we introduce the new quota APIs in MESOS-8068, we should deprecate the `/quota` endpoint. We should mark this as deprecated and hide it in our documentation.",Improvement,Major,Resolved,"2019-03-21 23:26:40","2019-03-21 23:26:40",1
"Apache Mesos","Add authorization support for the new `GET_QUOTA` call.","The new `GET_QUOTA` call will return QUOTA_CONFIGS:    // Used in GET_QUOTA and returned by GET /quota  //  // Overall cluster quota status, including all roles, their quota configurations and current state (e.g. consumed and effective limits)  message QuotaStatus {         repeated QuotaInfo infos [deprecated = true];         repeated QuotaConfig configs;   }    Currently, the GET_QUOTA authorizable action set both value  and quota_info fields. The value field is set due to  backward compatibility for the GET_QUOTA_WITH_ROLE action.    We should make the GET_QUOTA action only set the value  field with the role name. Since the quota.QuotaInfo field  is being deprecated, it should not be set (the local authorizer  only looks at the value field, it is also probably the case  for any external authorizer modules).",Improvement,Major,Resolved,"2019-03-21 23:20:24","2019-03-21 23:20:24",2
"Apache Mesos","Check failure when executor for task using resource provider resources subscribes before agent is registered","When an executor for a task using resource provider resources subscribes before the agent has registered with the master, we trigger a fatal assertion,    The reason for this failure is that we attempt to publish resources to the resource provider via the resource provider manager, but the resource provider manager is only created once the agent has registered with the master.    As a workaround one can terminate the executors and their tasks, and let the framework relaunch the tasks (provided it supports that).    A possible workaround could be to prevent such executors from subscribing until the resource provider manager is available.",Bug,Blocker,Resolved,"2019-03-21 17:03:32","2019-03-21 17:03:32",3
"Apache Mesos","Specifying custom CXXFLAGS breaks Mesos build","The environment variable CXXFLAGS (as well CFLAGS and CPPFLAGS) is intended to give the user a way to add custom compiler flags to the build at both configure-time and build-time.    For example, a user wishing to use the address-sanitizer feature for a development build could run configure like    or a user wishing to investigate a particular binary might want to rebuild that framework with additional debug information:      Therefore, providing custom CXXFLAGS should not break the build. However, we currently add some essential flags (like '-std=c++11') into CXXFLAGS, and a user specifying custom CXXFLAGS has to replicate all of these before he can provide his own.    Instead, we should try to restrict CXXFLAGS to some harmless default (e.g. '-g -O2') and move essential flags into some other variable MESOS_CXXFLAGS that is always added to the mesos build.",Bug,Major,Open,"2019-03-21 09:55:25","2019-03-21 09:55:25",3
"Apache Mesos","Allow for optionally unbundled ZooKeeper from CMake builds.","Following the example of unbundled libevent and libarchive, we should allow for unbundled zookeeper if the user wishes so.",Improvement,Minor,Reviewable,"2019-03-18 21:07:33","2019-03-18 21:07:33",2
"Apache Mesos","Agent crashes when SLRP recovers dropped operations.","MESOS-9537 is fixed by persisting dropped operations in SLRP, but the recovery codepath doesn't account for that:  [https://github.com/apache/mesos/blob/master/src/resource_provider/storage/provider.cpp#L1278]  Which caused the agent to crash with the following message during SLRP recovery:  ",Bug,Blocker,Resolved,"2019-03-18 18:12:54","2019-03-18 18:12:54",1
"Apache Mesos","Improving SLRP tests for preprovisioned volumes.","We should improve SLRP tests for preprovisioned volumes:  1. Test that {{CREATE_DISK}} fails if the specified profile is unknown.  2. Update test {{AgentRegisteredWithNewId}} to ensure that a recovered published volumes can be consumed by new tasks.",Improvement,Major,Resolved,"2019-03-15 03:41:40","2019-03-15 03:41:40",2
"Apache Mesos","Design for docker registry v2 schema2 basic support.",,Task,Major,Resolved,"2019-03-13 15:31:05","2019-03-13 15:31:05",8
"Apache Mesos","Recover frameworks from reregistered agents with operations","Once MESOS-8582 is completed, we need to update the master to recover frameworks when an agent sends an {{UpdateSlaveMessage}} containing operations from a framework which has not already been recovered.",Task,Major,Open,"2019-03-11 22:46:45","2019-03-11 22:46:45",5
"Apache Mesos","Make operation reconciliation send asynchronous updates","Following discussion amongst the community, it seems better for frameworks to perform operation reconciliation the same way we do task state reconciliation: a reconciliation request from the scheduler should trigger an asynchronous stream of operation status updates on the scheduler's event stream, rather than a synchronous HTTP response which contains all updates.",Task,Major,Resolved,"2019-03-11 22:13:32","2019-03-11 22:13:32",8
"Apache Mesos","Look into enabling the libarchive extraction flag ARCHIVE_EXTRACT_SECURE_NOABSOLUTEPATHS by default","The libarchive source provides the following flag:     https://github.com/libarchive/libarchive/blob/master/libarchive/archive.h#L672-L674    We should check if the default behavior is unsecure (i.e. allowing a fetched artifact to affect files outside the sandbox).",Improvement,Major,"In Progress","2019-03-11 17:47:58","2019-03-11 17:47:58",2
"Apache Mesos","Make setting volume ownership asynchronous in volume gid manager","Setting volume ownership can be expensive because we need to traverse all the directories and files in it, so we'd better to do it asynchronously in volume gid manager.",Improvement,Major,Resolved,"2019-03-11 14:58:35","2019-03-11 14:58:35",3
"Apache Mesos","Avoid reading host mount table when allocating a gid in GIDManager.",,Improvement,Major,Resolved,"2019-03-08 00:54:53","2019-03-08 00:54:53",3
"Apache Mesos","Support GID manager with non-sharable persistent volume.",,Improvement,Major,Accepted,"2019-03-08 00:53:32","2019-03-08 00:53:32",3
"Apache Mesos","Add authorization support for `UPDATE_QUOTA` call.","For the new `UPDATE_QUOTA` call, we need to add the corresponding authorization support. Unfortunately, there is already an action named `update_quotas`. We can use `update_quota_configs` instead.",Improvement,Major,Resolved,"2019-03-07 23:55:46","2019-03-07 23:55:46",3
"Apache Mesos","Make CSI plugin RPC metrics agnostic to CSI versions.","Currently SLRP provides per-CSI-call metrics, e.g.:    If we are to continue to provide such fine-grained metrics, when operators upgrade their CSI plugins to CSI v1, then SLRP would report another set of metrics for v1, which would be inconvenient to operators.    Also the fine-grained metrics are not very useful for operators, as most information are highly correlated to per-operation metrics. So most likely operators would simply aggregate the per-CSI-call metrics for monitoring CSI plugins, and use per-operation metrics to monitor volume creation/destroy/etc.    So instead of provide such fine-grained metrics, we could just provide a set of aggregated rpc metrics that are agnostic to CSI versions, such as:  ",Task,Critical,Resolved,"2019-03-07 20:11:14","2019-03-07 20:11:14",1
"Apache Mesos","Impossible to CREATE a volume on resource provider resources over the operator API","Currently the master HTTP handler for operator API {{CREATE}} requests strips away the whole {{DiskInfo}} in any passed resources to calculate the consumed resources.    This is incorrect for resource provider disk resources where the {{DiskInfo}} contains information unrelated to the persistence. The handler should remove exclusively information created by the operation.",Bug,Critical,Resolved,"2019-03-07 12:44:05","2019-03-07 12:44:05",1
"Apache Mesos","OperationReconciliationTest.AgentPendingOperationAfterMasterFailover is flaky again (3x) due to orphan operations","This test fails consistently when run while the system is stressed:  ",Bug,Blocker,Resolved,"2019-03-06 20:04:07","2019-03-06 20:04:07",3
"Apache Mesos","Refactor SLRP with a CSI service manager.","The CSI volume manager relies on service containers, which should be agnostic to CSI versions. As the first step of MESOS-9622, we should first refactor SLRP with a CSI service manager that manages service container lifecycles before refactoring it with a CSI volume manager.",Task,Critical,Resolved,"2019-03-04 23:35:23","2019-03-04 23:35:23",5
"Apache Mesos","Test CSI v1 in SLRP unit tests.","We could add a command line flag in the test CSI plugin to switch to either v0 and v1, and parameterize the CSI version in SLRP unit tests.",Task,Major,Resolved,"2019-03-01 19:45:26","2019-03-01 19:45:26",3
"Apache Mesos","Make SLRP pick the appropriate CSI versions for plugins.","To detect the CSI version supported by a plugin, we could call {{v1.Probe}}, and fallback to {{v0.Probe}} if the previous call fails.    Alternatively, we could introduce a field in {{CSIPluginInfo}} to specify the plugin version to use.",Task,Critical,Resolved,"2019-03-01 19:40:13","2019-03-01 19:40:13",3
"Apache Mesos","Make `DiskProfileAdaptor` agnostic to CSI spec version.","To support multiple CSI versions, the {{DiskProfileAdaptor}} module needs to be decoupled from CSI version. Mainly, we'll have to introduce a version-agnostic {{VolumeCapability}}.",Task,Critical,Resolved,"2019-03-01 19:36:35","2019-03-01 19:36:35",3
"Apache Mesos","Bundle CSI spec v1.0 in Mesos.","We need to bundle both CSI v0 and v1 in Mesos. This requires some redesign of the source code filesystem layout.",Task,Critical,Resolved,"2019-03-01 19:31:18","2019-03-01 19:31:18",3
"Apache Mesos","Implement CSI volume manager with CSI v1.",,Task,Critical,Resolved,"2019-03-01 19:27:41","2019-03-01 19:27:41",3
"Apache Mesos","Refactor SLRP with a CSI volume manager.","To support both CSI v0 and v1, SLRP needs to be agnostic to CSI versions. This could be achieved by refactoring all CSI volume management code into a CSI volume manager that can be implemented with CSI v0 and v1. Also, the volume state proto needs to be agnostic to CSI spec version as well.    Design doc: https://docs.google.com/document/d/1LPy839zwFw6UcRhmr65iKeMaHcoj6uUX25yJVbMknlY/edit#heading=h.1iswiwd3imin",Task,Critical,Resolved,"2019-03-01 19:25:37","2019-03-01 19:25:37",5
"Apache Mesos","Mesos failed to build due to error LNK2019 on Windows using MSVC.","Issue description:    Mesos failed to build due to error    Could you please take a look?    Reproduce steps:    ErrorMessage:          ",Bug,Major,Resolved,"2019-03-01 09:11:46","2019-03-01 09:11:46",1
"Apache Mesos","Add metrics for volume gid manager","We need to add some metrics for volume gid manager, e.g., total number of gids configured for volume gid manager and the number of the free gids available to volume gid manager, and add the new metrics into [this doc|https://github.com/apache/mesos/blob/master/docs/monitoring.md].",Task,Major,Resolved,"2019-03-01 07:37:34","2019-03-01 07:37:34",3
"Apache Mesos","Mesos Master Crashes with Launch Group when using Port Resources","Original Issue: [https://lists.apache.org/thread.html/979c8799d128ad0c436b53f2788568212f97ccf324933524f1b4d189@%3Cuser.mesos.apache.org%3E]     When the ports resources is removed, Mesos functions normally (I'm able to launch the task as many times as possible, while it always fails continually).    Attached is a snippet of the mesos master log from OFFER to crash.    ",Bug,Critical,Resolved,"2019-03-01 00:22:30","2019-03-01 00:22:30",3
"Apache Mesos","Display quota consumption in the webui.","Currently, the Roles table in the webui displays allocation and quota guarantees / limits. However, quota consumption is different from allocation, in that reserved resources are always considered consumed against the quota.    This discrepancy has led to confusion from users. One exampled occurred when an agent was added with a large reservation exceeding the memory quota guarantee. The user sees memory chopping in offers, and since the scheduler didn't want to use the reservation, it can't launch its tasks.    If consumption is shown in the UI, we should include a tool tip that indicates how consumed is calculated so that users know how to interpret it.",Improvement,Major,Resolved,"2019-02-28 16:58:04","2019-02-28 16:58:04",3
"Apache Mesos","`Filters.refuse_seconds` declines resources not in offers.","The [documentation|http://mesos.apache.org/documentation/latest/scheduler-http-api/#accept] of {{Filters.refuse_seconds}} says:  {quote}  Also, any of the offer’s resources not used in the ACCEPT call (e.g., to launch a task or task group) are considered declined and might be reoffered to other frameworks, meaning that they will not be reoffered to the scheduler for the amount of time defined by the filter.  {quote}  Consider an {{ACCEPT}} call with just a {{CREATE}} operation, but no {{LAUNCH}} or {{LAUNCH_GROUP}}. The {{CREATE}} call will generate a persistent volume resource that is *not* in the offer's resources, but it will still not be reoffered to the scheduler for the amount of time defined by the filter.    Also, the term *used* is vague here. If we have an {{ACCEPT}} call with a {{CREATE}} on a disk followed by a {{DESTROY}} on the created persistent volume, would the disk be considered *used*?",Bug,Major,Resolved,"2019-02-27 23:12:27","2019-02-27 23:12:27",2
"Apache Mesos","Example framework for feedback on agent default resources","We need a framework that can be used to test operations on agent default resources which request operation feedback.",Task,Major,Resolved,"2019-02-27 18:37:07","2019-02-27 18:37:07",5
"Apache Mesos","Support seccomp `unconfined` option for whitelisting.","Support seccomp `unconfined` option for whitelisting. Authorization needs to be implemented for this protobuf option.",Improvement,Major,Resolved,"2019-02-26 21:54:13","2019-02-26 21:54:13",3
"Apache Mesos","Resource provider manager assumes all operations are triggered by frameworks","When the agent tries to apply an operation to resource provider resources, it invokes {{ResourceProviderManager::applyOperation}} which in turn invokes {{ResourceProviderManagerProcess::applyOperation}}. That function currently assumes that the received message contains a valid {{FrameworkID}},      Since {{FrameworkID}} is not a trivial proto types, but instead one with a {{required}} field {{value}}, the message composed with the {{frameworkId}} below cannot be serialized which leads to a failure below which in turn triggers a {{CHECK}} failure in the agent's function interfacing with the manager.    A typical scenario where we would want to support operator API calls here is to destroy leftover persistent volumes or reservations.",Bug,Blocker,Resolved,"2019-02-26 17:14:35","2019-02-26 17:14:35",5
"Apache Mesos","Fetcher vulnerability - escaping from sandbox","I have noticed that there is a possibility to exploit fetcher and  overwrite any file on the agent host.    scenario to reproduce:    1) prepare a file with any content and name a file like ../../../etc/test and archive it. We can use python and zipfile module to achieve that:    2) prepare a service that will use our artifact (exploit.zip)    3) run service    at the end in /etc we will get our file. As you can imagine there is a lot possibility how we can use it.          ",Bug,Blocker,Resolved,"2019-02-26 02:05:10","2019-02-26 02:05:10",3
"Apache Mesos","Master check failure when marking agent unreachable.","{code}  Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815433    13 http.cpp:1185] HTTP POST for /master/api/v1/scheduler from 10.142.0.5:55133  Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815588    13 master.cpp:5467] Processing DECLINE call for offers: [ 5e57f633-a69c-4009-b773-990b4b8984ad-O58323 ] for framework 5e57f633-a69c-4009-b7  Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815693    13 master.cpp:10703] Removing offer 5e57f633-a69c-4009-b773-990b4b8984ad-O58323  Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820142    10 master.cpp:8227] Marking agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49 at slave(1)@10.142.0.10:5051 (tf-mesos-agent-t7c8.c.bitcoin-engi  Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820367    10 registrar.cpp:495] Applied 1 operations in 86528ns; attempting to update the registry  Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820572    10 registrar.cpp:552] Successfully updated the registry in 175872ns  Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820642    11 master.cpp:8275] Marked agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49 at slave(1)@10.142.0.10:5051 (tf-mesos-agent-t7c8.c.bitcoin-engin  Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820957     9 hierarchical.cpp:609] Removed agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49  Mar 11 10:04:35 research docker[4503]: F0311 10:04:35.851961    11 master.cpp:10018] Check failed: 'framework' Must be non NULL  Mar 11 10:04:35 research docker[4503]: *** Check failure stack trace: ***  Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044a7d  google::LogMessage::Fail()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c6046830  google::LogMessage::SendToLog()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044663  google::LogMessage::Flush()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c6047259  google::LogMessageFatal::~LogMessageFatal()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c5258e14  google::CheckNotNull<>()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c521dfc8  mesos::internal::master::Master::__removeSlave()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c521f1a2  mesos::internal::master::Master::_markUnreachable()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c5f98f11  process::ProcessBase::consume()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb2a4a  process::ProcessManager::resume()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb65d6  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv  Mar 11 10:04:36 research docker[4503]: @     0x7f96c35d4c80  (unknown)  Mar 11 10:04:36 research docker[4503]: @     0x7f96c2de76ba  start_thread  Mar 11 10:04:36 research docker[4503]: @     0x7f96c2b1d41d  (unknown)  Mar 11 10:04:36 research docker[4503]: *** Aborted at 1520762676 (unix time) try date -d @1520762676 if you are using GNU date ***  Mar 11 10:04:36 research docker[4503]: PC: @     0x7f96c2a4d196 (unknown)  Mar 11 10:04:36 research docker[4503]: *** SIGSEGV (@0x0) received by PID 1 (TID 0x7f96b986d700) from PID 0; stack trace: ***  Mar 11 10:04:36 research docker[4503]: @     0x7f96c2df1390 (unknown)  Mar 11 10:04:36 research docker[4503]: @     0x7f96c2a4d196 (unknown)  Mar 11 10:04:36 research docker[4503]: @     0x7f96c604ce2c google::DumpStackTraceAndExit()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044a7d google::LogMessage::Fail()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c6046830 google::LogMessage::SendToLog()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044663 google::LogMessage::Flush()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c6047259 google::LogMessageFatal::~LogMessageFatal()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c5258e14 google::CheckNotNull<>()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c521dfc8 mesos::internal::master::Master::__removeSlave()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c521f1a2 mesos::internal::master::Master::_markUnreachable()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c5f98f11 process::ProcessBase::consume()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb2a4a process::ProcessManager::resume()  Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb65d6 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv  Mar 11 10:04:36 research docker[4503]: @     0x7f96c35d4c80 (unknown)  Mar 11 10:04:36 research docker[4503]: @     0x7f96c2de76ba start_thread  Mar 11 10:04:36 research docker[4503]: @     0x7f96c2b1d41d (unknown)  Mar 11 10:04:38 research systemd[1]: mesos-master2.service: main process exited, code=exited, status=139/n/a  Mar 11 10:04:38 research docker[18886]: mesos-master  Mar 11 10:04:38 research systemd[1]: Unit mesos-master2.service entered failed state.  {code}    Additional case:        After an analysis of the more recent logs included in the comments below, the following seems to occur:    1) The last of the framework’s tasks is removed:    Oct 27 23:21:18 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:21:18.493418    15 master.cpp:12171] Removing task 2 with resources cpus(allocated: *):1; disk(allocated: *):4024; mem(allocated: *):2048 of framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 on agent 522424c1-2fac-42ab-9a70-b424266218a9-S0 at slave(1)@10.0.143.144:5051 (10.0.143.144)  which means the framework’s entry in slave->tasks is erased: https://github.com/apache/mesos/blob/e13929d62663015162db7e66c6600fe414d03ec3/src/master/master.cpp#L13527-L13529    2) Later, the agent disconnects and since the framework is not checkpointing, it is removed from the Slave struct:    Oct 27 23:23:20 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:23:20.248260    14 master.cpp:1321] Removing framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 (toil) from disconnected agent 522424c1-2fac-42ab-9a70-b424266218a9-S0 at slave(1)@10.0.143.144:5051 (10.0.143.144) because the framework is not checkpointing  Oct 27 23:23:20 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:23:20.248289    14 master.cpp:11436] Removing framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 (toil) from agent 522424c1-2fac-42ab-9a70-b424266218a9-S0 at slave(1)@10.0.143.144:5051 (10.0.143.144)  Oct 27 23:23:20 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:23:20.248311    14 master.cpp:12211] Removing executor 'toil-440' with resources {} of framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 on agent 522424c1-2fac-42ab-9a70-b424266218a9-S0 at slave(1)@10.0.143.144:5051 (10.0.143.144)  We see no logging related to task removal since slave->tasks[framework->id()] was empty this time. however, since we use operator[] to inspect the task map here, we perform an insertion and it has a side effect: https://github.com/apache/mesos/blob/e13929d62663015162db7e66c6600fe414d03ec3/src/master/master.cpp#L11416  This means that slave->tasks[framework->id()] now exists but has been initialized to an empty map. ruh roh.    3) Very soon after, the framework failover timeout elapses and the framework is removed:    Oct 27 23:23:22 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:23:22.890070    11 master.cpp:10224] Framework failover timeout, removing framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 (toil)  4) Now when __removeSlave() iterates over the keys of slave->tasks, it finds a key which points to a framework that has already been removed: https://github.com/apache/mesos/blob/e13929d62663015162db7e66c6600fe414d03ec3/src/master/master.cpp#L11796-L11800    We need to prevent that unintended map insertion to avoid the crash.    The resolution of this ticket should also involve the writing of a regression test.",Bug,Critical,Resolved,"2019-02-25 23:15:09","2019-02-25 23:15:09",3
"Apache Mesos","Refactor and Improve `class ResourceQuantity`.","Currently, the `ResourceQuantity` only provides a minimal map interface with no built-in arithmetic and contains operations. This makes it unwieldy.    The intention was to avoid the ambiguities between absent-means-zero (guarantee like semantic) and absent-means-infinite (limits like semantic). Instead of only providing a minimal interface and leave the rest to the caller, we should provide two classes for each semantic:    - ResourceQuantities will have absent-means-zero semantic  - ResourceLimits will  have absent-means-infinite semantic",Improvement,Major,Resolved,"2019-02-25 22:05:22","2019-02-25 22:05:22",5
"Apache Mesos","Removing a resource provider with consumers breaks resource publishing.","Currently, the agent publishes all resources considered used via the resource provider manager whenever it is asked to publish a subportion. If a resource provider with active users (e.g., tasks or even just executors) was removed, but a user stays around this will fail _any resource publishing_ on that node since a used resource provider is not subscribed.    We should either update the agent code to just deltas, or provide a workaround of the same effect in the resource provider manager.",Bug,Major,Resolved,"2019-02-25 16:38:21","2019-02-25 16:38:21",2
"Apache Mesos","mesos/mesos-centos nightly docker image has to include the SHA of the build.","As a snapshot build, we need to identify the exact HEAD of the branch build. Our current snapshot builds lack this information due to the way the build is setup.    The current build identifies e.g. when running the agent like this;      Note we lack a user in the first output line and the GIT sha altogether. Only tagged builds should commonly lack the SHA as it is not needed.      ",Bug,Minor,Resolved,"2019-02-23 02:30:23","2019-02-23 02:30:23",2
"Apache Mesos","Clean up `QuotaRequest` and `QuotaInfo`.","Once quota limits are implementation complete, we should clean up the `QuotaInfo` and `QuotaRequest` proto along with other legacy code and tests. Specifically:    - Remove the experimental `limits` field in `QuotaInfo` and `QuotaRequest` (code in https://reviews.apache.org/r/65852/ and https://reviews.apache.org/r/65851/ and https://reviews.apache.org/r/65334/)  - Update the related `QuotaRequest` validation code and tests (code in https://reviews.apache.org/r/65784/ and https://reviews.apache.org/r/65785/)  - The `principal` field is no longer used. It was used to support the remove_quota acl which was already deprecated. (On second thought, while it is not used by the Mesos local authorizer, an external auth module could potentially depend on this. We need to keep this for backward compatibility.)",Task,Major,Resolved,"2019-02-22 19:54:25","2019-02-22 19:54:25",2
"Apache Mesos","Add quota limits metrics.",,Task,Major,Resolved,"2019-02-22 03:10:13","2019-02-22 03:10:13",3
"Apache Mesos","Provide backward compatibility for old quota configurations.","Current (old) masters only support quota guarantee which also servers as limits implicitly. When upgrading to new masters where guarantees and limits are decoupled, we need to ensure backward compatibility such that the existing (old) quota configurations are honored and there should be no change to the cluster behavior.    To this end, new masters should also be able to consume the old quota registry. The old guarantee field will be used to set both guarantee and limits.",Task,Major,Resolved,"2019-02-22 02:59:47","2019-02-22 02:59:47",2
"Apache Mesos","Persist `QuotaConfig`s in the registry.","We need to persist the new `QuotaConfig` in the registry.    One thing to note is, the old masters only support quota guarantee which also servers as limits implicitly. Once new masters start to support both guarantees and limits, there is no safe downgrade path without altering the cluster behavior (if the new quota semantics are used). Thus, we need to ensure that alerts are given if such downgrades are attempted.    To this end, if the quota is configured after this change, a new minimum capability `QUOTA_V2` will be persisted to the registry along with the new `QuotaConfig` message. Thanks to the minimum capability check, old masters (that do not possess the `QUOTA_V2` capability) will refuse to start in this case and we will print out suggestions to the operator.",Task,Major,Resolved,"2019-02-22 02:50:02","2019-02-22 02:50:02",5
"Apache Mesos","Deprecate `SET_QUOTA` and `REMOVE_QUOTA` calls in favor of `UPDATE_QUOTA`.","Once the `UPDATE_QUOTA` call (MESOS-9596) is implemented and wired, we should deprecate the existing calls `REMOVE_QUOTA` and `SET_QUOTA`. In the user-facing documentation, we should hide the old API and showcase the new one.",Task,Major,Resolved,"2019-02-22 02:22:56","2019-02-22 02:22:56",1
"Apache Mesos","Update `GET_QUOTA` to return both guarantees and limits. ","We should mark the existing `QuotaInfo` message as deprecated in favor of the new `QuotaConfig`:        We will continue to fill in the QuotaInfo though for backward compatibility. See the design doc: [New API|https://docs.google.com/document/d/13vG5uH4YVwM79ErBPYAZfnqYFOBbUy2Lym0_9iAQ5Uk/edit#heading=h.z2vfcyzabymz]",Task,Major,Resolved,"2019-02-22 02:17:34","2019-02-22 02:17:34",3
"Apache Mesos","Update GET `/quota` to return both guarantees and limits.","We should mark the existing `QuotaInfo` message as deprecated in favor of the new `QuotaConfig`:        We will continue to fill in the QuotaInfo though for backward compatibility. See the design doc: [New API|https://docs.google.com/document/d/13vG5uH4YVwM79ErBPYAZfnqYFOBbUy2Lym0_9iAQ5Uk/edit#]    Note, we only update this v0 endpoint for the GET method. There is no plan to support configuring quota limits from this endpoint. V1 calls should be used.",Task,Major,Resolved,"2019-02-22 02:14:39","2019-02-22 02:14:39",1
"Apache Mesos","Status update streams for operations affecting agent default resources should be stored under meta/slaves/<slave_id>/operations/","The streams are currently created under {{meta/operations/}} but not recovered if {{meta/slaves/latest}} doesn't exist.    After discussing this with [~<USER> and with [~<USER>, we agreed that they should be created under {{meta/slaves/<slave_id>/operations/}} instead.    NOTE: don't forget to add the corresponding entry in the ascii drawing in {{slave/paths.hpp}.",Bug,Major,Resolved,"2019-02-22 01:55:29","2019-02-22 01:55:29",2
"Apache Mesos","Add a new `UPDATE_QUOTA` operator call.","The new `UPDATE_QUOTA` call will support quota limits and also allow atomic update to quotas of different roles.          See the [design doc|https://docs.google.com/document/d/13vG5uH4YVwM79ErBPYAZfnqYFOBbUy2Lym0_9iAQ5Uk/edit#] here for more details.",Task,Major,Resolved,"2019-02-22 01:37:06","2019-02-22 01:37:06",5
"Apache Mesos","Test `StorageLocalResourceProviderTest.RetryRpcWithExponentialBackoff` is flaky.","Observed on ASF CI:    Full log:  [^RetryRpcWithExponentialBackoff-badrun.txt] ",Bug,Major,Resolved,"2019-02-21 21:40:18","2019-02-21 21:40:18",3
"Apache Mesos","Mesos CI sometimes, incorrectly, overwrites already-pushed mesos master nightly images with new images built from non-master branches.","I pulled image mesos/mesos-centos:master-2019-02-15 some time on the 15th and worked with it locally, on my laptop, for about a week. Part of that work included downloading the related mesos-xxx-devel.rpm from the same CI build that produced the image so that I could build 3rd party mesos modules from the master base image. The rpm was labeled as pre-1.8.0.    This worked great until I tried to repeat the work on another machine. The other machine pulled the same dockerhub image (mesos/mesos-centos:master-2019-02-15) which was somehow built with a mesos-xxx.rpm labeled as pre-1.7.2. I couldn't build my docker image using this strangely new base because the mesos-xxx-devel.rpm I had hardcoded into the dockerfile no longer aligned with the version of the mesos RPM that was shipping in the base image.    The base image had changed, such that the mesos RPM version went from 1.8.0 to 1.7.2. This should never happen.    [~<USER> investigated and found that the problem appears to happen at random. Current thinking is that one of the mesos CI boxes uses a version of git that's too old, and that the CI scripts are incorrectly ignoring a git command failure: the git command fails because the git version is too old, and the script subsequently ignores any failures from the command pipeline in which this command is executed. With the result being that the version of the branch being built cannot be detected and therefore defaults to master - overwriting *actual* master image builds.    [~<USER> also wrote some patches, which I'll link here:    * https://reviews.apache.org/r/70024/  * https://reviews.apache.org/r/70025/",Bug,Major,Resolved,"2019-02-20 22:06:47","2019-02-20 22:06:47",1
"Apache Mesos","Reviewbot jenkins jobs stops validating any reviews as soon as it sees a patch which does not apply","The reviewbot Jenkins setup fetches all Mesos reviews since some time stamp, filters that list down to reviews which need to be validated, and then one by one validates each of the remaining review requests.    In doing that it applies patches with {{support/apply-reviews.py}} which is invoked by shelling out wth a function {{shell}} in {{support/verify-reviews.py}}. If that function sees any error from the shell command {{exit(1)}} is called which immediately terminates the Jenkins job.    As {{support/apply-reviews.py}} can fail if a patch does not apply cleanly anymore this means that any review requests which cannot be applied can largely disable reviewbot.    We should avoid calling {{exit}} in low-level functions in {{support/verify-reviews.py}} and instead bubble the error up to be handled at a larger scope. It looks like the script was alreadt designed to handle exceptions which might work much better here.",Bug,Blocker,Resolved,"2019-02-17 09:45:44","2019-02-17 09:45:44",2
"Apache Mesos","Mesos package naming appears to be undeterministic.","Transcribed from slack; https://mesos.slack.com/archives/C7N086PK2/p1550158266006900    It appears there are a number of RPM packages called “mesos-1.7.1-2.0.1.el7.x86_64.rpm” in the wild.    I’ve caught specimens with build dates February 1st, 7th and 13th. While it’s somewhat troubling in itself, none of these packages is the one referred to in Yum repository metadata (repos.<USER>com), which is a package built today on the 14th, so I can’t install Mesos right now.    Could it be that your pipeline is creating a new package with the same verson and release in every nightly build?    Repository metadata  ",Bug,Major,Resolved,"2019-02-16 02:51:17","2019-02-16 02:51:17",1
"Apache Mesos","ExecutorHttpApiTest.HeartbeatCalls is flaky.","I just saw this failing on our internal CI:  ",Bug,Critical,Accepted,"2019-02-15 23:14:12","2019-02-15 23:14:12",2
"Apache Mesos","Document per framework minimal allocatable resources in framework development guides","With MESOS-9523 we introduced fields into {{FrameworkInfo}} to give frameworks a way to express their resource requirements. We should document this feature in the framework development guide(s).",Task,Blocker,Resolved,"2019-02-15 14:20:52","2019-02-15 14:20:52",1
"Apache Mesos","Provide a configuration option to disallow logrotate stdout/stderr options in task env","See MESOS-9564 for context.    The configuration option could be module flag for the logrotate module.",Task,Major,Accepted,"2019-02-14 18:23:56","2019-02-14 18:23:56",3
"Apache Mesos","Operation status update streams are not properly garbage collected.","After successfully handling the acknowledgment of a terminal operation status update for an operation affecting agent's default resources, the agent should garbage collect the corresponding operation status update stream.",Bug,Major,Resolved,"2019-02-13 22:48:47","2019-02-13 22:48:47",2
"Apache Mesos","Agent should not try to recover operation status update streams that haven't been created yet.","If the agent fails over after having checkpointed a new operation but before the operation status update stream is created, the recovery process will fail.    This happens because agent will try to recover the operation status update streams even if it hasn't been created yet.    In order to prevent recovery failures, the agent should obtain the ids of the streams to recover by walking the directory in which operation status updates streams are stored.    The agent should also garbage collect streams if the checkpointed state doesn't contain a corresponding operation.",Bug,Major,Resolved,"2019-02-13 22:44:42","2019-02-13 22:44:42",2
"Apache Mesos","Release Mesos 1.7.2",,Task,Major,Resolved,"2019-02-13 20:10:15","2019-02-13 20:10:15",3
"Apache Mesos","Release Mesos 1.6.2",,Task,Major,Resolved,"2019-02-13 20:08:27","2019-02-13 20:08:27",3
"Apache Mesos","Manual testing of agent operation checkpointing/recovery","Since there are no synchronization points in the code to allow us to write automated tests for the agent operation checkpointing/recovery logic, we need to perform rigorous manual testing of those code paths.",Task,Major,Resolved,"2019-02-13 18:14:46","2019-02-13 18:14:46",5
"Apache Mesos","SLRP does not clean up mount directories for destroyed MOUNT disks.","When staging or publishing a CSI volume, SLRP will create the following mount points for these operations:    These directories are cleaned up when the volume is unpublished/unstaged. However, their parent directory, namly {{<work_dir>/csi/<rp_type>/<rp_name>/mounts/<volume_id>}} is never cleaned up.",Bug,Major,Resolved,"2019-02-13 01:37:25","2019-02-13 01:37:25",2
"Apache Mesos","Unit tests for creating and destroying persistent volumes in SLRP.","The plan is to add/update the following unit tests to test persistent volume destroy:   * CreateDestroyDisk   * CreateDestroyDiskWithRecovery   * CreateDestroyPersistentMountVolume   * CreateDestroyPersistentMountVolumeWithRecovery   * CreateDestroyPersistentMountVolumeWithReboot   * CreateDestroyPersistentBlockVolume   * DestroyPersistentMountVolumeFailed   * DestroyUnpublishedPersistentVolume   * DestroyUnpublishedPersistentVolumeWithRecovery   * DestroyUnpublishedPersistentVolumeWithReboot",Task,Major,Resolved,"2019-02-12 05:56:42","2019-02-12 05:56:42",5
"Apache Mesos","Logrotate container logger lets tasks execute arbitrary commands in the Mesos agent's namespace","The non-default {{LogrotateContainerLogger}} module allows tasks to configure sandbox log rotation (See http://mesos.apache.org/documentation/latest/logging/#Containers ).  The {{logrotate_stdout_options}} and {{logrotate_stderr_options}} in particular let the task specify free-form text, which is written to a configuration file located in the task's sandbox.  The module does not sanitize or check this configuration at all.    The logger itself will eventually run {{logrotate}} against the written configuration file, but the logger is not isolated in the same way as the task.  For both the Mesos and Docker containerizers, the logger binary will run in the same namespace as the Mesos agent.  This makes it possible to affect files outside of the task's mount namespace.    Two modes of attack are known to be problematic:  * Changing or adding entries to the configuration file.  Normally, the configuration file contains a single file to rotate:    It is trivial to add text to the {{logrotate_stdout_options}} to add a new entry:    * Logrotate's {{postrotate}} option allows for execution of arbitrary commands.  This can again be supplied with the {{logrotate_stdout_options}} variable.      Some potential fixes to consider:  * Overwrite the .logrotate.conf files each time. This would give only milliseconds between writing and calling logrotate for a thirdparty to modify the config files maliciously. This would not help if the task itself had postrotate options in its environment variables.  * Sanitize the free-form options field in the environment variables to remove postrotate or injection attempts like }\n/path/to/some/file\noptions{.  * Refactor parts of the Mesos isolation code path so that the logger and IO switchboard binary live in the same namespaces as the container (instead of the agent). This would also be nice in that the logger's CPU usage would then be accounted for within the container's resources.",Bug,Critical,Resolved,"2019-02-12 01:19:32","2019-02-12 01:19:32",5
"Apache Mesos","Authorization for DESTROY and UNRESERVE is not symmetrical.","For [the {{UNRESERVE}} case|https://github.com/apache/mesos/blob/5d3ed364c6d1307d88e6b950ae0eef423c426673/src/master/master.cpp#L3661-L3677], if the principal was not set, {{.has_principal()}} will be {{false}}, hence we will not call {{authorizations.push_back()}}, and hence we will not create an authz request with this resource as an object. For [the {{DESTROY}} case|https://github.com/apache/mesos/blob/5d3ed364c6d1307d88e6b950ae0eef423c426673/src/master/master.cpp#L3772-L3773], if the principal was not set, a default value {{}} for string will be used and hence we will create an authz request with this resource as an object.     We definitely need to make the behaviour consistent. I'm not sure which approach is correct.",Improvement,Major,Accepted,"2019-02-08 13:44:45","2019-02-08 13:44:45",5
"Apache Mesos","ContentType/AgentAPITest.MarkResourceProviderGone/1 is flaky","We observed a segfault in {{ContentType/AgentAPITest.MarkResourceProviderGone/1}} on test teardown.  ",Bug,Critical,Resolved,"2019-02-07 07:25:53","2019-02-07 07:25:53",5
"Apache Mesos","OPERATION_UNREACHABLE and OPERATION_GONE_BY_OPERATOR updates don't include the agent/RP IDs",,Bug,Major,Resolved,"2019-02-06 21:57:25","2019-02-06 21:57:25",2
"Apache Mesos","Operations are leaked in Framework struct when agents are removed","Currently, when agents are removed from the master, their operations are not removed from the {{Framework}} structs. We should ensure that this occurs in all cases.",Bug,Major,Resolved,"2019-02-06 18:12:33","2019-02-06 18:12:33",2
"Apache Mesos","Establish a well-defined agent state diagram","The agent's lifecycle is currently not well-defined. There are some agent states which are not represented with distinct agent state values in the code, and we have no documentation which clearly lays out the state diagram for an agent, including the events which will transition an agent from one state to another.    We should design this state diagram to ensure that all agents are always in a well-defined state which is represented in the code and visible to users via our APIs.    This work will include examining the {{Master::_removeSlave()}} function, which currently handles three cases of agent removal:  * Starting maintenance on an agent via the 'startMaintenance()' handler  * When an agent submits a new registration from a previously-known IP:port, via the _registerSlave() method (aka the 'deleted latest symlink' case)  * When an agent shuts itself down via an UnregisterSlaveMessage (aka the SIGUSR1 case)    In these cases, the agent is not transitioned to a new state in the master, it is simply removed. We should define agent states for these cases and ensure that the master stores these agent IDs and/or agent infos.    The outcome of this ticket should be a design doc describing the agent state diagram, and a high-level view of how this could be implemented. New tickets for the implementation should also be created.",Improvement,Major,Accepted,"2019-02-06 16:43:47","2019-02-06 16:43:47",8
"Apache Mesos","Allocator CHECK failure: reservationScalarQuantities.contains(role).","We recently upgraded our Mesos cluster from version 1.3 to 1.5, and since then have been getting periodic master crashes due to this error:    Full stack trace is at the end of this issue description. When the master fails, we automatically restart it and it rejoins the cluster just fine. I did some initial searching and was unable to find any existing bug reports or other people experiencing this issue. We run a cluster of 3 masters, and see crashes on all 3 instances.    Right before the crash, we saw a {{Removed agent:...}} log line noting that it was agent 9b912afa-1ced-49db-9c85-7bc5a22ef072-S6 that was removed.    I saved the full log from the master, so happy to provide more info from it, or anything else about our current environment.    Full stack trace is below.  ",Bug,Critical,Resolved,"2019-02-05 22:26:57","2019-02-05 22:26:57",3
"Apache Mesos","Allocator might skip allocations because a single framework is incapable of receiving certain resources.","Currently in the hierarchical allocator allocation loops we compute {{available}} resources by taking into account the capabilities of the current framework. Further down in the loop we might then {{break}} out of the iteration under the assumption that no other framework can receive the resources in question.    This is only correct if all considered frameworks have identical capabilities.",Bug,Major,Resolved,"2019-02-05 16:33:08","2019-02-05 16:33:08",5
"Apache Mesos","nvidia/cuda 10 does not work on GPU isolator.","I verified that nvidia/cuda 9 (i.e., 9.2-devel-ubuntu18.04) works with GPU isolator.    The unit test NvidiaGpuTest.ROOT_INTERNET_CURL_CGROUPS_NVIDIA_GPU_NvidiaDockerImage captures this, and is currently failing on GPU hosts since it uses latest nvidia/cuda image.    If fails with  {format}  sh: 1: nvidia-smi: not found  {format}",Bug,Critical,Resolved,"2019-02-01 22:44:58","2019-02-01 22:44:58",3
"Apache Mesos","Marking an unreachable agent as gone should transition the tasks to terminal state","If an unreachable agent is marked as gone, currently master just marks that agent in the registry but doesn't do anything about its tasks. So the tasks are in UNREACHABLE state in the master forever, until the master fails over. This is not great UX. We should transition these to terminal state instead.    This fix should also include a test to verify.",Improvement,Major,Resolved,"2019-01-31 22:48:56","2019-01-31 22:48:56",3
"Apache Mesos","SLRP does not clean up destroyed persistent volumes.","When a persistent volume created on a {{ROOT}} disk is destroyed, the agent will clean up its data: https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp#L4397  However, this is not the case for PVs on SLRP disks. The agent relies on the SLRP to do the cleanup:  https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp#L4472  But SLRP simply updates its metadata and do nothing:  https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/resource_provider/storage/provider.cpp#L2805    This would lead to data leakage if the framework does not call `CREATE_DISK` but just unreserve it.",Bug,Blocker,Resolved,"2019-01-31 02:29:38","2019-01-31 02:29:38",5
"Apache Mesos","Hierarchical allocator check failure when an operation on a shutdown framework finishes","When a non-speculated operation like e.g., {{CREATE_DISK}} becomes terminal after the originating framework was torn down, we run into an assertion failure in the allocator.    With non-speculated operations like e.g., {{CREATE_DISK}} it became possible that operations outlive their originating framework. This was not possible with speculated operations like {{RESERVE}} which were always applied immediately by the master.    The master does not take this into account, but instead unconditionally calls {{Allocator::updateAllocation}} which asserts that the framework is still known to the allocator.    Reproducer:   * register a framework with the master.   * add a master with a resource provider.   * let the framework trigger a non-speculated operation like {{CREATE_DISK.}}   * tear down the framework before a terminal operation status update reaches the master; this causes the master to e.g., remove the framework from the allocator.   * let a terminal, successful operation status update reach the master   * ?     To solve this we should cleanup the lifetimes of operations. Since operations can outlive their framework (unlike e.g., tasks), we probably need a different approach here.",Bug,Blocker,Resolved,"2019-01-29 12:09:40","2019-01-29 12:09:40",5
"Apache Mesos","Transition agent operations to some lost state when the agent is removed.","MESOS-8782 and MESOS-8783 transition operations to {{OPERATION_GONE_BY_OPERATOR}} or {{OPERATION_UNREACHABLE}} when their agents are marked as gone or unreachable respectively. However, there are other cases where agents can be removed and forgot by the master:  1) When an agent tries to register with a new ID from the same IP:  https://github.com/apache/mesos/blob/f130544bdb8a9849096ee2cb35ebcbc7d8a326d8/src/master/master.cpp#L6836-L6849  2) When an agent requests to unregister:  https://github.com/apache/mesos/blob/f130544bdb8a9849096ee2cb35ebcbc7d8a326d8/src/master/master.cpp#L7817-L7840    In these tasks, the master explicitly sends {{TASK_LOST}} for task status updates (this also means that [this documentation|https://github.com/apache/mesos/blob/f130544bdb8a9849096ee2cb35ebcbc7d8a326d8/include/mesos/mesos.proto#L2287-L2288] is wrong), but does nothing for operations. We should design proper operation status transitions for these cases.",Improvement,Major,Open,"2019-01-29 00:26:26","2019-01-29 00:26:26",5
"Apache Mesos","Support `DESTROY_DISK` on preprovisioned CSI volumes.","Currently the experimental {{DESTROY_DISK}} operation only applies to {{BLOCK}} or {{MOUNT}} disk resources. We should consider supporting {{DESTROY_DISK}} on {{RAW}} disk resources with source IDs as well. This could be handy in e.g., the following scenario:    1. The framework issued a {{CREATE_DISK}}.  2. The SLRP received {{CREATE_DISK}} and translated to a {{CreateVolume}} CSI call.  3. While the {{CreateVolume}} is ongoing, the agent was restarted with a new agent ID, causing the SLRP to lose its bookkeeping and start with a new RP ID as well, and hence the {{CREATE_DISK}} operation was lost.  4. The {{CreateVolume}} call succeeded and the new SLRP picked it up as a preprovisioned CSI volume.    In the above case, the framework should be able to choose to either import the CSI volume through {{CREATE_DISK}}, or directly reclaim the space through {{DESTROY_DISK}}. Currently the framework needs to always import the CSI volume before reclaiming the space.",Improvement,Blocker,Resolved,"2019-01-28 20:09:55","2019-01-28 20:09:55",2
"Apache Mesos","Agent `ReconcileOperations` handler should handle operation affecting default resources","{{Slave::reconcileOperations()}} has to be updated to send {{OPERATION_DROPPED}} for unknown operations that don't have a resource provider ID.",Task,Major,Resolved,"2019-01-25 22:38:26","2019-01-25 22:38:26",3
"Apache Mesos","SLRP sends inconsistent status updates for dropped operations.","The bug manifests in the following scenario:  1. Upon receiving profile updates, the SLRP sends an {{UPDATE_STATE}} to the agent with a new resource version.  2. At the same time, the agent sends an {{APPLY_OPERATION}} to the SLRP with the original resource version.  3. The SLRP asks the status update manager (SUM) to reply with an {{OPERATION_DROPPED}} to the framework because of the resource version mismatch. The status update is required to be acked. Then, it simply discards the operation (i.e., no bookkeeping).  4. The agent finds a missing operation in the {{UPDATE_STATE}} so it sends a {{RECONCILE_OPERATIONS}}.  5. The SLRP asks the SUM to reply with an {{OPERATION_DROPPED}} to the agent (without a framework ID set) because it no longer knows about the operation.  6. The SUM returns an error because the latter {{OPERATION_DROPPED}} is inconsistent with the earlier one since it does not have a framework ID.",Bug,Blocker,Resolved,"2019-01-25 20:21:43","2019-01-25 20:21:43",3
"Apache Mesos","Nested container launched with non-root user may not be able to write to its sandbox via the environment variable `MESOS_SANDBOX`","Launch a nested container to write to its sandbox via the env var `MESOS_SANDBOX`. The nested container is launched with a non-root user (e.g., `nobody`) and its parent container (i.e., the default executor) is launched with root since `mesos-execute` is executed with `sudo` in the example below.    The nested container will fail.    In the stderr of the nested container, we can see it has no permission to do the write.     ",Bug,Critical,Accepted,"2019-01-25 08:12:55","2019-01-25 08:12:55",3
"Apache Mesos","Master should clean up operations from downgraded agents","If a Mesos agent is upgraded to provide reliable feedback for operations on agent default resources and then later downgraded, the master may possess in-memory state related to operations requesting feedback which should be cleaned up. We should update the master to detect downgraded agents and clean up appropriately.    This ticket does not include sending best-effort feedback to schedulers for operations on downgraded agents.    The upgrade documentation should also be updated with a note about the impact of downgrades on operation feedback.",Task,Major,Resolved,"2019-01-23 17:44:21","2019-01-23 17:44:21",3
"Apache Mesos","CniIsolatorTest.ROOT_CleanupAfterReboot is flaky.","    It was from this commit https://github.com/apache/mesos/commit/c338f5ada0123c0558658c6452ac3402d9fbec29",Bug,Major,Resolved,"2019-01-23 01:00:40","2019-01-23 01:00:40",2
"Apache Mesos","ResourceOffersTest.ResourceOfferWithMultipleSlaves is flaky.","    caused by this commit https://github.com/apache/mesos/commit/07bccc6377a180267d4251897a765acba9fa0c4d",Bug,Major,Resolved,"2019-01-22 22:21:50","2019-01-22 22:21:50",2
"Apache Mesos","chown error handling is incorrect in createSandboxDirectory.",,Bug,Major,Resolved,"2019-01-22 21:58:47","2019-01-22 21:58:47",2
"Apache Mesos","Agent capability for operation feedback on default resources","We should add an agent capability to prevent the master from sending operations on agent default resources which request feedback to older agents which are not able to handle them.",Task,Major,Resolved,"2019-01-16 18:17:59","2019-01-16 18:17:59",3
"Apache Mesos","Add per-framework allocatable resources matcher/filter.","Currently, Mesos has a single global flag `min_allocatable_resources` that provides some control over the shape of the offer. But, being a global flag, finding a one-size-fits-all shape is hard and less than ideal. It will be great if frameworks can specify different shapes based on their needs.     In addition to extending this flag to be per-framework. It is also a good opportunity to see if it can be more than `min_alloctable` e.g. providing more predicates such as max,  (not) contain and etc. ",Improvement,Major,Resolved,"2019-01-15 17:21:13","2019-01-15 17:21:13",5
"Apache Mesos","Unable to build Mesos with CMake on Ubuntu 14.04.","Running the following command to build Mesos on Ubuntu 14.04 will lead to the error shown below:      The reason is that gRPC's CMake rules does not disable ALPN on systems with OpenSSL 1.0.1.",Bug,Major,Resolved,"2019-01-10 23:52:18","2019-01-10 23:52:18",2
"Apache Mesos","SLRP should treat gRPC timeouts as non-terminal errors, instead of reporting OPERATION_FAILED.","1. framework executes a CREATE_DISK operation.  2. The SLRP issues a CreateVolume RPC to the plugin  3. The RPC call times out  4. The agent/SLRP translates non-terminal gRPC timeout errors (DeadlineExceeded) for CreateVolume calls into OPERATION_FAILED, which is terminal.  5. framework receives a *terminal* OPERATION_FAILED status, so it executes another CREATE_DISK operation.  6. The second CREATE_DISK operation does not timeout.  7. The first CREATE_DISK operation was actually completed by the plugin, unbeknownst to the SLRP.  8. There's now an orphan volume in the storage system that no one is tracking.    Proposed solution: the SLRP makes more intelligent decisions about non-terminal gRPC errors. For example, timeouts are likely expected for potentially long-running storage operations and should not be considered terminal. In such cases, the SLRP should NOT report OPERATION_FAILED and instead should re-issue the **same** (idempotent) CreateVolume call to the plugin to ascertain the status of the requested volume creation.    Agent logs for the 3 orphan vols above:  ",Bug,Critical,Resolved,"2019-01-09 16:49:01","2019-01-09 16:49:01",8
"Apache Mesos","Reviewboard bot fails on verify-reviews.py.","Seeing this on our Azure based Mesos CI for review requests.        This is happening pretty much exactly since we landed https://github.com/apache/mesos/commit/3badf7179992e61f30f5a79da9d481dd451c7c2f#diff-0bcbb572aad3fe39e0e5c3c8a8c3e515",Bug,Major,Resolved,"2019-01-08 23:12:37","2019-01-08 23:12:37",2
"Apache Mesos","Investigate command health check performance","Users have reported performance issues caused by too many command health checks performed too quickly by the default executor. Use of the agent's LAUNCH_NESTED_CONTAINER_SESSION call can impact the containerizer's ability to successfully launch containers in general.    We need to investigate this issue and decide on a path forward to improve the performance of command health checks.",Task,Major,Resolved,"2019-01-08 19:29:59","2019-01-08 19:29:59",5
"Apache Mesos","Disallowed nan, inf and so on in `Value::Scalar`.","Mesos does not expect `Value::Scalar` to be nan, inf and etc. We should validate this when parsing.",Improvement,Major,Resolved,"2019-01-07 04:48:06","2019-01-07 04:48:06",1
"Apache Mesos","Benchmark command health checks in default executor","TCP/HTTP health checks were extensively scale tested as part of https://<USER>com/blog/introducing-mesos-native-health-checks-apache-mesos-part-2/.     We should do the same for command checks by default executor because it uses a very different mechanism (agent fork/execs the check command as a nested container) and will have very different scalability characteristics.    We should also use these benchmarks as an opportunity to produce perf traces of the Mesos agent (both with and without process inheritance) so that a thorough analysis of the performance can be done as part of MESOS-9513.",Task,Major,Resolved,"2019-01-03 18:02:35","2019-01-03 18:02:35",5
"Apache Mesos","Official 1.7.0 tarball can't be built on Ubuntu 16.04 LTS.","I installed Ubuntu 16.04.5 LTS in a VM and precisely followed the steps in [http://mesos.apache.org/documentation/latest/building/] to build Mesos (fetching the 1.7.0 release). Nevertheless what I get is the following error message:  {code:sh}  make[4]: Entering directory '/home/max/mesos-1.7.0/build/3rdparty/grpc-1.10.0'    DEPENDENCY ERROR    The target you are trying to run requires an OpenSSL implementation.  Your system doesn't have one, and either the third_party directory  doesn't have it, or your compiler can't build BoringSSL.  ",Bug,Blocker,Resolved,"2019-01-03 07:50:53","2019-01-03 07:50:53",2
"Apache Mesos","Agent could not recover due to empty docker volume checkpointed files.","Agent could not recover due to empty docker volume checkpointed files. Please see logs:    This might happen after hard reboot. Docker volume isolator uses `state::checkpoint()` function which creates a temporary file, then writes the data, then renames the temporary file to destination file. This function is atomic and supports `fsync` for the data. However, Docker volume isolator does not use `fsync` option for performance reasons, hence the data might be lost if page cache is not synced before reboot.    Basically the docker volume is not mounted yet, so the docker volume isolator should skip recovering this volume.",Bug,Critical,Resolved,"2019-01-02 22:17:23","2019-01-02 22:17:23",5
"Apache Mesos","Master will leak operations when agents are removed","Usually, offer operations are removed when the framework acknowledges  a terminal operation status update.    However, currently only operations on registered agents can be  acknowledged, so operations on agents which don't come back will be permanently leaked.",Bug,Major,Resolved,"2019-01-02 16:30:12","2019-01-02 16:30:12",3
"Apache Mesos","`make check` failed with linking errors when c-ares is installed.","MacOS Mojave  autotool (using cmake works)        It failed like this        ",Bug,Major,Resolved,"2019-01-02 07:03:37","2019-01-02 07:03:37",2
"Apache Mesos","Use ResourceQuantities in the allocator and sorter to improve performance.","In allocator and sorter, we need to do a lot of quantity calculations. Currently, we use the full {{Resources}} type with utilities like {{createScalarResourceQuantities()}}, even though we only care about quantities. Replace {{Resources}} with {{ResourceQuantities}}.    See:    https://github.com/apache/mesos/blob/386b1fe99bb9d10af2abaca4832bf584b6181799/src/master/allocator/sorter/drf/sorter.hpp#L444-L445  https://reviews.apache.org/r/70061/    With the addition of ResourceQuantities, callers can now just do {{ResourceQuantities.fromScalarResources(r.scalars())}} instead of using {{Resources::createStrippedScalarQuantity()}}, which should actually be a bit more efficient since we only copy the shared pointers rather than construct new `Resource` objects.",Improvement,Major,Resolved,"2018-12-31 18:53:19","2018-12-31 18:53:19",3
"Apache Mesos","IOswitchboard cleanup could get stuck due to FD leak from a race.","Our check container got stuck during destroy which in turned stucks the parent container. It is blocked by the I/O switchboard cleanup:    1223 18:04:41.000000 16269 switchboard.cpp:814] Sending SIGTERM to I/O switchboard server (pid: 62854) since container 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09e is being destroyed  ....  1227 04:45:38.000000  5189 switchboard.cpp:916] I/O switchboard server process for container 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09e has terminated (status=N/A)    Note the timestamp.    *Root Cause:*  Fundamentally, this is caused by a race between *.discard()* triggered by Check Container TIMEOUT and IOSB extracting ContainerIO object. This race could be exposed by overloaded/slow agent process. Please see how this race be triggered below:    # Right after IOSB server process is running, Check container Timed out and the checker process returns a failure, which would close the HTTP connection with agent.  # From the agent side, if the connection breaks, the handler will trigger a discard on the returned future and that will result in containerizer->launch()'s future transitioned to DISCARDED state.  # In containerizer, the DISCARDED state will be propagated back to IOSB prepare(), which stop its continuation on *extracting the containerIO* (it implies the object being cleaned up and FDs(one end of pipes created in IOSB) being closed in its destructor).  # Agent starts to destroy the container due to its discarded launch result, and asks IOSB to cleanup the container.  # IOSB server is still running, so agent sends a SIGTERM.  # SIGTERM handler unblocks the IOSB from redirecting (to redirect stdout/stderr from container to logger before exiting).  # io::redirect() calls io::splice() and reads the other end of those pipes forever.    This issue is *not easy to reproduce unless* on a busy agent, because the timeout has to happen exactly *AFTER* IOSB server is running and *BEFORE* IOSB extracts containerIO.",Bug,Critical,Resolved,"2018-12-28 06:17:31","2018-12-28 06:17:31",8
"Apache Mesos","Mesos executor fails to terminate and gets stuck after agent host reboot.","When an agent host reboots, all of its containers are gone but the agent will still try to recover from its checkpointed state after reboot.    The agent will soon discover that all the cgroup hierarchies are gone and assume (correctly) that the containers are destroyed.    However, when trying to terminate the executor, the agent will first try to wait for the exit status of its container:  https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/containerizer.cpp#L2631    Agent dose so by `waitpid` on the checkpointed child process pid. If, after the agent host reboot, a new process with the same pid gets spawned, then the parent will wait for the wrong child process. This could get stuck until the wrongly waited-for  process is somehow exited, see `ReaperProcess::wait()`: https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/reap.cpp#L88-L114    This will block the executor termination as well as future task status update (e.g. master might still think the task is running).",Bug,Critical,Resolved,"2018-12-27 01:51:16","2018-12-27 01:51:16",5
"Apache Mesos","Parallel reads for expensive master v1 read-only calls.","Similar to MESOS-9158 - we should make the operator API calls which serve master state perform computation of multiple such responses in parallel to reduce the performance impact on the master actor.    Note that this includes the initial expensive SUBSCRIBE payload for the event streaming API, which is less straightforward to incorporate into the parallel serving logic since it performs writes (to track the subscriber) and produces an infinite response, unlike the other state related calls.",Improvement,Major,Resolved,"2018-12-21 04:33:17","2018-12-21 04:33:17",5
"Apache Mesos","Test `MasterTest.CreateVolumesV1AuthorizationFailure` is flaky.","  This is because we authorize the retried registration before dropping it.    Full log: [^mesos-ec2-centos-7-CMake.Mesos.MasterTest.CreateVolumesV1AuthorizationFailure-badrun.txt]",Bug,Major,Resolved,"2018-12-20 22:17:38","2018-12-20 22:17:38",1
"Apache Mesos","Set up `object.value` for `CREATE_DISK` and `DESTROY_DISK` authorizations.","We should be defensive and set up {{object.value}} to the role of the resource for authorization actions {{CREATE_BLOCK_DISK}}, {{DESTROY_BLOCK_DISK}}, {{CREATE_MOUNT_DISK}} and {{DESTROY_MOUNT_DISK}} so an old-school authorizer can rely on the field to perform authorization.    This behavior is deprecated though, so will be removed once all {{*_WITH_ROLE}} authorization action aliases are removed through MESOS-7073.",Improvement,Major,Resolved,"2018-12-18 06:03:17","2018-12-18 06:03:17",1
"Apache Mesos","Unit test for master operation authorization.","We should create a unit test for MESOS-9474 and MESOS-9480. To make the test easier, we might want to consider using operation feedback once MESOS-9472 is done.",Task,Major,Resolved,"2018-12-18 05:58:11","2018-12-18 05:58:11",5
"Apache Mesos","Master may skip processing authorization results for `LAUNCH_GROUP`.","If there is a validation error for {{LAUNCH_GROUP}}, or if there are multiple authorization errors for some of the tasks in a {{LAUNCH_GROUP}}, the master will skip processing the remaining authorization results, which would result in these authorization results being examined by subsequent operations incorrectly:  https://github.com/apache/mesos/blob/3ade731d0c1772206c4afdf56318cfab6356acee/src/master/master.cpp#L5487-L5521    ",Bug,Blocker,Resolved,"2018-12-14 18:59:05","2018-12-14 18:59:05",2
"Apache Mesos","SLRP does not set RP ID in produced OperationStatus.",,Bug,Major,Resolved,"2018-12-14 14:00:03","2018-12-14 14:00:03",1
"Apache Mesos","Documentation for operation feedback","Review: https://reviews.apache.org/r/69871/",Task,Major,Resolved,"2018-12-13 18:42:16","2018-12-13 18:42:16",5
"Apache Mesos","Master does not respect authorization result for `CREATE_DISK` and `DESTROY_DISK`.","On our internal cluster with a custom authorizer module we observed the following problem:    The authorizer module caches authorization results, and that's why there was only one logged authorization requset. The problem is that, the logged request was for {{CREATE_MOUNT_DISK}}, and the result was {{deny}}, but despite the denial, all {{CREATE_DISK}} operations were processed, however another {{RESERVE}} operation was dropped because of this denial.    The bug is that the master pushed a authorization future in the {{futures}} vector in {{Master::accept}} for each {{DESTROY_DISK}}:   [https://github.com/apache/mesos/blob/18356bf3f4ac730b4a798261aad042555c4a4834/src/master/master.cpp#L4599-L4601]   However, the master never popped and checked the future in {{Master::_accept}}, but go ahead to process the operation:   [https://github.com/apache/mesos/blob/18356bf3f4ac730b4a798261aad042555c4a4834/src/master/master.cpp#L5706]   The future ended up mismatched with the {{RESERVE}} operation, causing it to be dropped.    Updated: a similar bug would happen if we have a validation error for {{LAUNCH_GROUP}}. See MESOS-9480.",Bug,Blocker,Resolved,"2018-12-12 20:10:48","2018-12-12 20:10:48",3
"Apache Mesos","Add end to end tests for operations on agent default resources.","Making note of particular cases we need to test:  * Verify that frameworks will receive OPERATION_GONE_BY_OPERATOR for operations on agent default resources when an agent is marked gone  * Verify that frameworks will receive OPERATION_GONE_BY_OPERATOR when they reconcile operations on agents which have been marked gone",Task,Major,Resolved,"2018-12-11 22:24:18","2018-12-11 22:24:18",8
"Apache Mesos","Unblock operation feedback on agent default resources.","# Remove {{CHECK}} marked with a TODO in {{Master::updateOperationStatus()}}.  # Update {{Master::acknowledgeOperationStatus()}}, remove the CHECK requiring a resource provider ID.  # Remove validation in {{Option<Error> validate(mesos::scheduler::Call& call, const Option<Principal>& principal)}}",Task,Major,Resolved,"2018-12-11 22:23:05","2018-12-11 22:23:05",3
"Apache Mesos","Master should track operations on agent default resources.","Make {{Master::updateSlave()}} add operations that the agent sends and the master doesn't know.    Right now only operations from SLRPs are added to the master's in-memory state.",Task,Major,Resolved,"2018-12-11 22:19:53","2018-12-11 22:19:53",3
"Apache Mesos","Master may send `FRAMEWORK_UPDATED` for a new framework ID in operator API.","In the operator streaming API, the master only sends {{FRAMEWORK_ADDED}} if a framework is subscribed with no ID:  [https://github.com/apache/mesos/blob/4803af11bcb038193c238c049ea67c73b1bfd9a1/src/master/master.cpp#L2653-L2679]  [https://github.com/apache/mesos/blob/4803af11bcb038193c238c049ea67c73b1bfd9a1/src/master/master.cpp#L2951-L2988]    That means after a master failover, if a framework is recovered from an agent or subscribed with an ID, a {{FRAMEWORK_UPDATED}} with a framework ID that is previously unknown will be sent to the subscribers.",Bug,Major,Open,"2018-12-11 22:14:49","2018-12-11 22:14:49",3
"Apache Mesos","Mesos does not validate framework-supplied FrameworkIDs","Since Mesos masters do not persist frameworks (MESOS-1719) we might subscribe schedulers with self-assigned {{FrameworkIDs}}.    While we cannot confirm that used {{FrameworkIDs}} were indeed assigned by Mesos masters, we should still validate the supplied values to make sure they do not break our internal assumptions (e.g., IDs might be used to construct filesystem paths).",Bug,Major,Resolved,"2018-12-11 12:59:32","2018-12-11 12:59:32",1
"Apache Mesos","Devices in a container are inaccessible due to `nodev` on `/var/run`.","A recent [patch|https://reviews.apache.org/r/69086/] (commit ede8155d1d043137e15007c48da36ac5fa0b5124) changes the behavior of how standard device nodes (e.g., /dev/null, etc.) are setup. It uses bind mount (from host) now (instead of mknod).    The devices nodes are created under `/var/run/mesos/containers/<container_id>/devices`, and then bind mounted to the container root filesystem. This is problematic for those Linux distros that mount `/var/run` (or `/run`) as `nodev`. For instance, CentOS 7.4:      As a result, the `/dev/null` devices in the container will inherit the `nodev` from `/run` on the host      This will cause Permission Denied error when a process in the container tries to open the device node.    You can try to reproduce this issue using Mesos Mini      And the, go to Marathon UI (http://localhost:8080), and launch an app using the following config      You'll see the task failed with Permission Denied.    The task will run normally if you use `mesos/mesos-mini:master-2018-12-01`",Bug,Blocker,Resolved,"2018-12-07 19:04:31","2018-12-07 19:04:31",3
"Apache Mesos","Speculative operations may make master and allocator resource views out of sync.","When speculative operations (RESERVE, UNRESERVE, CREATE, DESTROY) are issued via the master operator API, the master updates the allocator state in {{Master::apply()}}, and then later updates its internal state in {{Master::_apply}}. This means that other updates to the allocator may be interleaved between these two continuations, causing the master state to be out of sync with the allocator state.    This bug could happen with the following sequence of events:    - agent (re)registers with the master  - multiple speculative operation calls are made to the master via the operator API  - the allocator is speculatively updated in https://github.com/apache/mesos/blob/1d1af190b0eb674beecf20646d0b6ce082db4ed0/src/master/master.cpp#L11326  - before agent resource gets updated, it sends `UpdateSlaveMessage` when getting the (re)registered message if it has the capability `RESOURCE_PROVIDER` or oversubscription is used (https://github.com/apache/mesos/blob/3badf7179992e61f30f5a79da9d481dd451c7c2f/src/slave/slave.cpp#L1560-L1566 and https://github.com/apache/mesos/blob/3badf7179992e61f30f5a79da9d481dd451c7c2f/src/slave/slave.cpp#L1643-L1648)  - as long as the first operation via the operator API has been added to the {{Slave}} struct at this point, then the master won't hit [this block here|https://github.com/apache/mesos/blob/1d1af190b0eb674beecf20646d0b6ce082db4ed0/src/master/master.cpp#L7940-L7945] and the `UpdateSlaveMessage` triggers allocator to update the total resources with STALE info from the {{Slave}} struct [here|https://github.com/apache/mesos/blob/1d1af190b0eb674beecf20646d0b6ce082db4ed0/src/master/master.cpp#L8207], thus the update from the previous operation is overwritten and LOST. Since the {{Slave}} struct has not yet been updated, the allocator update at that point uses stale resources from {{slave->totalResources}}.  - agent finishes the operation and informs the master through `UpdateOperationStatusMessage` but for the speculative operation, we do not update the allocator https://github.com/apache/mesos/blob/3badf7179992e61f30f5a79da9d481dd451c7c2f/src/master/master.cpp#L11187-L11189  - The resource views of the master/agent state and the allocator state are now inconsistent    This caused MESOS-7971 and likely MESOS-9458 as well.     It's unclear how this can be fixed in a reliable way. It's possible that ensuring that updates to the allocator state and the master state are performed in a single synchronous block of code could work, but in the case of operator-initiated operations this is difficult. It may also be possible to ensure consistency by ensuring that every time such updates are done in the master, the allocator is updated before the master state.    This ticket will be Done when a comprehensive solution for this issue is designed. A subsequent ticket for actual implementation of that solution should be filed.",Bug,Major,Accepted,"2018-12-07 05:10:24","2018-12-07 05:10:24",5
"Apache Mesos","PersistentVolumeEndpointsTest.StaticReservation is flaky","Observed this in ASF CI https://builds.apache.org/view/M-R/view/Mesos/job/Mesos-Buildbot-Test/310/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--disable-parallel-test-execution,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1%20MESOS_TEST_AWAIT_TIMEOUT=60secs,OS=ubuntu:16.04,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!ubuntu-4)&&(!H21)&&(!H23)&&(!H26)&&(!H27)/consoleText    ",Bug,Major,Accepted,"2018-12-05 19:57:54","2018-12-05 19:57:54",2
"Apache Mesos","SEGV During Initial Connect Processing","While running my Mesos Scheduler v1 implementation, the code sometimes gets a SEGV in the JNI code.  It appears to fail in the 'connected' method of the attached code.",Bug,Major,Open,"2018-12-05 15:19:06","2018-12-05 15:19:06",5
"Apache Mesos","Set `SCMP_FLTATR_CTL_LOG` attribute during initialization of Seccomp context","Since version 4.14 the Linux kernel supports SECCOMP_FILTER_FLAG_LOG flag which can be used for enabling logging for all Seccomp filter operations except SECCOMP_RET_ALLOW. If a Seccomp filter does not allow the system call, then the kernel will print a message into dmesg during invocation of this system call.    At the moment libseccomp ver. 2.3.3 does not provide this flag, but the latest master branch of libseccomp supports SECCOMP_FILTER_FLAG_LOG. So, we need to add    into `SeccompFilter::create()` when the newest version of libseccomp will be released (v2.3.4+).     ",Task,Major,Open,"2018-12-05 15:02:12","2018-12-05 15:02:12",3
"Apache Mesos","Completed framework update streams may retry forever","Since the agent/RP currently does not GC operation status update streams when frameworks are torn down, it's possible that active update streams associated with completed frameworks may remain and continue retrying forever. We should add a mechanism to complete these streams when the framework becomes completed.    A couple options which have come up during discussion:  * Have the master acknowledge updates associated with completed frameworks. Note that since completed frameworks are currently only tracked by the master in memory, a master failover could prevent this from working perfectly.  * Extend the RP API to allow the GC of particular update streams, and have the agent GC streams associated with a framework when it receives a {{ShutdownFrameworkMessage}}. This would also require the addition of a new method to the status update manager.",Bug,Major,Resolved,"2018-12-01 03:47:51","2018-12-01 03:47:51",2
"Apache Mesos","Develop a sound plan on how to deal with disabled tests.","We currently have disabled a bunch of tests due to their extreme flakiness. While that is a good idea for a clean start or to bridge resource starvation, it can only be a stop-gap-solution.    We should develop something that allows us to keep such tests on the radar and forces us to get them back up, bit by bit.",Task,Major,Accepted,"2018-11-29 23:12:51","2018-11-29 23:12:51",3
"Apache Mesos","Revisit quota documentation.","At this point the quota documentation in the docs/ folder has become rather stale. It would be good to at least update any inaccuracies and ideally re-write it to better reflect the current thinking.",Documentation,Major,Resolved,"2018-11-29 23:10:01","2018-11-29 23:10:01",5
"Apache Mesos","ZK master detection can become forever pending.","The following agent logs are observed on an agent that cannot join the cluster after a network partition:    The hypothesis is that, when the leading master and an agent get in the same network partition, and the other two masters form a new quorum, the future returned from the following line can become forever pending on the partitioned agent:   [https://github.com/apache/mesos/blob/bf4e8b392b3fa58ffdbf5f14ce3f0ba7a1674a0c/src/zookeeper/group.cpp#L316]    One possible resolution is to trigger an update periodically. Another possibility is to make the master to check the quorum periodically and commit suicide if it is partitioned to trigger a ZK disconnection on any connected agent.    NOTE: At the creation of the ticket, I haven't got the log file of the partitioned master at the time frame when the network partition happened. I'll update the ticket once I have more information.",Bug,Critical,Accepted,"2018-11-29 23:02:49","2018-11-29 23:02:49",5
"Apache Mesos","ZooKeeperTest.Auth is flaky.",,Bug,Major,Open,"2018-11-28 13:44:11","2018-11-28 13:44:11",3
"Apache Mesos","Executor to framework message crashes master if framework has not re-registered.","If the executor sends a framework message after a master failover, and the framework has not yet re-registered with the master, this will crash the master:        This is because Framework::send proceeds if the framework is disconnected. In the case of a recovered framework, it will not have a pid or http connection yet:    https://github.com/apache/mesos/blob/9b889a10927b13510a1d02e7328925dba3438a0b/src/master/master.hpp#L2590-L2610        The executor to framework path does not guard against the framework being disconnected, unlike the status update path:    https://github.com/apache/mesos/blob/9b889a10927b13510a1d02e7328925dba3438a0b/src/master/master.cpp#L6472-L6495    vs.    https://github.com/apache/mesos/blob/9b889a10927b13510a1d02e7328925dba3438a0b/src/master/master.cpp#L8371-L8373    It was reported that this crash didn't occur for the user on 1.2.0, however the issue appears to present there as well, so we will try to backport a test to see if it's indeed not occurring in 1.2.0.",Bug,Blocker,Resolved,"2018-11-26 23:21:23","2018-11-26 23:21:23",3
"Apache Mesos","Rework FindPackageHelper to allow for locating xxx_ROOT_DIR.","With the introduction of unbundled libevent, libarchive and leveldb CMake builds, code duplication within the respective FindXXX.cmake modules became apparent and a refactoring seems adequate.    See this TODO: https://github.com/apache/mesos/blob/e6b507b6469553725c98b953f9578ab3f1a40b93/3rdparty/cmake/FindLIBARCHIVE.cmake#L19  ",Improvement,Minor,Open,"2018-11-25 14:28:16","2018-11-25 14:28:16",2
"Apache Mesos","Validation of JWT tokens using HS256 hashing algorithm is not thread safe.","from the [OpenSSL documentation|https://www.openssl.org/docs/man1.0.2/crypto/hmac.html]:    {quote}  It places the result in {{md}} (which must have space for the output of the hash function, which is no more than {{EVP_MAX_MD_SIZE}} bytes). If {{md}} is {{NULL}}, the digest is placed in a static array. The size of the output is placed in {{md_len}}, unless it is {{NULL}}. Note: passing a {{NULL}} value for {{md}} to use the static array is not thread safe.  {quote}    We are calling {{HMAC()}} as follows:        Given that this code does not run inside a process, race conditions could occur.",Bug,Major,Resolved,"2018-11-21 13:46:03","2018-11-21 13:46:03",5
"Apache Mesos","Implement Seccomp isolator tests",,Task,Major,Resolved,"2018-11-20 17:11:27","2018-11-20 17:11:27",3
"Apache Mesos","Allow for optionally unbundled leveldb from CMake builds.","Following the example of unbundled libevent and libarchive, we should allow for unbundled leveldb if the user wishes so.    For leveldb, this task is not as trivial as one would hope due to the fact that we link leveldb statically. This forces us to satisfy leveldb's strong dependencies against gpertools (tcmalloc) as well as snappy.  Alternatively, we would resort into linking leveldb dynamically, solving these issues.  ",Improvement,Minor,Resolved,"2018-11-20 01:07:25","2018-11-20 01:07:25",2
"Apache Mesos","Allow for optionally unbundled protobuf from CMake builds.","Following the example of unbundled libevent and lib archive, we should allow for unbundled protobuf if the user wishes so.",Improvement,Minor,Open,"2018-11-20 01:04:32","2018-11-20 01:04:32",2
"Apache Mesos","Allow for optionally unbundled glog from CMake builds. ","Following the example of unbundled libevent and libarchive, we should allow for unbundled glog if the user wishes so.",Improvement,Minor,Open,"2018-11-20 00:48:59","2018-11-20 00:48:59",2
"Apache Mesos","Update 'mesos task list' to only list running tasks","Doing a {{mesos task list}} currently returns all tasks that have ever been run (not just running tasks). The default behavior should be to return only the running tasks and offer an option to return all of them. To tell them apart, there should be a state field in the table returned by this command.",Task,Major,Resolved,"2018-11-19 11:11:12","2018-11-19 11:11:12",2
"Apache Mesos","Use the built CLI binary when running new CLI integration tests in CI","We currently use the CLI in the virtual environment which is just a Python file being interpreted, we should instead use the binary built by PyInstaller as it is what is gonna be used in production.",Task,Major,Resolved,"2018-11-16 12:50:44","2018-11-16 12:50:44",3
"Apache Mesos","Check failure on `StorageLocalResourceProviderProcess::applyCreateDisk`.","Observed the following agent failure on one of our staging clusters:  ",Bug,Minor,Resolved,"2018-11-16 12:10:05","2018-11-16 12:10:05",5
"Apache Mesos","Fetcher crashes extracting archives with non-ASCII filenames.","        So archive_strncpy_l() fails with -1. best_effort_strncat_in_locale() has this wonky-looking code:    ",Bug,Critical,Accepted,"2018-11-16 00:55:18","2018-11-16 00:55:18",3
"Apache Mesos","Implement tests for Seccomp parser",,Task,Major,Resolved,"2018-11-15 15:26:29","2018-11-15 15:26:29",3
"Apache Mesos","Add the capability to specify min/max disk size range for a profile.","Since CSI plugins run {{mkfs}} lazily when a volume is first published on an agent, it is possible that a CSI plugin allows a framework to create a volume of size that is too small or too large for the target filesystem when the framework issues a {{CREATE_DISK}}, but the volume cannot be used by any task since all tasks will fail because the volume cannot be published.    To avoid this and improve user experience, we could extend the module interface of the disk profile adaptor to specify a min/max size range for a given profile, so Mesos can validate the size upfront.",Improvement,Major,Open,"2018-11-14 21:56:47","2018-11-14 21:56:47",5
"Apache Mesos","Surface errors for publishing CSI volumes in task status updates.","Currently if a CSI volumes is failed to publish (e.g., due to {{mkfs}} errors), the framework will get a {{TASK_FAILED}} with reason {{REASON_CONTAINER_LAUNCH_FAILED}} or {{REASON_CONTAINER_UPDATE_FAILED}} and message {{Failed to publish resources for resource provider XXX: Received FAILED status}}, which is not informative. We should surface the actual error message to the {{TASK_FAILED}} status update.",Improvement,Major,Open,"2018-11-14 21:47:42","2018-11-14 21:47:42",5
"Apache Mesos","Implement Seccomp profile inheritance for POD containers","Child containers inherit its parent container's Seccomp profile by default. Also, Seccomp profile can be overridden by a Framework for a particular child container by specifying a path to the Seccomp profile.    Mesos containerizer persists information about containers on disk via `ContainerLaunchInfo` proto, which includes `ContainerSeccompProfile` proto. Mesos containerizer should use this proto to load the parent's profile for a child container. When a child inherits the parent's Seccomp profile, Mesos agent doesn't have to re-read a Seccomp profile from the disk, which was used for the parent container. Otherwise, we would have to check that a file content hasn't changed since the last time the parent was launched.",Task,Major,Resolved,"2018-11-14 16:38:45","2018-11-14 16:38:45",5
"Apache Mesos","Resource providers reported by master should reflect connected resource providers","Currently, the master will remember any resource provider it saw ever and report it e.g., in {{GET_AGENTS}} responses, regardless of whether the resource provider is currently connected or not. This is not very intuitive.    The master should instead only report resource providers which are currently connected. Agents can still report even disconnected resource providers.",Improvement,Major,Resolved,"2018-11-13 09:49:01","2018-11-13 09:49:01",5
"Apache Mesos","Design gRPC-based Mesos module interfaces.","We could consider designing how to have gRPC-based Mesos module interfaces. This will enable users to write their own modules through more language bindings. For synchronous module interfaces, MESOS-7749 already providers the gRPC client support.    We could move this to another epic in the future.",Wish,Major,Open,"2018-11-08 19:58:18","2018-11-08 19:58:18",13
"Apache Mesos","Support v1 Executor API over gRPC.","Supporting v1 Executor API over gRPC will enable people to write custom executors with more language bindings. The main work includes:   1. Define the Executor gRPC service in {{service.proto}}. The proto will be in proto3, but the request and response messages will still be in proto2, since this is what the current v1 API based on.   2. Refactor the agent code to support both HTTP and gRPC connections to reuse most of the current code for HTTP Executor API.   3. Implement handlers for gRPC calls.",Task,Major,Open,"2018-11-08 19:32:01","2018-11-08 19:32:01",8
"Apache Mesos","Support v1 Operator Master streaming API over gRPC.","Supporting v1 Operator Master streaming API over gRPC will enable people to interact with Mesos through more language bindings, and the UI can be based on gRPC. The main work includes:  1. Add the streaming calls to the gRPC service in {{service.proto}}. The proto will be in proto3, but the request and response messages will still be in proto2, since this is what the current v1 API based on.  2. Refactor the master code to support both HTTP and gRPC connections to reuse most of the current code for HTTP Operator streaming API.  3. Implement handlers for gRPC calls.",Task,Major,Open,"2018-11-08 19:15:41","2018-11-08 19:15:41",8
"Apache Mesos","Support v1 Operator Agent synchronous API over gRPC.","Supporting v1 Operator Agent API for synchronous calls over gRPC will enable people to query the agent with more language bindings, and the UI can be based on gRPC. The main work includes:  1. Define the Agent gRPC service in {{service.proto}}. The proto will be in proto3, but the request and response messages will still be in proto2, since this is what the current v1 API based on.  2. Implement handlers for gRPC calls to delegate them to the original agent call handlers.",Task,Major,Open,"2018-11-08 19:10:20","2018-11-08 19:10:20",8
"Apache Mesos","Support v1 Operator Master synchronous API over gRPC.","Supporting v1 Operator Master API for synchronous calls over gRPC will enable people to query the master with more language bindings, and the UI can be based on gRPC. The main work includes:  1. Define the Master gRPC service in {{service.proto}}. The proto will be in proto3, but the request and response messages will still be in proto2, since this is what the current v1 API based on.  2. Implement handlers for gRPC calls to delegate them to the original master call handlers.",Task,Major,Open,"2018-11-08 19:07:54","2018-11-08 19:07:54",8
"Apache Mesos","Support v1 Scheduler API over gRPC.","Supporting v1 Scheduler API over gRPC will enable people to write frameworks with more language bindings. The main work includes:   1. Define the Scheduler gRPC service in {{service.proto}}. The proto will be in proto3, but the request and response messages will still be in proto2, since this is what the current v1 API based on.   2. Refactor the master code to support both HTTP and gRPC connections to reuse most of the current code for HTTP Scheduler API.   3. Implement handlers for gRPC calls.",Task,Major,Open,"2018-11-08 19:01:20","2018-11-08 19:01:20",8
"Apache Mesos","Support gRPC server streaming calls in libprocess.","Supporting gRPC server for server streaming calls will enable using Mesos streaming API (such as the scheduler and operator API) over gRPC.",Task,Major,Open,"2018-11-08 18:51:33","2018-11-08 18:51:33",5
"Apache Mesos","Support gRPC server for unary calls in libprocess.","Supporting gRPC server for unary calls will enable using Mesos synchronous API (such as the master and agent API) over gRPC.",Task,Major,Open,"2018-11-08 18:46:38","2018-11-08 18:46:38",5
"Apache Mesos","Design gRPC server and streaming support in libprocess.","Currently libprocess only supports gRPC client for unary gRPC calls through MESOS-7749. To have full gRPC support to enable V1 gRPC API, we have to lay out the design for:   1. gRPC server support for unary gRPC calls.   2. gRPC server streaming support.    Optionally, we could consider support the following:   3. gRPC client streaming support. Only very few API calls use this so not as important as above.   4. gRPC bidirectional streaming support. Mesos API does not use this pattern currently.",Task,Major,Open,"2018-11-08 18:40:44","2018-11-08 18:40:44",8
"Apache Mesos","Unable to build new Mesos CLI with PyInstaller and Python 3.7.","Building the new Mesos CLI with Python 3.7 and PyInstaller 3.3.1 (our current dependencu) on your machine currently creates a binary that is not working:        As seen in https://github.com/pyinstaller/pyinstaller/issues/3219, this is due to PyInstaller.  We need to update the PyInstaller dependency to have a version that supports Python 3.7.",Bug,Major,Resolved,"2018-11-05 15:35:25","2018-11-05 15:35:25",1
"Apache Mesos","Test `HealthCheckTest.HealthyTaskNonShell` can hang.","In {{HealthCheckTest.HealthyTaskNonShell}} the {{statusRunning}} future is incorrectly checked before being waited:  [https://github.com/apache/mesos/blob/d8062f231b9f27889b7cae7a42eef49e4eed79ec/src/tests/health_check_tests.cpp#L673]  As a result, if for some arbitrary reason there is only one task status update sent (e.g., {{TASK_FAILED}}), {{statusRunning->state()}} will make the test hang forever:    (The line number above are not correct because of additional logging I added to triage this error.)",Bug,Major,Resolved,"2018-11-01 03:10:17","2018-11-01 03:10:17",3
"Apache Mesos","Test `CgroupsIsolatorTest.ROOT_CGROUPS_CreateRecursively` is flaky.","This fails with    See the full log attached below.",Bug,Major,Resolved,"2018-10-29 22:49:03","2018-10-29 22:49:03",3
"Apache Mesos","Test `SlaveRecoveryTest.AgentReconfigurationWithRunningTask` is flaky.","The fails with:        It is flaky because after partially accept the first offer, the scheduler will filter the agent for 5 seconds, and depending on the system load that may last more than 15 seconds:        I think we can either remove the expectation of the second offer (which I think is not essential to the test) or manually control the clock.    Full log attached.  ",Bug,Major,Resolved,"2018-10-27 00:34:47","2018-10-26 23:34:47",2
"Apache Mesos","FetcherTest.DuplicateFileURI fails on macos","I see {{FetcherTest.DuplicateFileURI}} fail pretty reliably on macos, e.g., 10.14.  ",Bug,Major,Resolved,"2018-10-25 22:35:25","2018-10-25 21:35:25",1
"Apache Mesos","Make agent atomically checkpoint operations and resources","See https://docs.google.com/document/d/1HxMBCfzU9OZ-5CxmPG3TG9FJjZ_-xDUteLz64GhnBl0/edit for more details.",Task,Major,Resolved,"2018-10-25 17:54:15","2018-10-25 16:54:15",5
"Apache Mesos","Persistence volume does not unmount correctly with wrong artifact URI","DCOS service json file is like following. When you type wrong uri, for example, file://root/test/http.tar.bz2, but the correct one is file:///root/test/http.tar.bz2. Then it will leave all the persistence mount on the agent, and after gc_delay timeout, the mount path is still there.    It means if it failed 10 times, then there is 10 persistence volume mount on the agent.    *Excepted Result: When task is failed,  dangling mount points should be unmounted correctly.*  ",Bug,Critical,Accepted,"2018-10-25 08:27:36","2018-10-25 07:27:36",3
"Apache Mesos","Data in persistent volume deleted accidentally when using Docker container and Persistent volume","Using docker image w/ persistent volume to start a service, it will cause data in persistent volume deleted accidentally when task killed and restarted, also old mount points not unmounted, even the service already deleted.     *The expected result should be data in persistent volume kept until task deleted completely, also dangling mount points should be unmounted correctly.*         *Step 1:* Use below JSON config to create a Mysql server using Docker image and Persistent Volume    *Step 2:* Kill mysqld process to force rescheduling new Mysql task, but found 2 mount points to the same persistent volume, it means old mount point did not be unmounted immediately.    !image-2018-10-24-22-20-51-059.png!    *Step 3:* After GC, data in persistent volume was deleted accidentally, but mysqld (Mesos task) still running    !image-2018-10-24-22-21-13-399.png!    *Step 4:* Delete Mysql service from Marathon, all mount points unable to unmount, even the service already deleted.",Bug,Critical,Open,"2018-10-24 15:26:13","2018-10-24 14:26:13",5
"Apache Mesos","Refactor the _ROOT_DIR logic into FindPackageHelper for CMake.","The unbundled libevent build has a TODO that recommends moving the logic for determining the root-dir of a package into our FindPackageHelper.  See https://github.com/apache/mesos/blob/ba0f78112b766e2a1d6e43c40d7fb2f7569c2580/3rdparty/cmake/FindLIBEVENT.cmake#L20-L40    That logic is pretty neat for e.g. homebrew users as it makes use of the installation prefix information gathered from the {{brew}} tool.    Instead of copy&pasting the same thing over and over into respective {{FindLIBxxx.cmake}} packages, we should move it up into {{FindPackageHelper.cmake}}.  ",Improvement,Minor,Open,"2018-10-24 14:47:03","2018-10-24 13:47:03",2
"Apache Mesos","CLI build step is broken with CMake due to missing file.","The file {{mesos.py}} was not added to {{CLI_FILES}} and this is now an issue when building the CLI using CMake.",Bug,Major,Resolved,"2018-10-24 10:29:38","2018-10-24 09:29:38",1
"Apache Mesos","Add test for `mesos task attach` on task launched without a TTY","As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py",Task,Major,Resolved,"2018-10-22 06:57:19","2018-10-22 05:57:19",3
"Apache Mesos","Add test(s) for `mesos task attach` on task launched with a TTY ","As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py",Task,Major,Resolved,"2018-10-22 06:56:04","2018-10-22 05:56:04",3
"Apache Mesos","Add interactive test(s) for `mesos task exec`","As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py  This will require new helper functions to get the input/output of the command.",Task,Major,Resolved,"2018-10-22 06:54:18","2018-10-22 05:54:18",3
"Apache Mesos","Add non-interactive test(s) for `mesos task exec`","As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py",Task,Major,Resolved,"2018-10-22 06:52:24","2018-10-22 05:52:24",3
"Apache Mesos","Container stuck at ISOLATING state due to libevent poll never returns.","We found UCR container may be stuck at `ISOLATING` state:     In the above logs, the state of container `1e5b8fc3-5c9e-4159-a0b9-3d46595a5b54` was transitioned to `ISOLATING` at 09:13:23, but did not transitioned to any other states until it was destroyed due to the executor registration timeout (10 mins). And the destroy can never complete since it needs to wait for the container to finish isolating.",Bug,Critical,Resolved,"2018-10-19 11:02:10","2018-10-19 10:02:10",5
"Apache Mesos","Document usage and build of new Mesos CLI","Stating how to compile and use the Mesos CLI + its limitations (only Mesos containerizer, exec DEBUG follows task-user).",Task,Major,Resolved,"2018-10-18 18:10:41","2018-10-18 17:10:41",3
"Apache Mesos","Nested container should run as the same user of its parent container by default.","Currently when launching a debug container, by default Mesos agent will use the executor's user as the debug container's user if the `user` field is not specified in the debug container's `commandInfo` (see [this code|https://github.com/apache/mesos/blob/1.7.0/src/slave/http.cpp#L2559] for details). This is OK for the command task since the command executor's user is same with command task's user (see [this code|https://github.com/apache/mesos/blob/1.7.0/src/slave/slave.cpp#L6068:L6070] for details), so the debug container will be launched as the same user of the task. But for the task in a task group, the default executor's user is same with the framework user (see [this code|https://github.com/apache/mesos/blob/1.7.0/src/slave/slave.cpp#L8959] for details), so in this case the debug container will be launched as the same user of the framework rather than the task. So in a scenario that framework user is a normal user but the task user is root, the debug container will be launched as the normal which is not desired, the expectation is the debug container should run as the same user of the container it debugs.",Bug,Major,Resolved,"2018-10-18 09:30:58","2018-10-18 08:30:58",3
"Apache Mesos","Some library functions ignore failures from ::close which should probably be handled.","Multiple functions e.g., in {{stout}} ignore the return value of {{::close}} with the following rationale,    I believe this is incorrect in general. Especially when not calling {{::fsync}} after a write operation, the kernel might buffer writes and only trigger write-related failures when the file descriptor is closed, see e.g., the note on error handling [here|http://man7.org/linux/man-pages/man2/close.2.html#NOTES].    We should audit our code to make sure that failures to {{::close}} file descriptors are properly propagated when needed.",Bug,Major,Resolved,"2018-10-17 21:44:44","2018-10-17 20:44:44",1
"Apache Mesos","Optimize `Resources::filter` operation.","`Resources::filter()` is a heavily used function. Currently it is O(n^2) due to the `add()` operation for each `Resource`:        `add()` is O( n ). This is not necessary. `filter()` operation should only remove `Resource` entries. We should be able to  `push_back`  the resource to the vector without scanning, making the `filter()` O( n ).  ",Improvement,Major,Resolved,"2018-10-17 00:12:27","2018-10-16 23:12:27",1
"Apache Mesos","Resource fragmentation: frameworks may be starved of port resources in the presence of large number roles with quota.","In our environment where there are 1.5k frameworks and quota is heavily utilized, we would experience a severe resource fragmentation issue. Specifically, we observed a large number of port-less offers circulating in the cluster. Thus frameworks that need port resources are not able to launch tasks even if their roles have quota (because currently, we can only set quota for scalar resources, not port range resources).    While most of the 1.5k frameworks do not suppress today and we believe the situation will significantly improve once they do. Still, I think there are some improvements the Mesos allocator can make to help.    h3. How resource becomes fragmented  The origin of these port-less offers stems from quota chopping. Specifically, when chopping an agent to satisfy a role’s quota, we will also hand out resources that this role does not have quota for (as long as it does not break other role’s quota). These “extra resources” certainly includes ALL the remaining port resources on the agent. After this offer, the agent will be left with no port resources even though it still has CPUs and etc. Later, these resources may be offered to other frameworks but they are useless due to no ports. Now we have some “bad offers” in the cluster.    h3. How resource fragmentation prolonged  A resource offer, once it is declined (e.g. due to no ports), is recovered by the allocator and offered to other frameworks again. Before this happens, it is possible that this offer might be able to merge with either the remaining resources or other declined resources on the same agent. However, it is conceivable that not uncommonly, the declined offer will be hand out again *as-is*.  This is especially probable if the allocator makes offers faster than the framework offer response time. As a result, we will observe the circulation of bad offers across different frameworks. These bad offers will exist for a long time before being consolidated again. For how long? *The longevity of the bad offer will be roughly proportional to the number of active frameworks*. In the worse case, once all the active frameworks have (hopefully long) declined the bad offer, the bad offer will have nowhere to go and finally start to merge with other resources on that agent.    Note, since the allocator performance has greatly improved in the past several months. The scenario described here could be increasingly common. Also, as we introduce quota limits and hierarchical quota, there will be much more agent chopping, making resource fragmentation even worse.    h3. Near-term Mitigations  As mentioned above, the longevity of a bad offer is proportional to the active frameworks. Thus framework suppression will certainly help. In addition, from the Mesos side, a couple of mitigation measures are worth considering (other than the long-term optimistic allocation strategy):    1. Adding a defragment interval once in a while in the allocator. For example, each minute or a dozen allocation cycles or so, we will pause the allocation, rescind all the offers and start allocating again. This essentially eliminates all the circulating bad offers by giving them a chance to be consolidated. Think of this as a periodic “reboot” of the allocator.  2. Consider chopping non-quota resources as well. Right now, for resources such as ports (or any other resources that the role does not have quota for), all are allocated in a single offer. We could choose to chop these non-quota resources as well. For example, port resources can be distributed proportionally to allocated CPU resources.  3. Provide support for specifying port quantities. With this, we can utilize the existing quota or `min_allocatable_resources` APIs to guarantee a certain number of port resources.",Bug,Major,Resolved,"2018-10-16 22:52:36","2018-10-16 21:52:36",3
"Apache Mesos","Add an optional `vendor` field in `Resource.DiskInfo.Source`.","This will allow the framework to recover volumes reported by the corresponding CSI plugin across agent ID changes.    When an agent changes its ID, all reservation information related to resources coming from a given resource provider will be lost, so frameworks needs an unique identifier to identify if a new volume associated with the new agent ID is the same volume. Since CSI volume ID are not unique across different plugins, we will need to add a new {{vendor}} field, which together with the existing {{id}} field can provide the means to globally uniquely identify this source.",Improvement,Critical,Resolved,"2018-10-16 05:40:51","2018-10-16 04:40:51",5
"Apache Mesos","UCR container launch stuck at PROVISIONING during image fetching.","We observed mesos containerizer stuck at PROVISIONING when launching a mesos container using docker image: `kvish/jenkins-dev:595c74f713f609fd1d3b05a40d35113fc03227c9`:    The image pulling never finishes. Insufficient image contents are still in image store staging directory /var/lib/mesos/slave/store/docker/staging/egLYqO, forever.      It is not clear yet why the SHA pulling does not finish, so we use the same image on another empty machine with UCR. The other machine has the container RUNNING correctly, and has the following staging directory before moving to the layers dir:      By comparing two cases, we can see one layer `8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324` is missing on the problematic agent node, and it is the last layer to fetch.    Here is the manifest as a reference:      This should not be related: when we try to find the extracted layers on the layers dir, we could only find two:      These two are base layers that were downloaded earlier from other images. We still need to figure out why there is one layer fetch not finished. (no curl process and tar process running stuck at background)",Bug,Major,Resolved,"2018-10-16 02:10:30","2018-10-16 01:10:30",5
"Apache Mesos","Consider providing better operation status updates while an RP is recovering","Consider the following scenario:    1. A framework accepts an offer with an operation affecting SLRP resources.  2. The master forwards it to the corresponding agent.  3. The agent forwards it to the corresponding RP.  4. The agent and the master fail over.  5. The master recovers.  6. The agent recovers while the RP is still recovering, so it doesn't include the pending operation on the {{RegisterMessage}}.  7. A framework performs an explicit operation status reconciliation.    In this case the master will currently respond with {{OPERATION_UNKNOWN}}, but it should be possible to respond with a more fine-grained and useful state, such as {{OPERATION_RECOVERING}}.",Task,Major,Resolved,"2018-10-15 21:38:53","2018-10-15 20:38:53",5
"Apache Mesos","Some master endpoints do not handle failed authorization properly.","When we authorize _some_ actions (right now I see this happening to create / destroy volumes, reserve / unreserve resources) *and* {{authorizer}} fails (i.e. returns the future in non-ready state), an assertion is triggered:    This is due to incorrect assumption in our code, see for example [https://github.com/apache/mesos/blob/a063afce9868dcee38a0ab7efaa028244f3999cf/src/master/master.cpp#L3752-L3763]:    Futures returned from {{await}} are guaranteed to be in terminal state, but not necessarily ready! In the snippet above, {{!authorization.get()}} is invoked without being checked ⇒ assertion fails.    Full stack trace:  ",Bug,Blocker,Resolved,"2018-10-15 14:03:48","2018-10-15 13:03:48",5
"Apache Mesos","Adding support for implicit allocation of mandatory custom resources in Mesos","I sent a an email (attached) few days ago to propose the introduction of a new hook to append resources implicitly to tasks for mandatory resources. This would allow Mesos to support mandatory resources like network bandwidth or disk IO for instance.     In a nutshell, we propose to add a hook with the following signature    and call it in the master in the ACCEPT message handler.",Improvement,Minor,Resolved,"2018-10-15 10:00:55","2018-10-15 09:00:55",5
"Apache Mesos","Consider introducing a ScalarResourceQuantity protobuf message.","As part of introducing quota limits, we're adding a new master::Call for updating quota. This call can take a simplified message that expresses scalar resource quantities:        This greatly simplified the validation code, as well as the UX of the API when it comes to knowing what kind of data to provide.    Ideally, the new quota paths can use this message in lieu of Resource objects, but we'll have to explore backwards compatibility (e.g. registry data).",Improvement,Major,Resolved,"2018-10-13 00:39:22","2018-10-12 23:39:22",5
"Apache Mesos","URI disk profile adaptor could deadlock.","The loop here can be infinit:  https://github.com/apache/mesos/blob/1.7.0/src/resource_provider/storage/uri_disk_profile_adaptor.cpp#L61-L80    ",Bug,Critical,Resolved,"2018-10-11 06:15:41","2018-10-11 05:15:41",1
"Apache Mesos","Mesos containerizer can get stuck during cgroup cleanup","I observed a task group's executor container which failed to be completely destroyed after its associated tasks were killed. The following is an excerpt from the agent log which is filtered to include only lines with the container ID, {{d463b9fe-970d-4077-bab9-558464889a9e}}:    The last log line from the containerizer's destroy path is:    (that is the second such log line, from {{LinuxLauncherProcess::_destroy}})  Then we just see    repeatedly, which occurs because the agent's {{GET_CONTAINERS}} call is being polled once per minute. This seems to indicate that the container in question is still in the agent's {{containers_}} map.    So, it seems that the containerizer is stuck either in the Linux launcher's {{destroy()}} code path, or the containerizer's {{destroy()}} code path.",Bug,Critical,Resolved,"2018-10-10 21:19:17","2018-10-10 20:19:17",5
"Apache Mesos","Create cgoup recursively to workaround systemd deleting cgroups_root.","This is my case:    My cgroups_root of mesos-slave is some_user/mesos under /sys/fs/cgroup。    It happens that this some_user dir may be gone for some unknown reason, in which case I can no longer create any cgroup and any task will fail.    So I would like to change          to    in CgroupsIsolatorProcess::prepare in src/slave/containerizer/mesos/isolators/cgroups/cgroups.cpp.    However, I'm not sure if there's any potential problem doing so. Any advice?     ",Improvement,Critical,Resolved,"2018-10-10 03:55:12","2018-10-10 02:55:12",3
"Apache Mesos","Mesos fails to build on Fedora 28","Trying to compile a fresh Mesos checkout on a Fedora 28 system with the following configuration flags:    and the following compiler    fails the build due to two warnings (even though --disable-werror was passed):  ",Bug,Major,Resolved,"2018-10-09 13:57:51","2018-10-09 12:57:51",2
"Apache Mesos","Master's /state-summary and /state spends a lot of time coalescing ranges.","In a scale test, the /state-summary endpoint was actually costing as much as the /state endpoint. I'm not sure how the request rate compared, but I noticed that /state-summary spends a lot of time coalescing ranges, which seems unnecessary.    The coalescing is also present in the /state endpoint perf data. We have several places where we track non-scalar resources across slaves, which is incorrect (see below). Adding these and printing them out can be pretty expensive.    E.g. see the total tracking in the master's framework struct:  https://github.com/apache/mesos/blob/1.7.0/src/master/master.hpp#L2475-L2498      This ticket tracks the work required to investigate this performance issue and propose some solutions to the problem.",Improvement,Major,Accepted,"2018-10-05 22:01:31","2018-10-05 21:01:31",5
"Apache Mesos","Nested container launch could fail if the agent upgrade with new cgroup subsystems.","Nested container launch could fail if the agent upgrade with new cgroup subsystems, because the new cgroup subsystems do not exist on parent container's cgroup hierarchy.",Bug,Major,Resolved,"2018-10-05 01:35:50","2018-10-05 00:35:50",5
"Apache Mesos","Document which fields are ignored during framework reregistration","When a framework reregisters, some fields in the {{FrameworkInfo}} are ignored, and the master simply uses whatever values were provided during the initial registration. We should document this behavior.",Documentation,Major,Open,"2018-10-05 00:55:14","2018-10-04 23:55:14",2
"Apache Mesos","If a framework looses operation information it cannot reconcile to acknowledge updates.","Normally, frameworks are expected to checkpoint agent ID and resource provider ID before accepting an offer with an OfferOperation. From this expectation comes the requirement in the v1 scheduler API that a framework must provide the agent ID and resource provider ID when acknowledging an offer operation status update. However, this expectation breaks down:    1. the framework might lose its checkpointed data; it no longer remembers the agent ID or the resource provider ID    2. even if the framework checkpoints data, it could be sent a stale update: maybe the original ACK it sent to Mesos was lost, and it needs to re-ACK. If a framework deleted its checkpointed data after sending the ACK (that's dropped) then upon replay of the status update it no longer has the agent ID or resource provider ID for the operation.    An easy remedy would be to add the agent ID and resource provider ID to the OperationStatus message received by the scheduler so that a framework can build a proper ACK for the update, even if it doesn't have access to its previously checkpointed information.    I'm filing this as a BUG because there's no way to reliably use the offer operation status API until this has been fixed.",Bug,Major,Resolved,"2018-10-04 18:56:02","2018-10-04 17:56:02",3
"Apache Mesos","Rejected quotas request error messages should specify which resources were overcommitted.","If we reject a quota request due to not having enough available resources, we fail with the following error:      but we don't print *which* resource was not available. This can be confusing to operators when the quota was attempted to be set for multiple resources at once.",Improvement,Major,Resolved,"2018-10-04 16:23:01","2018-10-04 15:23:01",1
"Apache Mesos","Allow operators to limit the number of Docker containers on a host","Since we have observed performance issues on machines where a large number (hundreds) of Docker containers are running simultaneously, we should consider adding a Mesos configuration flag to allow operators to limit the number of Docker containers that can be launched on a single host.",Improvement,Major,Open,"2018-10-02 23:19:08","2018-10-02 22:19:08",3
"Apache Mesos","Docker containerizer actor can get backlogged with large number of containers.","We observed during some scale testing that we do internally.    When launching 300+ Docker containers on a single agent box, it's possible that the Docker containerizer actor gets backlogged. As a result, API processing like `GET_CONTAINERS` will become unresponsive. It'll also block Mesos containerizer from launching containers if one specified `--containers=docker,mesos` because Docker containerizer launch will be invoked first by the composing containerizer (and queued).    Profiling results show that the bottleneck is `os::killtree`, which will be invoked when the Docker commands are discarded (e.g., client disconnect, etc.).    For this particular case, killtree is not really necessary because the docker command does not fork additional subprocesses. If we use the argv version of `subprocess` to launch docker commands, we can simply use os::kill instead. We confirmed that, by switching to os::kill, the performance issues goes away, and the agent can easily scale up to 300+ containers.",Bug,Blocker,Resolved,"2018-10-02 06:51:59","2018-10-02 05:51:59",3
"Apache Mesos","SLRP gets a stale checkpoint after system crash.","SLRP checkpoints a pending operations before issuing the corresponding CSI call through {{slave::state::checkpoint}}, which writes a new checkpoint to a temporary file then do a {{rename}}. However, because we don't do any {{fsync}}, {{rename}} is not atomic w.r.t. system crash. As a result, if the operation is processed during a system crash, it is possible that the CSI call has been executed, but the SLRP gets back a stale checkpoint after reboot and totally doesn't know about the operation.    To address this problem, we need to ensure the followings before issuing the CSI call:   1. The temp file is synced to the disk.   2. The rename is committed to the disk.    A possible solution is to do an {{fsync}} after writing the temp file, and do another {{fsync}} on the checkpoint dir after the {{rename}}.",Bug,Blocker,Resolved,"2018-10-01 23:05:42","2018-10-01 22:05:42",5
"Apache Mesos","Add an operation status update manager to the agent","Review here: https://reviews.apache.org/r/69505/",Task,Major,Resolved,"2018-09-28 23:27:48","2018-09-28 22:27:48",3
"Apache Mesos","Allow optional `profile` to be specified in `CREATE_DISK` offer operation.","This will allow the framework to import pre-existing volumes reported by the corresponding CSI plugin.    For instance, the LVM CSI plugin might detect some pre-existing volumes that Dan has created out of band. Currently, those volumes will be represented as RAW disk resource with a volume ID, but no volume profile by the SLRP. When a framework tries to use the RAW volume as either MOUNT or BLOCK volume, it'll issue a CREATE_DISK operation. The corresponding SLRP will handles the operation, and validate against a default profile for MOUNT volumes. However, this prevents the volume to have a different profile that the framework might want.    Ideally, we should allow the framework to optionally specify a profile that it wants the volume to have during CREATE_DISK because it might have some expectations on the volume. The SLRP will validate with the corresponding CSI plugin using the ValidateVolumeCapabilities RPC call to see if the profile is applicable to the volume.",Improvement,Blocker,Resolved,"2018-09-27 20:17:54","2018-09-27 19:17:54",8
"Apache Mesos","v1 JAVA scheduler library can drop TEARDOWN upon destruction.","Currently the v1 JAVA scheduler library neither ensures {{Call}} s are sent to the master nor waits for responses. This can be problematic if the library is destroyed (or garbage collected) right after sending a {{TEARDOWN}} call: destruction of the underlying {{Mesos}} actor races with sending the call.    ",Bug,Major,Resolved,"2018-09-27 15:22:39","2018-09-27 14:22:39",3
"Apache Mesos","Get rid of dependency on `net-tools` in network/cni isolator.","The `network/cni` isolator has a dependency on `net-tools`. The last release of `net-tools` was released in 2001. The tools were deprecated many years ago (see [Debian|https://lists.debian.org/debian-devel/2009/03/msg00780.html], [RH|https://bugzilla.redhat.com/show_bug.cgi?id=687920], and [LWN|https://lwn.net/Articles/710533/]) and no longer installed by default.    [https://github.com/apache/mesos/blob/983607e/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L2248]",Task,Minor,Resolved,"2018-09-27 01:00:21","2018-09-27 00:00:21",3
"Apache Mesos","Whenever our packaging tasks trigger errors we run into permission problems.","As shown in MESOS-9238, failures within our packaging cause permission failures on cleanup.        We should clean that up.",Bug,Minor,Resolved,"2018-09-26 15:01:01","2018-09-26 14:01:01",2
"Apache Mesos","Analyse and pinpoint libprocess SSL failures when using libevent 2.1.8.","Mesos SSL based on libevent >2.1.5beta fails to function properly. Depending on the underlying open SSL version, failures happen on accept or on receive.  The issue has been properly described https://issues.apache.org/jira/browse/MESOS-7076. We landed a workaround by bundling libevent 2.0.22. This ticket is meant to track further analysis of the true reason for the issue - so we fix it, instead of relying on a hard to maintain bandaid. ",Task,Major,Resolved,"2018-09-26 14:34:58","2018-09-26 13:34:58",8
"Apache Mesos","Prevent subscribers to the master's event stream from leaking connections","Some reverse proxies (e.g., ELB using an HTTP listener) won't close the upstream connection to Mesos when they detect that their client is disconnected.    This can make Mesos leak subscribers, which generates unnecessary authorization requests and affects performance.    We should evaluate methods (e.g., heartbeats) to enable Mesos to detect that a subscriber is gone, even if the TCP connection is still open.  ",Improvement,Critical,Resolved,"2018-09-26 01:13:40","2018-09-26 00:13:40",5
"Apache Mesos","Make SLRP be able to update its volumes and storage pools.","We should consider making SLRP update its resources periodically, or adding an endpoint to trigger that, for the following reasons:    1. Mesos currently assumes all profiles have disjoint storage pools. This is because Mesos models each resource independently. However, in practice an operator can set up, say two profiles, one for linear volumes and one for raid volumes, and an LVM resource provider that can provision both linear and raid volumes. The correlation between the storage pools of the linear and raid profiles would reduce one's pool capacity when a volume of the other type is provisioned. To reflect the actual sizes of correlated storage pools, we need a way to make SLRP update its resources.    2. The SLRP now only queries the CSI plugin to report a list of volumes during startup, so if a new device is added, the operator will have to restart the agent to trigger another SLRP startup, which is inconvenient.",Improvement,Critical,Resolved,"2018-09-25 01:00:02","2018-09-25 00:00:02",5
"Apache Mesos","Introduce a copy-on-write type to enforce safe mutation with exclusive ownership.","Resource copy-on-write optimization has been landed (MESOS-6765). But the no-mutation-without-exclusive-ownership rule is not enforced. This is error-prone as ownership needs to be manually checked every time:    {code:c++}      if (resource_.use_count() > 1) {        resource_ = make_shared<Resource_>(*resource_);      }  {code}    It will be great to introduce a templated COW wrapper class that enforces such semantic, such construct could also be reused in other places.",Improvement,Major,Open,"2018-09-21 23:43:43","2018-09-21 22:43:43",5
"Apache Mesos","MasterAPITest.EventAuthorizationFiltering is flaky","Saw this failure on a CentOS 6 SSL build in our internal CI. Build log attached. For some reason, it seems that the initial {{TASK_ADDED}} event is missed:  ",Bug,Minor,Open,"2018-09-21 21:51:03","2018-09-21 20:51:03",3
"Apache Mesos","XXXX_HTTP_CALL macros do not work, tests are disabled","Currently, all tests which make use of the {{FUTURE_HTTP_CALL}} and {{DROP_HTTP_CALL}} macros are disabled. It seems that these macros have not been updated to accommodate other changes. These macros should be debugged and the relevant tests re-enabled.    For example, see {{SlaveRecoveryTest, DISABLED_ReconnectHTTPExecutor}} in {{src/tests/slave_recovery_tests.cpp}}.",Improvement,Major,Accepted,"2018-09-18 19:54:24","2018-09-18 18:54:24",8
"Apache Mesos","Delimiters in endpoint names are inconsistent across mesos components.","At the moment endpoints in Mesos components have both {{-}} and {{_}} as delimiters:    This is inconsistency for no good reason.",Improvement,Minor,Open,"2018-09-18 15:21:51","2018-09-18 14:21:51",3
"Apache Mesos","`docker inspect` may return an unexpected result to Docker executor due to a race condition.","In the Docker container (`src/docker/executor`), we call `docker inspect` right after `docker run` ([https://github.com/apache/mesos/blob/1.6.0/src/docker/executor.cpp#L230:L242),] there is a small chance for `docker inspect` to return an unexpected result which does not contain the Docker container ID, so we will see an error like below:    If that happens, Docker executor will not send `TASK_RUNNING` status update, so the task will be stuck at `TASK_STARTING`.",Bug,Major,Resolved,"2018-09-14 00:35:32","2018-09-13 23:35:32",3
"Apache Mesos","Install Python3 on ubuntu-16.04-arm docker image","With the upgrade to Python 3 in the Mesos codebase builds which rely on docker images started to fail since they were missing a `python3` installation.    We fixed those issues for most of the Docker images in https://issues.apache.org/jira/browse/MESOS-8957. We still miss Python 3 on the Ubuntu-16.04-arm image which can be found in `support/mesos-build/ubuntu-16.04-arm`.",Task,Minor,Resolved,"2018-09-13 13:05:19","2018-09-13 12:05:19",2
"Apache Mesos","SLRP does not clean up plugin containers after it is removed.",,Bug,Blocker,Resolved,"2018-09-12 22:00:49","2018-09-12 21:00:49",5
"Apache Mesos","Github's mesos/modules does not build.","The examples modules repo at GitHub.com named mesos/modules does currently not build against the latest Apache Mesos. We should update that system. ",Bug,Major,Resolved,"2018-09-10 15:51:21","2018-09-10 14:51:21",2
"Apache Mesos","De-duplicate read-only requests to master based on principal.","Identical read-only requests can be batched and answered together. With batching available (MESOS-9158), we can now deduplicate requests based on principal.",Improvement,Major,Resolved,"2018-09-10 14:46:18","2018-09-10 13:46:18",5
"Apache Mesos","Storage local provider does not sufficiently handle container launch failures or errors","The storage local resource provider as currently implemented does not handle launch failures or task errors of its standalone containers well enough, If e.g., a RP container fails to come up during node start a warning would be logged, but an operator still needs to detect degraded functionality, manually check the state of containers with {{GET_CONTAINERS}}, and decide whether the agent needs restarting; I suspect they do not have always have enough context for this decision. It would be better if the provider would either enforce a restart by failing over the whole agent, or by retrying the operation (optionally: up to some maximum amount of retries).",Improvement,Critical,Resolved,"2018-09-10 12:10:21","2018-09-10 11:10:21",3
"Apache Mesos","Linking libevent should be avoided.","{{libevent}} is a combination of {{libevent_core}} and {{libevent_extra}}. Mesos/libprocess does not make use of any {{libevent_extra}} functionality and additionally it appears that its use is against best practices.    From http://www.wangafu.net/~nickm/libevent-book/Ref0_meta.html:      We should adapt our linkage accordingly.",Bug,Minor,Resolved,"2018-09-09 18:01:17","2018-09-09 17:01:17",1
"Apache Mesos","If some image layers are large, the image pulling may stuck due to the authorized token expired.","The image layer blobs pulling happen asynchronously but in the same libprocess process. There is a chance that one layer get the token then the thread switch to another layer curling which may take long. When the original layer curling resumes, the token already expired (e.g., after 60 seconds).        The impact is the task launch stuck and all subsequent task using this image would also stuck because it waits for the same image pulling future to become ready.    Please note that this issue is not likely to be reproduced, unless on a busy system using images containing large layers.",Bug,Major,Resolved,"2018-09-08 02:43:30","2018-09-08 01:43:30",5
"Apache Mesos","LongLivedDefaultExecutorRestart is flaky.",,Bug,Major,Resolved,"2018-09-07 11:02:47","2018-09-07 10:02:47",2
"Apache Mesos","Avoid double copying of master->framework messages when incrementing metrics.","When incrementing metrics, we currently do stuff like    which is not efficient. We should update such callsites to avoid gratuitous conversions which could degrade performance when many events are being sent.",Improvement,Major,Resolved,"2018-09-06 21:34:15","2018-09-06 20:34:15",1
"Apache Mesos","Mesos v1 scheduler library does not properly handle SUBSCRIBE retries","After the authentication related refactor done as part of [https://reviews.apache.org/r/62594/], the state of the scheduler is checked in `send` ([https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L234)]  but it is changed in `_send` ([https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L234).] As a result, we can have 2 SUBSCRIBE calls in flight at the same time on the same connection! This is not good and not spec compliant of a HTTP client that is expecting a streaming response.    We need to fix the library to either drop the retried SUBSCRIBE call if one is in progress (as it was before the refactor) or close the old connection and start a new connection to send the retried SUBSCRIBE call.               ",Bug,Major,Resolved,"2018-09-05 22:17:24","2018-09-05 21:17:24",3
"Apache Mesos","Add docs for UPDATE_OPERATION_STATUS event","We need to add the {{UPDATE_OPERATION_STATUS}} event to the docs for the v1 scheduler API event stream.",Documentation,Critical,Resolved,"2018-08-31 22:23:33","2018-08-31 21:23:33",1
"Apache Mesos","Removing rootfs mounts may fail with EBUSY.","We observed in production environment that this      Consider fixing the issue by using detach unmount when unmounting container rootfs. See MESOS-3349 for details.    The root cause on why Device or resource busy is received when doing rootfs unmount is still unknown.    _UPDATE_: The production environment has a cronjob that scan filesystems to build index (updatedb for mlocate). This can explain the EBUSY we receive when doing `unmount`.    _UPDATE_: Splunk that's scanning `/var/lib/mesos` could also be a source of triggers.",Bug,Blocker,Resolved,"2018-08-30 23:36:51","2018-08-30 22:36:51",5
"Apache Mesos","Extend request batching to '/roles' endpoint","For consistency and improved performance under load, the `/roles` endpoint should use the same request batching mechanism as `/state`, '/tasks`, ...",Improvement,Major,Resolved,"2018-08-30 12:31:45","2018-08-30 11:31:45",3
"Apache Mesos","Mesos build fail with Clang 3.5.","1. The `-Wno-inconsistent-missing-override` option added in https://reviews.apache.org/r/67953/      is not recognized by clang 3.5.  2. The same issue described in https://reviews.apache.org/r/55400/ would make      `src/resource_provider/storage/provider.cpp` fail to compile.",Bug,Blocker,Resolved,"2018-08-30 05:36:34","2018-08-30 04:36:34",3
"Apache Mesos","Docker command executor may stuck at infinite unkillable loop.","Due to the change from https://issues.apache.org/jira/browse/MESOS-8574, the behavior of docker command executor to discard the future of docker stop was changed. If there is a new killTask() invoked and there is an existing docker stop in pending state, the old one would call discard and then execute the new one. This is ok for most of cases.    However, docker stop could take long (depends on grace period and whether the application could handle SIGTERM). If the framework retry killTask more frequently than grace period (depends on killpolicy API, env var, or agent flags), then the executor may be stuck forever with unkillable tasks. Because everytime before the docker stop finishes, the future of docker stop is discarded by the new incoming killTask.    We should consider re-use grace period before calling discard() to a pending docker stop future.",Bug,Major,Accepted,"2018-08-29 08:05:42","2018-08-29 07:05:42",3
"Apache Mesos","Test `StorageLocalResourceProviderTest.ROOT_CreateDestroyDiskRecovery` is flaky.","The test is flaky in 1.7.x:    This is because of `DESTRY_DISK` races with a profile poll. If the poll finishes first, SLRP will start reconciling storage pools, and drop certain incoming operations during reconciliation.",Bug,Major,Resolved,"2018-08-29 02:11:27","2018-08-29 01:11:27",2
"Apache Mesos","Expose more network statistics for containers on CNI network in the `network/cni` isolator.","This is a second step after MESOS-5647. We need to expose more networking metrics for containers on CNI network in the `network/cni` isolator. At the moment `usage()` method calls `getifaddrs(3)` to get limited statistics such as received/transmitted bytes, packets, errors and dropped packets. It works on any modern linux-based system. `usage()` method could use Netlink Protocol Library to get more metrics and expose it into `ResourceStatistics`. Please create the netlink socket in the corresponding network namespace through the `NamespaceRunner` and re-use the socket in `usage()`.",Task,Major,Open,"2018-08-28 21:52:11","2018-08-28 20:52:11",5
"Apache Mesos","Add allocator benchmark to allow multiple framework/agent profiles.","We want to add some test harness that allows us to test allocator performance when run with multiple agent and framework profiles.",Bug,Major,Resolved,"2018-08-28 20:44:23","2018-08-28 19:44:23",5
"Apache Mesos","Failed to build Mesos with Python 3.7 and new CLI enabled","I've tried to build Mesos with the flag 'enable-new-cli' and Python 3.7 and it failed with this error message:  ",Bug,Minor,Resolved,"2018-08-27 17:17:22","2018-08-27 16:17:22",2
"Apache Mesos","An attempt to remove or destroy container in composing containerizer leads to segfault.","`LAUNCH_NESTED_CONTAINER` and `LAUNCH_NESTED_CONTAINER_SESSION` leads to segfault in the agent when the parent container is unknown to the composing containerizer. If the parent container cannot be found during an attempt to launch a nested container via `ComposingContainerizerProcess::launch()`, the composing container returns an error without cleaning up the container. On `launch()` failures, the agent calls `destroy()` which accesses uninitialized `containerizer` field.",Bug,Major,Resolved,"2018-08-27 16:11:53","2018-08-27 15:11:53",3
"Apache Mesos","Improve `class Slave` in the allocator.","Currently, there are several issues with Slave class in the allocator:    (1) Resources on an agent are characterized by two variables: total and allocated. However, these two related fields are currently mutated separately by different member functions, leading to temporary inconsistencies. This is fragile and has produced several odd logic flows.    (2) While we track aggregated allocated resources on the agent, we do not know which frameworks those resources are allocated to. This lack of information makes several things difficult. For example, the odd agent removal logic described in MESOS-621. And also, currently, we can not update the framework sorter by simply looking at the Slave class. This leads to convoluted update/tracking (un)allocated resources logic.",Improvement,Major,Open,"2018-08-24 23:51:05","2018-08-24 22:51:05",5
"Apache Mesos","./support/python3/mesos-gtest-runner.py --help crashes",,Bug,Minor,Resolved,"2018-08-23 09:13:45","2018-08-23 08:13:45",1
"Apache Mesos","Mesos master segfaults when responding to /state requests.",,Bug,Blocker,Resolved,"2018-08-22 15:34:22","2018-08-22 14:34:22",3
"Apache Mesos","Zookeeper doesn't compile with newer gcc due to format error",RR: https://reviews.apache.org/r/68370/,Bug,Major,Resolved,"2018-08-21 13:51:51","2018-08-21 12:51:51",2
"Apache Mesos","Ignore pre-existing CSI volumes known to SLRP.","A pre-existing CSI volume can be known to an SLRP if there is a CSI volume state checkpoint for the volume, but the corresponding resource checkpoint is missing. This typically happen when the agent ID changes which would make the SLRP lose all its resource provider state. In such a case, we should not treat these volumes as unmanaged pre-existing volumes (i.e., disk resources without profiles). For now we should not report them, and consider designing a way to recover these volumes.",Improvement,Major,Resolved,"2018-08-17 22:04:10","2018-08-17 21:04:10",1
"Apache Mesos","Subprocess should unset CLOEXEC on whitelisted file descriptors.","The libprocess subprocess API accepts a set of whitelisted file descriptors that are supposed to  be inherited to the child process. On windows, these are used, but otherwise the subprocess API just ignores them. We probably should make sure that the API clears the {{CLOEXEC}} flag on this descriptors so that they are inherited to the child.",Bug,Major,Resolved,"2018-08-17 19:01:10","2018-08-17 18:01:10",3
"Apache Mesos","`UriDiskProfileAdaptor` should not update profiles when a poll returns a non-OK HTTP status.","Currently if the {{UriDiskProfileAdatpor}} receives an non-OK status, e.g., {{404 Not Found}}, from a URL poll, it would still read the response body (which could be empty or malformed) and update the profile matrix. The expected behavior should be skip this poll and retry later.",Bug,Major,Resolved,"2018-08-17 18:16:26","2018-08-17 17:16:26",1
"Apache Mesos","Unkillable pod container stuck in ISOLATING","We have a simple test that launches a pod with two containers (one writes in a file and the other reads it). This test is flaky because the container sometimes fails to start.  Marathon app definition:        During the test, Marathon tries to launch the pod but doesn't receive a {{TASK_RUNNING}} for the first container and so after 2min decides to kill the pod which also fails.     Agent sandbox (attached to this ticket minus docker layers, since they're too big to attach) shows that one of the containers wasn't started properly - the last line in the agent log says:    Until then the log looks pretty unspektakular.     Afterwards, Marathon tries to kill the container repeatedly, but doesn't succeed - the executor receives the reuests but doesn't send anything back:      Relevant Ids for grepping the logs:  ",Bug,Major,Resolved,"2018-08-17 14:11:45","2018-08-17 13:11:45",5
"Apache Mesos","Failed to compile gRPC when the build path contains symlinks.","  Apparently, in GRPC makefile, it uses realpath (no symlinks) when computing the build directory, whereas, Mesos builds use `abspath` (doesn't resolve symlinks). So there is a target mismatch if any directory in your Mesos path is a symlink.",Bug,Major,Resolved,"2018-08-16 23:05:41","2018-08-16 22:05:41",1
"Apache Mesos","Parallel serving of state-related read-only requests in the Master.","Similar to MESOS-9122, make all read-only master state endpoints batched.",Improvement,Major,Resolved,"2018-08-16 17:20:16","2018-08-16 16:20:16",5
"Apache Mesos","StorageLocalResourceProviderProcess can deadlock","On fatal conditions the {{StorageLocalResourceProviderProcess}} triggers its {{fatal}} function which causes its {{Driver}} process to be torn down. Invocations of {{fatal}} need to be properly {{defer}}'d and must never execute on the {{Driver}} process.    We saw an invocation of {{fatal}} deadlock in our internal CI since its invocation in {{StorageLocalResourceProviderProcess::sendResourceProviderStateUpdate}} wasn't explicitly {{defer}}'d, and by accident was executing on the {{Driver}}'s process.",Bug,Major,Resolved,"2018-08-15 13:35:11","2018-08-15 12:35:11",1
"Apache Mesos","MasterTest.TaskStateMetrics is flaky","Observed on Ubuntu 16.04, cmake build:      Full log attached.",Bug,Major,Resolved,"2018-08-14 17:05:42","2018-08-14 16:05:42",1
"Apache Mesos","Close all file descriptors except whitelist_fds in posix/subprocess.","Close all file descriptors except whitelist_fds in posix/subprocess (currently whitelist_fds are not honored yet). This would avoid the fd being leaked. Please follow the steps from this commit to make corresponding change:   https://issues.apache.org/jira/browse/MESOS-8917?focusedCommentId=16522629&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16522629",Bug,Major,Resolved,"2018-08-10 23:45:56","2018-08-10 22:45:56",5
"Apache Mesos","Container stuck at ISOLATING due to FD leak","When containers are launching on a single agent at scale, one container stuck at ISOLATING could occasionally happen. And this container becomes un-destroyable due to containerizer destroy always wait for isolate() finish to continue.    We add more logging to debug this issue:     which shows that the await() in CNI::attach() stuck at the second future (io::read() for stdout).    By looking at the df of this stdout:      We found              pipe 275230 is held by the agent process and the sleep process at the same time!    The reason why the leak is possible is because we don't use `pipe2` to create a pipe with `O_CLOEXEC` in subprocess:  https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess_posix.cpp#L61    Although we do set cloexec on those fds later:  https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess.cpp#L366-L373    There is a race where a fork happens after `pipe()` call, but before cloexec is called later. This is more likely on a busy system (this explains why it's not hard to repo the issue when launching a lot of containers on a single box).",Bug,Critical,Resolved,"2018-08-10 23:41:18","2018-08-10 22:41:18",8
"Apache Mesos","Failed to build gRPC on Linux without OpenSSL.","Building Mesos on Ubuntu 16.04 without SSL headers installed yields the following message:  ",Bug,Blocker,Resolved,"2018-08-10 22:34:47","2018-08-10 21:34:47",2
"Apache Mesos","Agent and scheduler driver authentication retry backoff time could overflow.","In the agent we have the following retry backoff calculation logic:    https://github.com/apache/mesos/blob/874c752316b14055c0a5a7b67f97ccf912abcc3c/src/slave/slave.cpp#L1401-L1418    {code:c++}      Duration backoff =        flags.authentication_backoff_factor * std::pow(2, failedAuthentications);  {code}    Since the `Duration` uses `int64_t` to hold nanosecond, if we set the `authentication_backoff_factor` to 1 second, we will overflow after 34 failed authentications (from second to nanosecond we lose 30 bits and 2^34 in the `pow()`).    The effect is we do not backoff at all, we will just retry immediately after the 5s timeout:  https://github.com/apache/mesos/blob/874c752316b14055c0a5a7b67f97ccf912abcc3c/src/master/master.cpp#L9615-L9619    The scheduler driver also has the same issue.    We should also audit all the other backoff logic.",Bug,Blocker,Resolved,"2018-08-10 01:44:35","2018-08-10 00:44:35",3
"Apache Mesos","Agent has a fragile burn-in 5s authentication timeout.","Agent has a fragile burn-in 5s authentication timeout. This also includes the trip to master and queueing delay on the master message queue. We should either increase this or make it configurable.",Bug,Blocker,Resolved,"2018-08-09 19:42:47","2018-08-09 18:42:47",2
"Apache Mesos","Master authentication handling leads to request amplification.","In `Master::authenticate(const UPID& from, const UPID& pid)`, we have:    https://github.com/apache/mesos/blob/master/src/master/master.cpp#L9594    {code:c++}    if (authenticating.contains(pid)) {      LOG(INFO) << Queuing up authentication request from  << pid                <<  because authentication is still in progress;        // Try to cancel the in progress authentication by discarding the      // future.      authenticating[pid].discard();        // Retry after the current authenticator session finishes.      authenticating[pid]        .onAny(defer(self(), &Self::authenticate, from, pid));        return;    }  {code}    Let's say the master is processing authentication request R1 (whose associating future is held in `authenticating`). Now it receives another request R2 from the same client (due to e.g. re-try), according to the above code logic, we will (1) discard R1; (2) enqueue R2 as a callback which will be triggered when R1 is discarded, and we will redo `:: authenticate` with R2.    Here the master assumes that R2 is the most current request. This is true in the above example. However, this assumption could easily break when auth requests come faster than they are discarded. If we have 3 requests (R1, R2, R3) in the event queue, then we could trigger `::authenticate` SIX times in total, once for R1, twice for R2 and three times for R3. This grows in quadratic to the number of enqueued requests and the master will be overwhelmed.    This issue couples with MESOS-9146 and MESOS-9145 makes the master authentication fragile and can easily be overwhlemed.",Bug,Blocker,Resolved,"2018-08-09 18:24:40","2018-08-09 17:24:40",3
"Apache Mesos","MasterQuotaTest.RemoveSingleQuota is flaky.",,Bug,Major,Resolved,"2018-08-09 15:26:53","2018-08-09 14:26:53",2
"Apache Mesos","Use composing containerizer by default in tests.","If we assign docker,mesos to the `containerizers` flag for an agent, then {{ComposingContainerizer}} will be used for many tests that do not specify {{containerizers}} flag. That's the goal of this task.    I tried to do that by adding {{flags.containerizers = docker,mesos}}, but it turned out that some tests are started to hang due to a paused clocks, while docker c'zer and docker library use libprocess clocks.    After setting composing c'zer by default, some tests (e.g. {{AgentAPITest.AttachContainerInputValidation}}) started to hang due to a paused clocks and the use of clock-dependent methods, like {{await()}}, {{delay()}}, etc. by the docker library.    It hangs in {{Docker::validateVersion()}}, which is called from {{Docker::create()}}. After I added {{Clock::resume()}} before calling {{version.await(DOCKER_VERSION_WAIT_TIMEOUT)}}, tests have started to hang due to the hanging docker recovery: docker c'zer launches {{docker ps -a}} subprocess and subscribes for its termination. As a reaper process uses {{delay()}}, this leads to a hanging recovery process for the docker c'zer.",Improvement,Major,Accepted,"2018-08-07 08:07:49","2018-08-07 07:07:49",8
"Apache Mesos","GRPC build fails to pass compiler flags","The GRPC build integration fails to pass compiler flags down from the main build into the GRPC component build. This can make the build fail in surprising ways.    For example, if you use {{CXXFLAGS=-fsanitize=thread CFLAGS=-fsanitize=thread}}, the build fails because of the inconsistent application of these flags across bundled components.    In this build log, libprotobuf was built using the correct flags, which then causes GRPC to fail because it is missing the flags:  ",Bug,Blocker,Resolved,"2018-08-06 18:12:20","2018-08-06 17:12:20",2
"Apache Mesos","Health checks launching nested containers while a container is being destroyed lead to unkillable tasks.","A container might get stuck in {{DESTROYING}} state if there's a command health check that starts new nested containers while its parent container is getting destroyed.    Here are some logs which unrelated lines removed. The `REMOVE_NESTED_CONTAINER`/`LAUNCH_NESTED_CONTAINER_SESSION` keeps looping afterwards.      This stops when the framework reconciles and instructs Mesos to kill the task. Which also results in a    Nothing else related to this container is logged following this line.",Bug,Blocker,Resolved,"2018-08-03 11:44:35","2018-08-03 10:44:35",6
"Apache Mesos","Test `StorageLocalResourceProviderTest.ROOT_ContainerTerminationMetric` is flaky.","This test is flaky and can fail with the following error:    The actual error is the following:    The root cause is that the SLRP calls {{ListVolumes}} and {{GetCapacity}} when starting up, and if the plugin container is killed when these calls are ongoing, gRPC will return an {{OS Error}} which will lead the SLRP to fail.    This flakiness will be fixed once we finish https://issues.apache.org/jira/browse/MESOS-8400.",Bug,Major,Resolved,"2018-08-02 23:55:54","2018-08-02 22:55:54",1
"Apache Mesos","Port mapper CNI plugin should use '-n' option with 'iptables --list'","Without the {{-n}} option, [this iptables command|https://github.com/apache/mesos/blob/9457dce1d99b5616d1b5eeb9a344733f6320d7b5/src/slave/containerizer/mesos/isolators/network/cni/plugins/port_mapper/port_mapper.cpp#L313] could result in a large number of reverse hostname lookups, which could take a while.    We should use the -n option there to avoid this issue.",Improvement,Major,Resolved,"2018-08-02 02:12:56","2018-08-02 01:12:56",1
"Apache Mesos","Metrics Improvements","This epic is meant to collect tickets for improvements to metrics in all Mesos components.",Epic,Major,Open,"2018-08-02 01:41:03","2018-08-02 00:41:03",13
"Apache Mesos","Port mapper CNI plugin might fail with Resource temporarily unavailable","https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/isolators/network/cni/plugins/port_mapper/port_mapper.cpp#L345    Looks like we're missing a `-w` for the iptable command. This will lead to issues like      This becomes more likely if there are many concurrent launches of Mesos contianers that uses port mapper on the box.",Bug,Major,Resolved,"2018-08-01 20:04:30","2018-08-01 19:04:30",1
"Apache Mesos","Agent reconfiguration can cause master to unsuppress on scheduler's behalf.","When agent reconfiguration was enabled in Mesos, the allocator was also updated to remove all offer filters associated with an agent when that agent's attributes change. In addition, whenever filters for an agent are removed, the framework is unsuppressed for any roles that had filters on the agent.    While this ensures that schedulers will have an opportunity to use resources on an agent after reconfiguration, modifying the scheduler's suppression may put the scheduler in an inconsistent state, where it believes it is suppressed in a particular role when it is not.",Bug,Major,Resolved,"2018-07-31 19:04:24","2018-07-31 18:04:24",3
"Apache Mesos","Expose quota consumption metrics.","Currently, quota related metrics exposes quota guarantee and allocated quota. We should expose consumed which is allocated quota plus unallocated reservations. We already have this info in the allocator as `consumedQuotaScalarQuantities`, just needs to expose it.",Improvement,Major,Resolved,"2018-07-31 17:55:44","2018-07-31 16:55:44",5
"Apache Mesos","Parallel serving of '/state' requests in the Master.","To reduce the impact of '/state'-related workloads on the Master actor and to increase the average response time when multiple '/state' requests are in the Master's mailbox, accumulate '/state' requests and process them in parallel while blocking the master actor only once.",Improvement,Blocker,Resolved,"2018-07-31 14:25:14","2018-07-31 13:25:14",5
"Apache Mesos","Optimize range addition operation.","MESOS-9086 made range subtraction operation faster than addition which shows room for improvement for addition.",Improvement,Major,Resolved,"2018-07-30 18:24:40","2018-07-30 17:24:40",3
"Apache Mesos","Launch nested container session fails due to incorrect detection of `mnt` namespace of command executor's task.","Launch nested container call might fail with the following error:    This happens when the containerizer launcher [tries to enter|https://github.com/apache/mesos/blob/077f122d52671412a2ab5d992d535712cc154002/src/slave/containerizer/mesos/launch.cpp#L879-L892] `mnt` namespace using the pid of a terminated process. The pid [was detected|https://github.com/apache/mesos/blob/077f122d52671412a2ab5d992d535712cc154002/src/slave/containerizer/mesos/containerizer.cpp#L1930-L1958] by the agent before spawning the containerizer launcher process, because the process was running back then.    The issue can be reproduced using the following test (pseudocode):    When we start the very first nested container, `getMountNamespaceTarget()` returns a PID of the task (`sleep 1000`), because it's the only process whose `mnt` namespace differs from the parent container. This nested container becomes a child of PID 1 process, which is also a parent of the command executor. It's not an executor's child! It can be seen in attached `pstree.png`.    When we start a second nested container, `getMountNamespaceTarget()` might return PID of the previous nested container (`echo echo`) instead of the task's PID (`sleep 1000`). It happens because the first nested container entered `mnt` namespace of the task. Then, the containerizer launcher (nanny process) attempts to enter `mnt` namespace using the PID of a terminated process, so we get this error.",Bug,Critical,Resolved,"2018-07-27 19:17:43","2018-07-27 18:17:43",8
"Apache Mesos","mesos-style reports violations on a clean checkout","When running {{support/mesos-style.py}} on a clean checkout of e.g., {{e879e920c35}} Python style violations are reported,        I would expect a clean checkout to not report any violations.",Bug,Major,Resolved,"2018-07-25 11:31:19","2018-07-25 10:31:19",2
"Apache Mesos","Add move support to the Resources / Resource_ wrappers.","Currently, the Resources / Resource_ wrappers do not have move support. Since copying resources are rather expensive (especially when there's large ports ranges or metadata like reservations / labels), we should add move support to reduce copying automatically where possible:    * Constructors for both Resources and Resource_  * +, +=, addition operations: with addition we sometimes need to add the resource to the vector in which case we can move if taking an rvalue",Improvement,Major,Resolved,"2018-07-24 20:05:13","2018-07-24 19:05:13",3
"Apache Mesos","Test `ROOT_DOCKER_DockerAndMesosContainerizers/DefaultExecutorTest.TaskWithFileURI` is flaky.","The test is flaky and segfault on CI ubuntu-16.04-SSL, log attached.    Looks like this is due to a race condition during the test destruction sequence:  The test     {code:c++}    Future<v1::scheduler::Event::Update> startingUpdate;    Future<v1::scheduler::Event::Update> runningUpdate;    Future<v1::scheduler::Event::Update> finishedUpdate;    EXPECT_CALL(*scheduler, update(_, _))      .WillOnce(          DoAll(              FutureArg<1>(&startingUpdate),              v1::scheduler::SendAcknowledge(frameworkId, agentId)))      .WillOnce(          DoAll(              FutureArg<1>(&runningUpdate),              v1::scheduler::SendAcknowledge(frameworkId, agentId)))      .WillOnce(          DoAll(              FutureArg<1>(&finishedUpdate),              v1::scheduler::SendAcknowledge(frameworkId, agentId)));      mesos.send(        v1::createCallAccept(            frameworkId,            offer,            {v1::LAUNCH_GROUP(                executorInfo, v1::createTaskGroupInfo({taskInfo}))}));      AWAIT_READY(startingUpdate);    ASSERT_EQ(v1::TASK_STARTING, startingUpdate->status().state());    ASSERT_EQ(taskInfo.task_id(), startingUpdate->status().task_id());      AWAIT_READY(runningUpdate);    ASSERT_EQ(v1::TASK_RUNNING, runningUpdate->status().state());    ASSERT_EQ(taskInfo.task_id(), runningUpdate->status().task_id());      AWAIT_READY(finishedUpdate);    ASSERT_EQ(v1::TASK_FINISHED, finishedUpdate->status().state());    ASSERT_EQ(taskInfo.task_id(), finishedUpdate->status().task_id());  }  {code}    The sending acknowledgment of the last task status update (TASK_FINISHED) could race with the test tear down. Specifically, the `EXPECT_CALL` on the `update()` captures a pointer to arg0 which is `mesos`. However, `mesos` could be destructed during the test teardown leaving arg0 a nullptr and consequently the test segfaults when it tries to call arg0->...send().    One quick fix is to remove the last acknowledgment. However, a sound fix is to make the `mesos` pointer a shared one. This would entail a lot of interface changes. Since our only concern is the capture in the `EXPECT_CALL`, maybe we only need to change the interface of that to take a shared pointer.",Bug,Major,Open,"2018-07-24 06:41:42","2018-07-24 05:41:42",1
"Apache Mesos","Add seccomp filter into containerizer launcher.","Containerizer launcher should create an instance of the `SeccompFilter` class, which will be used to setup/load a Seccomp filter rules using the given `ContainerSeccompProfile` message.",Task,Major,Resolved,"2018-07-23 14:42:42","2018-07-23 13:42:42",3
"Apache Mesos","Implement Docker Seccomp profile parser.","The parser should translate Docker seccomp profile into the `ContainerSeccompProfile` protobuf message.",Task,Major,Resolved,"2018-07-23 14:07:03","2018-07-23 13:07:03",5
"Apache Mesos","Refactor capability related logic in the allocator.","- Add a function for returning the subset of frameworks that are capable of receiving offers from the agent. This moves the capability checking out of the core allocation logic and means the loops can just iterate over a smaller set of framework candidates rather than having to write 'continue' cases. This covers the GPU_RESOURCES and REGION_AWARE capabilities.    - Similarly, add a function that allows framework capability based filtering of resources. This pulls out the filtering logic from the core allocation logic and instead the core allocation logic can just all out to the capability filtering function. This covers the SHARED_RESOURCES, REVOCABLE_RESOURCES and RESERVATION_REFINEMENT capabilities. Note that in order to implement this one, we must refactor the shared resources logic in order to have the resource generation occur regardless of the framework capability (followed by getting filtered out via this new function if the framework is not capable).",Improvement,Major,Resolved,"2018-07-21 00:21:51","2018-07-20 23:21:51",3
"Apache Mesos","Refactor the old allocator benchmarks to utilize the allocator benchmark fixture.","Refactor/migrate/sanitize old allocator benchmarks in `src/tests/hierarchical_allocator_tests.cpp` to utilize the new allocator benchmark fixture in `src/tests/hierarchical_allocator_benchmarks.cpp`, this will help to:  - Reduce code duplication;  - Make more accurate measurements by excluding event-driven allocations;    Also some of the old benchmarks may no longer be ",Improvement,Major,"In Progress","2018-07-21 00:14:25","2018-07-20 23:14:25",3
"Apache Mesos","Add allocator quota tests regarding reserve/unreserve already allocated resources.","Add allocator quota tests regarding reserve/unreserve already allocated resources:    - Reserve already allocated resources should not affect quota headroom;  - The same applies to unreserve allocated resources.",Task,Major,Resolved,"2018-07-20 20:04:22","2018-07-20 19:04:22",3
"Apache Mesos","Linux launcher may report `Failed to clone: Success` on error.","The clone function passed to {{subprocess}} in {{LinuxLauncherProcess::fork}} is implemented in a way that if `{{ns::clone}}` returns an error, it would return -1, which makes {{subprocess}} assume that the error sets up {{errno}} and returns a {{Failed to clone: Success}} error. We should try to detect this and return the proper error.",Improvement,Minor,Open,"2018-07-19 23:31:42","2018-07-19 22:31:42",1
"Apache Mesos","Consider including public protobuf definitions in generated jar","We currently do not package public proto sources alongside other resources in the jar. This is inconsistent with what we do e.g., for packages or {{install rules}} on the C++ side.    Frameworks seem to work around this by forking required proto sources into their own source code, or (slightly less worse) fetching them from potentially poorly versioned internet resources. Both approaches can lead to complicate dependencies between used jar and proto sources.    We should include them in the jar we publish, e.g., by declaring them as {{resources}}.",Improvement,Major,Open,"2018-07-19 12:07:04","2018-07-19 11:07:04",1
"Apache Mesos","On macOS libprocess_tests fail to link when compiling with gRPC","Seems like this was introduces with commit {{a211b4cadf289168464fc50987255d883c226e89}}. Linking {{libprocess-tests}} on macOS with enabled gRPC fails with  ",Bug,Major,Resolved,"2018-07-19 08:31:59","2018-07-19 07:31:59",2
"Apache Mesos","Test `PartitionTest.PartitionAwareTaskCompletedOnPartitionedAgent` is flaky.","Test is flaky on CI centos-7-CMake,      ",Bug,Major,Resolved,"2018-07-18 21:21:06","2018-07-18 20:21:06",1
"Apache Mesos","`createStrippedScalarQuantity()` should clear all metadata fields.","Currently `createStrippedScalarQuantity()` strips resource meta-data and transforms dynamic reservations into a static reservation. However, no current code depends on the reservations in resources returned by this helper function. This leads to boilerplate code around call sites and performance overhead.",Improvement,Major,Resolved,"2018-07-18 20:46:37","2018-07-18 19:46:37",3
"Apache Mesos","Optimize range subtraction operation.","Based on the profiling result of MESOS-8989, the range subtraction operation is about 2~3 times more expensive than that of addition. It's not obvious that this has to be the case.    The current range subtraction implementation relies on boost IntervalSet, the construction cost of the IntervalSet could be the culprit:  https://github.com/apache/mesos/blob/9147283171d761a4d38710f24ba654f8a96e325c/src/common/values.cpp#L378-L387    I think we could do better by writing a one-pass (with sorting) algorithm like that of addition.",Improvement,Major,Resolved,"2018-07-18 18:57:40","2018-07-18 17:57:40",3
"Apache Mesos","Test ReservationEndpointsTest.ReserveAndUnreserveNoAuthentication is flaky.","Flaky on CI debian-9-SSL_GRPC    Error Message      ",Bug,Major,Accepted,"2018-07-17 19:53:16","2018-07-17 18:53:16",2
"Apache Mesos","Avoid two trips through the master mailbox for state.json requests.","Currently, a state.json request travels through the master's mailbox twice: before authorization and after. This increases the overall state.json response time by around 30%.    To remove one mailbox trip, we can perform the initial portion (validation and authorization) of state and /state off the master actor by using a top-level {{Route}}, then dispatch onto the master actor only for json / protobuf serialization. This should drop the authorization time down to near 0 if it's indeed mostly queuing delay.",Task,Major,Open,"2018-07-17 15:21:16","2018-07-17 14:21:16",5
"Apache Mesos","Virtualenv management in support directory is buggy.","When switching back and forth from Python 2 to 3, the virtualenv does not get correctly reinstalled.",Bug,Major,Resolved,"2018-07-13 17:14:55","2018-07-13 16:14:55",2
"Apache Mesos","Pylint is too noisy when using mesos-style.py","  The score needs to be removed.",Bug,Minor,Resolved,"2018-07-13 12:12:33","2018-07-13 11:12:33",1
"Apache Mesos","Tox doesn't run in the support virtualenv when using Python 3 mesos-style.py",,Bug,Major,Resolved,"2018-07-13 09:10:34","2018-07-13 08:10:34",1
"Apache Mesos","Improve performance of metrics snapshot.","As we add more metrics, the response time of the metrics snapshot is going up. We should optimize the generation of the snapshot to improve response times.    One of the ways of improving the performance would be to migrate from pull to push gauges.",Task,Major,Accepted,"2018-07-12 21:26:19","2018-07-12 20:26:19",5
"Apache Mesos","Improve containerizer logging for isolator prepare/update/cleanup.","It has been very hard to debug container startup/teardown issues if the containers are stuck in some isolator prepare/update/cleanup steps. We should add more logging in the Mesos containerizer for each isolator calls.",Task,Critical,Open,"2018-07-12 00:25:28","2018-07-11 23:25:28",3
"Apache Mesos","Support systemd and freezer cgroup subsystems bind mount for container with rootfs.","From MESOS-8327, cgroup subsystems are bind mounted to the container's rootfs, but systemd and freezer cgroup are not bind mounted yet since they are not subsystems under the cgroup isolator but from the linux launcher.    Some applications (e.g., dockerd) may check the /proc/self/cgorup for enabled subsystems and check them at /proc/self/mountinfo to make sure there are those mounts. Here is an example:      The first one is a task without image, the second one is a task using debian image. So any app relies on systemd and freezer cgroup would may fail:      So, we should consider add systemd and freezer cgroup bind mount at the cgroup isolator and make a *NOTE* for this behavior.",Task,Major,Resolved,"2018-07-11 22:35:55","2018-07-11 21:35:55",3
"Apache Mesos","Changing `CREATE_VOLUME` and `CREATE_BLOCK` to `CREATE_DISK`.","Mesos 1.5 introduced four new operations for better storage support through CSI. These operations are:      * CREATE_VOLUME converts RAW disks to MOUNT or PATH disks.    * DESTROY_VOLUME converts MOUNT or PATH disks back to RAW disks.    * CREATE_BLOCK converts RAW disks to BLOCK disks.    * DESTROY_BLOCK converts BLOCK disks back to RAW disks.    However, the following two issues are raised for these operations:    1. Volume is overloaded and leads to conflicting/inconsistent naming.  2. The concept of PATH disks does not exist in CSI, which could be problematic.    To address this, we could change CREATE_VOLUME/CREATE_BLOCK to CREATE_DISK, and DESTROY_VOLUME/DESTROY_BLOCK to DESTROY_DISK, and make CREATE_DISK support only MOUNT and BLOCK disks.",Task,Major,Resolved,"2018-07-10 04:18:35","2018-07-10 03:18:35",3
"Apache Mesos","Add global and per-framework metrics for invalid and unauthorized operations","The master should expose global and per-framework metrics which track the number of offer operations that fail because they are invalid, or because of authorization.",Task,Major,Open,"2018-07-09 19:05:31","2018-07-09 18:05:31",2
"Apache Mesos","mesos-style.py messaging is poor","After running into all sorts of issues connected to the {{-c}} option, at some point I tried the {{-k}} option and got the following output;    It appears that the messaging here is worth reconsidering;   - No C++ files to lint, No JavaScript files to lint, Total errors found: 0 seems to have little value for the normal user - how about we make that a user activated verbose output?",Bug,Minor,Resolved,"2018-07-08 01:36:03","2018-07-08 00:36:03",1
"Apache Mesos","Make gRPC call deadline configurable.","Currently, the deadline for a gRPC call to become terminal is hard-coded to 5 seconds. This would cause problems on slow machines. Ideally, we should make this deadline configurable.",Improvement,Major,Resolved,"2018-07-07 00:08:17","2018-07-06 23:08:17",3
"Apache Mesos","Default executor should commit suicide if it cannot receive HTTP responses for LAUNCH_NESTED_CONTAINER calls.","If there is a network problem (e.g., a routing problem), it is possible that the agent has received {{LAUNCH_NESTED_CONTAINER}} calls from the default executor and launched the nested container, but the executor does not get the HTTP response. This would result in tasks stuck at {{TASK_STARTING}} forever. We should consider making the default executor commit suicide if it does not receive the response in a reasonable amount of time. ",Bug,Major,Open,"2018-07-05 19:05:50","2018-07-05 18:05:50",3
"Apache Mesos","Agent GC could unmount a dangling persistent volume multiple times.","When the agent GC an executor dir and the sandbox of one of its run that contains a dangling persistent volume, the agent might try to unmount the persistent volume twice, which leads to an {{EINVAL}} when trying to unmount the target for the second time.    Here is the log from a failure run of {{GarbageCollectorIntegrationTest.ROOT_DanglingMount}}:  ",Bug,Minor,Resolved,"2018-07-03 21:08:31","2018-07-03 20:08:31",2
"Apache Mesos","Build and persist quota headroom info across allocation cycle.","Currently, in the allocator, quota headroom info is built up from scratch at the beginning of each allocation iteration. This affects performance and increases code complexity. We should be able to track and persist this info as we make new allocations.",Improvement,Major,Accepted,"2018-07-02 22:46:26","2018-07-02 21:46:26",3
"Apache Mesos","Move SASL based CRAM-MD5 authentication out of libmesos.","We might want to reduce the hard dependencies of libmesos against third party libraries, simplifying deployment and speeding up load times. In case of the SASL based CRAM-MD5 authentication, we already have (test-)modules built which provide the needed services.",Improvement,Minor,Open,"2018-06-30 19:44:53","2018-06-30 18:44:53",8
"Apache Mesos","CNI isolator recovery should wait until unknown orphan cleanup is done","Currently, CNI isolator will cleanup unknown orphaned containers in an asynchronous way (see [here|https://github.com/apache/mesos/blob/1.6.0/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L439] for details) during recovery, that means agent recovery can finish while the cleanup of unknown orphaned containers is still ongoing which is not ideal. So we need to make CNI isolator recovery waits until unknown orphan cleanup is done.",Bug,Major,Resolved,"2018-06-29 02:40:50","2018-06-29 01:40:50",2
"Apache Mesos","Archiver utility extracts links within subdirectories incorrectly","The fix for MESOS-9008 did not correctly fix extraction of links with the archiver utility.    Two problems:  1) Links that were originally relative within the archiver are transformed to point to absolute locations after extraction.  The link targets should remain relative.  2) Links within subdirectories (i.e. path/to/link -> path/to/target) will lose part of the target path upon extraction (i.e. path/to/link -> target)",Bug,Critical,Resolved,"2018-06-29 00:36:16","2018-06-28 23:36:16",2
"Apache Mesos","DefaultExecutorTest.SigkillExecutor is flaky","Failed on windows CI but does not look like windows specific:            Subsequent offers calls should be ignored.  ",Bug,Major,Resolved,"2018-06-29 00:23:06","2018-06-28 23:23:06",1
"Apache Mesos","Document `linux/seccomp` isolator",,Documentation,Major,Resolved,"2018-06-28 17:41:05","2018-06-28 16:41:05",5
"Apache Mesos","Implement `linux/seccomp` isolator","The main purpose of this isolator is to prepare `ContainerSeccompProfile` for a containerizer launcher. `ContainerSeccompProfile` message is generated by the isolator from a JSON-file that contains declaration of Seccomp filter rules.    In addition, seccomp isolator should check for a Seccomp support by the Linux kernel.",Task,Major,Resolved,"2018-06-28 17:30:34","2018-06-28 16:30:34",5
"Apache Mesos","Implement a wrapper class for `libseccomp` API","The main purpose of this class is to provide translation of `SeccompProfile` protobuf into invocations of `libseccomp` API. The main user of this class is a containerizer launcher.",Task,Major,Resolved,"2018-06-28 16:37:56","2018-06-28 15:37:56",5
"Apache Mesos","Add Seccomp-related protobufs","Define `SeccompProfile` and `SeccompInfo` messages and then update appropriate messages, including `LinuxInfo` and `ContainerLaunchInfo`.",Task,Major,Resolved,"2018-06-28 16:02:25","2018-06-28 15:02:25",3
"Apache Mesos","Update build scripts to support `seccomp-isolator` flag and `libseccomp` library","This ticket consists of the following subtasks:  1) Bundle `libseccomp` tarball as a third-party library  2) Add build rules to compile `libseccomp` library and to link Mesos agent against this library  3) Add `enable-seccomp-isolator` flag to the build scripts",Task,Major,Resolved,"2018-06-28 15:48:22","2018-06-28 14:48:22",5
"Apache Mesos","Mesos CNI portmap plugins' iptables rules doesn't allow connections via host ip and port from the same bridge container network","using `mesos-cni-port-mapper` with folllowing config:     - 2 services running on the same mesos-slave using unified containerizer in different tasks and communicating via host ip and host port   - connection timeouts due to iptables rules per container CNI-XXX chain   - actually timeouts are caused by    rule #1 is executed and no masquerading happens.    there are multiple solutions:   - -simpliest and fastest one is not to add that ACCEPT- - NOT A SOLUTION. it's happening in `bridge` plugin and `cni/portmap` shows that snat/masquerade should be done during portmapping as well.   - perhaps, there's a better change in iptables rules that can fix it   - proper one (imho) is to finally implement cni spec 0.3.x in order to be able to use chaining of plugins and use cni's `bridge` and `portmap` plugins in chain (and get rid of mesos-cni-port-mapper completely eventually).",Bug,Major,Open,"2018-06-27 12:09:39","2018-06-27 11:09:39",3
"Apache Mesos","De-couple suppression API from clearing filters.","Currently, we have the following semantics:    SUPPRESS: stop sending additional offers to the scheduler  REVIVE: resume sending additional offers to the scheduler AND clear all filters    SUPPRESS is idempotent and can be retried without issue, however, since revive clears all filters, it is not idempotent and it is risky to call it periodically.    Suppression state and filter management should be done with separate APIs, e.g.:    SUPPRESS: stop sending additional offers to the scheduler  UNSUPPRESS: resume sending additional offers to the scheduler  CLEAR_FILTERS: clear filters  REVIVE: (for backwards compatibility) UNSUPPRESS + CLEAR_FILTERS",Improvement,Major,"In Progress","2018-06-26 03:25:08","2018-06-26 02:25:08",5
"Apache Mesos","GPU Isolator still depends on cgroups/devices agent flag given cgrous/all is supported.","GPU Isolator still depends on cgroups/devices agent flag given cgrous/all is supported.",Bug,Major,Resolved,"2018-06-25 23:23:22","2018-06-25 22:23:22",2
"Apache Mesos","The container which joins CNI network and has checkpoint enabled will be mistakenly destroyed by agent","Reproduce steps:    1) Run {{mesos-execute}} to launch a task which joins a CNI network {{net1}} and has checkpoint enabled:    2) After task is in the {{TASK_RUNNING}} state, restart the agent process, and then in the agent log, we will see the container is destroyed.    And {{mesos-execute}} will receive a {{TASK_GONE}} for the task:  ",Bug,Blocker,Resolved,"2018-06-24 04:13:09","2018-06-24 03:13:09",3
"Apache Mesos","Seccomp design doc","Design doc:    https://docs.google.com/document/d/146FJJ0sDi1sp_HQxVUg-vhqVSTEsdCeD4If3b1xCeec",Documentation,Major,Resolved,"2018-06-22 14:40:29","2018-06-22 13:40:29",8
"Apache Mesos","Validate that container paths are unique in `ContainerInfo.volumes`.","Currently we allow two volumes to have the same {{container_path}}. The bind-mount of a later volume would overwrite that of an earlier one.    However, if the two volumes are file-based secrets, the containerizer will generate pre-exec commands similar to the following:      The second {{mv}} would rename {{secret2}} to {{source}}, but {{target}} remains bounded to the gone {{secret1}}, and this would make the last {{mount}} result in an {{ENOENT}}.    In general, allowing multiple with the same {{container_path}} is not useful, so we should disallow it and validate that in advance instead of getting a failure during container launch.",Improvement,Major,Open,"2018-06-22 03:55:34","2018-06-22 02:55:34",2
"Apache Mesos","Make all SLRP tests become non-root tests.","All SLRP and Resource Provider Config API tests enables {{filesystem/linux}} isolation and thus require root permission. However, we remove the isolation and unmount all paths under the test sandboxes instead, to make them non-root tests.",Task,Major,Open,"2018-06-21 04:37:47","2018-06-21 03:37:47",2
"Apache Mesos","Allow resources to be removed when updating the sorter.","Currently we do not allow resource conversions to change the resource quantities when updating the sorter; we only allow the metadata for their consumed resources to be modified.    However, this restricts Mesos from supporting operations that remove resources. For example, when a CSI volume with a stale profile is destroyed, it would be better to convert it into an empty resource since the disk space is no longer available. See https://issues.apache.org/jira/browse/MESOS-8825.    To make the allocator more flexible, we should allow resource conversions to remove resources when updating the sorter.",Improvement,Major,Resolved,"2018-06-21 02:03:48","2018-06-21 01:03:48",5
"Apache Mesos","MasterAPITest.SubscribersReceiveHealthUpdates is flaky","This test fails flaky on CI. Log attached.  ",Bug,Major,Resolved,"2018-06-20 23:07:22","2018-06-20 22:07:22",1
"Apache Mesos","`UPDATE_STATE` can race with `UPDATE_OPERATION_STATUS` for a resource provider.","Since a resource provider and its operation status update manager run in different actors, for a completed operation, its `UPDATE_OPERATION_STATUS` call may race with an `UPDATE_STATE`. When the `UPDATE_STATE` arrives to the agent earlier, the total resources will be updated, but the terminal status of the completed operation will be ignored since it is known by both the agent and the resource provider. As a result, when the `UPDATE_OPERATION_STATUS` arrives later, the agent will try to apply the operation, but this is incorrect since the total resources has already been updated.",Bug,Blocker,Resolved,"2018-06-20 04:09:47","2018-06-20 03:09:47",2
"Apache Mesos","Support for creation non-existing host paths in a whitelist as source paths","Docker creates a directory specified in {{docker run}}'s {{--volume}}/{{-v}} option as the source path that will get bind-mounted into the container, if the source location didn't originally exist on the host.    Unlike Docker, UCR bails on launching containers if any of their host mount paths doesn't originally exist. While this is more secure and eliminates unnecessary side effects, it breaks transparent compatibility when trying to migrate from Docker.    As a trade-off, we should allow host path creation in a restricted manner, by introducing a new Mesos agent flag ({{--host_path_volume_force_creation}}) as a colon-separated whitelist (similar to the format of POSIX's {{$PATH}} environment variable), under whose items' subdirectories the host paths are allowed to be created.  ",Task,Major,Resolved,"2018-06-19 19:51:45","2018-06-19 18:51:45",5
"Apache Mesos","Fetcher fails to extract some archives containing hardlinks","We recently switched the infrastructure to e.g., extract archives to libarchive which seems to have narrower support for e.g., hardlinks in archives, see e.g., [https://github.com/libarchive/libarchive/wiki/Hardlinks] upstream (likely outdated).    In a particular case, we tried to extract https://artifacts.elastic.co/downloads/kibana/kibana-5.6.5-linux-x86_64.tar.gz which the fetcher successfully extracted in e.g., 1.6.0, but which now leads to failures like  ",Bug,Blocker,Resolved,"2018-06-19 17:49:06","2018-06-19 16:49:06",1
"Apache Mesos","The agent's GET_AGENT leaks resource information when using authorization","While the master's {{GET_AGENTS}} call e.g., filters resources (by using an approver with {{VIEW_ROLE}}) so that it does not leak resources the querying principal should not be able to see, no such filtering is done in the corresponding agent's {{GET_AGENT}} call.    This call should be authorized as well to not expose information we expect to be not visible.",Bug,Critical,Accepted,"2018-06-18 11:51:15","2018-06-18 10:51:15",3
"Apache Mesos","Operator API event stream can miss task status updates.","As of now, the master only sends TaskUpdated messages to subscribers when the latest known task state on the agent changed:        The latest state is set like this:        However, the `TaskStatus` message included in an `TaskUpdated` event is the event at the bottom of the queue when the update was sent.    So we can easily get in a situation where e.g. the first TaskUpdated has .status.state == TASK_STARTING and .state == TASK_RUNNING, and the second update with .status.state == TASK_RUNNNING and .state == TASK_RUNNING would not get delivered because the latest known state did not change.    This implies that schedulers can not reliably wait for the status information corresponding to specific task state, since there is no guarantee that subscribers get notified during the time when this status update will be included in the status field.",Bug,Major,Resolved,"2018-06-15 12:20:44","2018-06-15 11:20:44",3
"Apache Mesos","Add default bodies for libprocess HTTP error responses.","By default on error libprocess would only return a response  with the correct status code and no response body.    However, most browsers do not visually indicate the response  status code, so if any error occurs anyone using a browser will only  see a blank page, making it hard to figure out what happened.",Improvement,Major,Resolved,"2018-06-15 12:14:39","2018-06-15 11:14:39",3
"Apache Mesos","Allow for unbundled libevent in CMake builds to work around 2.1.x SSL issues.","On macOS and Ubuntu 17, libevent-openssl >= 2.1.x is broken in conjunction with libprocess.    We tried to pinpoint the issue but so far with no success. For enabling CMake SSL builds on those systems , we have to support prior libevent versions. Allowing building against a preinstalled libevent version paves that way.",Improvement,Blocker,Resolved,"2018-06-15 03:05:31","2018-06-15 02:05:31",2
"Apache Mesos","Add SLRP unit tests for missing profiles.","We need to add unit tests to verify SLRP correctness when the set of known profiles are shrunk by the disk profile adaptor. Here lists a couple scenarios worth testing:   1. `CREATE_VOLUME` should succeed if it is submitted before the profile becomes stale.  2. `CREATE_VOLUME` should be dropped if it is submitted after the profile becomes stale.  3. `DESTROY_VOLUME` should free spaces for a stale profile.",Task,Major,Resolved,"2018-06-14 21:03:36","2018-06-14 20:03:36",5
"Apache Mesos","Add a better benchmark for range type resources.","While investigating the performance issue with the `available()` function in the allocator, the arithmetic of range type resources, namely ports, appear to be most expensive. And the subtraction proves to be significantly more expensive than the addition whose cause is not immediately clear to me.    While we do have a benchmark `[Resources_Scalar_Arithmetic_BENCHMARK_Test|https://github.com/apache/mesos/blob/1b851877ff22cf2da193bb649dc838bb50c26b34/src/tests/resources_tests.cpp#L3610-L3626]` that contains a range parametrized test, its result is somewhat misleading. It evaluates the addition performance by repeatedly summing up the same port resources which is fine. However, when evaluating the subtraction, it took the result from the addition and repeatedly subtracting the same port resources. The result is that after the first subtraction, the subtrahend becomes empty and all the subsequent subtractions are shorthanded, producing uninstructive result.",Improvement,Minor,Resolved,"2018-06-14 00:16:07","2018-06-13 23:16:07",2
"Apache Mesos","Master asks agent to shutdown upon auth errors.","The Mesos master sends a {{ShutdownMessage}} to an agent if there is an [authentication|https://github.com/apache/mesos/blob/d733b1031350e03bce443aa287044eb4eee1053a/src/master/master.cpp#L6532-L6543] or an [authorization|https://github.com/apache/mesos/blob/d733b1031350e03bce443aa287044eb4eee1053a/src/master/master.cpp#L6622-L6633] error during agent registration.     Upon receipt of this message, the agent kills alls its tasks and commits suicide. This means that transient auth errors can lead to whole agents being killed along with it's tasks.    I think the master should stop sending the {{ShutdownMessage}} in these cases, or at least let the agent retry the registration a few times before asking it to shutdown.",Bug,Blocker,Resolved,"2018-06-12 23:46:01","2018-06-12 22:46:01",5
"Apache Mesos","Posting to the operator api with 'accept recordio' header can crash the agent","It's possible to crash the mesos agent by posting a reasonable request to the operator API.  h3. Background:    Sending a request to the v1 api endpoint with an unsupported 'accept' header:    Results in the following friendly error message:    h3. Reproducible crash:    However, sending the same request with 'application/recordio' 'accept' header:    causes the agent to crash (no response is received).    Crash log is shown below, full log from the agent is attached here:  ",Bug,Major,Resolved,"2018-06-08 00:02:59","2018-06-07 23:02:59",2
"Apache Mesos","SlaveRecoveryTest/0.PingTimeoutDuringRecovery is flaky","During an unrelated change in a PR, the apache build bot sent the following error:      ",Bug,Major,Accepted,"2018-06-06 13:52:04","2018-06-06 12:52:04",3
"Apache Mesos","Problem and solution overview for the slow API issue.","Collect data from the clusters regarding {{state.json}} responsiveness, figure out, where the bottlenecks are, and prepare an overview of solutions.",Task,Major,Resolved,"2018-06-04 15:10:29","2018-06-04 14:10:29",5
"Apache Mesos","External Resource Provider Design","We need a design for external resource provider and how external resources are used. How external resources are offered to the frameworks is a separated issue and is not covered in this design.",Task,Major,Resolved,"2018-05-31 16:13:13","2018-05-31 15:13:13",8
"Apache Mesos","Wire `UPDATE_QUOTA` call.","Wire the existing master, auth, registar, and allocator pieces together to complete the `UPDATE_QUOTA` call.    This would enable the master capability `QUOTA_V2`.    This also fixes the ignoring zero resource quota bug in the old quota implementation, namely:    Currently, Mesos discards resource object with zero scalar value when parsing resources. This means quota set to zero would be ignored and not enforced. For example, role with quota set to cpu:10;mem:10;gpu:0 intends to get no GPU. Due to the above issue, the allocator can only see the quota as cpu:10;mem:10, and no quota GPU means no guarantee and NO limit. Thus GPUs may still be allocated to this role.     With the completion of `UPDATE_QUOTA` which takes a map of name, scalar values, zero value will no longer be dropped.  ",Bug,Major,Resolved,"2018-05-31 00:17:21","2018-05-30 23:17:21",5
"Apache Mesos","Executor crash trying to print container ID.","As observed in an internal cluster:        The issue is caused by not this block in CheckerProcess not checking that previousCheckContainerId is still some after it had yielded control:  ",Bug,Major,Resolved,"2018-05-28 16:58:03","2018-05-28 15:58:03",3
"Apache Mesos","Output of tasks gets corrupted if task defines the same environment variables as the executor container","The issue is easily reproducible if one launches a task group and the taks nested container defines the same set of environment variables as the executor. In those circumstances, the following [snippet is activated|https://github.com/apache/mesos/blob/285d82080748cd69044c226950274c7046048c4b/src/slave/containerizer/mesos/launch.cpp#L1057]:        But this is not the only time that this file writes into {{cout}}.    This may be a bad idea because applications which consume the standard output of a task may end up being corrupted by the container manager output. In these cases, writing to {{cerr}} should be the right approach. ",Task,Minor,Resolved,"2018-05-28 11:48:58","2018-05-28 10:48:58",1
"Apache Mesos","Install Python 3 on Mesos CI instances",,Task,Major,Resolved,"2018-05-25 16:32:16","2018-05-25 15:32:16",3
"Apache Mesos","python3/post-reviews.py errors due to TypeError.","  The review still get posted.",Bug,Major,Resolved,"2018-05-25 11:58:49","2018-05-25 10:58:49",1
"Apache Mesos","Improve the container preparing logging in IOSwitchboard and volume/secret isolator.","Improve the container preparing logging in IOSwitchboard and volume/secret isolator.",Improvement,Major,Resolved,"2018-05-24 02:10:13","2018-05-24 01:10:13",2
"Apache Mesos","Master check failure due to CHECK_SOME(providerId).",,Bug,Critical,Resolved,"2018-05-23 19:29:02","2018-05-23 18:29:02",2
"Apache Mesos","Add metrics about CSI calls.","We should add metrics for CSI calls so operators can be alerted on flapping CSI plugins.",Task,Major,Resolved,"2018-05-23 00:12:18","2018-05-22 23:12:18",5
"Apache Mesos","Master streaming API does not send (health) check updates for tasks.","Currently, Master API subscribers get task status updates when task state changes (the actual logic is [slightly more complex|https://github.com/apache/mesos/blob/d7d7cfbc3e5609fc9a4e8de8203a6ecb11afeac7/src/master/master.cpp#L10794-L10841]). We use task status updates to deliver health and check information to schedulers, in which case task state does not change. Hence these updates are filtered out and the subscribers do not get any task health updates.    Here is a test that confirms the described behaviour: https://gist.github.com/rukletsov/c079d95479fb134d137ea3ae8b7ae874",Task,Major,Resolved,"2018-05-22 21:57:02","2018-05-22 20:57:02",2
"Apache Mesos","Add allocator counter metric for number of different resource types allocated","Currently we have gauges for how many cpus/mem/disk are offered to a role. It would be valuable to also have counters for these to diagnose/debug situations like MESOS-8935.",Task,Major,Open,"2018-05-19 02:33:05","2018-05-19 01:33:05",3
"Apache Mesos","Per Framework Offer metrics with a specific resource type","MESOS-8845 counts the number of offers sent to a framework but it would be useful to also know how many of these offers contained cpus, how many contained mem etc. This is so that we can diagnose/debug offer starvation situations like MESOS-8935 better.",Task,Major,Resolved,"2018-05-19 02:28:21","2018-05-19 01:28:21",2
"Apache Mesos","Benchmark tests to reproduce quota limit chopping behavior","We need a benchmark style test to reproduce the behavior described in MESOS-8935 so that we can track improvement with subsequent fixes.",Task,Major,"In Progress","2018-05-19 01:50:39","2018-05-19 00:50:39",8
"Apache Mesos","Implement a Random Sorter for offer allocations.","The only sorter that Mesos supports today is the DRF sorter. But, there are cases when DRF sorting causes offer fragmentation when dealing with non-revocable resources and multiple frameworks. One of the options to improve this situation is to introduce a new random sorter instead of DRF sorter.    See [https://docs.google.com/document/d/1uvTmBo_21Ul9U_mijgWyh7hE0E_yZXrFr43JIB9OCl8/edit#heading=h.nfye94rqpotp] for additional details.          ",Task,Major,Resolved,"2018-05-19 01:46:40","2018-05-19 00:46:40",5
"Apache Mesos","Quota limit chopping can lead to cpu-only and memory-only offers.","When we allocate resources to a role, we'll chop the available resources of the agent up to the quota limit for the role (per MESOS-7099). This prevents the role from exceeding its quota limit.    This has the unintended consequence of creating cpu-only and memory-only offers.    Consider agents with 10 cpus and 100 GB mem and roles with quota guarantee/limit of 5 cpus, 10 GB mem. The following allocations will occur:    agent 1:   r1 -> 5 cpus 10GB mem   r2 -> 5 cpus 10GB mem   r3 -> 0 cpus 10GB mem (quota allocates even if it can make progress towards a single resource and MESOS-1688 allows this)   r4 -> 0 cpus 10GB mem   ...   r10 -> 0 cpus 10GB mem    agent 2:   r3 -> 5 cpus 0GB mem (r3 is already at its 10GB mem limit)   r4 -> 5 cpus 0GB mem   r11 -> 0 cpus 10GB mem   ...   r20 -> 0 cpus 10GB mem    Here, roles 3-20 receive memory only and cpu only offers. This gets further exacerbated if DRF chooses the same ordering between roles across cycles. ",Bug,Major,Resolved,"2018-05-19 00:00:44","2018-05-18 23:00:44",3
"Apache Mesos","Quota guarantee metric does not handle removal correctly.","The quota guarantee metric is not removed when the quota gets removed:  https://github.com/apache/mesos/blob/1.6.0/src/master/allocator/mesos/metrics.cpp#L165-L174    The consequence of this is that the metric will hold the initial value that gets set and all subsequent removes / sets will not be exposed via the metric.",Bug,Major,Resolved,"2018-05-17 23:04:20","2018-05-17 22:04:20",2
"Apache Mesos","Refactor the libprocess gRPC warpper.","Refactor {{process::grpc::client::Runtime}} for better naming and interface, and fewer synchronizations.",Improvement,Major,Resolved,"2018-05-16 19:43:32","2018-05-16 18:43:32",5
"Apache Mesos","Autotools don't work with newer OpenJDK versions","There are three distinct issues with modern Java and Linux versions:    1. Mesos configure script expects `libjvm.so` at `$JAVA_HOME/jre/lib/<arch>/server/libjvm.so`, but in the newer openjdk versions, `libjvm.so` is found at `$JAVA_HOME/lib/server/libjvm.so`.    2. On some distros (e.g., Ubuntu 18.04), JAVA_HOME env var might be missing. In such cases, the configure is able to compute it by looking at `java` and `javac` paths and succeeds. However, some maven plugins require JAVA_HOME to be set and could fail if it's not found.    Because configure scripts generate an automake variable `JAVA_HOME`, we can simply invoke maven in the following way to fix this issue:     These two behaviors were observed with OpenJDK 1.11 on Ubuntu 18.04 but I suspect that the behavior is present on other distros/OpenJDK versions.    3. `javah` has been removed as of OpenJDK 1.10. Instead `javac -h` is to be used as a replacement. See [http://openjdk.java.net/jeps/313] for more details.",Bug,Major,Resolved,"2018-05-16 06:23:25","2018-05-16 05:23:25",3
"Apache Mesos","Per Framework SUBSCRIBE metrics.","Per Framework SUBSCRIBE metrics.",Improvement,Major,Resolved,"2018-05-16 01:32:45","2018-05-16 00:32:45",2
"Apache Mesos","Agent leaking file descriptors into forked processes","If not all file descriptors are carefully {{open}}'ed with {{O_CLOEXEC}} the Mesos agent might leak them into forked processes e.g., executors. This presents a potential security issue as such processes can interfere with the agent.    The current approach is to fix all invocations of {{open}} to always set {{O_CLOEXEC}}, but this approach breaks down when using 3rdparty libraries as there is no reliable way to patch unbundled dependencies.    It seems a more reliable approach would be to {{close}} all but a whitelisted set of file descriptors when after {{fork}}, but before the {{exec*}}. It should be possible to assemble such a whitelist for the typical use cases (e.g., in for the Mesos containerizer's  {{launch}}) and pass it to a modified functions to start subprocess. We might need to audit uses of raw {{fork}} in the code.",Bug,Major,Resolved,"2018-05-15 12:51:51","2018-05-15 11:51:51",3
"Apache Mesos","Allocation logic cleanup.","The allocation logic has grown organically and is now very hard to read and maintain. This epic will track cleanups to improve the readability of the core allocation logic:    * Add a function for returning the subset of frameworks that are capable of receiving offers from the agent. This moves the capability checking out of the core allocation logic and means the loops can just iterate over a smaller set of framework candidates rather than having to write 'continue' cases. This covers the GPU_RESOURCES and REGION_AWARE capabilities.  * Similarly, add a function that allows framework capability based filtering of resources. This pulls out the filtering logic from the core allocation logic and instead the core allocation logic can just all out to the capability filtering function. This covers the SHARED_RESOURCES, REVOCABLE_RESOURCES and RESERVATION_REFINEMENT capabilities. Note that in order to implement this one, we must refactor the shared resources logic in order to have the resource generation occur regardless of the framework capability (followed by getting filtered out via this new function if the framework is not capable).  * Update the scalar quantity related functions to also strip static reservation metadata. Currently there is extra code in the allocator across many places (including the allocation logic) to perform this in the call-sites.  * Track across allocation cycles or pull out the following into functions: quantity of quota that is currently charged to a role, amount of headroom that is needed/available for unsatisfied quota guarantees.  * Pull out the resource shrinking function.",Epic,Major,Resolved,"2018-05-15 01:21:03","2018-05-15 00:21:03",5
"Apache Mesos","Per Framework terminal task state metrics","Counter metriss about number of tasks that reached terminal states (FINISHED, FAILED etc.)    These counter metrics will have granularity of task states and reasons (i.e., number of tasks that are FINISHED due to REASON `foo` from SOURCE `master`).",Task,Major,Resolved,"2018-05-14 18:43:07","2018-05-14 17:43:07",2
"Apache Mesos","Add framework metrics benchmark test.","Add framework metrics benchmark test.",Improvement,Major,Resolved,"2018-05-14 17:28:21","2018-05-14 16:28:21",3
"Apache Mesos","Docker image fetcher fails with HTTP/2.","    Note that curl is saying the HTTP version is HTTP/2. This happens on modern curl that automatically negotiates HTTP/2, but the docker fetcher isn't prepared to parse that.    ",Bug,Major,Resolved,"2018-05-11 05:08:36","2018-05-11 04:08:36",3
"Apache Mesos","`UriDiskProfileAdaptor` fails to update profile selectors.","The {{UriDiskProfileAdaptor}} ignores the polled profile matrix if the polled one has the same size as the current one: https://github.com/apache/mesos/blob/1.5.x/src/resource_provider/storage/uri_disk_profile.cpp#L282-L286  {code:cxx}    // Profiles can only be added, so if the parsed data is the same size,    // nothing has changed and no notifications need to be sent.    if (parsed.profile_matrix().size() <= profileMatrix.size()) {      return;    }  {code}  However, this prevents the profile selector from being updated, which is not the desired behavior.",Bug,Major,Resolved,"2018-05-11 04:23:10","2018-05-11 03:23:10",2
"Apache Mesos","Update the Python CLI to use Python 3",,Task,Major,Resolved,"2018-05-10 18:48:49","2018-05-10 17:48:49",5
"Apache Mesos","MasterSlaveReconciliationTest.ReconcileDroppedOperation is flaky","This was observed on a Debian 9 SSL/GRPC-enabled build. It appears that a poorly-timed {{UpdateSlaveMessage}} leads to the operation reconciliation occurring before the expectation for the {{ReconcileOperationsMessage}} is registered:      Full log is attached as {{MasterSlaveReconciliationTest.ReconcileDroppedOperation.txt}}.",Bug,Major,Resolved,"2018-05-08 03:16:44","2018-05-08 02:16:44",1
"Apache Mesos","Improve debuggability of `MockResourceProvider::send()`.","{{MockResourceProvider::send}} uses {{Driver::send}} to send HTTP requests for v1 API calls, which returns a failure if the request is failed. However, we haven't established a programming pattern to reflect the failures properly in tests, and it makes debugging new tests hard.    To improve the debuggability of the mock resource provider, we could keep track of all futures returned by {{Driver::send}}, and await for them to become ready before tearing down the mock resource provider.    NOTE: At a first look, we could improve the debuggability by registering the following callback:  {code:java}  process::Future<Nothing> send(const Call& call)  {    return driver->send(call)      .onAny([](const process::Future<Nothing>& future) {        ASSERT_TRUE(future.isReady()) << future;      });  }  {code}  However, this could potentially introduce some test flakiness, as if the test is torn down before the driver process the HTTP response, the above future will fail due to disconnection when the the 202 Accept HTTP response is in transmission (so this cannot be fixed by simply adding a {{Clock::settle}} call:  ",Improvement,Major,Open,"2018-05-07 21:26:55","2018-05-07 20:26:55",2
"Apache Mesos","Add minimum capabilities in the master.","See Epic MESOS-8878 for motivation.    Add a new `minimum_capabilities` field in the master registry.  Upon recovery, the master should check the minimum capabilities record against its own capability and refuse to boot if it lacks any minimum capability.    We probably want to backport this change to previous versions.",Improvement,Major,Resolved,"2018-05-04 01:42:20","2018-05-04 00:42:20",3
"Apache Mesos","Docker container's resources will be wrongly enlarged in cgroups after agent recovery","Reproduce steps:    1. Run `mesos-execute --master=10.0.49.2:5050 --task=[file:///home/qzhang/workspace/config/task_docker.json] --checkpoint=true` to launch a Docker container.    2. When the Docker container is running, we can see its resources in cgroups are correctly set, so far so good.    3. Restart Mesos agent, and then we will see the resources of the Docker container will be wrongly enlarged.  ",Bug,Critical,Resolved,"2018-05-03 10:25:53","2018-05-03 09:25:53",3
"Apache Mesos","Normal exit of Docker container using rexray volume results in TASK_FAILED.","In the fix to  MESOS-8488, we reap the Docker container process directly in Docker executor, and it will wait for `docker run` to return for at most 3 seconds. However, in some cases, the `docker run` command will indeed need more than 3 seconds to return, e.g., the Docker container uses an external rexray volume (see the attached task json as an example), for such container, there will be about 5 seconds between container process exits and the `docker run` returns (I suspect Docker daemon was doing some stuff related to rexray volume during this time), so we will reap this container, and send a {{TASK_FAILED}}.",Bug,Major,Resolved,"2018-05-03 05:20:26","2018-05-03 04:20:26",2
"Apache Mesos","ResourceProviderManagerHttpApiTest.ResubscribeResourceProvider is flaky.","This test is flaky on CI:      This is different from https://issues.apache.org/jira/browse/MESOS-8315.",Bug,Major,Resolved,"2018-05-03 01:14:11","2018-05-03 00:14:11",1
"Apache Mesos","StorageLocalResourceProviderTest.ROOT_ZeroSizedDisk is flaky.","This test is flaky on CI:  ",Bug,Major,Resolved,"2018-05-03 01:08:49","2018-05-03 00:08:49",2
"Apache Mesos","OperationReconciliationTest.AgentPendingOperationAfterMasterFailover is flaky.","This test is flaky on CI:  ",Bug,Major,Resolved,"2018-05-03 01:04:54","2018-05-03 00:04:54",1
"Apache Mesos","Agent may fail to recover if the agent dies before image store cache checkpointed.","    This may happen if the agent dies after the file is created but before the contents are persisted on disk.",Bug,Major,Resolved,"2018-05-02 17:58:28","2018-05-02 16:58:28",3
"Apache Mesos","Master does not correctly reconcile dropped operations after agent failover","When an operation does not reach the agent before an agent failover, the master currently does not detect the dropped operation when the agent reregisters and sends the list of its operation in an {{UpdateSlaveMessage}}.",Bug,Blocker,Resolved,"2018-05-02 15:34:10","2018-05-02 14:34:10",2
"Apache Mesos","CMake builds are missing byproduct declaration for jemalloc.","The {{jemalloc}} dependency is missing a byproduct declaration in the CMake configuration. As a result, building Mesos with enabled {{jemalloc}} using CMake and Ninja will fail.",Bug,Minor,Resolved,"2018-05-02 11:31:12","2018-05-02 10:31:12",1
"Apache Mesos","Suspicious enum value comparisons in scheduler Java bindings","Clang reports suspicious comparisons of enum values in the scheduler Java bindings,      While the current implementation might just work since the different enum values might by accident map onto the same integer values (needs to be confirmed), this seems brittle and against the type safety the languages offers. We should fix this code.",Bug,Major,Resolved,"2018-05-02 09:58:29","2018-05-02 08:58:29",1
"Apache Mesos","Introduce a push-based gauge.","Currently, we only have pull-based gauges which have significant performance downsides.    A push-based gauge differs from a pull-based gauge in that the client is responsible for pushing the latest value into the gauge whenever it changes. This can be challenging in some cases as it requires the client to have a good handle on when the gauge value changes (rather than just computing the current value when asked).    It is highly recommended to use push-based gauges if possible as they provide significant performance benefits over pull-based gauges. Pull-based gauge suffer from delays getting processed on the event queue of a Process, as well as incur computation cost on the Process each time the metrics are collected. Push-based gauges, on the other hand, incur no cost to the owning Process when metrics are collected, and instead incur a trivial cost when the Process pushes new values in.",Improvement,Major,Resolved,"2018-04-26 22:47:56","2018-04-26 21:47:56",3
"Apache Mesos","Per Framework resource allocation metrics","We should provide greater visibility into the allocator state for each framework.    Metrics could include:  * Max/min DRF position of the framework during last allocation cycle  * Number of times the framework has had resources filtered during allocation  * The distribution of 'refuse_seconds' durations provided in decline calls",Task,Major,Resolved,"2018-04-26 18:51:17","2018-04-26 17:51:17",8
"Apache Mesos","Per Framework Offer metrics","Metrics regarding number of offers (sent, accepted, declined, rescinded) on a per framework basis.",Task,Major,Resolved,"2018-04-26 18:49:06","2018-04-26 17:49:06",2
"Apache Mesos","Per Framework task state metrics","Gauge metrics about current number of tasks in active states (RUNNING, STAGING etc).",Task,Major,Resolved,"2018-04-26 18:48:08","2018-04-26 17:48:08",3
"Apache Mesos","Per Framework Operation metrics","Metris for number of operations sent via ACCEPT calls by framework.",Task,Major,Resolved,"2018-04-26 18:44:51","2018-04-26 17:44:51",2
"Apache Mesos","Per Framework EVENT metrics","Metrics for number of events sent by the master to the framework.",Task,Major,Resolved,"2018-04-26 18:43:56","2018-04-26 17:43:56",2
"Apache Mesos","Per Framework CALL metrics","Metrics about number of different kinds of calls sent by a framework to master.",Task,Major,Resolved,"2018-04-26 18:43:15","2018-04-26 17:43:15",2
"Apache Mesos","Flaky `MasterAllocatorTest/0.SingleFramework`","      ",Bug,Major,Resolved,"2018-04-26 14:31:55","2018-04-26 13:31:55",2
"Apache Mesos","`cpu.cfs_quota_us` may be accidentally set for command task using docker during agent recovery.","Prior to Mesos 1.3, docker containerizer does not honor the flag `–cgroups_enable_cfs` for command task when creating the container, a patch ported this flag to docker command executor only up to 1.3 (MESOS-6134)     However, docker containerizer honors the flag when updating containers:    https://github.com/apache/mesos/blob/7559c9352c78912526820f6222ed2b17ad3b19cf/src/slave/containerizer/docker.cpp#L1726    For non-command tasks, docker containerizer always `update` the resources during launch:    https://github.com/apache/mesos/blob/7559c9352c78912526820f6222ed2b17ad3b19cf/src/slave/containerizer/docker.cpp#L1325-L1330    For command tasks, it is not the case:    https://github.com/apache/mesos/blob/7559c9352c78912526820f6222ed2b17ad3b19cf/src/slave/containerizer/docker.cpp#L1271-L1277    However, when recovering the executor, `update` is called for both command and non-command tasks.    This means, for command task, the cpu cgroup cfs settings would change when a command executor is recovered. Specifically, recovered command executors will have cfs set while all other command executors will not. This may lead to a drastic change in the container resource usage depending on the system load.    To maintain backward compatibility, we probably want to avoid setting the `cpu.cfs_quota_us` field in `update` if the field is not already set.  ",Bug,Critical,Accepted,"2018-04-25 20:29:25","2018-04-25 19:29:25",5
"Apache Mesos","Resource provider manager registrar recovery can race with agent on agent state leading to hard failures","When running in the agent the resource provider manager persists its state into the agent's state. The agent uses a LevelDB state which protects against concurrent access. The way we modelled LevelDB an {{fetch}} when a lock is present leads to a failed {{Future}} result. When the resource provider manager encounters a failed recovery it emits a fatal error, e.g.,    We should not fail hard for such recoverable failure scenarios.",Bug,Major,Resolved,"2018-04-25 15:00:46","2018-04-25 14:00:46",3
"Apache Mesos","Consider validating that resubscribing resource providers do not change their name or type","The agent currently uses a resource provider's name and type to construct e.g., paths for persisting resource provider state and their recovery. With that we should likely prevent resource providers from changing that information since we might otherwise be unable to recover them successfully.",Bug,Major,Resolved,"2018-04-25 12:15:34","2018-04-25 11:15:34",2
"Apache Mesos","Add test of resource provider manager recovery",See https://reviews.apache.org/r/66546/.,Task,Major,Resolved,"2018-04-25 11:54:37","2018-04-25 10:54:37",1
"Apache Mesos","Add a tests of recovery of the resource provider manager registrars.",,Bug,Major,Resolved,"2018-04-25 11:53:02","2018-04-25 10:53:02",2
"Apache Mesos","mesos-tests takes a long time to execute no tests","Executing {{mesos-tests}} takes substantially more time than running {{stout-tests}} or {{libprocess-tests}} when no tests are executed, e.g., for a release build with debug symbols        Looking at where time is spent with {{perf}} points to two {{parameters}} functions in {{src/tests/resources_tests.cpp}}. These functions construct large collections of {{Resource}} when registering {{Resources_Contains_BENCHMARK_Test}} and {{Resources_Scalar_Arithmetic_BENCHMARK_Test}}, i.e., these functions will be executed even if the corresponding test is filtered out.",Bug,Minor,Resolved,"2018-04-25 10:30:29","2018-04-25 09:30:29",1
"Apache Mesos","Get rid of extra `containerizer->wait()` calls in tests.","Since both `wait()` and `destroy()` return the same result, we can get rid of extra `containerizer->wait()` call in tests. E.g [here|https://github.com/apache/mesos/blob/c662048ae365630e3249b51102c9f7f962cc24d3/src/tests/slave_recovery_tests.cpp#L2292-L2300] and [there|https://github.com/apache/mesos/blob/c662048ae365630e3249b51102c9f7f962cc24d3/src/tests/cluster.cpp#L654-L668] as well as in some other places.",Improvement,Major,Resolved,"2018-04-24 16:27:33","2018-04-24 15:27:33",3
"Apache Mesos","Remove storage pools associated with missing profiles.","When a profile no longer exists, the storage pool associated with it should be removed.",Task,Major,Resolved,"2018-04-24 02:59:01","2018-04-24 01:59:01",5
"Apache Mesos","mesos.pom file hardcodes developers","Currently {{src/java/mesos.pom.in}} hardcodes developers. The information there duplicates {{docs/comitters.md}} and is currently likely outdated and will get out of sync again in the future.    It seems we should either automatically populate this field during the release process or drop this field without replacement. We already point to the dev mailing list which can be used to reach Mesos developers.",Task,Minor,Resolved,"2018-04-23 12:39:13","2018-04-23 11:39:13",1
"Apache Mesos","VolumeSandboxPathIsolatorTest.SharedParentTypeVolume fails on macOS","This test fails on macOS with:      Likely a regression introduced in commit {{189efed864ca2455674b0790d6be4a73c820afd6}} which removed {{volume/sandbox_path}} for POSIX.",Bug,Major,Resolved,"2018-04-23 10:07:47","2018-04-23 09:07:47",2
"Apache Mesos","Mount the volume based on `Volume.mode`.","See [design doc|https://docs.google.com/document/d/1QyeDDX4Zr9E-0jKMoPTzsGE-v4KWwjmnCR0l8V4Tq2U/edit#heading=h.kck8nfvxr80w] for why we need to do this.",Task,Major,Resolved,"2018-04-20 14:09:13","2018-04-20 13:09:13",5
"Apache Mesos","Support multiple tasks with different users can access a persistent volume.","See [design doc|https://docs.google.com/document/d/1QyeDDX4Zr9E-0jKMoPTzsGE-v4KWwjmnCR0l8V4Tq2U/edit#heading=h.f4x59l41lxwx] for why we need to do this.",Task,Major,Resolved,"2018-04-20 14:04:16","2018-04-20 13:04:16",8
"Apache Mesos","Grant non-root task user the permissions to access the DOCKER_VOLUME volume","See [design doc|https://docs.google.com/document/d/1QyeDDX4Zr9E-0jKMoPTzsGE-v4KWwjmnCR0l8V4Tq2U/edit#heading=h.e6p985n775m] for why we need to do this.",Task,Major,Reviewable,"2018-04-20 14:01:05","2018-04-20 13:01:05",3
"Apache Mesos","Grant non-root task user the permissions to access the image volume","See [design doc|https://docs.google.com/document/d/1QyeDDX4Zr9E-0jKMoPTzsGE-v4KWwjmnCR0l8V4Tq2U/edit#heading=h.s78760cmtdz6] for why we need to do this.",Task,Major,Reviewable,"2018-04-20 08:57:40","2018-04-20 07:57:40",3
"Apache Mesos","Grant non-root task user the permissions to access the SANDBOX_PATH volume of PARENT type","See [design doc|https://docs.google.com/document/d/1QyeDDX4Zr9E-0jKMoPTzsGE-v4KWwjmnCR0l8V4Tq2U/edit#heading=h.s6f8rmu65g2p] for why we need to do this.",Task,Major,Resolved,"2018-04-20 08:54:47","2018-04-20 07:54:47",13
"Apache Mesos","Add functions for manipulating POSIX ACLs into stout","We need to add functions for setting/getting POSIX ACLs into stout so that we can leverage these functions to grant volume permissions to the specific task user.    This will introduce a new dependency {{libacl-devel}} when building Mesos.",Task,Major,Resolved,"2018-04-20 08:43:11","2018-04-20 07:43:11",3
"Apache Mesos","Make remaining quota to offer event-driven in the allocator.","Currently, in the allocator, role consumed quota info is built up from scratch at the beginning of each allocation iteration. This affects performance and increases code complexity. We should be able to track and persist this info as we make new allocations.",Improvement,Major,Accepted,"2018-04-18 19:21:50","2018-04-18 18:21:50",3
"Apache Mesos","Add jemalloc as optional third-party memory allocator","As seen MESOS-7876, using jemalloc over the default memory allocator can have performance benefits.         Additionally, this is also supports the use case of MESOS-7944 by providing an out-of-the-box option to enable memory profiling. (which is also the ticket referenced in the mailing list discussion about this)",Improvement,Major,Resolved,"2018-04-18 16:34:32","2018-04-18 15:34:32",3
"Apache Mesos","Master should show dynamic resources in state endpoint","The master currently only shows static agent resources, i.e., resources defined in the agent's {{SlaveInfo}} in its state endpoint. We should fix this code to show the dynamic resources so that at least resource provider resources are shown. We might need to filter out oversubscribed resources for backward-compatibility.",Task,Major,Resolved,"2018-04-17 22:27:07","2018-04-17 21:27:07",2
"Apache Mesos","Build the unsecure gRPC libraries to remove SSL dependency.","GRPC can be built without SSL (the unsecure libraries) so we should use these libraries to avoid a build dependency between gRPC and OpenSSL.",Improvement,Critical,Resolved,"2018-04-17 18:28:37","2018-04-17 17:28:37",3
"Apache Mesos","Catch up new CLI features to be the same as the old one.",https://github.com/apache/mesos/tree/master/src/cli,Task,Major,Resolved,"2018-04-17 10:49:13","2018-04-17 09:49:13",8
"Apache Mesos","Support docker image tarball hdfs based fetching.","Support docker image tarball hdfs based fetching.",Task,Blocker,Resolved,"2018-04-17 06:28:00","2018-04-17 05:28:00",13
"Apache Mesos","Deprecate Role::resources in favor of Role::allocated and Role::offered.","There are upcoming enhancements around role related resource accounting. The changes will add a more detailed role related resources accounting.     We need to retire the {{resources}} member of the {{Role}} Message in mesos.proto (V0 + V1). This in turn means that we follow this deprecation on the role-related endpoints as well, adding {{allocated}} to both /roles as well as GET_ROLES.  ",Improvement,Minor,Accepted,"2018-04-16 03:21:53","2018-04-16 02:21:53",2
"Apache Mesos","/roles and webui roles table should display distinct offered and allocated resources.","The role endpoints currently show accumulated values for resources (allocated), containing offered resources. For gaining an overview showing our allocated resources separately from the offered resources could improve the signal quality, depending on the use case.    This also affects the UI display, for example the Roles tab.  ",Improvement,Major,Resolved,"2018-04-15 00:17:53","2018-04-14 23:17:53",3
"Apache Mesos","RP-related API should be experimental.","The new offer operations and resource provider API introduced in Mesos 1.5.0 should be marked as experimental.",Bug,Blocker,Resolved,"2018-04-14 02:02:52","2018-04-14 01:02:52",1
"Apache Mesos","CgroupIsolatorProcess accesses subsystem processes directly.","The {{CgroupsIsolatorProcess}} interacts with the different cgroups subsystems via {{Processes}} dealing with a dedicated subsystem each. Each {{Process}} is held by {{CgroupsIsolatorProcess}} directly and e.g., no intermediate wrapper class is involved performing {{dispatch}} to an underlying process.    Since no wrapper around these {{Subsystem}} processes is used, a user needs to make sure to only {{dispatch}} to the process himself, he should e.g., never directly invoke functions on the {{Process}} or else inconsistencies or races can arise inside the {{Subsystem}} process; if e.g., a {{Subsystem}} dispatches to itself, {{CgroupsIsolatorProcess}} might concurrently invoke {{Subsystem}} functions.      {{CgroupsIsolatorProcess}} does not always {{dispatch}} to these process, but invokes them directly. We should fix this by either introducing wrappers around the {{Subsystem}} wrappers, or by explicitly fixing {{CgroupsIsolatorProcess}} to always use {{dispatch}} to interact with its subsystems. While the first approach seems cleaner and more future-proof, the latter might be less effort _now_.",Bug,Major,Resolved,"2018-04-13 14:02:13","2018-04-13 13:02:13",5
"Apache Mesos","OPERATION_DROPPED operation status updates should include the operation/framework IDs","The agent should include the operation/framework IDs in operation status updates sent in response to a reconciliation request from the master. These status updates have the operation status: {{OPERATION_DROPPED}}.",Bug,Blocker,Resolved,"2018-04-13 00:41:30","2018-04-12 23:41:30",3
"Apache Mesos","Transition pending operations to OPERATION_UNREACHABLE when an agent is removed.","Pending operations on an agent should be transitioned to `OPERATION_UNREACHABLE` when an agent is marked unreachable. We should also make sure that we pro-actively send operation status updates for these operations when the agent becomes unreachable.    We should also make sure that we send new operation updates if/when the agent reconnects - perhaps this is already accomplished with the existing operation update logic in the agent?",Bug,Critical,Resolved,"2018-04-13 00:39:36","2018-04-12 23:39:36",3
"Apache Mesos","Transition operations to OPERATION_GONE_BY_OPERATOR when marking an agent gone.","The master should transition operations to the state {{OPERATION_GONE_BY_OPERATOR}} when an agent is marked gone, sending an operation status update to the frameworks that created them.    We should also remove them from {{Master::frameworks}}.",Bug,Critical,Resolved,"2018-04-13 00:37:10","2018-04-12 23:37:10",3
"Apache Mesos","Mesos master shouldn't silently drop operations","We should make sure that all call places of {{void Master::drop(Framework*, const Offer::Operation&, const string&)}} send a status update if an operation ID was specified. OR we should make sure that they do NOT send one, and make that method send one.",Bug,Major,Resolved,"2018-04-13 00:34:33","2018-04-12 23:34:33",3
"Apache Mesos","Expose Check and HealthCheck information on Mesos HTTP endpoints.","Is the information about task health check definition not exposed on Mesos HTTP endpoints ({{/master/tasks}} or {{/slave/state}} ) for some specific reason? I'm working on integration with Hashicorp Consul and it would allow me to synchronize the definitions of health checks only by using HTTP API. If this information is not exposed by accident, I will gladly make a pull request.    This is related to both {{HealthCheck}} and {{CheckInfo}} in both {{v0}} and {{v1}} APIs.",Improvement,Minor,Accepted,"2018-04-12 14:10:50","2018-04-12 13:10:50",2
"Apache Mesos","Fatal error in `DRFSorter::unallocated()` in `SharedPersistentVolumeRescindOnDestroy` test.","      Observed this failure in internal CI for test  ",Bug,Minor,Open,"2018-04-12 12:27:44","2018-04-12 11:27:44",2
"Apache Mesos","Support `STAGE_UNSTAGE_VOLUME` CSI capability in SLRP","CSI v0.2 introduces a new `STAGE_UNSTAGE_VOLUME` node capability. If a plugin has this capability, SLRP needs to call `NodeStageVolume` before publishing a volume, and call `NodeUnstageVolume` after unpublishing a volume.",Task,Blocker,Resolved,"2018-04-12 02:45:41","2018-04-12 01:45:41",5
"Apache Mesos","Restructure Web UI to make updates simpler to do.","The controllers are currently all in a file called controllers.js and the webui has a very nested structure, both can be improved.",Improvement,Major,Open,"2018-04-11 16:19:17","2018-04-11 15:19:17",3
"Apache Mesos","Authenticate and authorize calls to the resource provider manager's API","The resource provider manager is exposed via an agent endpoint against which resource providers subscribe or perform other actions. We should authenticate and authorize any interactions there.    Since currently local resource providers run on agents who manages their lifetime it seems natural to extend the framework used for executor authentication to resource providers as well. The agent would then generate a secret token whenever a new resource provider is started and inject it into the resource providers it launches. Resource providers in turn would use this token when interacting with the manager API.",Task,Major,Resolved,"2018-04-11 12:35:13","2018-04-11 11:35:13",8
"Apache Mesos","Add slave recovery test for default executor.",,Improvement,Major,Resolved,"2018-04-11 00:30:54","2018-04-10 23:30:54",2
"Apache Mesos","Use Python3 for Mesos support scripts","Our Python scripts under {{support/}} currently implicitly assume that developers have a python2 environment as their primary Python installation.    We should consider updating these scripts so that they can be used with a python3 installation as well. There exist [some resources|http://python-future.org/overview.html#automatic-conversion-to-py2-3-compatible-code] on the web documenting best practices and tools for automatic rewrites which should get us a long way.",Task,Major,Resolved,"2018-04-10 16:01:39","2018-04-10 15:01:39",8
"Apache Mesos","Agent crashes when CNI config not defined","I was deploying an application through marathon in an integration test that looked like this:   * Mesos container (UCR)   * container network   * some network name specified    Given network name did not exist, I did not even passed CNI config to the agent.    After Mesos tried to deploy my task, the agent crashed because of missing CNI config.    ",Bug,Critical,Resolved,"2018-04-10 08:07:37","2018-04-10 07:07:37",1
"Apache Mesos","Make resource provider aware of workloads.","Since the {{NodePublishVolume}} CSI call is supposed to be called for each workload, SLRP it self should be aware of workloads. Potentially, we could have the following event in the resource provider API:    For SLRP or any local resource provider, a workload is a container, and SLRP can implement {{ApplyResourceUsage}} by checking if a resource is used by a new workload, and call {{NodeUnpublishVolume}} and {{NodePublishVolume}} accordingly.    For ERP, a workload can be a framework, so the resource provider can checkpoint which framework is using what resources and provide such information to the allocator after a failover.    Note that the {{ApplyResourceUsage}} call should report *all* resources being used on an agent, so it can handle resources without identifiers (such as cpus, mem) correctly.",Task,Major,Resolved,"2018-04-05 00:14:44","2018-04-04 23:14:44",3
"Apache Mesos","CSI proto is always included in the build when using CMake","cmake ../mesos  make LDFLAGS='-lexecinfo -lm'    ",Bug,Minor,Resolved,"2018-03-29 23:05:10","2018-03-29 22:05:10",1
"Apache Mesos","Create ACL for grow and shrink volume","As follow up work of MESOS-4965, we should make sure new operations are properly protected in ACL and authorizer.",Task,Blocker,Resolved,"2018-03-28 19:46:39","2018-03-28 18:46:39",3
"Apache Mesos","Support resizing persistent volume through operator API","MESOS-4965 tracks the implementation through framework offer operation, while this task extends the support to operator API.",Task,Blocker,Resolved,"2018-03-28 19:30:59","2018-03-28 18:30:59",5
"Apache Mesos","Expose inactive resource providers through the `GET_RESOURCE_PROVIDERS` agent call.","Once a resource provider config is placed through the {{ADD_RESOURCE_PROVIDER_CONFIG}} or {{UPDATE_RESOURCE_PROVIDER_CONFIG}} call, there is no way to tell if the resource provider fails to start. This makes it very hard for users to debug misconfigured resource providers, as they will need to dig into the agent log to figure out the error. We should report resource providers that are not registered yet in the {{GET_RESOURCE_PROVIDERS}} call, with the current statuses and error messages, to surface any error that fails a resource provider.",Improvement,Major,Open,"2018-03-28 05:17:52","2018-03-28 04:17:52",2
"Apache Mesos","Agent resource provider config API calls should be idempotent.","There are some issues w.r.t. using the current agent resource provider config API calls:    1. {{UPDATE_RESOURCE_PROVIDER_CONFIG}}: If the caller fail to receive the HTTP response code, there is no way to retry the operation without triggering an RP restart.  2. {{REMOVE_RESOURCE_PROVIDER_CONFIG}}: If the caller fail to receive the HTTP response code, a retry will return a 404 Not Found. But due to MESOS-7697, there is no way for the caller to know if the 404 is due to a previous successful config removal or not.    To address these issues, we should make these calls idempotent, such that they return 200 OK when the caller retry. It would be nice if {{ADD_RESOURCE_PROVIDER_CONFIG}} is also idempotent for consistency.",Bug,Blocker,Resolved,"2018-03-28 00:26:19","2018-03-27 23:26:19",2
"Apache Mesos","`Add` to sequence will not run if it races with sequence destruction","Adding item to sequence is realized by dispatching `add()` to the sequence actor. However, this could race with the sequence actor destruction.:    After the dispatch but before the dispatched `add()` message gets processed by the sequence actor, if the sequence gets destroyed, a terminate message will be injected to the *head* of the message queue. This would result in the destruction of the sequence without the `add()` call ever gets processed. User would end up with a pending future and the future's `onDiscarded' would not be triggered during the sequence destruction.    The solution is to set the `inject` flag to `false` so that the terminating message is enqueued to the end of the sequence actor message queue. All `add()` messages that happen before the destruction will be processed before the terminating message.",Bug,Major,Resolved,"2018-03-27 21:30:39","2018-03-27 20:30:39",1
"Apache Mesos","Update description of a Containerizer interface.","[Containerizer interface|https://github.com/apache/mesos/blob/master/src/slave/containerizer/containerizer.hpp] must be updated with respect to the latest changes. In addition, it should clearly describe semantics of `wait()` and `destroy()` methods, including cases with a nested containers.",Documentation,Major,Resolved,"2018-03-27 19:05:02","2018-03-27 18:05:02",2
"Apache Mesos","Update composing containerizer tests.","Composing containerizer tests need to be updated after changing type and semantics of return value for `destroy()` method.",Task,Major,Resolved,"2018-03-27 18:02:04","2018-03-27 17:02:04",3
"Apache Mesos","Implement a test which ensures that `wait` and `destroy` return the same result for a terminated nested container.","This test verifies that both mesos and composing containerizers maintain the contract described in the Containerizer API regarding availability of a termination status for terminated nested containers.",Task,Major,Resolved,"2018-03-27 17:34:57","2018-03-27 16:34:57",3
"Apache Mesos","Implement recovery for resource provider manager registrar","In order to properly persist and recover resource provider information in the resource provider manager we should   # Include a registrar in the manager, and   # Implement missing recovery functionality in the registrar so it can return a recovered registry.",Task,Major,Resolved,"2018-03-27 16:32:48","2018-03-27 15:32:48",5
"Apache Mesos","Restore `WaitAfterDestroy` test to check termination status of a terminated nested container.","It's important to check that after termination of a nested container, its termination status is available. This property is used in default executor.  Note that the test uses Mesos c'zer and checks above-mentioned property only for Mesos c'zer.    Right now, if we remove [this section of code|https://github.com/apache/mesos/blob/5b655ce062ff55cdefed119d97ad923aeeb2efb5/src/slave/containerizer/mesos/containerizer.cpp#L2093-L2111], no test will be broken!    https://reviews.apache.org/r/65505",Task,Major,Resolved,"2018-03-27 16:28:48","2018-03-27 15:28:48",3
"Apache Mesos","OversubscriptionTest.ForwardUpdateSlaveMessage is flaky","Observed this failure in CI,  ",Bug,Major,Resolved,"2018-03-27 16:03:43","2018-03-27 15:03:43",1
"Apache Mesos","Use composing containerizer in some agent tests.","If we assign docker,mesos to the `containerizers` flag for an agent, then `ComposingContainerizer` will be used for many tests that do not specify `containerizers` flag. That's the goal of this task.    I tried to do that by adding [`flags.containerizers = docker,mesos;`|https://github.com/apache/mesos/blob/master/src/tests/mesos.cpp#L273], but it turned out that some tests are started to hang due to a paused clocks, while docker c'zer and docker library use libprocess clocks.",Task,Major,Resolved,"2018-03-27 13:58:58","2018-03-27 12:58:58",8
"Apache Mesos","Libprocess: deadlock in process::finalize","Since we are calling [`libprocess::finalize()`|https://github.com/apache/mesos/blob/02ebf9986ab5ce883a71df72e9e3392a3e37e40e/src/slave/containerizer/mesos/io/switchboard_main.cpp#L157] before returning from the IOSwitchboard's main function, we expect that all http responses are going to be sent back to clients before IOSwitchboard terminates. However, after [adding|https://reviews.apache.org/r/66147/] `libprocess::finalize()` we have seen that IOSwitchboard might get stuck in `libprocess::finalize()`. See attached stacktrace.",Bug,Major,Accepted,"2018-03-23 20:02:25","2018-03-23 20:02:25",5
"Apache Mesos","Don't print full usage for invocation errors","The current usage string for mesos-master comes in at 399 lines, and for mesos-agent at 685 lines.         Printing such a wall of text will overflow most terminal windows, making it necessary to scroll up to see the actual error when invoking mesos with an incorrect command line.",Improvement,Major,Resolved,"2018-03-23 16:28:29","2018-03-23 16:28:29",2
"Apache Mesos","G++ Warning about libc system macros `major` and `minor` prevents Mesos build","On linux systems, the header `<sys/sysmacros.h>` defines three macros called makedev(), major() and minor(). (See also [http://man7.org/linux/man-pages/man3/makedev.3.html])    Trying to compile Mesos using g++ 7.2.0 leads to the following warning:    The root cause is that csi.proto defines the following protobuf message:    The generated C++ in `csi.pb.h` headers will contain, amongst others, the following function:    And the recursive include structure of the header `<string>` leads to `stdlib.h` as follows:     ",Bug,Blocker,Resolved,"2018-03-22 15:45:34","2018-03-22 15:45:34",2
"Apache Mesos","Mesos configured with `--enable-grpc` doesn't compile on non-Linux builds","Commit {{59cca968e04dee069e0df2663733b6d6f55af0da}} added {{examples/test_csi_plugin.cpp}} to non-Linux builds that are configured using the {{--enable-grpc}} flag. As {{examples/test_csi_plugin.cpp}} includes {{fs/linux.hpp}}, it can only compile on Linux and needs to be disabled for non-Linux builds.",Bug,Major,Resolved,"2018-03-22 09:04:26","2018-03-22 09:04:26",2
"Apache Mesos","Add the fields `ExposedPorts` and `Volumes` into Docker v1 image spec","This ticket is to address the TODO below in the [docker/v1.proto|https://github.com/apache/mesos/blob/1.5.0/include/mesos/docker/v1.proto#L70:L71]:    And similar to the field `ExposedPorts` mentioned in the above TODO, we should also add the field `Volumes` which is also a string-message pair.    Once these two fields are added, we could consider to build features on top of them in the `docker/runtime` isolator.     ",Improvement,Major,Reviewable,"2018-03-22 08:20:22","2018-03-22 08:20:22",2
"Apache Mesos","Support CSI v0.2 in SLRP.","SLRP needs to be modified to talk to plugins using CSI v0.2.",Task,Blocker,Resolved,"2018-03-22 01:04:03","2018-03-22 01:04:03",5
"Apache Mesos","Consider removing conditional inclusion in the public header `csi/spec.hpp`.","Currently we conditionally include {{csi.grpc.pb.h}} in {{csi/spec.hpp}} based on the configuration config {{ENABLE_GRPC}}, which is not ideal since this makes the public header depends on an some-what internal configuration flag. We could consider one of the following approaches to remove such dependency:    1. Generate a blank {{csi.grpc.pb.h}} when gRPC is not enabled.  2. Split {{csi/spec.hpp}} into {{csi/messages.hpp}} and {{csi/services.hpp}}, and do the conditional inclusion of {{csi/services.hpp}} in the implementation files.  3. Only include {{csi.pb.h}} in {{csi/spec.hpp}} since Mesos is only publicly dependent on the proto messages. Have a {{src/csi/services.hpp}} to include {{csi.grpc.pb.h}}.  4. Remove this wrapper header file and directly include {{csi.pb.h}} and {{csi.grpc.pb.h}}.",Improvement,Minor,Resolved,"2018-03-21 19:12:38","2018-03-21 19:12:38",2
"Apache Mesos","Cleanup `containers_` hashmap once container exits","To clean up a `containers_` hash map in composing c'zer, we need to subscribe on a container termination event in `_launch` method. Also, it's desirable to limit the number of places where we do the clean up.",Task,Major,Resolved,"2018-03-21 18:57:51","2018-03-21 18:57:51",3
"Apache Mesos","Synchronize result of `wait` and `destroy` composing c'zer methods","Make sure both `wait` and `destroy` methods always return the same result.  For example, if we call `destroy` for a terminated nested container, then composing c'zer returns `false`/`None`, while `wait` method calls `wait` for a parent container, which might read a container termination status from the file. Probably, we need to implement a test for this case, if it doesn't exist.",Task,Major,Resolved,"2018-03-21 17:56:32","2018-03-21 17:56:32",3
"Apache Mesos","Remove `destroyed` promise from `Container` struct","[`destroyed` promise|https://github.com/apache/mesos/blob/5d8a9c1b77f96151da859b4c0c3607d22c36cd18/src/slave/containerizer/composing.cpp#L138] is not needed anymore, since we can use the property that `wait` and `destroy` methods depend on the same container termination promise. This change should affect only composing c'zer.",Task,Major,Resolved,"2018-03-21 17:26:54","2018-03-21 17:26:54",3
"Apache Mesos","SlaveTest.ChangeDomain is disabled.","This test has been disabled in https://github.com/apache/mesos/commit/c0468b240842d4aaf04249cb0a58c59c43d1850d. We should either fix or remove it.",Bug,Major,Resolved,"2018-03-21 16:14:39","2018-03-21 16:14:39",1
"Apache Mesos","Update tests after changing return type of `wait` method","Changing return type of `wait` methods requires corresponding changes in tests.  Here is a known list of test source files that need to be updated:  ",Task,Major,Resolved,"2018-03-21 15:50:02","2018-03-21 15:50:02",3
"Apache Mesos","Unify return type of `wait` and `destroy` containerizer methods","We want to unify return type of both `destroy` and `wait` methods for a containerizer, because they depend on the same container termination promise in our built-in mesos and docker containerizers. That gives us an opportunity to simplify launch and destroy logic in composing containerizer.    The return type of `destroy()` methods should be changed from:    to  ",Task,Major,Resolved,"2018-03-21 15:07:36","2018-03-21 15:07:36",5
"Apache Mesos","Replace the manual parsing in Mesos code with the native protobuf map support","In MESOS-7656, we have updated the JSON <=> protobuf message conversion in stout for map support which means we can use the native protobuf map feature now in Mesos code. So we should replace the manual parsing for the following fields with the native protobuf map.    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/docker/v1.proto#L65:L68]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L33:L36]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L61:L64]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L88:L91]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L107:L110]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L151:L154]    Please note, for [Appc image manifest|https://github.com/apache/mesos/blob/1.5.0/include/mesos/appc/spec.proto#L43], we also have a field {{repeated Label labels = 4}}, but we should not replace it with the native protobuf map support, because according to the [Appc image spec|https://github.com/appc/spec/blob/master/spec/aci.md#image-manifest-schema], this field is not a map, instead it is a list of objects.    And in {{mesos.proto}}, we also have a couple protobuf messages which have field like {{optional Labels labels = 10}}, .e.g. {{TaskInfo.labels}}, I would not suggest to replace them with native protobuf map since that would be an API changes which may break framework's code.",Improvement,Major,Resolved,"2018-03-21 02:29:28","2018-03-21 02:29:28",2
"Apache Mesos","Preparation for v2 API: v1 Tech Debt","This epic is meant to collect tech debt-related issues which we would like to address before/during the development of the v2 API. Some guidelines:   * Do not include v1's deprecated fields in v2.",Epic,Major,Open,"2018-03-20 16:36:36","2018-03-20 16:36:36",13
"Apache Mesos","Enable storage local resource provider in CMake.","We should be able to generate gRPC files for CSI and build storage local resource provider with CMake.",Task,Blocker,Resolved,"2018-03-19 23:58:13","2018-03-19 23:58:13",3
"Apache Mesos","Make gRPC-related tests cross-platform.","Since gRPC support in-process channels, we should use them in the related unit tests to make the tests cross-platform.",Task,Major,Resolved,"2018-03-19 23:36:38","2018-03-19 23:36:38",2
"Apache Mesos","Bump gRPC bundle to 1.10.0.","Bump gRPC to 1.10.0 for better CMake support and new header paths.",Task,Major,Resolved,"2018-03-15 17:31:42","2018-03-15 17:31:42",2
"Apache Mesos","Build CSI proto in CMake.","We should be able to build CSI proto with CMake.",Task,Major,Resolved,"2018-03-09 19:17:15","2018-03-09 19:17:15",3
"Apache Mesos","Improve stout JSON -> protobuf message conversion to handle more valid JSONs","The followings are valid in JSON which can be successfully parsed by Google's protobuf utility function {{JsonStringToMessage()}} but can NOT be parsed our stout, i.e., call {{JSON::parse()}} to convert the JSON to a stout JSON object and then call {{protobuf::parse()}} to parse the JSON object to a protobuf message.    The second step ({{protobuf::parse()}}) will fail with an error like this:    This error comes from {{Try<Nothing> operator()(const JSON::String& string)}} which currently can only convert {{JSON::String}} to protobuf string/bytes/enum, so we need to enhance it to be able to convert {{JSON::String}} to bools and integers.    What we can parse currently are:    We should make the behavior of our stout JSON -> protobuf conversion consistent with Google's.",Improvement,Major,Resolved,"2018-03-09 08:32:22","2018-03-09 08:32:22",2
"Apache Mesos","Make the CSI client to support CSI v0.2.","CSI v0.2 is incompatible with v0.1, thus we need to modify the CSI client to support the new CSI API. We may consider supporting both v0.1 and v0.2 in Mesos 1.6, or just deprecating v0.1.",Task,Blocker,Resolved,"2018-03-09 02:26:03","2018-03-09 02:26:03",2
"Apache Mesos","Bump CSI bundle to v0.2.","Upgrade CSI spec bundle in {{3rdparty/}}.",Task,Blocker,Resolved,"2018-03-09 01:57:32","2018-03-09 01:57:32",2
"Apache Mesos","Enable resource provider agent capability by default","In 1.5.0 we introduced a resource provider agent capability which e.g., enables a modified operation protocol. We should enable this capability by default.         If tests explicitly depend on the agent being fully operational, they should be adjusted for the modified protocol. It is e.g., not enough to wait for a {{dispatch}} to the agent's recovery method, but instead one should wait for a dedicated {{UpdateSlaveMessage}} from the agent.",Bug,Major,Resolved,"2018-03-08 16:26:28","2018-03-08 16:26:28",2
"Apache Mesos","ballon-executor is hard to run as unprivileged user",,Bug,Major,Resolved,"2018-03-06 13:07:19","2018-03-06 13:07:19",2
"Apache Mesos","Validate `DockerInfo` exists when container's type is `DOCKER`","Currently when framework launches a task whose ContainerInfo's type is DOCKER (i.e., Docker containerizer will be used to launch the container), we do not validate if the `DockerInfo` exists in the ContainerInfo, so such task will be sent from master to agent, and will eventually fail due to pulling image with empty name.    Actually we have a validation in [this code|https://github.com/apache/mesos/blob/1.5.0/src/docker/docker.cpp#L605:L607], but it is too late (i.e., when Docker executor tries to create the Docker container), we should do the validation much earlier, e.g., in master.",Improvement,Major,Resolved,"2018-03-06 02:04:31","2018-03-06 02:04:31",2
"Apache Mesos","The 'allocatable' check in the allocator is problematic with multi-role frameworks","The [allocatable|https://github.com/apache/mesos/blob/1.5.x/src/master/allocator/mesos/hierarchical.cpp#L2471-L2479] check in the allocator (shown below) was originally introduced to help alleviate the situation where a framework receives only disk, but not cpu/memory, thus cannot launch a task.        When we introduce multi-role capability to the frameworks, this check makes less sense now. For instance, consider the following case:  1) There is a single agent and a single framework in the cluster  2) The agent has cpu/memory reserved to role A, and disk reserved to B  3) The framework subscribes to both role A and role B  4) The framework expects that it'll receive an offer containing the resources on the agent  5) However, the framework receives no disk resources due to the following [code|https://github.com/apache/mesos/blob/1.5.x/src/master/allocator/mesos/hierarchical.cpp#L2078-L2100]. This is counter intuitive.        Two comments:  1) If `allocatable` check is still necessary (see MESOS-7398)?  2) If we want to keep `allocatable` check for the original purpose, we should do that based on framework not role, given that a framework can subscribe to multiple roles now?    Some related JIRAs:  MESOS-1688  MESOS-7398  ",Bug,Major,Resolved,"2018-03-02 00:49:22","2018-03-02 00:49:22",3
"Apache Mesos","Valid tasks may be explicitly dropped by agent due to race conditions","Tasks may be explicitly dropped by the agent if all the following conditions are met:  (1) Several `LAUNCH_TASK` or `LAUNCH_GROUP` calls use the same executor.  (2) The executor currently does not exist on the agent.  (3) Due to some race conditions, these tasks are trying to launch on the agent in a different order from their original launch order. (See below how this could happen)    In this case, tasks that are trying to launch on the agent before the first task in the original order will be explicitly dropped by the agent (TASK_DROPPED` or `TASK_LOST` will be sent)).     Up until now, Mesos does not guarantee in-order task launch on the agent. Let's say Mesos master sends two `launchTask` messages (launch Task1 and Task2) to an agent. In most cases (except MESOS-3870), these messages are delivered to the agent in order. However, currently, there are two asynchronous steps (unschedule GC and task authorization) in the agent task launch path. Depending on the CPU scheduling order, task2 launch may finish these two steps earlier than task1 and get to the launch executor stage before task1.    In this case, prior to MESOS-1720, these two tasks will still get launched. If task1 and task2 use the same executor, whoever reaches the launch executor stage first, will launch the executor.    However, after resolving MESOS-1720, agents start to enforce some order for tasks using the same executor. Specifically, when master crafts the launch task message, it will specify the `launch_executor` flag. Thus Task1 in the above case will have `launch_executor` flag set to true. And task2 (and any subsequent tasks that use the same executor) will have the flag set to false.    If task2 reaches the launch executor stage before task1 (due to the race condition described above), the agent will see that its `launch_executor ` is false but the executor specified in the `launchTask` message is not running. As a result, it will explicitly drop task2 as in:    https://github.com/apache/mesos/blob/32f6d4eec2724414e217875f4f7d3b2538db5381/src/slave/slave.cpp#L2888    Based on discussion with [~<USER> and [~<USER>,  we should take an explicit approach of using process:: Sequence to ensure ordered task delivery (on both the master and agent).",Bug,Blocker,Resolved,"2018-03-01 20:19:52","2018-03-01 20:19:52",8
"Apache Mesos","Add a `/debug` allocator endpoint to expose allocator state for debugging.","Currently it is hard to debug issues related to framework not being able to get offers. We could add a {{/debug}} endpoint that help the debugging easier, starting by adding the information about active offer filters. Since this endpoint is for debugging purpose, there might be no guarantee for backward compatibility in the future.",Improvement,Major,Accepted,"2018-02-28 00:42:27","2018-02-28 00:42:27",8
"Apache Mesos","Containers stuck in FETCHING possibly due to unresponsive server.","Two nested containers were launched and transitioned to FETCHING nearly at the same time, and tried to fetch the same artifacts. The first one failed to fetch some artifacts and transitioned to DESTROYING. However, the second nested container got stock in FETCHING and the LAUNCH_NESTED_CONTAINER call never returned.        After a closer look at the sandbox logs, these two containers bypassed the fetcher cache when downloading artifacts, and thus should not be interfering with each other. However, the log of the stuck container stopped at Downloading resource from .... This might indicate that the server (here it's Amazon S3) accepted the connection but never finished a HTTP response. To avoid containers being stuck in FETCHING, we should add a download timeout to abort the fetching when the download speed is too low.",Bug,Major,Resolved,"2018-02-28 00:31:08","2018-02-28 00:31:08",3
"Apache Mesos","Test `MasterAllocatorTest/*.TaskFinished` is flaky.","Occasionally the test would crash with the following logs:      This has been observed multiple times and every time the test crashed right after      Attached logs of 4 crash instances.",Bug,Major,Resolved,"2018-02-26 21:45:14","2018-02-26 21:45:14",2
"Apache Mesos","Terminal task status update will not send if 'docker inspect' is hung","When the agent processes a terminal status update for a task, it calls {{containerizer->update()}} on the container before it forwards the update: https://github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/slave.cpp#L5509-L5514    In the Docker containerizer, {{update()}} calls {{Docker::inspect()}}, which means that if the inspect call hangs, the terminal update will not be sent: https://github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/containerizer/docker.cpp#L1714",Bug,Major,Resolved,"2018-02-23 23:21:20","2018-02-23 23:21:20",3
"Apache Mesos","Quota headroom tracking may be incorrect in the presence of hierarchical reservation.","When calculating the global quota headroom, we subtract all unallocated reservations by doing        We only traverse roles with reservation. In the presence of hierarchal reservation, this is problematic. Consider a child role (e.g. a/b) with no reservations, it can still get reserved resources if its ancestor has reservations (e.g. a has reservations). However, allocated reserved resources of role “a/b” will be ignored given the above code.    The consequence is that availableHeadroom will be underestimated because allocated reservations are underestimated. This would lead to excessive resources set aside for quota headroom.",Bug,Major,Resolved,"2018-02-23 21:55:23","2018-02-23 21:55:23",2
"Apache Mesos","Master crashes during slave reregistration after failover.","The following happened after a master failover.  During slave reregistration, new tasks were added and the new leading master notified all of its subscribers, and triggered the following check failure:      This was because the master tried to get the framework info when sending the notification: https://github.com/apache/mesos/blob/1.5.x/src/master/master.cpp#L11190    But it added the framework after that:  https://github.com/apache/mesos/blob/1.5.x/src/master/master.cpp#L6963",Bug,Blocker,Resolved,"2018-02-22 19:07:43","2018-02-22 19:07:43",3
"Apache Mesos","Allow empty resource provider selector in `UriDiskProfileAdaptor`.","Currently in {{UriDiskProfileAdaptor}}, it is invalid for a profile to have a resource provider selector with 0 resource providers. However, one can put non-existent provider types and names into the selector to achieve the same effect, and this is semantically inconsistent. We should allow an empty list of resource providers directly.",Bug,Major,Resolved,"2018-02-20 18:40:29","2018-02-20 18:40:29",1
"Apache Mesos","Add Docker inspect timeout when collecting container statistics","In cases where the Docker daemon is hung, many processes calling {{docker inspect}} may accumulate due to repeated attempts to collect the container statistics of Docker containers. We should add a timeout to these {{Docker::inspect()}} calls to avoid this accumulation.",Improvement,Major,Accepted,"2018-02-20 17:30:46","2018-02-20 17:30:46",3
"Apache Mesos","Mesos master stack overflow in libprocess socket send loop.","Mesos master crashes under load. Attached are some infos from the `lldb`:    To quote [~<USER>  {quote}it’s the stack overflow bug in libprocess due to the way `internal::send()` and `internal::_send()` are implemented in `process.cpp`  {quote}",Bug,Blocker,Resolved,"2018-02-19 13:52:10","2018-02-19 13:52:10",1
"Apache Mesos","Avoid failure for invalid profile in `UriDiskProfileAdaptor`","We should be defensive and not fail the profile module when the user provides an invalid profile in the profile matrix.",Bug,Major,Resolved,"2018-02-17 00:51:01","2018-02-17 00:51:01",1
"Apache Mesos","Add infra to test a hung Docker daemon","We should add infrastructure to our tests which enables us to test the behavior of the Docker executor and containerizer in the presence of a hung Docker daemon.    One possible first-order solution is to build a simple binary which never returns. We could initialize the agent/executor with this binary instead of the Docker CLI in order to simulate a Docker daemon which hangs on every call.",Improvement,Major,Resolved,"2018-02-16 21:47:28","2018-02-16 21:47:28",3
"Apache Mesos","Pass FrameworkInfo to agents when applying operations","Currently an {{Operation}} only contains a {{FrameworkID}} of originating frameworks, but e.g., not the full {{FrameworkInfo}}. This is problematic in master failover scenarios where a master might learn about an operation triggered by a framework unknown to it. The way the master implementation is structured, we would like to create tracking structures for that framework (e.g., to sync with the allocator down the line), but cannot do so since we can only learn this information when either the framework reregisters, or an agent running tasks of that framework reconciles with the master. We also cannot use conjured uo dummy information until we  learn the true {{FrameworkInfo}} since some required fields in {{FrameworkInfo}} (namely {{FrameworkInfo.user}}) cannot be updated, see MESOS-703.    We should introduce a channel for agents to learn the full {{FrameworkInfo}} for all frameworks executing operations on its resources. For simplicity and symmetry with {{RunTaskMessage}} it seems that adding an explicit {{FrameworkInfo}} field to {{Operation}} would do the job (e.g., allow atomic information transfer when operations are sent to the agent or on reconciliation with newly elected masters.",Improvement,Major,Accepted,"2018-02-14 14:00:45","2018-02-14 14:00:45",3
"Apache Mesos","Destroy nested container if `LAUNCH_NESTED_CONTAINER_SESSION` fails","Currently, if `attachContainerOutput()` [fails|https://github.com/apache/mesos/blob/bc6b61bca37752689cffa40a14c53ad89f24e8fc/src/slave/http.cpp#L3550-L3552] for `LAUNCH_NESTED_CONTAINER_SESSION` call, then we return HTTP 500 error to the client, but we don't destroy the nested container.   However, we [destroy|https://github.com/apache/mesos/blob/bc6b61bca37752689cffa40a14c53ad89f24e8fc/src/slave/http.cpp#L3607-L3612] a nested container if `attachContainerOutput()` returns a failure.",Bug,Major,Resolved,"2018-02-13 14:21:36","2018-02-13 14:21:36",3
"Apache Mesos","Improve discard handling of 'Docker::inspect()'","In the call path of {{Docker::inspect()}}, each continuation currently checks if {{promise->future().hasDiscard()}}, where the {{promise}} is associated with the output of the {{docker inspect}} call. However, if the call to {{docker inspect}} becomes hung indefinitely, then continuations are never invoked, and a subsequent discard of the returned {{Future}} will have no effect. We should add proper {{onDiscard}} handling to that {{Future}} so that appropriate cleanup is performed in such cases.",Improvement,Major,Resolved,"2018-02-13 08:15:36","2018-02-13 08:15:36",3
"Apache Mesos","Improve discard handling for 'Docker::stop' and 'Docker::pull'.","The functions in the Docker library which issue Docker CLI commands should be updated so that when the {{Future}} they return is discarded, any subprocesses which have been spawned will be cleaned up.",Improvement,Major,Resolved,"2018-02-13 08:11:11","2018-02-13 08:11:11",3
"Apache Mesos","Docker executor makes no progress when 'docker inspect' hangs","In the Docker executor, many calls later in the executor's lifecycle are gated on an initial {{docker inspect}} call returning: https://github.com/apache/mesos/blob/bc6b61bca37752689cffa40a14c53ad89f24e8fc/src/docker/executor.cpp#L223    If that first call to {{docker inspect}} never returns, the executor becomes stuck in a state where it makes no progress and cannot be killed.    It's tempting for the executor to simply commit suicide after a timeout, but we must be careful of the case in which the executor's Docker container is actually running successfully, but the Docker daemon is unresponsive. In such a case, we do not want to send TASK_FAILED or TASK_KILLED if the task's container is running successfully.",Improvement,Major,Resolved,"2018-02-13 08:01:01","2018-02-13 08:01:01",5
"Apache Mesos","Container stuck in PULLING when Docker daemon hangs","When the {{force}} argument is not set to {{true}}, {{Docker::pull}} will always perform a {{docker inspect}} call before it does a {{docker pull}}. If either of these two Docker CLI calls hangs indefinitely, the Docker container will be stuck in the PULLING state. This means that we make no further progress in the {{launch()}} call path, so the executor binary is never executed, the {{Future}} associated with the {{launch()}} call is never failed or satisfied, and {{wait()}} is never called on the container. The agent chains the executor cleanup onto that {{wait()}} call which is never made. So, when the executor registration timeout elapses, {{containerizer->destroy()}} is called on the executor container, but the rest of the executor cleanup is never performed, and no terminal task status update is sent.    This leaves the task destined for that Docker executor stuck in TASK_STAGING from the framework's perspective, and attempts to kill the task will fail.",Improvement,Major,Resolved,"2018-02-13 07:55:34","2018-02-13 07:55:34",3
"Apache Mesos","Make Docker executor/containerizer resilient to Docker daemon failures.","Experience has shown that the Docker CLI can hang indefinitely at times. There are many variations of this behavior, and it occurs across many versions of Docker. For these reasons, and since many users of Mesos still make heavy use of the Docker containerizer and the Docker executor, it will improve the user experience to make the Docker containerizer/executor resilient to such Docker daemon failures.",Epic,Major,Accepted,"2018-02-13 07:19:35","2018-02-13 07:19:35",13
"Apache Mesos","Allow newline characters when decoding base64 strings in stout.","Current implementation of `stout::base64::decode` errors out on encountering a newline character (\n or \r\n) which is correct wrt [RFC4668#section-3.3|https://tools.ietf.org/html/rfc4648#section-3.3]. However, most implementations insert a newline to delimit encoded string and ignore (instead of erroring out) the newline character while decoding the string. Since stout facilities are used by third-party modules to encode/decode base64 data, it is desirable to allow decoding of newline-delimited data.",Task,Major,Resolved,"2018-02-12 22:06:58","2018-02-12 22:06:58",2
"Apache Mesos","Command checks should always call `WAIT_NESTED_CONTAINER` before `REMOVE_NESTED_CONTAINER`","After successful launch of a nested container via `LAUNCH_NESTED_CONTAINER_SESSION` in a checker library, it calls [waitNestedContainer |https://github.com/apache/mesos/blob/0a40243c6a35dc9dc41774d43ee3c19cdf9e54be/src/checks/checker_process.cpp#L657] for the container. Checker library [calls|https://github.com/apache/mesos/blob/0a40243c6a35dc9dc41774d43ee3c19cdf9e54be/src/checks/checker_process.cpp#L466-L487] `REMOVE_NESTED_CONTAINER` to remove a previous nested container before launching a nested container for a subsequent check. Hence, `REMOVE_NESTED_CONTAINER` call follows `WAIT_NESTED_CONTAINER` to ensure that the nested container has been terminated and can be removed/cleaned up.    In case of failure, the library [doesn't call|https://github.com/apache/mesos/blob/0a40243c6a35dc9dc41774d43ee3c19cdf9e54be/src/checks/checker_process.cpp#L627-L636] `WAIT_NESTED_CONTAINER`. Despite the failure, the container might be launched and the following attempt to remove the container without call `WAIT_NESTED_CONTAINER` leads to errors like:      The checker library should always call `WAIT_NESTED_CONTAINER` before `REMOVE_NESTED_CONTAINER`.",Bug,Blocker,Resolved,"2018-02-12 14:02:26","2018-02-12 14:02:26",5
"Apache Mesos","Test UriDiskProfileTest.FetchFromHTTP is flaky.","The {{UriDiskProfileTest.FetchFromHTTP}} test is flaky on Debian 9:      I also run it in repetition and got the following error log (although the test itself is passed):  ",Bug,Major,Resolved,"2018-02-10 02:31:26","2018-02-10 02:31:26",3
"Apache Mesos","Persistent volumes are not visible in Mesos UI when launching a pod using default executor.","When user launches a pod to use a persistent volume in DC/OS, the nested containers in the pod can access the PV successfully and the PV directory of the executor shown in Mesos UI has all the contents written by the tasks, but the PV directory of the tasks shown in DC/OS UI and Mesos UI is empty.",Bug,Major,Resolved,"2018-02-10 00:44:28","2018-02-10 00:44:28",2
"Apache Mesos","Test profile checkpointing in SLRP","Once MESOS-8492 is addressed, we should add a test to verify that RP can recover and complete pending `CREATE_VOLUME` and `CREATE_BLOCK` operations even if the disk profile adaptor no longer knows about the profiles these operations use.    Note that this is currently not doable since there is no way to pause the progress of a `CREATE_VOLUME` and fail the resource provider. Once MESOS-9003 is done this becomes viable.",Task,Major,Open,"2018-02-09 18:44:50","2018-02-09 18:44:50",3
"Apache Mesos","Test resource provider selection for URI disk profile adaptor.","The {{DiskProfileAdaptor}} module provides an interface to filter resource providers for a given profile, and is implemented in {{UriDiskProfileAdaptor}}. We should add tests for the filtering feature.",Task,Major,Open,"2018-02-09 18:40:58","2018-02-09 18:40:58",2
"Apache Mesos","Default executor should allow decreasing the escalation grace period of a terminating task","The command executor supports [decreasing the escalation grace period of a terminating task|https://github.com/apache/mesos/blob/c665dd6c22715fa941200020a8f7209f1f5b1ca1/src/launcher/executor.cpp#L800-L803].    For consistency, this should also be supported by the default executor.",Bug,Major,Accepted,"2018-02-09 01:52:36","2018-02-09 01:52:36",5
"Apache Mesos","Enhance V1 scheduler send API to receive sync response from Master","Current scheduler HTTP API doesn't provide a way for the scheduler to get a synchronous response back from the Master. A synchronous API means the scheduler wouldn't have to wait on the event stream to check the status of operations that require master-only validation/approval/rejection.",Task,Major,Resolved,"2018-02-08 20:15:14","2018-02-08 20:15:14",3
"Apache Mesos","CGROUPS_ROOT_PidNamespaceForward and CGROUPS_ROOT_PidNamespaceBackward tests fail","  ",Bug,Critical,Resolved,"2018-02-08 11:04:07","2018-02-08 11:04:07",2
"Apache Mesos","Port libprocess HTTPTest.QueryEncodeDecode","The test cases:   * -HTTPTest, EndpointsHelp-   * -HTTPTest, EndpointsHelpRemoval-   * -HTTPTest, NestedGet-   * HTTPTest, QueryEncodeDecode    Are disabled in the Windows build because they fail.",Task,Major,Resolved,"2018-02-07 17:47:11","2018-02-07 17:47:11",1
"Apache Mesos","Bug in `Master::detected()` leads to coredump in `MasterZooKeeperTest.MasterInfoAddress`.","  This failure is most likely caused by calling [leader->has_domain()|https://github.com/apache/mesos/blob/994213739b1afc473bbd9d15ded7c3fd26eaa924/src/master/master.cpp#L2159] on empty `leader`, from logs:  ",Bug,Major,Resolved,"2018-02-07 16:24:50","2018-02-07 16:24:50",2
"Apache Mesos","Test StorageLocalResourceProviderTest.ROOT_Metrics is flaky","The SLRP Metrics test is flaky because the agent might got two {{SlaveRegisteredMessage}}s due to its retry logic for registration, and thus it would send two {{UpdateSlaveMessage}}s. As a result, the futures waiting for these messages will be ready before the plugin is actually launched. This will lead to a race between the SIGKILL and LAUNCH_CONTAINER in the test, and if the kill happens before SLRP gets connected to the plugin, SLRP will wait for 1 minutes before giving up, which is too long for the test to wait for a second launch.",Bug,Major,Resolved,"2018-02-06 05:24:19","2018-02-06 05:24:19",2
"Apache Mesos","PythonFramework test fails with cache write failure.","After some recent changes, the  {{ExamplesTest.PythonFramework}} fails on centos and ubuntu rather frequently (but not always).    The symptom always is like this (taken from an ASF CI run):      ",Bug,Major,Resolved,"2018-02-05 19:39:29","2018-02-05 19:39:29",2
"Apache Mesos","AgentAPIStreamingTest.AttachInputToNestedContainerSession is flaky.",,Bug,Major,Resolved,"2018-02-05 19:07:50","2018-02-05 19:07:50",8
"Apache Mesos","The default executor can wrongly indicate that tasks from all task groups are unhealthy","The default executor sets a global [unhealthy|https://github.com/apache/mesos/blob/7ace6c4c263ebdd8f1605b1c478cb2e7eb9e83a1/src/launcher/default_executor.cpp#L1286] field to {{true}} once it kills a task due to failed health checks.    When a task from any task group exits, it will [check|https://github.com/apache/mesos/blob/7ace6c4c263ebdd8f1605b1c478cb2e7eb9e83a1/src/launcher/default_executor.cpp#L877-L882] that global healthy field and set the task status update's {{healthy}} field accordingly. This means that once an unhealthy task belonging to a task group is killed, task status updates for tasks belonging to other task groups, which can contain only healthy tasks, will be sent with their {{healthy}} field set to {{false}}.",Bug,Major,Accepted,"2018-02-05 18:33:57","2018-02-05 18:33:57",3
"Apache Mesos","Add metrics about CSI plugin terminations.","So that operators can be alerted on flapping CSI plugin container.",Task,Major,Resolved,"2018-02-02 22:39:53","2018-02-02 22:39:53",2
"Apache Mesos","Consider adding a timeout to Docker executor task launch","In order to be more resilient to an unresponsive Docker daemon on an agent, the Docker executor could utilize a timeout for its task launches. If its initial {{docker inspect}} call fails to return within this timeout, the executor could commit suicide.    However, we must be careful to properly clean up in such a case. For example, if the executor's {{docker run}} command was successful, but then {{docker inspect}} failed to return, we would want to be sure that the Docker containerizer would destroy the running container in this case. Furthermore, it's possible that it could lead to a state where the executor terminates, then a TASK_FAILED is forwarded to the master, but the task container continues to run on the agent until the daemon becomes responsive again. If a launch timeout is implemented, care should be taken to avoid such inconsistent states.",Improvement,Major,Open,"2018-02-02 22:05:40","2018-02-02 22:05:40",5
"Apache Mesos","Default executor doesn't wait for status updates to be ack'd before shutting down","The default executor doesn't wait for pending status updates to be acknowledged before shutting down, instead it sleeps for one second and then terminates:        The event handler should exit if upon receiving a {{Event::ACKNOWLEDGED}} the executor is shutting down, no tasks are running anymore, and all pending status updates have been acknowledged.",Bug,Major,Resolved,"2018-02-02 20:38:29","2018-02-02 20:38:29",3
"Apache Mesos","Pending offer operations on resource provider resources not properly accounted for in allocator","The master currently does not accumulate the resources used by offer operations on master failover. While we create a datastructure to hold this information, we missed updating it.      Here {{usedByOperations}} is not updated.    This leads to problems when the operation becomes terminal and we try to recover the used resources which might not be known to the framework sorter inside the hierarchical allocator.",Bug,Major,Resolved,"2018-02-02 14:05:44","2018-02-02 14:05:44",2
"Apache Mesos","The default executor doesn't retry kills that it initiated","The default executor might initiate a task kill due to health check failures or to a task in the same task group failing.    If the kill call fails, the executor won't retry it, so the task will get stuck in a killing state.",Bug,Major,Accepted,"2018-02-01 21:49:24","2018-02-01 21:49:24",5
"Apache Mesos","Some task status updates sent by the default executor don't contain a REASON.","The default executor doesn't set a reason when sending {{TASK_KILLING}}, {{TASK_KILLED}},   and {{TASK_FAILED}} task status update.",Bug,Major,Accepted,"2018-02-01 21:46:51","2018-02-01 21:46:51",3
"Apache Mesos","Default executor tasks can get stuck in KILLING state","The default executor will transition a task to {{TASK_KILLING}} and mark its container as being killed before issuing the {{KILL_NESTED_CONTAINER}} call.    If the kill call fails, the task will get stuck in {{TASK_KILLING}}, and the executor won't allow retrying the kill.  ",Bug,Critical,Resolved,"2018-02-01 21:43:54","2018-02-01 21:43:54",5
"Apache Mesos","Design Doc for Storage External Resource Provider (SERP) support.",,Task,Major,Resolved,"2018-02-01 18:43:04","2018-02-01 18:43:04",8
"Apache Mesos","When `UPDATE_SLAVE` messages are received, offers might not be rescinded due to a race ","When an agent with enabled {{RESOURCE_PROVIDER}} capability (re-)registers with the master it sends a {{UPDATE_SLAVE}} after being (re-)registered. In the master, the agent is added (back) to the allocator, as soon as it's (re-)registered, i.e. before {{UPDATE_SLAVE}} is being send. This triggers an allocation and offers might get sent out to frameworks. When {{UPDATE_SLAVE}} is being handled in the master, these offers have to be rescinded, as they're based on an outdated agent state.  Internally, the allocator defers a offer callback in the master ({{Master::offer}}). In rare cases a {{UPDATE_SLAVE}} message might arrive at the same time and its handler in the master called before the offer callback (but after the actual allocation took place). In this case the (outdated) offer is still sent to frameworks and never rescinded.    Here's the relevant log lines, this was discovered while working on https://reviews.apache.org/r/65045/:  ",Bug,Major,Open,"2018-02-01 14:34:37","2018-02-01 14:34:37",2
"Apache Mesos","SLRP failed to connect to CSI endpoint.","After bumping the gRPC bundle to 1.8.3, there are some flakiness in SLRP tests caused by SLRP not being able to connect to a CSI endpoint. The reason is that it seems to take longer for gRPC 1.8 to prepare a domain socket (i.e., the time between the {{bind}} and {{accept}} calls are longer), and as a result, SLRP cannot talk to a CSI plugin immediately after the socket file is created.",Bug,Blocker,Resolved,"2018-01-31 01:33:06","2018-01-31 01:33:06",2
"Apache Mesos","Noisy transport endpoint is not connected logs on closing sockets.","When within libprocess a socket is closing, we try to shut it down. That shutdown fails as the socket is not connected. This is intended behavior. The error code returned {{ENOTCONN}} tells us that there is nothing to see here for such common scenario.    The problem appears to be the logging of such event - that might appear as not useful - no matter which log-level is used.        We should try to prevent this specific, non actionable logging entirely while making sure we do not hinder debugging scenarios.  ",Bug,Minor,Resolved,"2018-01-31 00:51:10","2018-01-31 00:51:10",3
"Apache Mesos","Fetcher doesn't log it's stdout/stderr properly to the log file","The fetcher doesn't log it's stdout or stderr to the task's output files as it does on Linux. This makes it extraordinarily difficult to diagnose fetcher failures (bad URI, or permissions problems, or whatever).    It does not appear to be a glog issue. I added output to the fetcher via cout and cerr, and that output didn't show up in the log files either. So it appears to be a logging capture issue.    Note that the container launcher, launched from src/slave/containerizer/mesos/launcher.cpp, does appear to log properly. However, when launching the fetcher itself from src/slave/containerizer/fetcher.cpp (FetcherProcess::run), logging does not happen properly.",Bug,Major,Resolved,"2018-01-30 23:29:04","2018-01-30 23:29:04",2
"Apache Mesos","URI disk profile adaptor does not consider plugin type for a profile.","Currently, the URI disk profile adaptor will fetch an URI, the content of which contains a profile matrix. However, there's no field in the profile matrix for the adaptor to tell which plugin type a profile is for.    We should consider adding a `plugin_type` field in `CSIManifest`.",Bug,Major,Resolved,"2018-01-30 21:32:43","2018-01-30 21:32:43",3
"Apache Mesos","Missing map header when compiling against unbundled protobuf","When compiling mesos against the system-default version of protobuf on Ubuntu 17.04, the build fails due to a missing include.         Explanation for the error by [~<USER>:  Note that the reason why this doesn't compile in protobuf 3.0.x is due to how the c++ files are generated.  In protobuf 3.0.x (and 3.1.x and 3.2.x) generated code only includes the protobuf map headers if there is a map present in the .proto file:[https://github.com/google/protobuf/blob/3.0.x/src/google/protobuf/compiler/cpp/cpp_file.cc#L817-L827]    From 3.3.x onwards, all generated files include {{google/protobuf/generated_message_table_driven.h}}, which in turn includes the map headers:[https://github.com/google/protobuf/blob/3.3.x/src/google/protobuf/compiler/cpp/cpp_file.cc#L1006]",Bug,Major,Resolved,"2018-01-30 13:57:59","2018-01-30 13:57:59",1
"Apache Mesos","Improve UI when displaying frameworks with many roles.","The /frameworks UI endpoint displays all the roles of each framework in a table:    !Screen Shot 2018-01-29 à 10.38.05.png!    This is not readable if a framework has many roles. We thus need to provide a solution to only display a few roles per framework and show more when a user wants to see all of them.",Task,Major,Resolved,"2018-01-29 09:40:43","2018-01-29 09:40:43",5
"Apache Mesos","The test `DefaultExecutorTest.KillTaskGroupOnTaskFailure` is flaky","  From the detailed log in the attachment, it seems the root cause is that agent did not get a chance to forward TASK_RUNNING to master for the first task because it failed immediately.",Bug,Major,Accepted,"2018-01-28 02:37:31","2018-01-28 02:37:31",3
"Apache Mesos","Docker parameter `name` does not work with Docker Containerizer.","When deploying a marathon app with Docker Containerizer (need to check Mesos Containerizer) and the parameter name set, Mesos is not able to recognize/control/kill the started container.    Steps to reproduce    # Deploy the below marathon app definition   #  Watch task being stuck in staging and mesos not being able to kill it/communicate with it   ##   {quote}e.g., Agent Logs: W0126 18:38:50.000000  4988 slave.cpp:6750] Failed to get resource statistics for executor ‘instana-agent.1a1f8d22-02c8-11e8-b607-923c3c523109’ of framework 41f1b534-5f9d-4b5e-bb74-a0e387d5739f-0001: Failed to run ‘docker -H unix:///var/run/docker.sock inspect mesos-1c6f894d-9a3e-408c-8146-47ebab2f28be’: exited with status 1; stderr=’Error: No such image, container or task: mesos-1c6f894d-9a3e-408c-8146-47ebab2f28be{quote}   # Check on node and see container running, but not being recognized by mesos    ",Bug,Critical,Resolved,"2018-01-26 19:27:40","2018-01-26 19:27:40",2
"Apache Mesos","Checkpoint profiles in storage local resource provider.","SLRP should be able to handle missing profiles from an arbitrary disk profile module, and probably need to checkpoint them for recovery.",Task,Blocker,Resolved,"2018-01-25 22:09:42","2018-01-25 22:09:42",3
"Apache Mesos","LinuxCapabilitiesIsolatorFlagsTest.ROOT_IsolatorFlags is flaky","Observed this on internal Mesosphere CI.    h2. Steps to reproduce   # Add {{::sleep(1);}} before [removing|https://github.com/apache/mesos/blob/e91ce42ed56c5ab65220fbba740a8a50c7f835ae/src/linux/cgroups.cpp#L483] test cgroup   # recompile   # run `GLOG_v=2 sudo GLOG_v=2 ./src/mesos-tests --gtest_filter=LinuxCapabilitiesIsolatorFlagsTest.ROOT_IsolatorFlags --gtest_break_on_failure --gtest_repeat=10 --verbose`    h2. Race description    While recovery is in progress for [the first slave|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/tests/containerizer/linux_capabilities_isolator_tests.cpp#L733], calling [`StartSlave()`|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/tests/containerizer/linux_capabilities_isolator_tests.cpp#L738] leads to calling [`slave::Containerizer::create()`|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/tests/cluster.cpp#L431] to create a containerizer. An attempt to create a mesos c'zer, leads to calling [`cgroups::prepare`|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/slave/containerizer/mesos/linux_launcher.cpp#L124]. Finally, we get to the point, where we try to create a [test container|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/linux/cgroups.cpp#L476]. So, the recovery process for the second slave [might detect|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/slave/containerizer/mesos/linux_launcher.cpp#L268-L301] this test container as an orphaned container.    Thus, there is the race between recovery process for the first slave and an attempt to create a c'zer for the second agent.",Bug,Major,Resolved,"2018-01-25 12:19:10","2018-01-25 12:19:10",8
"Apache Mesos","Docker bug can cause unkillable tasks.","Due to an [issue on the Moby project|https://github.com/moby/moby/issues/33820], it's possible for Docker versions 1.13 and later to fail to catch a container exit, so that the {{docker run}} command which was used to launch the container will never return. This can lead to the Docker executor becoming stuck in a state where it believes the container is still running and cannot be killed.    We should update the Docker executor to ensure that containers stuck in such a state cannot cause unkillable Docker executors/tasks.    One way to do this would be a timeout, after which the Docker executor will commit suicide if a kill task attempt has not succeeded. However, if we do this we should also ensure that in the case that the container was actually still running, either the Docker daemon or the DockerContainerizer would clean up the container when it does exit.    Another option might be for the Docker executor to directly {{wait()}} on the container's Linux PID, in order to notice when the container exits.",Improvement,Major,Resolved,"2018-01-25 01:16:45","2018-01-25 01:16:45",2
"Apache Mesos","Design API changes for supporting quota limits.","Per MESOS-8068, the introduction of a quota limit requires introducing this in the API. We should send out the proposed changes more broadly in the interest of being more rigorous about API changes.    [Design doc|https://docs.google.com/document/d/13vG5uH4YVwM79ErBPYAZfnqYFOBbUy2Lym0_9iAQ5Uk/edit#]",Task,Major,Resolved,"2018-01-25 00:35:01","2018-01-25 00:35:01",13
"Apache Mesos","Webui should display role limits.","With the addition of quota limits (see MESOS-8068), the UI should be updated to display the per role limit information. Specifically, the 'Roles' tab needs to be updated.",Task,Major,Resolved,"2018-01-25 00:25:58","2018-01-25 00:25:58",1
"Apache Mesos","MasterTest.RegistryGcByCount is flaky","Observed this while testing Mesos 1.5.0-rc1 in ASF CI.         ",Bug,Major,Resolved,"2018-01-24 19:02:32","2018-01-24 19:02:32",3
"Apache Mesos","stout test NumifyTest.HexNumberTest fails. ","The current Mesos master shows the following on my machine:        This problem disappears for me when reverting the latest boost upgrade.",Bug,Blocker,Resolved,"2018-01-24 18:07:38","2018-01-24 18:07:38",1
"Apache Mesos","ExampleTests PythonFramework fails with sigabort.","Starting the {{PythonFramework}} manually results in a sigabort:          When running the {{PythonFramework}} via lldb, I get the following stacktrace:        Given that this example works (most of the time) on other macOS systems, I am assuming this is a problem of my system.",Bug,Blocker,Resolved,"2018-01-24 14:25:03","2018-01-24 14:25:03",3
"Apache Mesos","Signed/Unsigned comparisons in tests","Many tests in mesos currently have comparisons between signed and unsigned integers, eg    or comparisons between values of different enums, e.g. TaskState and v1::TaskState:    Usually, the compiler would catch these and emit a warning, but these are currently silenced because gtest headers are included using the {{-isystem}} command line flag.",Bug,Major,Resolved,"2018-01-24 12:41:06","2018-01-24 12:41:06",1
"Apache Mesos","Mesos returns high resource usage when killing a Docker task.","The way we get resource statistics for Docker tasks is through getting the cgroup subsystem path through {{/proc/<pid>/cgroup}} first (taking the {{cpuacct}} subsystem as an example):    Then read {{/sys/fs/cgroup/cpuacct//docker/66fbe67b64ad3a86c6e080e18578bc9e540e55ee0bdcae09c2e131a4264a3a3b/cpuacct.stat}} to get the statistics:    However, when a Docker container is being teared down, it seems that Docker or the operation system will first move the process to the root cgroup before actually killing it, making {{/proc/<pid>/docker}} look like the following:    This makes a racy call to [{{cgroup::internal::cgroup()}}|https://github.com/apache/mesos/blob/master/src/linux/cgroups.cpp#L1935] return a single '/', which in turn makes [{{DockerContainerizerProcess::cgroupsStatistics()}}|https://github.com/apache/mesos/blob/master/src/slave/containerizer/docker.cpp#L1991] read {{/sys/fs/cgroup/cpuacct///cpuacct.stat}}, which contains the statistics for the root cgroup:    This can be reproduced by [^test.cpp] with the following command:  ",Bug,Major,Resolved,"2018-01-23 21:33:57","2018-01-23 21:33:57",2
"Apache Mesos","Make clean fails without Python artifacts.","Make clean may fail if there are no Python artifacts created by previous builds.             Triggered by [https://github.com/apache/mesos/blob/62d392704c499e06da0323e50dfd016cdac06f33/src/Makefile.am#L2218-L2219]",Bug,Major,Resolved,"2018-01-23 03:57:44","2018-01-23 03:57:44",1
"Apache Mesos","Test StorageLocalResourceProviderTest.ROOT_ConvertPreExistingVolume is flaky","Observed on our internal CI on ubuntu16.04 with SSL and GRPC enabled,  ",Bug,Major,Resolved,"2018-01-22 20:47:47","2018-01-22 20:47:47",2
"Apache Mesos","Authorize `GET_OPERATIONS` calls.","The {{GET_OPERATIONS}} call lists all known operations on a master or agent. Authorization has to be added to this call.",Task,Major,Resolved,"2018-01-22 13:50:57","2018-01-22 13:50:57",3
"Apache Mesos","CHECK failure in DRFSorter due to invalid framework id.","A framework registering with a custom {{FrameworkID}} containing slashes such as {{/foo/bar}} will trigger a CHECK failure at https://github.com/apache/mesos/blob/177a2221496a2caa5ad25e71c9982ca3eed02fd4/src/master/allocator/sorter/drf/sorter.cpp#L167:    The sorter should be defensive with any {{FrameworkID}} containing slashes.",Bug,Major,Resolved,"2018-01-20 02:12:34","2018-01-20 02:12:34",2
"Apache Mesos","Mesos master might drop some events in the operator API stream","Inside `Master::updateTask`, we call `Subscribers::send` which asynchronously calls `Subscribers::Subscriber::send` on each subscriber.    But the problem is that inside `Subscribers:Subscriber::send` we are looking up the state of the master (e.g., getting Task* and Framework*) which might have changed between `Subscribers::send ` and `Subscribers::Subscriber::send`.         For example, if a terminal task received an acknowledgement the task might be removed from master's state, causing us to drop the TASK_UPDATED event.         We noticed this in an internal cluster, where a TASK_KILLED update was sent to one subscriber but not the other.               ",Bug,Critical,Resolved,"2018-01-20 01:31:41","2018-01-20 01:31:41",3
"Apache Mesos","`LAUNCH_GROUP` failure tears down the default executor.","The following code in the default executor (https://github.com/apache/mesos/blob/12be4ba002f2f5ff314fbc16af51d095b0d90e56/src/launcher/default_executor.cpp#L525-L535) shows that if a `LAUNCH_NESTED_CONTAINER` call is failed (say, due to a fetcher failure), the whole executor will be shut down:  {code:cpp}  // Check if we received a 200 OK response for all the  // `LAUNCH_NESTED_CONTAINER` calls. Shutdown the executor  // if this is not the case.  foreach (const Response& response, responses.get()) {    if (response.code != process::http::Status::OK) {      LOG(ERROR) << Received ' << response.status << ' (                 << response.body << ) while launching child container;      _shutdown();      return;    }  }  {code}    This is not expected by a user. Instead, one would expect that a failed `LAUNCH_GROUP` won't affect other task groups launched by the same executor, similar to the case that a task failure only takes down its own task group. We should adjust the semantics to make a failed `LAUNCH_GROUP` not take down the executor and affect other task groups.",Bug,Critical,Resolved,"2018-01-19 21:59:22","2018-01-19 21:59:22",5
"Apache Mesos","Destroyed executors might be used after `Slave::publishResource()`.","In the following code from [https://github.com/apache/mesos/blob/7b30b9ccd63dbcd3375e012dae6e2ffb9dc6a79f/src/slave/slave.cpp#L2652:]  {code:cpp}  publishResources()    .then(defer(self(), [=] {      return containerizer->update(          executor->containerId,          executor->allocatedResources());    }))  {code}  A destroyed executor might be dereferenced if it has been move to {{Framework.completedExecutors}} and kicked out from this circular buffer. We should refactor {{Slave::publishResources()}} and its uses to make the code less fragile.",Bug,Critical,Resolved,"2018-01-19 18:04:34","2018-01-19 18:04:34",2
"Apache Mesos","Look for unnecessary 'Clock::resume()' at the end of tests","[r65232|https://reviews.apache.org/r/65232/] fixes the destructor of the test agent to resume the clock during container destruction, which is necessary because the {{cgroups::destroy()}} code path makes use of {{delay()}}. After that change, we should look for unnecessary calls to {{Clock::resume()}} in tests which were added to accommodate this issue in {{Slave::~Slave()}}.",Improvement,Major,Open,"2018-01-19 17:40:59","2018-01-19 17:40:59",2
"Apache Mesos","Test MasterAllocatorTest/1.SingleFramework is flaky","Observed in our internal CI on a ubuntu-16 setup in a plain autotools build,  ",Bug,Major,Resolved,"2018-01-19 09:07:39","2018-01-19 09:07:39",3
"Apache Mesos","Unit test for `Slave::detachFile` on removed frameworks.","We should add a unit test for MESOS-8460.",Task,Major,Resolved,"2018-01-19 02:24:11","2018-01-19 02:24:11",2
"Apache Mesos","`Slave::detachFile` can segfault because it could use invalid Framework*.","Observed this SEGV in an internal cluster    {code}    {code}",Bug,Major,Resolved,"2018-01-18 20:33:14","2018-01-18 20:33:14",3
"Apache Mesos","Response from /help includes broken links.","Links in response from {{http://<master-ip:port>/help/}} contain duplicated /help prefix, e.g., {{http://<master-ip:port>/help/help/version}}.    Links in response from {{http://<master-ip:port>/help/master/}} contain duplicated /master prefix, e.g., {{http://<master-ip:port>/help/master/master/flags}}.    Without trailing slash all links are fine.",Bug,Major,Accepted,"2018-01-18 17:33:48","2018-01-18 17:33:48",3
"Apache Mesos","Allocator should allow roles to burst above guarantees but below limits.","Currently, allocator only allocates resources for quota roles up to their guarantee in the first allocation stage. The allocator should continue allocating resources to these roles in the second stage below their quota limit. In other words, allocator should allow roles to burst above their guarantee but below the limit.",Improvement,Major,Resolved,"2018-01-17 21:08:38","2018-01-17 21:08:38",8
"Apache Mesos","Avoid unnecessary copying of protobuf in the v1 API.","Now that we have move support for protobufs, we can avoid the unnecessary copying of protobuf in the v1 API to improve the performance.",Improvement,Major,Resolved,"2018-01-17 19:03:52","2018-01-17 19:03:52",3
"Apache Mesos","Add a download link for master and agent logs in WebUI","Just like task sandboxes, it would be great for us to provide a download link for mesos and agent logs in the WebUI. Right now the the log link opens up the pailer, which is not really convenient to do `grep` and such while debugging.",Improvement,Major,Resolved,"2018-01-17 17:26:03","2018-01-17 17:26:03",3
"Apache Mesos","ExecutorAuthorizationTest.RunTaskGroup segfaults.","  Full log attached.",Bug,Major,Resolved,"2018-01-17 15:44:37","2018-01-17 15:44:37",3
"Apache Mesos","Add missing fields to agent v1 operator API","Some fields which are available via the agent {{/state}} endpoint are not accessible via the v1 API.",Improvement,Major,Accepted,"2018-01-16 22:24:29","2018-01-16 22:24:29",2
"Apache Mesos","Incomplete output of apply-reviews.py --dry-run","The script {{support/apply-reviews.py}} has a flag {{--dry-run}} which should dump the commands which would be performed. This flag is useful to e.g., reorder patch chains or to manually resolve intermediate conflicts while still being able to pull a full chain.    The output looks like this    Trying to replay that dry run leads to an error since the commands to create the commit message files are not printed.    We should add these commands to the output.",Bug,Major,Resolved,"2018-01-16 09:42:24","2018-01-16 09:42:24",1
"Apache Mesos","Agent miss to detach `virtualLatestPath` for the executor's sandbox during recovery","In {{Framework::recoverExecutor()}}, we attach {{executor->directory}} to 3 virtual paths:  (1) /agent_workdir/frameworks/FID/executors/EID/runs/CID  (2) /agent_workdir/frameworks/FID/executors/EID/runs/latest  (3) /frameworks/FID/executors/EID/runs/latest  But in this method, when we find the executor completes, we only do detach for (1) and (2) but not (3). We should do detach for (3) too as what we do in {{Slave::removeExecutor}}, otherwise, it will be a leak.",Bug,Major,Resolved,"2018-01-15 13:45:40","2018-01-15 13:45:40",1
"Apache Mesos","Test that `UPDATE_STATE` of a resource provider doesn't have unwanted side-effects in master or agent","While we test the correct behavior of {{UPDATE_STATE}} sent by resource providers when an operation state changes or after (re-)registration, this call might also get sent independent from any such event, e.g., if resources are added to a running resource provider. Correct behavior of master and agent need to be tested. Outstanding offers should be rescinded and internal states updated.",Task,Major,Resolved,"2018-01-15 09:43:35","2018-01-15 09:43:35",3
"Apache Mesos","GC failure causes agent miss to detach virtual paths for the executor's sandbox","I launched a task via {{mesos-execute}} which just did a {{sleep 10}}, when the task finished, {{Slave::removeExecutor()}} and {{Slave::removeFramework()}} were called and they will try to gc 3 directories:  # /<slave-work-dir>/slaves/<slaveID>/frameworks/<frameworkID>/executors/<executorID>/runs/<containerID>  # /<slave-work-dir>/slaves/<slaveID>/frameworks/<frameworkID>/executors/<executorID>  # /<slave-work-dir>/slaves/<slaveID>/frameworks/<frameworkID>    For 1 and 2, the code to gc them is like this:      So here {{then()}} is used which means we will only do the detach when the gc succeeds. But the problem is the order of 1, 2 and 3 deleted by gc can not be guaranteed, from my test, 3 will be deleted first for most of times. Since 3 is the parent directory of 1 and 2, so the gc for 1 and 2 will fail:    So we will NOT do the detach for 1 and 2 which is a leak.",Bug,Major,Resolved,"2018-01-14 09:47:08","2018-01-14 09:47:08",2
"Apache Mesos","Source tree contains generated endpoint documentation","Even though we generate documentation automatically in CI, the source tree still contains checked in, generated endpoint documentation in {{docs/endpoints}}.    We should remove these source files from the tree. We need to make sure to    * not break automatic website generation with {{support/mesos-website/build.sh}},  * not break the local website generation workflow with {{site/mesos-website-dev.sh}}, and  * not break local website generation workflow with {{rake}} via {{site/Rakefile}}.",Task,Major,Resolved,"2018-01-12 22:37:41","2018-01-12 22:37:41",1
"Apache Mesos","Revisit the operation status de-duplication logic in the master","If the master gets two different terminal operation status updates, it could end up with different states in {{operation.latest_state}} and the front of statuses list ({{operation.statuses}}.    This is consistent with how task status updates are handled, but should be revisited.  ",Task,Major,Open,"2018-01-12 21:56:37","2018-01-12 21:56:37",3
"Apache Mesos","Refactor master's 'updateOperation()' to not accept an update message","We've discovered that we need to update operations in the master state upon receipt of either UpdateSlaveMessage or UpdateOperationStatusMessage - whichever arrives first. We should change the {{updateOperation()}} helper in the master to not accept an {{UpdateOperationStatusMessage}} as a parameter (perhaps accepting the statuses directly instead), in order to make this helper friendlier to the callsite in {{updateSlave()}}.",Improvement,Minor,Open,"2018-01-11 22:19:57","2018-01-11 22:19:57",3
"Apache Mesos","Make logging of operations and their updates consistent","The logging of operations and their updates is not consistent across the master/agent/RP manager/RP code. We should make this logging consistent, and ensure that it adequately handles cases of missing operation ID, missing framework ID, etc.",Improvement,Major,Open,"2018-01-11 17:51:33","2018-01-11 17:51:33",3
"Apache Mesos","Non-leading master should reply to some API requests","Currently, non-leading masters forward all v1 API requests to the leader. However, responses for some requests like GET_FLAGS, GET_HEALTH, GET_MASTER, and GET_VERSION could be provided by non-leading masters as well.    We should update the master code to reply to such requests directly rather than redirecting them.",Improvement,Major,Open,"2018-01-10 21:42:52","2018-01-10 21:42:52",3
"Apache Mesos","Clean up endpoint socket if the container daemon is destroyed while waiting.","SLRP uses a post-stop hook to ask the container daemon to clean up the endpoint socket after its plugin container is terminated. However, if the container daemon is destructed while waiting for the container it monitors before the container itself is terminated, the socket file will remain there, making SLRP unable to recover.    There might be two solutions:  1. During SLRP recovery, check if the plugin container is still running.  2. Start the container daemon in the waiting phase.",Bug,Major,Resolved,"2018-01-10 20:02:52","2018-01-10 20:02:52",3
"Apache Mesos","SLRP recovery tests leak file descriptors.","The {{CreateDestroyVolumeRecovery}} (formerly {{NewVolumeRecovery}}) and {{PublishResourcesRecovery}} (formerly {{LaunchTaskRecovery}}) tests leak fds. When running them in repetition, either the following error will manifest:    or the plugin container will exit possibly due to no fd.",Bug,Major,Resolved,"2018-01-10 19:20:09","2018-01-10 19:20:09",2
"Apache Mesos","Clean up residual CSI endpoints for SLRP tests.","Since the CSI endpoints are not in the sandbox directory of the unit tests, they need to be explicitly cleaned up.",Improvement,Major,Resolved,"2018-01-10 19:02:14","2018-01-10 19:02:14",2
"Apache Mesos","Speed up SLRP tests","Each of the current SLRP unit tests takes seconds to run. This can be improved by reducing the allocation interval and declining offers with filters.",Improvement,Major,Resolved,"2018-01-10 18:58:46","2018-01-10 18:58:46",2
"Apache Mesos","Validation for resource provider config agent API calls.","Currently the API returns 200 OK if the config is put in the resource provider config directory, even if the config is not valid (e.g., don't specify a controller plugin). We should consider validating the config when the call is processed.",Bug,Major,Resolved,"2018-01-10 18:56:21","2018-01-10 18:56:21",2
"Apache Mesos","Test that operations are correctly reported following a master failover","As the master keeps track of operations running on a resource provider, it needs to be updated on these operations when agents reregister after a master failover. E.g., an operation that has finished during the failover should be reported as finished by the master after the agent on which the resource provider is running has reregistered.",Task,Major,Resolved,"2018-01-10 10:37:05","2018-01-10 10:37:05",3
"Apache Mesos","Master's UpdateSlave handler not correctly updating terminated operations","I created a test that verifies that operation status updates are resent to the master after being dropped en route to it (MESOS-8420).    The test does the following:    # Creates a volume from a RAW disk resource.  # Drops the first `UpdateOperationStatusMessage` message from the agent to the master, so that it isn't acknowledged by the master.  # Restarts the agent.  # Verifies that the agent resends the operation status update.    The good news are that the agent is resending the operation status update, the bad news are that it triggers a CHECK failure that crashes the master.    Here are the relevant sections of the log produced by the test:        We can see that once the SLRP reregisters with the agent, the following happens:    # The agent will send an {{UpdateSlave}} message to the master including the converted resources and the {{CREATE_VOLUME}} operation with the status {{OPERATION_FINISHED}}.  # The master will update the agent's resources, including the volume created by the operation.  # The agent will resend the operation status update.  # The master will try to apply the operation and crash, because it already updated the agent's resources on step #2.",Bug,Blocker,Resolved,"2018-01-10 00:59:37","2018-01-10 00:59:37",5
"Apache Mesos","Test that operation status updates are retried after being dropped en-route to the master.",https://reviews.apache.org/r/65057/,Task,Major,Resolved,"2018-01-09 23:44:57","2018-01-09 23:44:57",3
"Apache Mesos","RP manager incorrectly setting framework ID leads to CHECK failure","The resource provider manager [unconditionally sets the framework ID|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/manager.cpp#L637] when forwarding operation status updates to the agent. This is incorrect, for example, when the resource provider [generates OPERATION_DROPPED updates during reconciliation|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/storage/provider.cpp#L1653-L1657], and leads to protobuf errors in this case since the framework ID's required {{value}} field is left unset.",Bug,Blocker,Resolved,"2018-01-09 09:32:47","2018-01-09 09:32:47",1
"Apache Mesos","Mesos can get stuck when a Process throws an exception.","When a {{Process}} throws an exception, we log it, terminate the throwing {{Process}}, and continue to run. However, currently there exists no known user-level code that I'm aware of that handles the unexpected termination due to an uncaught exception.    Generally, this means that when an exception is thrown (e.g. a bad call to {{std::map::at}}), the {{Process}} terminates with a log message but things get stuck and the user has to debug what is wrong / kill the process.    Libprocess would likely need to provide some primitives to better support handling unexpected termination of a {{Process}} in order for us to provide a strategy where we continue running.    In the short term, it would be prudent to abort libprocess if any {{Process}} throws an exception so that users can observe the issue and we can get it fixed.",Bug,Major,Resolved,"2018-01-09 02:55:03","2018-01-09 02:55:03",1
"Apache Mesos","CHECK failure if trying to recover nested containers but the framework checkpointing is not enabled.","    If the framework does not enable the checkpointing. It means there is no slave state checkpointed. But containers are still checkpointed at the runtime dir, which mean recovering a nested container would cause the CHECK failure due to its parent's sandbox dir is unknown.",Bug,Blocker,Resolved,"2018-01-08 23:35:41","2018-01-08 23:35:41",5
"Apache Mesos","Add an SLRP test for agent  reboot.","We should add a test for the following scenario: when an agent is rebooted, all previously published CSI volumes would become unmounted. So SLRP should remount them when a task is going to use the volumes.",Task,Major,Resolved,"2018-01-08 20:41:13","2018-01-08 20:41:13",2
"Apache Mesos","Killing a queued task can lead to the command executor never terminating.","If a task is killed while the executor is re-registering, we will remove it from queued tasks and shut down the executor if all the its initial tasks could not be delivered. However, there is a case (within {{Slave::___run}}) where we leave the executor running, the race is:    # Command-executor task launched.  # Command executor sends registration message. Agent tells containerizer to update the resources before it sends the tasks to the executor.  # Kill arrives, and we synchronously remove the task from queued tasks.  # Containerizer finishes updating the resources, and in {{Slave::___run}} the killed task is ignored.  # Command executor stays running!    Executors could have a timeout to handle this case, but it's not clear that all executors will implement this correctly. It would be better to have a defensive policy that will shut down an executor if all of its initial batch of tasks were killed prior to delivery.    In order to implement this, one approach discussed with [~<USER> is to look at the running + terminated but unacked + completed tasks, and if empty, shut the executor down in the {{Slave::___run}} path. This will require us to check that the completed task cache size is set to at least 1, and this also assumes that the completed tasks are not cleared based on time or during agent recovery.",Bug,Critical,Resolved,"2018-01-08 01:13:13","2018-01-08 01:13:13",5
"Apache Mesos","Reconfiguration policy fails to handle mount disk resources.","We deployed {{--reconfiguration_policy=additive}} on a number of Mesos agents that had mount disk resources configured, and it looks like the agent confused the size of the mount disk with the size of the work directory resource:          The {{--resources}} flag is  ",Bug,Blocker,Resolved,"2018-01-06 01:59:41","2018-01-06 01:59:41",3
"Apache Mesos","Add an SLRP test for agent registered with a new ID.","When an agent is registered with a new ID, SLRP should be assigned with a different ID, and all previously created volumes would become pre-existing volumes without profile.",Task,Major,Resolved,"2018-01-06 00:42:42","2018-01-06 00:42:42",2
"Apache Mesos","Add an SLRP test for CSI plugin restart.","We will add a unit test that keeps killing the CSI plugin to verify that SLRP can restart the plugin and work properly.",Task,Major,Resolved,"2018-01-05 23:12:36","2018-01-05 23:12:36",3
"Apache Mesos","Add SLRP unit tests for profile updates and corner cases.","The following tests will be added:  1. RP update state with no resources, and recover from this state  2. Pre-existing CSI volume with zero size  3. RP updates state with no profile, then another update state which contains resources associated with a profile  ",Task,Major,Resolved,"2018-01-05 22:02:29","2018-01-05 22:02:29",3
"Apache Mesos","Add agent HTTP API operator call to mark local resource providers as gone","It is currently not possible to mark local resource providers as gone (e.g., after agent reconfiguration). As resource providers registered at earlier times could still be cached in a number of places, e.g., the agent or the master, the only way to e.g., prevent this cache from growing too large is to fail over caching components (to e.g., prevent an agent cache to update a fresh master cache during reconciliation).    Showing unavailable and known to be gone resource providers in various endpoints is likely also confusing to users.    We should add an operator call to mark resource providers as gone. While the entity managing resource provider subscription state is the resource provider manager, it still seems to make sense to add this operator call to the agent API as currently only local resource providers are supported. The agent would then forward the call to the resource provider manager which would transition its state for the affected resource provider, e.g., setting its state to {{GONE}} and removing it from the list of known resource providers, and then send out an update to its subscribers.",Improvement,Critical,Resolved,"2018-01-05 15:03:14","2018-01-05 15:03:14",3
"Apache Mesos","Resource provider manager should persist resource provider information","Currently, the resource provider manager used to abstract away resource provider subscription and state does not persist resource provider information. It has no notion of e.g., disconnected or forcibly removed resource providers. This makes it hard to implement a number of features, e.g.,    * removal of a resource provider and make it possible to garbage collect its cached state (e.g., in the resource provider manager, agent, or master), or  * controlling resource provider resubscription, e.g., by observing and enforcing resubscription timeouts.    We should extend the resource provider manager to persist the state of each resource provider (e.g., {{CONNECTED}}, {{DISCONNECTED}}, its resources and other attributes). This information should also be exposed in resource provider reconciliation, and be reflected in master or agent endpoints.",Improvement,Major,Resolved,"2018-01-05 14:50:15","2018-01-05 14:50:15",8
"Apache Mesos","Handle plugin crashes gracefully in SLRP recovery.","When a CSI plugin crashes, the container daemon in SLRP will reset its corresponding {{csi::Client}} service future. However, if a CSI call races with a plugin crash, the call may be issued before the service future is reset, resulting in a failure for that CSI call. MESOS-9517 partly addresses this for {{CreateVolume}} and {{DeleteVolume}} calls, but calls in the SLRP recovery path, e.g., {{ListVolume}}, {{GetCapacity}}, {{Probe}}, could make the SLRP unrecoverable.    There are two main issues:   1. For {{Probe}}, we should investigate if it is needed to make a few retry attempts, then after that, we should recover from failed attempts (e.g., kill the plugin container), then make the container daemon relaunch the plugin instead of failing the daemon.    2. For other calls in the recovery path, we should either retry the call, or make the local resource provider daemon be able to restart the SLRP after it fails.",Improvement,Blocker,Reviewable,"2018-01-05 04:33:56","2018-01-05 04:33:56",5
"Apache Mesos","Use unique ID for CSI plugin containers in SLRP.","If an agent crashed abnormally and the runtime directory is lost, then a standalone container previously launched by SLRP will be considered orphan when the agent restarts. Since the orphan containers are cleaned up asynchronously, it is possible that the cleanup is racing with SLRP launching a new standalone container instance with the same ID. To avoid this race, we should use unique IDs for CSI plugin containers.",Bug,Major,Resolved,"2018-01-05 02:02:01","2018-01-05 02:02:01",3
"Apache Mesos","Made gRPC a requirement for Mesos builds.",,Task,Blocker,Resolved,"2018-01-04 17:38:16","2018-01-04 17:38:16",5
"Apache Mesos","Bump CSI to 0.1.0","We should bump our CSI bundle to the new 0.1.0 release.",Task,Blocker,Resolved,"2018-01-04 16:37:22","2018-01-04 16:37:22",2
"Apache Mesos","SLRP NewVolumeRecovery and LaunchTaskRecovery tests CHECK failures.","CHECK failures manifested on the two SLRP tests after resource upgrade/downgrade is introduced.",Bug,Blocker,Resolved,"2018-01-04 16:33:47","2018-01-04 16:33:47",2
"Apache Mesos","Mesos agent doesn't notice that a pod task exits or crashes after the agent restart","h4. (1) Agent doesn't detect that a pod task exits/crashes    # Create a Marathon pod with two containers which just do {{sleep 10000}}.  # Restart the Mesos agent on the node the pod got launched.  # Kill one of the pod tasks    *Expected result*: The Mesos agent detects that one of the tasks got killed, and forwards {{TASK_FAILED}} status to Marathon.    *Actual result*: The Mesos agent does nothing, and the Mesos master thinks that both tasks are running just fine. Marathon doesn't take any action because it doesn't receive any update from Mesos.    h4. (2) After the agent restart, it detects that the task crashed, forwards the correct status update, but the other task stays in {{TASK_KILLING}} state forever    # Perform steps in (1).  # Restart the Mesos agent    *Expected result*: The Mesos agent detects that one of the tasks got crashed, forwards the corresponding status update, and kills the other task too.    *Actual result*: The Mesos agent detects that one of the tasks got crashed, forwards the corresponding status update, but the other task stays in `TASK_KILLING` state forever.    Please note, that after another agent restart, the other tasks gets finally killed and the correct status updates get propagated all the way to Marathon.",Bug,Blocker,Resolved,"2018-01-04 02:27:08","2018-01-04 02:27:08",3
"Apache Mesos","Notion of transitioning agents in the master is now inaccurate.","While [~<USER> and I were discussing https://reviews.apache.org/r/57535/ we found a recent change that made the concept of in transition agents confusing. See my comment here: https://reviews.apache.org/r/52083/#review170066    Given the new semantics described in the summary of https://reviews.apache.org/r/52083, the need for a separate method {{transitioning}} no longer exists because now it just wraps around a single variable {{unrecovered}} and gives it an alias which is less intuitive (because when reading the word transitioning one would think it has a more general meaning).",Improvement,Major,Resolved,"2018-01-04 00:44:13","2018-01-04 00:44:13",3
"Apache Mesos","Notion of removable task in master code is inaccurate.","In the past, the notion of a removable task meant: the task is terminal and acknowledged. It appears now that a removable task is defined purely by its state (terminal or unreachable) but not whether the terminal update is acknowledged.    As a result, the code that is calling this function ({{isRemovable}}) ends up being unintuitive. One example of a confusing piece of code is within {{updateTask}}. Here, we have logic which says, if the task is removable, recover the resources *but don't remove it*. This seems more intuitive if directly described as: if the task is no longer consuming resources, then (e.g. transitioned to terminal or unreachable) then recover the resources.    If one looks up the documentation of {{isRemovable}}, it says When a task becomes removable, it is erased from the master's primary task data structures, but that isn't accurate since this function doesn't say whether the terminal task has been acknowledged, which is required for a task to be removable.    I think an easy improvement here would be to move this notion of removable towards something like {{isTerminalOrUnreachable}}. We could also think about how to name this concept more generally, like {{canReleaseResources}} to describe whether the task's resources are considered allocated.    If we do introduce a notion of {{isRemovable}}, it seems it should be saying whether the task could be removed from the master, which includes checking that terminal tasks have been acknowledged.",Improvement,Major,Resolved,"2018-01-04 00:41:00","2018-01-04 00:41:00",5
"Apache Mesos","Show LRP resources in master and agent endpoints.","Currently, only resource provider info is shown. We should also show the resources provided by resource providers.",Task,Major,Resolved,"2018-01-03 22:37:07","2018-01-03 22:37:07",2
"Apache Mesos","Add metrics for operations in Storage Local Resource Provider (SLRP).",,Task,Major,Resolved,"2018-01-03 22:26:32","2018-01-03 22:26:32",3
"Apache Mesos","Master should bookkeep local resource providers.","This will simplify the handling of `UpdateSlaveMessage`. ALso, it'll simplify the endpoint serving.",Task,Major,Resolved,"2018-01-03 22:25:33","2018-01-03 22:25:33",5
"Apache Mesos","Update WebUI to show local resource providers.",,Improvement,Critical,Resolved,"2018-01-03 22:09:27","2018-01-03 22:09:27",2
"Apache Mesos","Use protobuf reflection to simplify upgrading of resources.","This is the {{upgradeResources}} half of the protobuf-reflection-based upgrade/downgrade of resources: https://issues.apache.org/jira/browse/MESOS-8221    We will also add {{state::read}} to complement {{state::checkpoint}} which will be used to read protobufs from disk rather than {{protobuf::read}}.",Task,Blocker,Resolved,"2018-01-03 18:19:37","2018-01-03 18:19:37",3
"Apache Mesos","Test reconciliation after operation is dropped en route to agent","Since new code paths were added to handle operations on resources in 1.5, we should test that such operations are reconciled correctly after an operation is dropped on the way from the master to the agent.",Task,Major,Resolved,"2018-01-03 16:59:17","2018-01-03 16:59:17",3
"Apache Mesos","Create AuthN support for prune images API","We want to make sure there is a way to configure AuthZ for new API added in MESOS-8360.",Improvement,Major,Resolved,"2017-12-28 20:05:00","2017-12-28 20:05:00",3
"Apache Mesos","Verify that the master acknowledges operation status updates correctly",https://reviews.apache.org/r/65039,Task,Major,Resolved,"2017-12-28 00:44:41","2017-12-28 00:44:41",5
"Apache Mesos","Verify end-to-end operation status update retry after RP failover",,Task,Major,Resolved,"2017-12-28 00:37:13","2017-12-28 00:37:13",5
"Apache Mesos","Example frameworks to support launching mesos-local.","The scheduler driver and library support implicit launching of mesos-local for a convenient test setup. Some of our example frameworks account for this in supporting implicit ACL rendering and more.     We should unify the experience by documenting this behaviour and adding it to all example frameworks.",Improvement,Minor,Resolved,"2017-12-27 21:55:07","2017-12-27 21:55:07",3
"Apache Mesos","Add operator API 'PRUNE_IMAGES' for manual container image GC.",,Improvement,Major,Resolved,"2017-12-27 17:26:12","2017-12-27 17:26:12",3
"Apache Mesos","Create agent endpoints for pruning images","This is a follow up on MESOS-4945, but we agreed that we should create a HTTP endpoint on agent to manually trigger image gc.",Task,Major,Resolved,"2017-12-23 00:07:01","2017-12-23 00:07:01",5
"Apache Mesos","Example frameworks have an inconsistent UX.","Our example frameworks are a bit inconsistent when it comes to specifying things like the framework principal / secret etc..   Many of these examples have great value in testing a Mesos cluster. Unifying the parameterizing would improve the user experience when testing Mesos.    {{MESOS_AUTHENTICATE_FRAMEWORKS}} is being used by many examples for enabling / disabling authentication. {{load_generator_framework}} as one example however uses {{MESOS_AUTHENTICATE}} for that purpose. The credentials themselves are most commonly expected in environment variables {{DEFAULT_PRINCIPAL}} and {{DEFAULT_SECRET}} while in some cases we chose to use {{MESOS_PRINCIPAL}}, {{MESOS_SECRET}} instead.",Improvement,Minor,Resolved,"2017-12-22 20:10:11","2017-12-22 20:10:11",3
"Apache Mesos","Resources may get over allocated to some roles while fail to meet the quota of other roles.","In the quota role allocation stage, if a role gets some resources on an agent to meet its quota, it will also get all other resources on the same agent that it does not have quota for. This may starve roles behind it that have quotas set for those resources.    To fix that, we need to track quota headroom in the quota role allocation stage. In that stage, if a role has no quota set for a scalar resource, it will get that resource only when two conditions are both met:    - It got some other resources on the same agent to meet its quota; And    - After allocating those resources, quota headroom is still above the required amount.",Bug,Major,Resolved,"2017-12-21 04:01:49","2017-12-21 04:01:49",3
"Apache Mesos","Resource provider-capable agents not correctly synchronizing checkpointed agent resources on reregistration","For resource provider-capable agents the master does not re-send checkpointed resources on agent reregistration; instead the checkpointed resources sent as part of the {{ReregisterSlaveMessage}} should be used.    This is not what happens in reality. If e.g., checkpointing of an offer operation fails and the agent fails over the checkpointed resources would, as expected, not be reflected in the agent, but would still be assumed in the master.    A workaround is to fail over the master which would lead to the newly elected master bootstrapping agent state from {{ReregisterSlaveMessage}}.",Bug,Critical,Resolved,"2017-12-20 14:19:34","2017-12-20 14:19:34",2
"Apache Mesos","When a resource provider driver is disconnected, it fails to reconnect.","If the resource provider manager closes the HTTP connection of a resource provider, the resource provider should reconnect itself. For that, the resource provider driver will change its state to DISCONNECTED, call a {{disconnected}} callback and use its endpoint detector to reconnect.  This doesn't work in a testing environment where a {{ConstantEndpointDetector}} is used. While the resource provider is notified of the closed HTTP connection (and logs {{End-Of-File received}}), it never disconnects itself and calls the {{disconnected}} callback. Discarding {{HttpConnectionProcess::detection}} in {{HttpConnectionProcess::disconnected}} doesn't trigger the {{onAny}} callback of that future. This might not be a problem in {{HttpConnectionProcess}} but could be related to the test case using a {{ConstantEndpointDetector}}.",Bug,Major,Resolved,"2017-12-20 13:36:31","2017-12-20 13:36:31",2
"Apache Mesos","Resubscription of a resource provider will crash the agent if its HTTP connection isn't closed","A resource provider might resubscribe while its old HTTP connection wasn't properly closed. In that case an agent will crashm with, e.g., the following log:      This is due to a race condition in {{resource_provider/manager.cpp}} when handling closed HTTP connections of resource providers. If a resource provider resubscribes and its old HTTP connection is still open, the resource provider manager will close it. This is unexpected and will trigger closing the new HTTP connection which results in a failed {{CHECK}}.",Bug,Blocker,Resolved,"2017-12-19 12:46:02","2017-12-19 12:46:02",2
"Apache Mesos","Improve JSON v1 operator API performance.","According to some user reports, a simple comparison of the v1 operator API (using the GET_TASKS call) and the v0 /tasks HTTP endpoint shows that the v1 API suffers from an inefficient implementation:    {noformat: title=Curl Timing}  Operator HTTP API (GET_TASKS): 0.02s user 0.08s system 1% cpu 9.883 total  Old /tasks API: /tasks: 0.00s user 0.00s system 1% cpu 0.222 total  {noformat}    Looking over the implementation, it suffers from the same issues we originally had with the JSON endpoints:    * Excessive copying up the tree of state building calls.  * Building up the state object as opposed to directly serializing it.",Improvement,Major,Resolved,"2017-12-19 00:26:41","2017-12-19 00:26:41",2
"Apache Mesos","Agent can become stuck in (re-)registering state during upgrades","Currently, an agent will not be erased from the set of currently (re-)registering agents if     - it tries to (re-)register with a malformed version string   - it tries to (re-)register with a version smaller than the minimum supported version   - it tries to (re-)register with a domain when the master has no domain configured   - the operator marks the slave as gone while the (re-)registration is ongoing    Afterwards, all further (re-)registration attempts with the same agent id will be discarded, because the master still  thinks that the original (re-)registration is ongoing.    Since most realistic way to encounter this issue would be during cluster upgrades, and it will fix itself with a master restart, it is unlikely to be reported externally.    Review: https://reviews.apache.org/r/64506",Bug,Major,Resolved,"2017-12-18 11:26:43","2017-12-18 11:26:43",3
"Apache Mesos","Quota headroom may be insufficiently held when role has more reservation than quota.","If a role has more reservation than its quota, the current quota headroom calculation is insufficient in guaranteeing quota allocation.    Consider, role `A` with 100 (units of resource, same below) reservation and 10 quota and role `B` with no reservation and 90 quota. Let's say there is no allocation yet. The existing allocator would calculate that the required headroom is 100. And since unallocated quota role reserved resource is also 100, no additional resources would be held back for the headroom.    While role `A` would have no problem getting its quota satisfied. Role `B` may have difficulty getting any resources because the headroom can only be allocated to `A`.    The solution is to calculate per-role headroom before aggregating the quantity. And unallocated reservations should not count towards quota headroom. In the above case. The headroom for role `A` should be zero, the headroom for role `B` should be 90. Thus the aggregated headroom will be `90`.",Bug,Major,Resolved,"2017-12-15 23:47:48","2017-12-15 23:47:48",3
"Apache Mesos","MasterTest.RegistryUpdateAfterReconfiguration is flaky","Observed here: https://jenkins.<USER>com/service/jenkins/job/mesos/job/Mesos_CI-build/2399/FLAG=CMake,label=mesos-ec2-debian-8/testReport/junit/mesos-ec2-debian-8-CMake.Mesos/MasterTest/RegistryUpdateAfterReconfiguration/    The test here failed because the registry contained 2 slaves, when it should have only one.    Looking through the log, everything seems normal (in particular, only 1 slave id appears throughout this test). The only thing out of the ordinary seems to be the agent sending two `RegisterSlaveMessage`s and two `ReregisterSlaveMessage`s, but looking at the code for generating the random backoff factor in the slave that seems to be more or less normal, and shouldn't break the test.",Bug,Major,Resolved,"2017-12-15 16:54:18","2017-12-15 16:54:18",1
"Apache Mesos","Improve logs displayed after a slave failed recovery.","We have to add some steps to clean the Docker daemon state used by the Docker containerizer.",Improvement,Minor,Resolved,"2017-12-13 15:35:36","2017-12-13 15:35:36",2
"Apache Mesos","Add container-specific CGroup FS mounts under /sys/fs/cgroup/* to Mesos containers","Containers launched with Unified Containerizer do not include container-specific CGroup FS mounts under {{/sys/fs/cgroup}}, which are created by default by Docker (usually readonly for unprivileged containers). Let's honor the same convention for Mesos containers.    For example, this is needed by Uber's [{{automaxprocs}}|https://github.com/uber-go/automaxprocs] patch for Go programs, which amends {{GOMAXPROCS}} per CPU quota and requires access to the CPU cgroup subsystem.",Task,Critical,Resolved,"2017-12-13 06:37:44","2017-12-13 06:37:44",13
"Apache Mesos","Mesos containerizer does not properly handle old running containers","We were testing an upgrade scenario recently and encountered the following assertion failure:      Looking into {{Slave::_launch}}, indeed we find an unguarded access to the parent container's {{ContainerConfig}} [here|https://github.com/apache/mesos/blob/c320ab3b2dc4a16de7e060b9e15e9865a73389b0/src/slave/containerizer/mesos/containerizer.cpp#L1716].    We recently [added checkpointing|https://github.com/apache/mesos/commit/03a2a4dfa47b1d47c5eb23e81f5ef8213e46d545] of {{ContainerConfig}} to the Mesos containerizer. It seems that we are not appropriately handling upgrades, when there may be old containers running for which we do not expect to recover a {{ContainerConfig}}.",Bug,Blocker,Resolved,"2017-12-12 23:50:17","2017-12-12 23:50:17",2
"Apache Mesos","Tests that fetch docker images might be flaky due to insufficient wait timeout.",,Bug,Major,Resolved,"2017-12-08 16:42:33","2017-12-08 16:42:33",2
"Apache Mesos","Add authorization to display of resource provider information in API calls and endpoints","The {{GET_RESOURCE_PROVIDERS}} call is used to list all resource providers known to a Mesos agent. We akso display resource provider infos for the master's {{GET_AGENTS}} call. These call needs to be authorized.",Task,Major,Resolved,"2017-12-08 09:58:36","2017-12-08 09:58:36",3
"Apache Mesos","Pass resource provider information to master as part of UpdateSlaveMessage","We extended {{UpdateSlaveMessage}} so updates to an agent's total resources from resource providers are possible. We realized that will need to explicitly pass resource provider details (here for now: {{ResourceProviderInfo}}) to the master so it can be queried for the providers present on certain agents. This should happen as part of {{UpdateSlaveMessage}} so a single synchronization channel is used for this kind of information.    We need to adjust {{UpdateSlaveMessage}} for these requirements. This should happen before 1.5.0 gets released so we do not need to deprecate a never really used message format.",Bug,Major,Resolved,"2017-12-07 20:11:44","2017-12-07 20:11:44",5
"Apache Mesos","Document container image garbage collection.","Document container image garbage collection.",Documentation,Major,Resolved,"2017-12-07 17:14:46","2017-12-07 17:14:46",2
"Apache Mesos","Introduce a UUID message type","Currently when UUID need to be part of a protobuf message, we use a byte array field for that. This has some drawbacks, especially when it comes to outputting the UUID in logs: To stringify the UUID field, we first have to create a stout UUID, then call {{.toString()}} of that one. It would help to have a UUID type in {{mesos.proto}} and provide a stringification function for it in {{type_utils.hpp}}.",Task,Major,Resolved,"2017-12-07 08:16:57","2017-12-07 08:16:57",3
"Apache Mesos","DefaultExecutorTest.ROOT_MultiTaskgroupSharePidNamespace is flaky.","On Ubuntu 16.04:    Full log attached.    On Fedora 23:      The test became flaky shortly after MESOS-7306 has been committed and likely related to it.",Bug,Major,Accepted,"2017-12-06 14:22:42","2017-12-06 14:22:42",2
"Apache Mesos","Update CHANGELOG to call out agent reconfiguration feature","This is a feature worth calling out in the 1.5.0 CHANGELOG.",Documentation,Major,Resolved,"2017-12-05 23:46:25","2017-12-05 23:46:25",1
"Apache Mesos","Add user doc for agent reconfiguration",,Documentation,Major,Resolved,"2017-12-05 23:44:31","2017-12-05 23:44:31",3
"Apache Mesos","Built-in driver-based executors ignore kill task if the task has not been launched.","If docker executor receives a kill task request and the task has never been launch, the request is ignored. We now know that: the executor has never received the registration confirmation, hence has ignored the launch task request, hence the task has never started. And this is how the executor enters an idle state, waiting for registration and ignoring kill task requests.",Bug,Blocker,Resolved,"2017-12-04 17:59:02","2017-12-04 17:59:02",5
"Apache Mesos","Add excluded image parameter to containerizer::pruneImages() interface.","Add excluded image parameter to containerizer::pruneImages() interface.",Improvement,Major,Resolved,"2017-12-02 00:51:20","2017-12-02 00:51:20",2
"Apache Mesos","Support container image basic auto gc.","Add heuristic logic in the agent for basic auto image gc support.    Please see this section for the new interface design:  https://docs.google.com/document/d/1TSn7HOFLWpF3TLRVe4XyLpv6B__A1tk-tU16B1ZbsCI/edit#heading=h.iepp3ce9i22i  ",Improvement,Major,Resolved,"2017-12-02 00:45:59","2017-12-02 00:45:59",8
"Apache Mesos","Reservation may not be allocated when the role has no quota.","Reservations that belong to a role that has no quota may not be allocated even when the reserved resources are allocatable to the role.    This is because in the current implementation the reserved resources may be counted towards the headroom left for unallocated quota limit in the second stage allocation.    https://github.com/apache/mesos/blob/c844db9ac7c0cef59be87438c6781bfb71adcc42/src/master/allocator/mesos/hierarchical.cpp#L1764-L1767    Roles with quota do not have this issue because currently their reservations are taken care of in the first stage.",Bug,Critical,Resolved,"2017-12-01 23:44:45","2017-12-01 23:44:45",3
"Apache Mesos","Add documentation about fault domains","We need some user docs for fault domains.",Documentation,Major,Resolved,"2017-12-01 23:35:59","2017-12-01 23:35:59",3
"Apache Mesos","ReservationTest.MasterFailover is flaky when run with `RESOURCE_PROVIDER` capability.","On a system under load, {{ResourceProviderCapability/ReservationTest.MasterFailover/1}} can fail. {{GLOG_v=2}} of the failure:   ",Bug,Major,Resolved,"2017-12-01 14:04:03","2017-12-01 14:04:03",3
"Apache Mesos","SlaveTest.IgnoreV0ExecutorIfItReregistersWithoutReconnect is flaky.","  Full log attached.",Bug,Major,Resolved,"2017-12-01 13:15:36","2017-12-01 13:15:36",3
"Apache Mesos","Mesos Containerizer GC should set 'layers' after checkpointing layer ids in provisioner.","    Please neglect the debugging logs like '111111'. To reproduce this issue, just continuously trigger image gc. The log above was from a scenario that we launch two nested containers. One sleeps 1 second, another sleep forever.    This is related to this patch: https://github.com/apache/mesos/commit/e273efe6976434858edb85bbcf367a02e963a467#diff-a3593ed0ebd2b205775f7f04d9b5afe7    The root cause is that we did not set the 'layers' after we checkpoint the layer ids in provisioner. The log below is the prove:  ",Bug,Critical,Resolved,"2017-11-30 00:04:03","2017-11-30 00:04:03",3
"Apache Mesos","Persistent volumes are not visible in Mesos UI using default executor on Linux.","The reason is because on Linux, if multiple containers in a default executor want to share a persistent volume, it'll use SANDBOX_PATH volume source with type PARENT. This will be translated into a bind mount in the nested container's mount namespace, thus not visible in the host mount namespace. Mesos UI operates in the host mount namespace.    One potential solution for that is to create a symlink (instead of just a mkdir) in the sandbox. The symlink will be shadowed by the bind mount in the nested container, but in the host mount namespace, it'll points to the corresponding persistent volume.",Bug,Major,Resolved,"2017-11-29 23:23:22","2017-11-29 23:23:22",3
"Apache Mesos","Add an agent endpoint to list all active resource providers","Operators/Frameworks might need information about all resource providers currently running on an agent. An API endpoint should provide that information and include resource provider name and type.",Task,Major,Resolved,"2017-11-28 07:57:04","2017-11-28 07:57:04",3
"Apache Mesos","Support resource provider re-subscription in the resource provider manager","Resource providers may re-subscribe by sending a {{SUBSCRIBE}} call that includes a resource provider ID. Support for this has to be added to the resource provider manager. E.g., the manager should check if a resource provider with the ID exists and use the updated HTTP connection.",Task,Major,Resolved,"2017-11-28 07:55:15","2017-11-28 07:55:15",3
"Apache Mesos","NestedMesosContainerizerTest.ROOT_CGROUPS_RecoverLauncherOrphans is flaky.","  Full log attached.",Bug,Major,Resolved,"2017-11-27 10:32:25","2017-11-27 10:32:25",2
"Apache Mesos","Add state recovery for storage local resource provider.","The storage local resource provider needs to checkpoint its total resources and pending operations atomically, and recover them after failing over.",Task,Major,Resolved,"2017-11-27 00:31:54","2017-11-27 00:31:54",3
"Apache Mesos","ResourceProviderManagerHttpApiTest.ConvertResources is flaky","From a ASF CI run:      ",Bug,Major,Resolved,"2017-11-23 15:15:48","2017-11-23 15:15:48",2
"Apache Mesos","Mesos.DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer is flaky.","  Full log attached.",Bug,Major,Resolved,"2017-11-23 12:04:02","2017-11-23 12:04:02",2
"Apache Mesos","Introduce a way to resolve the profile for disk resources","This builds off MESOS-8060 which introduced the {{profile}} field to disk resources.    A volume profile will consist of a {{string}} which maps to a list of {{VolumeCapability}} and some free-form parameters (string key-value pairs).  See the {{message CreateVolumeResponse}} under https://github.com/container-storage-interface/spec/blob/master/csi.proto for more information about the fields.    We will introduce a module which will allow the operator to specify this mapping.  Unfortunately, due to the nature of volume profiles, we cannot provide a reasonable default mapping of profiles (because we don't have any idea what volumes are available).  Instead, there will be two implementations of the module:  1) The default will essentially turn volume profiles off.  Any attempt to use volume profiles will simply fail.  2) An optional module will allow the operator to specify a URI (file or link) that contains a JSON representation of the mapping.  The module will have some knobs to tune the frequency at which this URI is fetched from.",Task,Major,Resolved,"2017-11-21 01:29:09","2017-11-21 01:29:09",3
"Apache Mesos","Support image prune in mesos containerizer and provisioner.","Implement image prune in containerizer and the provisioner, by using mark and sweep to garbage collect unused layers.",Bug,Major,Resolved,"2017-11-20 20:38:41","2017-11-20 20:38:41",13
"Apache Mesos","SlaveRecoveryTest/0.ReconnectExecutor is flaky.","Observed it today in our CI. Logs attached.",Bug,Major,Resolved,"2017-11-17 09:58:41","2017-11-17 09:58:41",3
"Apache Mesos","Add operator API to reload local resource providers.","To add, remove and update local resource providers on the fly more conveniently and without restarting agents, we would like to introduce new operator API to add new config files in the resource provider config directory and trigger a reload for the resource provider.",Task,Major,Resolved,"2017-11-17 07:07:59","2017-11-17 07:07:59",5
"Apache Mesos","Add metrics for offer operation feedback",,Task,Blocker,Resolved,"2017-11-16 01:22:30","2017-11-16 01:22:30",5
"Apache Mesos","Add an option to build the new CLI and run unit tests.","An update of the discarded [https://reviews.apache.org/r/52543/]    Also needs to be available for CMake.",Improvement,Major,Resolved,"2017-11-15 23:12:56","2017-11-15 23:12:56",8
"Apache Mesos","Add recommended accept/decline/suppress/revive behavior to scheduler docs","We should update the scheduler documentation to provide recommendations to framework authors regarding how to use the ACCEPT / DECLINE / SUPPRESS / REVIVE calls, in order to maximize resource availability.",Task,Major,Reviewable,"2017-11-15 21:37:36","2017-11-15 21:37:36",2
"Apache Mesos","Strip (Offer|Resource).allocation_info for non-MULTI_ROLE schedulers.","In support of MULTI_ROLE capable frameworks, a Resource.allocation_info field was added and the Resource math of the Mesos library was updated to check for matching allocation_info when checking for (in)equality, addability, subtractability, containment, etc. To compensate for these changes, the demo frameworks of Mesos were updated to set the allocation_info for Resource objects during the matching phase in which offers' resources are evaluated in order for the framework to launch tasks. The Mesos demo frameworks NEEDED to be updated because the Resource algebra within Mesos now depended on matching allocation_info fields of Resource objects when executing algebraic operations. See https://github.com/apache/mesos/commit/c20744a9976b5e83698e9c6062218abb4d2e6b25#diff-298cc6a77862b7ff3422cd06c215ef28R91 .    This poses a unique problem for **external** libraries that both aim to support various frameworks, some that DO and some that DO NOT opt-in to the MULTI_ROLE capability; specifically those external libraries that implement Resource algebra that's consistent with what Mesos implements internally. One such example of a library is mesos-go, though there are undoubtedly others. The problem can be explained via this scenario:   {quote}  Flo's mesos-go framework is running well, it doesn't opt-in to MULTI_ROLE because it doesn't need multiple roles. His framework runs on a version of Mesos that existed prior to integration of MULTI_ROLE support. His DC operator upgrades the mesos cluster to the latest version. Flo rebuilds his framework on the latest version of mesos-go and re-launches it on the cluster. He observes that his framework receives offers, but rejects ALL of them. Digging into the code he realizes that Mesos is injecting allocation_info into Resource objects being offered to his framework, and mesos-go considers allocation_info when comparing Resource objects (because it's MULTI_ROLE compatible now), but his framework doesn't take this into consideration when preparing its own Resource objects prior to the resource matching phase. The consequence is that Flo's framework is trying to match against Resources that will never align because his framework isn't setting an allocation_info that might possibly match the allocation_info that Mesos is always injecting - regardless of the MULTI_ROLE capability (or lack thereof in this case) of his framework.  {quote}    If Mesos were to strip the allocation_info from Resource objects, prior to offering them to non-multi-role frameworks, then the problem illustrated above would go away.  ",Bug,Major,Resolved,"2017-11-15 20:48:29","2017-11-15 20:48:29",5
"Apache Mesos","Change the auto backend logic of backing filesystem from exclusive to inclusive.","Currently, the auto backend logic is exclusive for backing filesystem:      This might cause unstable issue on some specific backing fs. An inclusive logic should be supported. See:  https://docs.docker.com/engine/userguide/storagedriver/selectadriver/#supported-backing-filesystems",Improvement,Major,Reviewable,"2017-11-15 18:27:08","2017-11-15 18:27:08",3
"Apache Mesos","mesos.interface 1.4.0 cannot be installed with pip","This breaks some framework development tooling.    WIth latest pip:      This works fine for previous releases:      But it does not for 1.4.0:      Verbose output shows that pip skips the 1.4.0 distribution:  ",Task,Major,Resolved,"2017-11-14 19:31:32","2017-11-14 19:31:32",1
"Apache Mesos","Add resource versions to RunTaskMessage","To support speculative application of certain offer operations we have added resource versions to offer operation messages. This permits checking compatibility of master and agent state before applying operations.    Launch operations are not modelled with offer operation messages, but instead with {{RunTaskMessage}}. In order to provide the same consistency guarantees we need to add resource versions to {{RunTaskMessage}} as well. Otherwise we would only rely on resource containment checks in the agent to catch inconsistencies; these can be unreliable as there is no guarantee that the matched agent resource is unique (e.g., with two {{RESERVE}} operations on similar resorces triggered on the same agent and one of these failing, the other succeeding, we would end up potentially sending one framework a success status and the other a failed one, but would not do anything the make sure the speculative operation application matches the resources belonging to the sent offer operation status update).",Task,Major,Resolved,"2017-11-14 18:50:16","2017-11-14 18:50:16",3
"Apache Mesos","Use protobuf reflection to simplify downgrading of resources.","We currently have a {{downgradeResources}} function which is called on every  {{repeated Resource}} field in every message that we checkpoint. We should leverage  protobuf reflection to automatically downgrade any instances of {{Resource}} within any  protobuf message.",Improvement,Blocker,Resolved,"2017-11-14 18:08:42","2017-11-14 18:08:42",5
"Apache Mesos","Validate that any offer operation is only applied on resources from a single provider","Offer operations can only be applied to resources from one single resource provider. A number of places in the implementation assume that the provider ID obtained from any {Resource} in an offer operation is equivalent to the one from any other resource. We should update the master to validate that invariant and reject malformed operations.",Task,Major,Resolved,"2017-11-14 13:39:59","2017-11-14 13:39:59",1
"Apache Mesos","Support `RESERVE`/`CREATE` operations with resource providers","{{RESERVE}}/{{UNRESERVE}}/{{CREATE}}/{{DESTROY}} operations should work with resource provider resources like they do with agent resources. I.e. they will be speculatively applied and an offer operation will be sent to the respective resource provider.",Task,Major,Resolved,"2017-11-14 09:17:55","2017-11-14 09:17:55",5
"Apache Mesos","Handle agent local resources in offer operation handler","The master will send {{ApplyOfferOperationMessage}} instead of {{CheckpointResourcesMessage}} when an agent has the 'RESOURCE_PROVIDER' capability set. The agent handler for the message needs to be updated to support operations on agent resources.",Task,Major,Resolved,"2017-11-13 10:48:34","2017-11-13 10:48:34",5
"Apache Mesos","Reconcile offer operations between resource providers, agents, and master","We need to implement reconciliation of pending or unacknowledged offer operations between resource providers and agent, and agents and master. ",Task,Major,Resolved,"2017-11-10 15:37:33","2017-11-10 15:37:33",2
"Apache Mesos","Add the pip-requirements from other modules to the pylint virtual environment","We have a virtual environment in {{/support}} that is used by Pylint and ESlint. Pylint will then lint modules that need a different set of dependencies to work. If we do not pull the pip-requirements from other modules to the linter virtual environment, we will see {{import-error}}s when linting Python files.    One solution would be to use the {{--init-hook}} flag of Pylint in {{PyLinter}}. When using {{run_lint}}, we would group the modified files per module and then run pylint for all the modified files of each module.",Improvement,Major,Resolved,"2017-11-10 14:55:12","2017-11-10 14:55:12",3
"Apache Mesos","Eliminate agent failover after resource checkpointing failure","Currently, when the agent encounters an error checkpointing its resources to disk, the agent process will exit. Now that the master sends {{ApplyOperationMessage}} to the agent in order to apply operations, we can implement operation feedback on the agent and the agent no longer needs to unconditionally terminate when checkpointing fails.    For backward compatibility with older masters, the agent should still terminate if it receives a {{CheckpointResourcesMessage}} from the master and an error is encountered while checkpointing.    However, when checkpointing is attempted in the handler for {{ApplyOperationMessage}}, the agent can handle errors by sending a terminal operation update to the master.",Task,Major,Open,"2017-11-09 22:52:55","2017-11-09 22:52:55",3
"Apache Mesos","Add end to end tests for offer operation feedback",,Task,Major,Resolved,"2017-11-09 22:40:51","2017-11-09 22:40:51",8
"Apache Mesos","Add plumbing for explicit offer operation reconciliation between master, agent, and RPs.",,Task,Major,Resolved,"2017-11-09 22:01:25","2017-11-09 22:01:25",2
"Apache Mesos","Update the ReconcileOfferOperations protos","Some protos have been committed, but they follow an event-based API.    We decided to follow the request/response model for this API, so we need to update the protos.",Task,Major,Resolved,"2017-11-09 21:34:15","2017-11-09 21:34:15",1
"Apache Mesos","Implement a library to send offer operation status updates",,Task,Major,Resolved,"2017-11-09 21:26:53","2017-11-09 21:26:53",8
"Apache Mesos","Propagate failures from applying offer operations from resource providers","With MESOS-8087 we add support for sending offer operation status updates from resource providers. There we only implemented the success path, but we should still adjust the code so failures can be communicated. This would require generating new resource version UUIDs in the resource providers and transporting them to the master for bookkeeping.",Task,Major,Resolved,"2017-11-09 16:27:43","2017-11-09 16:27:43",3
"Apache Mesos","Implement explicit offer operation reconciliation between the master, agent and RPs.","Upon receiving an {{UpdateSlave}} message the master should compare its list of pending operations for the agent/LRPs to the list of pending operations contained in the message. It should then build a {{ReconcileOfferOperations}} message with all the operations missing in the {{UpdateSlave}} message and send it to the agent.    The agent will receive these messages and should handle them by itself if the operations affect the default resources, or forward them to the RP manager otherwise.    The agent/RP handler should check if the operations are pending. If an operation is not pending, then an {{ApplyOfferOperation}} message got dropped, and the agent/LRP should send an {{OFFER_OPERATION_DROPPED}} status update to the master.",Task,Major,Resolved,"2017-11-09 01:44:39","2017-11-09 01:44:39",3
"Apache Mesos","Agents should handle acks for operations affecting default resources.","The agent's acknowledgement handler should be updated to be able to handle acks for updates related to operations affecting agent default resources.",Task,Major,Resolved,"2017-11-09 01:37:56","2017-11-09 01:37:56",3
"Apache Mesos","Update master’s OfferOperationStatusUpdate handler to acknowledge updates to the agent if OfferOperationID is not set.",,Task,Major,Resolved,"2017-11-09 01:33:23","2017-11-09 01:33:23",2
"Apache Mesos","Update the scheduler library to support request/response API calls.","The scheduler client/library should be updated to add support for API calls following the request/response model, e.g., {{ReconcileOfferOperations}}.",Task,Major,Resolved,"2017-11-09 01:30:42","2017-11-09 01:30:42",5
"Apache Mesos","Implement ReconcileOfferOperations handler in the master","The master will synchronously respond to the framework with a {{OFFER_OPERATIONS_RECONCILIATION}} response.",Task,Major,Resolved,"2017-11-09 01:28:48","2017-11-09 01:28:48",5
"Apache Mesos","Update the master to accept OfferOperationIDs from frameworks.","Master’s {{ACCEPT}} handler should send failed operation updates when a framework sets the {{OfferOperationID}} on an operation destined for an agent without the {{RESOURCE_PROVIDER}} capability.",Task,Major,Resolved,"2017-11-09 01:25:02","2017-11-09 01:25:02",3
"Apache Mesos","Master’s OperationStatusUpdate handler should forward updates to the framework when OfferOperationID is set.",,Task,Major,Resolved,"2017-11-09 01:21:12","2017-11-09 01:21:12",2
"Apache Mesos","Enable agent to send operation status updates, checkpoint, and retry using the SUM",,Task,Major,Resolved,"2017-11-09 01:12:41","2017-11-09 01:12:41",8
"Apache Mesos","Enable LRP to send operation status updates, checkpoint, and retry using the SUM",,Task,Major,Resolved,"2017-11-09 01:12:29","2017-11-09 01:12:29",8
"Apache Mesos","Implement the agent's AcknowledgeOfferOperationMessage handler.","The handler should forward acks to the RP manager for resource provider operations.  Handling of operations on agent default resources will be taken care of as part of MESOS-8194",Task,Major,Resolved,"2017-11-09 01:10:18","2017-11-09 01:10:18",3
"Apache Mesos","Implement master's AcknowledgeOfferOperationMessage handler.","This handler should validate the message and forward it to the corresponding agent/ERP.",Task,Major,Resolved,"2017-11-09 01:09:05","2017-11-09 01:09:05",5
"Apache Mesos","Add a container daemon to monitor a long-running standalone container.","The `ContanierDaemon` class is responsible to monitor if a long-running service running in a standalone container is ready to serve, and restart the service container if not. It does not manage the lifecycle of the contanier it monitors, so the container persists across `ContainerDaemon`s.",Task,Major,Resolved,"2017-11-08 19:54:58","2017-11-08 19:54:58",3
"Apache Mesos","Scheduler library has incorrect assumptions about connections.","Scheduler library assumes that a connection cannot be interrupted between continuations, for example {{send()}} and {{_send()}}: [https://github.com/apache/mesos/blob/509a1ab3226bbec7c369f431656f4ec692da00ba/src/scheduler/scheduler.cpp#L553]. This is not true, {{detected()}} can fire in-between, leading to disconnection:      The bug has been introduced in https://reviews.apache.org/r/62594",Bug,Critical,Resolved,"2017-11-07 18:05:35","2017-11-07 18:05:35",1
"Apache Mesos","Using a failoverTimeout of 0 with Mesos native scheduler client can result in infinite subscribe loop","Over the past year, the Marathon team has been plagued with an issue that hits our CI builds periodically in which the scheduler driver enters a tight loop, sending 10,000s of SUBSCRIBE calls to the master per second. I turned on debug logging for the client and the server, and it pointed to an issue with the {{doReliableRegistration}} method in sched.cpp. Here's the logs:        In Marathon, when we are running our tests, we set the failoverTimeout to 0 in order to cause the Mesos master to immediately forget about a framework when it disconnects.    On line 860 of sched.cpp, the retry-delay is set to 1/10th the failoverTimeout, which provides the best explanation for why the value is 0:        Reading through the code, it seems that once this value is 0, it will always be zero, since backoff is multiplicative (0 * 2 == 0), and the failover_timeout / 10 limit is applied each time.    To make matters worse, failoverTimeout of {{0}} is the default:        I've confirmed that when using 1.4.0 of the Mesos client java jar, this default is used if failoverTimeout is not set:    ",Bug,Minor,Resolved,"2017-11-04 07:23:52","2017-11-04 07:23:52",2
"Apache Mesos","Review #62775 broke the build","Review https://reviews.apache.org/r/62775/ broke the Windows build.    Note: the Windows Review Bot posted a build break to the patch before it was reviewed and submitted. We need to do a post-mortem to determine why the build break, despite automation catching it, was still submitted.",Bug,Critical,Resolved,"2017-11-01 21:04:32","2017-11-01 21:04:32",1
"Apache Mesos","Add a mock resource provider manager.","To test a storage local resource provider, we need to inject a mock resource provider manager such that:  1. A full agent will start during the test so the resource provider can launch standalone containers for CSI plugins.  2. We can inject offer operations through the mock manager to test the resource provider.",Task,Major,Resolved,"2017-10-28 01:23:59","2017-10-28 00:23:59",3
"Apache Mesos","Publish and unpublish storage local resources through CSI plugins.","Storage local resource provider needs to call the following CSI API to publish CSI volumes for tasks to use:  1. ControllerPublishVolume (optional)  2. NodePublishVolume    Although we don't need to unpublish CSI volumes after tasks are completed, we still needs to unpublish them for DESTROY_VOLUME or DESTROY_BLOCK:  1. NodeUnpublishVolume  2. ControllerUnpublishVolume (optional)",Task,Major,Resolved,"2017-10-28 01:16:12","2017-10-28 00:16:12",3
"Apache Mesos","Add filesystem layout for storage resource providers.","We need directories for placing mount points and checkpoint CSI volume state for storage resource providers. Unlike resource checkpoints, CSI volume states should persist across agents since otherwise the CSI plugin might not work properly.",Task,Major,Resolved,"2017-10-27 23:14:50","2017-10-27 22:14:50",3
"Apache Mesos","Masters can lose track of tasks' executor IDs.","The response to the master's {{state}} endpoint sometimes doesn't include the executor id corresponding to some tasks.    Agents send to the leading master a list of tasks when reregistering. Tasks that are not started by the command executor should contain an executor id, but the following snippet can sometimes clear the executor id of tasks started by other executors: https://github.com/apache/mesos/blob/1.4.0/src/slave/slave.cpp#L1515-L1522    ",Bug,Major,Resolved,"2017-10-26 00:41:16","2017-10-25 23:41:16",4
"Apache Mesos","Design a library to send offer operation status updates","As detailed in the [offer operation feedback design doc|https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI/edit#], we need to add a library to do the following:  * Send offer operation status updates  * Checkpoint pending/unacknowledged operations  * Retry operation status updates until an acknowledgement is received    This should be a common library which can be used by the agent (for its default resources) and by local resource providers. In the future, it can also be used by external resource providers.    We should write a short design doc to explore precisely how this will be implemented. It can probably be modeled after the task status update manager in the agent.",Task,Major,Resolved,"2017-10-25 18:45:24","2017-10-25 17:45:24",3
"Apache Mesos","Add new protobuf messages for offer operation feedback","We should add the necessary protobuf messages for offer operation feedback as detailed in the [offer operation feedback design doc|https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI/edit#].",Task,Major,Resolved,"2017-10-25 18:37:47","2017-10-25 17:37:47",2
"Apache Mesos","Add placeholder handlers for offer operation feedback","In order to sketch out the flow of messages necessary to facilitate offer operation feedback, we should add some empty placeholder handlers to the master and agent as detailed in the [offer operation feedback design doc|https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI/edit#].",Task,Major,Resolved,"2017-10-25 18:35:33","2017-10-25 17:35:33",2
"Apache Mesos","Make os::pipe file descriptors O_CLOEXEC.","File descriptors from {{os::pipe}} will be inherited across exec. On Linux we can use [pipe2|http://man7.org/linux/man-pages/man2/pipe.2.html] to atomically make the pipe {{O_CLOEXEC}}.",Bug,Critical,Resolved,"2017-10-24 22:44:49","2017-10-24 21:44:49",3
"Apache Mesos","Agent should properly handle recovering an executor when its pid is reused","Here's how to reproduce this issue:    # Start a task using the Docker containerizer (the same will probably happen with the command executor).  # Stop the corresponding Mesos agent while the task is running.  # Change the executor's checkpointed forked pid, which is located in the meta directory, e.g., {{/var/lib/mesos/slave/meta/slaves/latest/frameworks/19faf6e0-3917-48ab-8b8e-97ec4f9ed41e-0001/executors/foo.13faee90-b5f0-11e7-8032-e607d2b4348c/runs/latest/pids/forked.pid}}. I used pid 2, which is normally used by {{kthreadd}}.  # Reboot the host",Bug,Critical,Resolved,"2017-10-24 00:44:27","2017-10-23 23:44:27",2
"Apache Mesos","GPU tests are failing due to TASK_STARTING.","For instance: NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_VerifyDeviceAccess    ",Bug,Major,Resolved,"2017-10-21 06:23:18","2017-10-21 05:23:18",1
"Apache Mesos","Unified Containerizer Auto backend should check xfs ftype for overlayfs backend.","when using xfs as the backing filesystem in unified containerizer, the `ftype` has to be equal to 1 if we are using the overlay fs backend. we should add the detection in auto backend logic because some OS (like centos 7.2) has xfs ftype=0 by default.    https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/",Bug,Major,Resolved,"2017-10-20 22:20:53","2017-10-20 21:20:53",3
"Apache Mesos","ROOT_DOCKER_DockerHealthyTask segfaults in debian 8.","This test consistently cannot recover the agent on two debian 8 builds: with SSL and CMake based. The error is always the same (full logs attached):  ",Bug,Major,Resolved,"2017-10-20 19:34:07","2017-10-20 18:34:07",3
"Apache Mesos","Add a master flag to disallow agents that are not configured with fault domain","Once mesos masters and agents in a cluster are *all* upgraded to a version where the fault domains feature is available, it is beneficial to enforce that agents without a fault domain configured are not allowed to join the cluster.     This is a safety net for operators who could forget to configure the fault domain of a remote agent and let it join the cluster. If this happens, an agent in a remote region will be considered a local agent by the master and frameworks (because agent's fault domain is not configured) causing tasks to potentially land in a remote agent which is undesirable.    Note that this has to be a configurable flag and not enforced by default because otherwise upgrades from a fault domain non-configured cluster to a configured cluster will not be possible.",Improvement,Major,Resolved,"2017-10-20 11:18:24","2017-10-20 10:18:24",3
"Apache Mesos","Process offer operations in storage local resource provider","The storage local resource provider receives offer operations for reservations and resource conversions, and invoke proper CSI calls to implement these operations.",Task,Major,Resolved,"2017-10-18 02:03:14","2017-10-18 01:03:14",3
"Apache Mesos","Add a call to update total resources in the resource provider API.","We should add a call for a resource provider to update the total resources, and remove {{resources}} from the {{SUBSCRIBE}} call and instead moving to a protocol where a resource provider first subscribes and then updates its resources.",Task,Major,Resolved,"2017-10-17 21:40:23","2017-10-17 20:40:23",3
"Apache Mesos","Docker fetcher plugin unsupported scheme failure message is not accurate.","https://github.com/apache/mesos/blob/1.4.0/src/uri/fetchers/docker.cpp#L843    This failure message is not accurate. For such a case, if the user/operator give a wrong credential to communicate to a BASIC auth based docker private registry. The authentication failed but the log is still saying: Unsupported auth-scheme: BASIC    ",Improvement,Major,Resolved,"2017-10-17 19:27:12","2017-10-17 18:27:12",3
"Apache Mesos","Add a test CSI plugin for storage local resource provider.","We need a dummy CSI plugin for testing storage local resoure providers. The test CSI plugin would just create subdirectories under its working directories to mimic the behavior of creating volumes, then bind-mount those volumes to mimic publish.",Task,Major,Resolved,"2017-10-17 01:58:06","2017-10-17 00:58:06",5
"Apache Mesos","Import resources from CSI plugins in storage local resource provider.","The following lists the steps to import resources from a CSI plugin:  1. Launch the node plugin      1.1 GetSupportedVersions      1.2 GetPluginInfo      1.3 ProbeNode      1.4 GetNodeCapabilities  2. Launch the controller plugin      2.1 GetSuportedVersions      2.2 GetPluginInfo      2.3 GetControllerCapabilities  3. GetCapacity  4. ListVolumes  5. Report to the resource provider through UPDATE_TOTAL_RESOURCES",Task,Major,Resolved,"2017-10-17 01:49:59","2017-10-17 00:49:59",5
"Apache Mesos","Authorize standalone container calls from local resource providers.","We need to add authorization for a local resource provider to call the standalone container API to prevent the provider from manipulating arbitrary containers. We can use the same JWT-based authN/authZ mechanism for executors, where the agent will create a auth token for each local resource provider instance:  ",Task,Major,Resolved,"2017-10-16 23:46:18","2017-10-16 22:46:18",5
"Apache Mesos","Add protobuf for checkpointing resource provider states.","Resource providers need to checkpoint operations and resource reservations/volume types/etc atomically, so we will need to add a {{src/resource_providers/state.proto}} file that contains protobuf messages for checkpointing.",Task,Major,Resolved,"2017-10-16 19:43:04","2017-10-16 18:43:04",2
"Apache Mesos","Add filesystem layout for local resource providers.","We need to add a checkpoint directory for local resource providers. The checkpoints should be tied to the slave ID, otherwise resources with the same ID appearing on different agents (due to agent failover and registering with a new ID) may confuse frameworks.",Task,Major,Resolved,"2017-10-16 19:23:43","2017-10-16 18:23:43",2
"Apache Mesos","Enqueueing events in MockHTTPScheduler can lead to segfaults.","Various tests segfault due to a yet unknown reason. Comparing logs (attached) hints that the problem might be in the scheduler's event queue.",Bug,Critical,Accepted,"2017-10-15 01:34:52","2017-10-15 00:34:52",5
"Apache Mesos","ResourceProviderRegistrarTest.AgentRegistrar is flaky.","Observed it in internal CI. Test log attached.",Bug,Major,Resolved,"2017-10-14 22:57:33","2017-10-14 21:57:33",2
"Apache Mesos","Leverage helper functions to reduce boilerplate code related to v1 API.","https://reviews.apache.org/r/61982/ created an example how test code related to scheduler v1 API can be simplified with appropriate usage of helper function. For example, instead of crafting a subscribe call manually like in    a helper function {{v1::scheduler::SendSubscribe()}} shall be invoked.    To find all occurrences that shall be fixed, one can grep the test codebase for {{call.set_type}}. At the moment I see the following files:      The same applies for sending status update acks; {{v1::scheduler::SendAcknowledge()}} action shall be used instead of manually crafting acks.",Improvement,Major,Open,"2017-10-14 07:25:20","2017-10-14 06:25:20",3
"Apache Mesos","Some tests miss subscribed event because expectation is set after event fires.","Tests    all have the same problem. They initiate a scheduler subscribe call in reaction to {{connected}} event. However, an expectation for {{subscribed}} event is created _afterwards_, which might lead to an uninteresting mock function call for {{subscribed}} followed by a failure to wait for {{subscribed}}, see attached log excerpt for more details. Problematic code is here: https://github.com/apache/mesos/blob/1c51c98638bb9ea0e8ec6a3f284b33d6c1a4e8ef/src/tests/containerizer/runtime_isolator_tests.cpp#L593-L615    A possible solution is to await for {{subscribed}} only, without {{connected}}, setting the expectation before a connection is attempted, see https://github.com/apache/mesos/blob/1c51c98638bb9ea0e8ec6a3f284b33d6c1a4e8ef/src/tests/default_executor_tests.cpp#L139-L159.",Bug,Major,Resolved,"2017-10-14 07:02:20","2017-10-14 06:02:20",2
"Apache Mesos","Allow the KillPolicy to specify a command","As specified in the design doc of MESOS-7951, the default executor should be extended to allow the specification of a command in the {{KillPolicy}}.",Improvement,Major,Open,"2017-10-13 21:15:56","2017-10-13 20:15:56",5
"Apache Mesos","Allow the KillPolicy to specify a signal","As specified in the design doc of MESOS-7951, the default executor should be updated to allow the framework to specify a particular signal to be used when initiating task termination.",Improvement,Major,Open,"2017-10-13 21:13:44","2017-10-13 20:13:44",3
"Apache Mesos","Add messages to publish resources on a resource provider","Before launching a task that uses resource provider resources, the resource provider needs to be informed to publish these resources as it may take some necessary actions. For external resource providers resources might also have to be unpublished when a task is finished. The resource provider needs to ack these calls after it's ready.",Task,Major,Resolved,"2017-10-13 12:59:41","2017-10-13 11:59:41",7
"Apache Mesos","Introduce Lamport timestamp for offer operations.","We need to use Lamport clock (https://en.wikipedia.org/wiki/Lamport_timestamps) to establish partial ordering between offer operation, and the resources the operation is operating on.    It is used to establish happens before relations so that RPs can reject those operations that applies to a stale snapshot of the resources due to speculation failures.    See more details in this doc:  https://docs.google.com/document/d/1RrrLVATZUyaURpEOeGjgxA6ccshuLo94G678IbL-Yco/edit#    Given that the Lamport clock needs to be transferred between agent and masters, it needs to be serialized to protobuf. We probably needs to define the following methods for it:  ```  merge(...); // Take a max between the two.  increment();  operation<(...);  copy and assignment operator  ```",Task,Major,Resolved,"2017-10-13 03:16:04","2017-10-13 02:16:04",2
"Apache Mesos","Add operation status update handler in Master.","Please follow this doc for details.  https://docs.google.com/document/d/1RrrLVATZUyaURpEOeGjgxA6ccshuLo94G678IbL-Yco/edit#    This handler will process operation status update from resource providers. Depends on whether it's old or new operations, the logic is slightly different.",Task,Major,Resolved,"2017-10-13 02:11:32","2017-10-13 01:11:32",5
"Apache Mesos","Double free corruption in tests due to parallel manipulation of signal and control handlers.","As mentioned in https://reviews.apache.org/r/51122, {{installCtrlHandler}} and {{configureSignal}} might be called from parallel threads, triggering double free corruption. This makes our example framework tests flaky.",Bug,Major,Resolved,"2017-10-12 22:09:46","2017-10-12 21:09:46",3
"Apache Mesos","updateAvailable races with a periodic allocation and leads to flaky tests.","When an operator requests a resource modification (reserve resources, create a persitent volume and so on), a corresponding endpoint handler can request allocator state modification twice: recover resources from rescinded offers and for update applied operation. These operations should happen atomically, i.e., no other allocator change can happen in-between. This is however not the case: a periodic allocation can kick in. Solutions to this race might be: moving offer management to the allocator, coupling operations in the allocator, pausing allocator.    While this race does not necessarily lead to bugs in production—as long as operators and tooling can handle failures and retry—, it makes some tests using resource modification flaky, because in tests we do not plan for failures and retries.",Bug,Major,Resolved,"2017-10-12 21:23:19","2017-10-12 20:23:19",5
"Apache Mesos","Checkpoint and recover layers used to provision rootfs in provisioner","This information will be necessary for {{provisioner}} to determine all layers of active containers, which we need to retain when image gc happens.",Task,Major,Resolved,"2017-10-12 16:54:47","2017-10-12 15:54:47",5
"Apache Mesos","Some fields went missing with no replacement in api/v1.","Hi friends,     These fields are available via the state.json but went missing in the v1 of the API:  -leader_info- -> available via GET_MASTER which should always return leading master info  start_time  elected_time    As we're showing them on the Overview page of the DC/OS UI, yet would like not be using state.json, it would be great to have them somewhere in V1.",Improvement,Critical,Resolved,"2017-10-12 16:45:51","2017-10-12 15:45:51",2
"Apache Mesos","PersistentVolumeTest.SharedPersistentVolumeRescindOnDestroy is flaky.","I'm observing {{ROOT_MountDiskResource/PersistentVolumeTest.SharedPersistentVolumeRescindOnDestroy/0}} being flaky on our internal CI. From what I see in the logs, when {{framework1}} accepts an offer, creates volumes, launches a task, and kills it right after, the executor might manage to register in-between and hence an unexpected {{TASK_RUNNING}} status update is sent. To fix this, one approach is to explicitly wait for {{TASK_RUNNING}} before attempting to kill the task.",Bug,Major,Resolved,"2017-10-12 01:04:05","2017-10-12 00:04:05",2
"Apache Mesos","Add ReadWriteLock to libprocess.","We want to add a new {{ReadWriteLock}} similar to {{Mutex}}, which can provide better concurrecy protection for mutual exclusive actions, but allow high concurrency for actions which can be performed at the same time.    One use case is image garbage collection: the new API {{provisioner::pruneImages}} needs to be mutually exclusive from {{provisioner::provision}}, but multiple {{{provisioner::provision}} can concurrently run safely.",Task,Major,Resolved,"2017-10-11 21:47:29","2017-10-11 20:47:29",5
"Apache Mesos","Change Libprocess actor state transitions verbose logs to use VLOG(3) instead of 2","Without claiming a general change or a holistic approach, the amount of logs concerning states being resumed when running a Mesos cluster with {{GLOG_v=2}} is quite noisy. We should thus use {{VLOG(3)}} for such messages.",Improvement,Minor,Resolved,"2017-10-11 14:44:35","2017-10-11 13:44:35",1
"Apache Mesos","Change Mesos common events verbose logs to use VLOG(2) instead of 1","The original commit https://github.com/apache/mesos/commit/fa6ffdfcd22136c171b43aed2e7949a07fd263d7 that started using VLOG(1) for the allocator does not state why this level was chosen and the periodic messages such as No allocations performed should be displayed at a higher level to simplify debugging.",Improvement,Minor,Resolved,"2017-10-11 10:37:52","2017-10-11 09:37:52",2
"Apache Mesos","Add agent capability for resource provider.","This capability will be used by master to tell if an agent is resource provider capable. There will be some logic in the master that depends on this capability.",Task,Major,Resolved,"2017-10-11 04:23:40","2017-10-11 03:23:40",2
"Apache Mesos","Bundled GRPC build does not build on Debian 8","Debian 8 includes an outdated version of libc-ares-dev, which prevents bundled GRPC to build.    I believe [~<USER> already has a fix.",Bug,Major,Resolved,"2017-10-11 01:51:27","2017-10-11 00:51:27",2
"Apache Mesos","v1 role-related endpoints need to reflect hierarchical accounting.","With the introduction of hierarchical roles, the role-related endpoints need to be updated to provide aggregated accounting information.    For example, information about how many resources are allocated to /eng should include the resources allocated to /eng/frontend and /eng/backend, since quota guarantees and limits are also applied on the aggregation.    This also affects the UI display, for example the 'Roles' tab.",Bug,Major,Accepted,"2017-10-10 23:20:35","2017-10-10 22:20:35",8
"Apache Mesos","Pylint report errors in apply-reviews.py on Ubuntu 14.04","    Error 1, Module 'ssl' has no 'SSLContext' member (no-member):  The line caught is part of a function that is only used on Windows (https://github.com/apache/mesos/blob/4fd018be674ad6badd23f6a1f0baea2d63fd7974/support/apply-reviews.py#L188). Having this code requires Python 2.7.9: https://docs.python.org/2/library/ssl.html#    Error 2, Unexpected keyword argument 'context' in function call (unexpected-keyword-arg):  This keyword argument is indeed added in Python 2.7.9.  ",Bug,Minor,Resolved,"2017-10-10 14:43:59","2017-10-10 13:43:59",1
"Apache Mesos","Introduce first class 'profile' for disk resources.","This is similar to storage classes. Instead of adding a bunch of storage backend specific parameters (e.g., rotational, type, speed, etc.) into the disk resources, and asking the frameworks to make scheduling decisions based on those vendor specific parameters. We propose to use a level of indirection here.  The operator will setup mappings between a profile name to a set of vendor specific disk parameters. The framework will do disk selection based on profile names.  The storage resource provider will provide a hook allowing operators to customize the profile name assignment for disk resources.",Task,Major,Resolved,"2017-10-06 22:41:00","2017-10-06 21:41:00",2
"Apache Mesos","Agent and master can race when updating agent state.","In {{2af9a5b07dc80151154264e974d03f56a1c25838}} we introduce the use of {{UpdateSlaveMessage}} for the agent to inform the master about its current total resources. Currently we trigger this message only on agent registration and reregistration.    This can race with operations applied in the master and communicated via {{CheckpointResourcesMessage}}.    Example:    1. Agent ({{cpus:4(\*)}} registers.  2. Master is triggered to apply an operation to the agent's resources, e.g., a reservation: {{cpus:4(\*) -> cpus:4(A)}}. The master applies the operation to its current view of the agent's resources and sends the agent a {{CheckpointResourcesMessage}} so the agent can persist the result.  3. The agent sends the master an {{UpdateSlaveMessage}}, e.g., {{cpus:4(\*)}} since it hasn't received the {{CheckpointResourcesMessage}} yet.  4. The master processes the {{UpdateSlaveMessage}} and updates its view of the agent's resources to be {{cpus:4(\*)}}.  5. The agent processes the {{CheckpointResourcesMessage}} and updates its view of its resources to be {{cpus:4(A)}}.  6. The agent and the master have an inconsistent view of the agent's resources.",Bug,Critical,Resolved,"2017-10-06 16:45:42","2017-10-06 15:45:42",2
"Apache Mesos","Design doc for offer operations feedback",https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI,Task,Major,Resolved,"2017-10-05 20:27:09","2017-10-05 19:27:09",8
"Apache Mesos","protoc not found when running make -j4 check directly in stout","If we run {{make -j4 check}} without running {{make}} first, we will get the following error message: ",Bug,Major,Resolved,"2017-10-04 19:16:56","2017-10-04 18:16:56",2
"Apache Mesos","Killing TASK_GROUP fail to kill some tasks","When starting following pod definition via marathon:        mesos will successfully kill all {{ct2}} containers but fail to kill all/some of the {{ct1}} containers. I've attached both master and agent logs. The interesting part starts after marathon issues 6 kills:        All {{.ct1}} tasks fail eventually (~30s) where {{.ct2}} are successfully killed.",Bug,Critical,Resolved,"2017-10-04 17:01:05","2017-10-04 16:01:05",2
"Apache Mesos","Mesos HTTP/HTTPS health checks for IPv6 docker containers.","Currently the MESOS HTTP/HTTPS health checks hardcode the IP address to 127.0.0.1 while performing the pings on the containers. With IPv6 containers, even dual stack kernels the container will have both the IPv4 and IPv6 loopback interfaces (127.0.0.1 and ::1). Further, its upto the application's discretion to either open a INET or an INET6 socket which would imply that to support IPv6 containers the MESOS HTTP/HTTPS health checks need to be configurable to perform health checks on 127.0.0.1 or ::1.   A proposal here would be to introduce the concept of a transport on which MESOS HTTP/HTTPS health checks work. That is the framework specifies whether MESOS HTTP healthchecks work over TCP or TCP6. ",Story,Major,Resolved,"2017-10-02 23:39:34","2017-10-02 22:39:34",5
"Apache Mesos","ReservationEndpointsTest.GoodReserveAndUnreserveACL is flaky.","As just observed on our internal CI;    Error Message      Log:  ",Bug,Major,Resolved,"2017-10-02 14:56:47","2017-10-02 13:56:47",2
"Apache Mesos","MasterTestPrePostReservationRefinement.ReserveAndUnreserveResourcesV1 is flaky.","As seen on our internal CI.    Error Message        Log:  ",Bug,Major,Resolved,"2017-10-02 14:17:39","2017-10-02 13:17:39",2
"Apache Mesos","ns::clone should spawn process, which is a direct child","`ns::clone` does double-fork in order to be able to enter given PID namespace and returns grandchild's pid, which is not a direct child of a parent process, hence parent process can not retrieve status of an exited grandchild process. As second fork is implemented via `os::clone`, we can pass `CLONE_PARENT` flag. Also, we have to handle both intermediate child process and grandchild process to avoid zombies.  Motivation behind this improvement is that both `docker exec` and `LXC attach` can enter process' PID namespace, while still controlling child's status code. ",Improvement,Major,Open,"2017-09-28 15:39:11","2017-09-28 14:39:11",8
"Apache Mesos","Launch CSI plugins in storage local resource provider.","Launching a CSI plugin requires the following steps:  1. Verify the configuration.  2. Prepare a directory in the work directory of the resource provider where the socket file should be placed, and construct the path of the socket file.  3. If the socket file already exists and the plugin is already running, we should not launch another plugin instance.  4. Otherwise, launch a standalone container to run the plugin and connect to it through the socket file.",Task,Major,Resolved,"2017-09-27 23:45:09","2017-09-27 22:45:09",5
"Apache Mesos","SLRP Configuration","A typical SLRP configuration could look like the following:  The {{csi_plugins}} field lists the configurations to launch standalone containers for CSI plugins. The plugins are specified through a map, then we use the {{controller_plugin_name}} and {{node_plugin_name}} fields to refer to the corresponding plugin. With this design, we can support both headless and split-component deployment for CSI.",Task,Major,Resolved,"2017-09-27 23:36:11","2017-09-27 22:36:11",3
"Apache Mesos","A resource provider for supporting local storage through CSI","The Storage Local Resource Provider (SLRP) is a resource provider component in Mesos to manage persistent local storage on agents. SLRP should support the following MVP functions: * Registering to the RP manager (P0) * Reporting available disk resources through a CSI controller plugin. (P0) * Processing resource converting operations (CREATE_BLOCK, CREATE_VOLUME, DESTROY_BLOCK, DESTROY_VOLUME) issued by frameworks to convert RAW disk resources to mount or block volumes through a CSI controller plugin (P0) * Publish/unpublish a disk resource through CSI controller/node plugins for a task (P0) * Support storage profiles through modules (P1) * Tracking and checkpointing resources and reservations (P1) ",Epic,Major,Resolved,"2017-09-27 23:16:46","2017-09-27 22:16:46",13
"Apache Mesos","Add Mesos CLI command to list agents","We should have a command listing the agents in a Mesos cluster.",Task,Major,Resolved,"2017-09-27 14:09:51","2017-09-27 13:09:51",2
"Apache Mesos","Update HTTP scheduler library to allow for modularized authenticatee.","Allow the scheduler library to load an HTTP authenticatee module providing custom mechanisms for authentication.",Improvement,Major,Resolved,"2017-09-26 21:58:24","2017-09-26 20:58:24",3
"Apache Mesos","Introduce a basic HTTP authenticatee.","Refactor the hardcoded basic HTTP authentication code from within the scheduler library into the (modularized) interface provided by MESOS-8016",Improvement,Major,Resolved,"2017-09-26 12:33:00","2017-09-26 11:33:00",2
"Apache Mesos","Introduce modularized HTTP authenticatee.","Define the implementation of a modularized interface for the scheduler library authentication, providing the means of an authenticatee. This interface will allow consumers of HTTP APIs to use replaceable authentication mechanisms via a defined interface.",Improvement,Major,Resolved,"2017-09-26 12:27:33","2017-09-26 11:27:33",2
"Apache Mesos","Design a scheduler (V1) HTTP API authenticatee mechanism.","Provide a design proposal for a scheduler HTTP API authenticatee module.",Improvement,Major,Resolved,"2017-09-25 22:35:47","2017-09-25 21:35:47",2
"Apache Mesos","Add test for blkio statistics","In [MESOS-6162|https://issues.apache.org/jira/browse/MESOS-6162], we have added the support for cgroups blkio statistics. In this ticket, we'd like to add a test to verify the cgroups blkio statistics can be correctly retrieved via Mesos containerizer's {{usage()}} method.",Task,Major,Resolved,"2017-09-25 15:26:58","2017-09-25 14:26:58",3
"Apache Mesos","Support Znode paths for masters in the new CLI","Right now the new Mesos CLI only works in single master mode with a single master IP and port. We should add support for finding the mesos leader in HA mode by hitting a set of zk instances similar to how {{mesos-resolve}} works.",Improvement,Major,Resolved,"2017-09-25 14:05:56","2017-09-25 13:05:56",3
"Apache Mesos","PersistentVolumeEndpointsTest.SlavesEndpointFullResources is flaky.","Observed on internal CI:    Full log attached.",Bug,Major,Resolved,"2017-09-21 18:04:05","2017-09-21 17:04:05",2
"Apache Mesos","PersistentVolumeEndpointsTest.NoAuthentication is flaky.","Observed a failure on internal CI:    Full log attached.",Bug,Major,Resolved,"2017-09-21 16:18:36","2017-09-21 15:18:36",2
"Apache Mesos","PersistentVolumeEndpointsTest.UnreserveVolumeResources is flaky.","Observed a failure on the internal CI:    Full log attached.",Bug,Major,Resolved,"2017-09-21 15:45:12","2017-09-21 14:45:12",2
"Apache Mesos","fatal, check failed !framework->recovered()","mesos master crashed on what appears to be framework recovery    mesos master version: 1.3.1  mesos agent version: 1.3.1        The issue happened again on Mesos 1.5 (docker mesos master from the <USER>docker repo):    The failure in this case seems to happen right after an agent drops out of the cluster - which is a similar failure condition to the first time.",Bug,Critical,Resolved,"2017-09-20 16:09:44","2017-09-20 15:09:44",3
"Apache Mesos","Use ASF CI for automating RPM packaging and upload to bintray.","RR: https://reviews.apache.org/r/63543/",Task,Major,Resolved,"2017-09-15 19:33:56","2017-09-15 18:33:56",5
"Apache Mesos","Activate apache/mesos org on bintray","This would allow us to host community-managed rpm/deb packages.",Task,Major,Resolved,"2017-09-15 19:32:54","2017-09-15 18:32:54",3
"Apache Mesos","Create Centos 6/7 RPM package.","Create SPEC file and a corresponding Docker file for CentOS 6 and 7.",Task,Major,Resolved,"2017-09-15 19:31:36","2017-09-15 18:31:36",5
"Apache Mesos","Stout fails to compile with libc >= 2.26.","Glibc 2.26 removes xlocale.h [1] which makes stout fail to compile. Stout should be using 'locale.h' instead.   [1]: https://sourceware.org/glibc/wiki/Release/2.26#Removal_of_.27xlocale.h.27",Bug,Major,Resolved,"2017-09-15 18:40:18","2017-09-15 17:40:18",2
"Apache Mesos","reviewboard's GUESS_FIELDS setting leads to redundant information in commit messages","Reviewboard can be set up to automatically guess a patch's summary and description when {{GUESS_FIELDS}} is set. For commits that have no dedicated description, it uses the commit summary as description as well. This leads to commits with redundant commit messages, e.g.,        When applying this commit with e.g., {{apply_reviews.py}} the redundant body is faithfully copied, but we should consider updating it to instead remove the redundant information automatically leading to e.g.,    ",Bug,Minor,Resolved,"2017-09-14 19:26:39","2017-09-14 18:26:39",1
"Apache Mesos","Lint javascript files to enable linting.","To enable the linting of our javascript codebase, the javascript files should first be linted so that new commits will not have to include fixes for current issues.",Bug,Trivial,Resolved,"2017-09-14 16:23:18","2017-09-14 15:23:18",1
"Apache Mesos","The command/default/docker executor can incorrectly send a TASK_FINISHED update even when the task is killed","Currently, when a task is killed, the default/command/docker executor incorrectly send a {{TASK_FINISHED}} status update instead of {{TASK_KILLED}}. This is due to an unfortunate missed conditional check when the task exits with a zero status code.    We should modify the code to correctly send {{TASK_KILLED}} status updates when a task is killed.",Bug,Critical,Resolved,"2017-09-13 23:01:35","2017-09-13 22:01:35",3
"Apache Mesos","SlaveTest.HTTPSchedulerSlaveRestart test is flaky.","Saw this on ASF CI when testing 1.4.0-rc5    ",Bug,Major,Resolved,"2017-09-12 19:52:40","2017-09-12 18:52:40",1
"Apache Mesos","PersistentVolumeEndpointsTest.EndpointCreateThenOfferRemove test is flaky","Saw this when testing 1.4.0-rc5        Looks a bit different than the flakiness observed in MESOS-6086",Bug,Critical,Accepted,"2017-09-12 19:47:51","2017-09-12 18:47:51",2
"Apache Mesos","Adding process::Executor::execute()","It would be easier to use {{process::Executor}} if we can add an {{execute()}} interface that runs a function asynchronously and returns a {{Future}}, so we do the following: ",Improvement,Major,Resolved,"2017-09-12 19:10:13","2017-09-12 18:10:13",2
"Apache Mesos","Handle cgroups v2 hierarchy when parsing /proc/self/cgroups.","Cgroups v2 hierarchies don't list the controllers field (e.g., 0::/user.slice/user-1000.slice/session-5.scope)  is empty [1] and hence the cgroup parser failes. We should simply skip over fields with empty controller field.    [1] See 6-2 in https://www.kernel.org/doc/Documentation/cgroup-v2.txt",Bug,Major,Resolved,"2017-09-12 19:05:30","2017-09-12 18:05:30",1
"Apache Mesos","Handle `/proc/self/ns/pid_for_children` when parsing available namespace.","Since Linux 4.12, /proc/self/ns/pid_for_children is a handle for the PID namespace of child processes created by this process. Since this is not a namespace type in its own, we should ignore this file when listing namespaces via `ls /proc/self/ns`.",Bug,Major,Resolved,"2017-09-12 19:01:02","2017-09-12 18:01:02",1
"Apache Mesos","check for maintenance on agent causes fatal error","We interact with the maintenance API frequently to orchestrate gracefully draining agents of tasks without impacting service availability.    Occasionally we seem to trigger a fatal error in Mesos when interacting with the api. This happens relatively frequently, and impacts us when downstream frameworks (marathon) react badly to leader elections.    Here is the log line that we see when the master dies:        It's quite possibly we're using the maintenance API in the wrong way. We're happy to provide any other logs you need - please let me know what would be useful for debugging.    Thanks.",Bug,Critical,Resolved,"2017-09-12 17:36:52","2017-09-12 16:36:52",5
"Apache Mesos","Heavy-duty GC makes the agent unresponsive","An agent is observed to performe heavy-duty GC every half an hour:   Each GC activity took 5+ minutes. During the period, the agent became unresponsive, the health check timed out, and no endpoint responded as well. When a disk-usage GC is trigged, around 300 pruning actors would be generated (https://github.com/apache/mesos/blob/master/src/slave/gc.cpp#L229). My hypothesis is that these actors would used all of the worker threads, and some of them took a long time to finish (possibly due to many files to delete, or too many fs operations at once, etc).",Bug,Major,Resolved,"2017-09-11 20:47:35","2017-09-11 19:47:35",2
"Apache Mesos","Design Doc for Extended KillPolicy","After introducing the {{KillPolicy}} in MESOS-4909, some interactions with framework developers have led to the suggestion of a couple possible improvements to this interface. Namely,  * Allowing the framework to specify a command to be run to initiate termination, rather than a signal to be sent, would allow some developers to avoid wrapping their application in a signal handler. This is useful because a signal handler wrapper modifies the application's process tree, which may make introspection and debugging more difficult in the case of well-known services with standard debugging procedures.  * In the case of terminations which do begin with a signal, it would be useful to allow the framework to specify the signal to be sent, rather than assuming SIGTERM. PostgreSQL, for example, permits several shutdown types, each initiated with a [different signal|https://www.postgresql.org/docs/9.3/static/server-shutdown.html].",Improvement,Major,Resolved,"2017-09-09 00:26:03","2017-09-08 23:26:03",8
"Apache Mesos","Add GC capability to nested containers","We should extend the existing API or add a new API for nested containers for an executor to tell the Mesos agent that a nested container is no longer needed and can be scheduled for GC.",Improvement,Major,Resolved,"2017-09-07 20:30:03","2017-09-07 19:30:03",8
"Apache Mesos","DefaultExecutorTest.SigkillExecutor test fails on Windows","  The above commit introduced the test {{MesosContainerizer/DefaultExecutorTest.SigkillExecutor}} which fails on Windows. At a rough glance, if this is dependent on the {{SIGKILL}} signal, it may not be applicable on Windows and just needs to be disabled.",Bug,Major,Resolved,"2017-09-07 20:02:18","2017-09-07 19:02:18",2
"Apache Mesos","MasterAPITest.EventAuthorizationFiltering is flaky.","    The above commit introduced the test {{MasterAPITest.EventAuthorizationFiltering}} which is flaky.",Bug,Major,Resolved,"2017-09-07 19:56:03","2017-09-07 18:56:03",2
"Apache Mesos","Implement jemalloc memory profiling support for Mesos","After investigation in MESOS-7876 and discussion on the mailing list, this task is for tracking progress on adding out-of-the-box memory profiling support using jemalloc to Mesos.",Bug,Major,Resolved,"2017-09-07 14:10:05","2017-09-07 13:10:05",8
"Apache Mesos","Send TASK_STARTING status from built-in executors","All executors have the option to send out a TASK_STARTING status update to signal to the scheduler that they received the command to launch the task.  It would be good if our built-in executors would do this, for reasons laid out in https://mail-archives.apache.org/mod_mbox/mesos-dev/201708.mbox/%3CCA%2B9TLTzkEVM0CKvY%2B%3D0%3DwjrN6hYFAt0401Y7b8tysDWx1WZzdw%40mail.gmail.com%3E  This will also fix MESOS-6790.",Improvement,Major,Resolved,"2017-09-06 15:52:34","2017-09-06 14:52:34",12
"Apache Mesos","Early disk usage check for garbage collection during recovery","Currently the default value for `disk_watch_interval` is 1 minute. This is not fast enough and could lead to the following scenario:  1. The disk usage was checked and there was not enough headroom:  But no container was pruned because no container had been scheduled for GC.  2. A task was completed. The task itself contained a lot of nested containers, each used a lot of disk space. Note that there is no way for Mesos agent to schedule individual nested containers for GC since nested containers are not necessarily tied to tasks. When the top-lovel container is completed, it was scheduled for GC, and the nested containers would be GC'ed as well:    3. Since the next disk usage check was still 40ish seconds away, no GC was performed even though the disk was full. As a result, Mesos agent failed to checkpoint the task status:   4. When the agent restarted, it tried to checkpoint the task status again. However, since the first disk usage check was scheduled 1 minute after startup, the agent failed before GC kicked in, falling into a restart failure loop:   We should kick in GC early, so the agent can recover from this state.  Related ticket: MESOS-7031",Bug,Major,Open,"2017-09-06 01:41:26","2017-09-06 00:41:26",2
"Apache Mesos","Move sandbox path volume logic to 'volume/sandbox_path' isolator.","Currently, the sandbox path volume logics are in two places: the 'filesystem/linux' isolator and the 'volume/sandbox_path' isolator, depending on the type of the sandbox path volume (SELF or PARENT).  We should move all the sandbox path volume related logics to 'volume/sandbox_path' isolator.",Task,Major,Resolved,"2017-09-04 16:33:02","2017-09-04 15:33:02",3
"Apache Mesos","OOM due to LibeventSSLSocket send incorrectly returning 0 after shutdown.","LibeventSSLSocket can return 0 from send incorrectly, which leads the caller to send the data twice!  See here: https://github.com/apache/mesos/blob/1.3.1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L396-L398  In some particular cases, it's possible that the caller keeps getting back 0 and loops infinitely, blowing up the memory and OOMing the process.  One example is when a send occurs after a shutdown:  ",Bug,Blocker,Resolved,"2017-09-02 00:53:24","2017-09-01 23:53:24",5
"Apache Mesos","`Metrics()` hangs on second call on Windows","An unfortunately difficult to debug problem has cropped up on Windows. While running the {{mesos-tests}} they will hang at:    {{GarbageCollectorTest.Schedule}} is the first test that will hang in an unfiltered run of mesos-tests.  This can be minimally reproduced by running any two tests which call {{Metrics()}} from {{utils.cpp}}. The following have been confirmed:    The second test will hang (indicating a race condition), waiting for a {{GET}} to {{/metrics/snapshot}} that never returns.  There appears to be a timing problem to this bug as well. If your CPU is heavily utilized (say, by running another build in the background), the tests will pass. They will pass if you attach Application Verifier to {{mesos-tests.exe}}, which slows down execution enough. Very slow machines (such as those used for CI) will also not exhibit this hang. Increasing the log verbosity using {{GLOG_v=2}} or higher will make it disappear (although {{--verbose}} has no effect).  Oddly, the bug will reproduce under the Visual Studio debugger, but all it shows us is a pending future waiting for the metrics request to come back.  In {{metrics.cpp}} there is a note that the request might timeout, but we're unsure if this is the same problem, or a different problem manifesting in the same way:    A {{git bisect}} revealed that:    Caused this bug to appear (but does not necessarily mean it created the bug). Reverting this commit allows all the tests to pass, but we believe this just hides the bug.  This bug has reproduced on Windows machines with and without Docker (and Windows containers) installed. (I only mention this because it was a variable on my machine when the bug first appeared, but have since ruled it out as relevant.)  We do not think that it is specific to {{libevent}}, as the bug does not appear to reproduce on a Linux VM built with {{libevent}} instead of {{libev}}.",Bug,Critical,Resolved,"2017-08-31 21:02:03","2017-08-31 20:02:03",2
"Apache Mesos","The composing containerizer leaks memory in some scenarios.","The composing containerizer does not remove an active containers from its internal {{containers}} hashmap containing the known active containers in some cases. This can happen when the container terminates on its own. This means that {{destroy()}} is not invoked for such containers.  Ideally, we should chain the {{destroy}} callback when launching the container itself.",Bug,Critical,Resolved,"2017-08-31 00:38:28","2017-08-30 23:38:28",2
"Apache Mesos","Add a javascript linter to the webui.","As far as I can tell, javascript linters (e.g. ESLint) help catch some functional errors as well, for example, we've made some strict mistakes a few times that ESLint can catch: MESOS-6624, MESOS-7912.",Improvement,Major,Resolved,"2017-08-29 01:39:58","2017-08-29 00:39:58",5
"Apache Mesos","Make args optional in mesos port mapper plugin","Current implementation of the mesos-port-mapper plugin fails if the args field is absent in the cni config which makes it very specific to mesos. Instead, if args could be optional then this plugin could be used in a more generic environment. ",Bug,Major,Resolved,"2017-08-28 23:00:15","2017-08-28 22:00:15",1
"Apache Mesos","Fix communication between old masters and new agents.","For re-registration, agents currently send the resources in tasks and executors to the master in the post-reservation-refinement format, which is incompatible for pre-1.4 masters. We should change the agent such that it always downgrades the resources to the pre-reservation-refinement format, and the master unconditionally upgrade the resources to post-reservation-refinement format.",Bug,Blocker,Resolved,"2017-08-28 19:30:44","2017-08-28 18:30:44",2
"Apache Mesos","ProcessManager::resume sometimes crashes accessing EventQueue.","The following segfault is found on [ASF|https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)/4159/] in {{MesosContainerizerSlaveRecoveryTest.ResourceStatistics}} but it's flaky and shows up in other tests and environments (with or without --enable-lock-free-event-queue) as well.    {noformat: title=Configuration}  ./bootstrap '&&' ./configure --verbose '&&' make -j6 distcheck      A <EMAIL> query shows many such instances: https://lists.apache.org/list.html?<EMAIL>:lte=1M:process%3A%3AEventQueue%3A%3AConsumer%3A%3Aempty",Bug,Blocker,Resolved,"2017-08-28 19:02:51","2017-08-28 18:02:51",8
"Apache Mesos","Docker statistics not reported on Windows.","On Windows, the JSON information provided by the agent at the /container API does not contain the expected {{statistics}} object for Docker containers on Windows. This breaks the dcos-metrics tool, required for DC/OS integration on Windows.",Bug,Major,Resolved,"2017-08-25 18:55:38","2017-08-25 17:55:38",3
"Apache Mesos","Improve the test coverage of the DefaultExecutor.","We should write tests for the {{DefaultExecutor}} to cover the following common scenarios:  # -Start a task that uses a GPU, and make sure that it is made available to the task.- # -Launch a Docker task with a health check.- # -Launch two tasks and verify that they can access a volume owned by the Executor via {{sandbox_path}} volumes.- # -Launch two tasks, each one in its own task group, and verify that they can access a volume owned by the Executor via {{sandbox_path}} volumes.- # -Launch a task that uses an env secret, make sure that it is accessible.- # -Launch a task using a URI and make sure that the artifact is accessible.- # -Launch a task using a Docker image + URIs, make sure that the fetched artifact is accessible.- # Launch one task and ensure that (health) checks can read from a persistent volume. # -Ensure that the executor's env is NOT inherited by the nested tasks.-",Improvement,Major,Resolved,"2017-08-25 00:05:16","2017-08-24 23:05:16",5
"Apache Mesos","Non-checkpointing framework's tasks should not be marked LOST when agent disconnects.","Currently, when framework with checkpointing disabled has tasks running on an agent and that agent disconnects from the master, the master will mark those tasks LOST and remove them from its memory. The assumption is that the agent is disconnecting because it terminated.    However, it's possible that this disconnection occurred due to a transient loss of connectivity and the agent re-connects while never having terminated. This case violates our assumption of there being no unknown tasks to the master:    ```   void Master::reconcileKnownSlave(   Slave* slave,   const vector<ExecutorInfo>& executors,   const vector<Task>& tasks)   {   ...    // TODO(<USER>: There's an implicit assumption here the slave   // cannot have tasks unknown to the master. This _should_ be the   // case since the causal relationship is:   // slave removes task -> master removes task   // Add error logging for any violations of this assumption!   ```    As a result, the tasks would remain on the agent but the master would not know about them!    A more appropriate action here would be:    # When an agent disconnects, mark the tasks as unreachable.  ## If the framework is not partition aware, only show it the last known task state.  ## If the framework is partition aware, let it know that it's now unreachable.  # If the agent re-connects:  ## And the agent had restarted, let the non-checkpointing framework know its tasks are GONE/LOST.  ## If the agent still holds the tasks, the tasks are restored as reachable.  # If the agent gets removed:  ## For partition aware non-checkpointing frameworks, let them know the tasks are unreachable.  ## For non partition aware non-checkpointing frameworks, let them know the tasks are lost and kill them if the agent comes back.",Bug,Critical,Accepted,"2017-08-23 20:50:35","2017-08-23 19:50:35",5
"Apache Mesos","Ordering dependency between 'linux/capabilities' and 'docker/runtime' isolator.","Looks like there is an unintentional ordering dependency between linux/capabilities isolator and docker/runtime isolator.   For the command task case, since both isolators set ContainerLaunchInfo.command. When merging ContainerLaunchInfo.command, docker/runtime isolator assumes its command is before linux/capabilities isolator's command because 'mesos-execute' should be used as argv[0].  We should try to eliminate this dependency.",Bug,Major,Resolved,"2017-08-23 05:04:55","2017-08-23 04:04:55",3
"Apache Mesos","GarbageCollectorIntegrationTest.ExitedFramework is flaky","Observed this on ASF CI.  ",Bug,Major,Open,"2017-08-21 19:17:32","2017-08-21 18:17:32",3
"Apache Mesos","Filter results of `/state` on agent by role.","The results returned by {{/state}} include data about resource reservations per each role, which should be filtered for certain users, particularly in a multi-tenancy scenario.  The kind of leaked data includes specific role names and their specific reservations.",Task,Major,Resolved,"2017-08-15 12:47:22","2017-08-15 11:47:22",3
"Apache Mesos","Avoid Multiple PROTOC invocations when generating Protobuf & GRPC code in libprocess","{{make}} current runs {{protoc}} twice for a proto file, one for generating the {{.pb.h}} header, the other for generating {{.pb.cc}} implementation. This can be avoided by rewriting the rules for protobuf code generation in the Autotools configs.",Improvement,Minor,Resolved,"2017-08-14 18:48:07","2017-08-14 17:48:07",1
"Apache Mesos","`GET_EXECUTORS` and `/state` is not consistent between master and agent","The master seem not to keep information about the executors since they are not returned either either by getting the master state (with either v0 and v1 API's) or with the call {{GET_EXECUTORS}}. Creating a cluster as follows:      And launch  a couple of frameworks as follows:      Not using the operator endpoints on the agent:    While the master does    These results are consistent using the `/state` endpoint on both, agent and master as well as using the {{GET_STATE}} v1 API call. The agent returns information about executors, while the master response has none.",Improvement,Major,Accepted,"2017-08-14 13:52:52","2017-08-14 12:52:52",3
"Apache Mesos","Quota heuristic check not accounting for mount volumes","This may be expected but came as a surprise to us. We are unable to create a quota bigger than the root disk space on slaves.    Given two clusters with the same number of slaves and root disk size, but one that also has mount volumes, is what the disk resources look like:            In {{fin-fang-foom}}, I was able to create a quota for {{143490mb}} which is the total of available disk resources, root in this case, as reported by Mesos. For {{hydra}}, I am only able to create a quota for {{143489mb}}. This is equivalent to the total of root disks available in {{hydra}} rather than the total available disks reported by Mesos resources which is {{254084mb}}.    With a modified Mesos that adds logging to {{quota_handler}}, we can see that only the {{disk(*)}} number increases in {{nonStaticClusterResources}} after every iteration. The final iteration is {{disk(*):143489}} which is the maximum quota I was able to create on {{hydra}}. We expected that quota heuristic check would also include resources such as {{disk(*)[MOUNT:/dcos/volume2]:7373}}      ",Bug,Major,Resolved,"2017-08-11 18:16:51","2017-08-11 17:16:51",1
"Apache Mesos","Mesos master rescinds all the in-flight offers from all the registered agents when a new maintenance schedule is posted for a subset of slaves","We are running mesos 1.1.0 in production. We use a custom autoscaler for scaling our mesos  cluster up and down. While scaling down the cluster, autoscaler makes a POST request to mesos master /maintenance/schedule endpoint with a set of slaves to move to maintenance mode. This forces mesos master to rescind all the in-flight offers from *all the slaves* in the cluster. If our scheduler accepts one of these offers, then we get a TASK_LOST status update back for that task. We also see such (https://gist.github.com/<USER>8858e7cb59a23e8e1762a27571824118) log lines in mesos master logs.  After reading the code(refs: https://github.com/apache/mesos/blob/master/src/master/master.cpp#L6772), it appears that offers are getting rescinded for all the slaves. I am not sure what is the expected behavior here, but it makes more sense if only resources from slaves marked for maintenance are reclaimed.  *Experiment:* To verify that it is actually happening, I checked out the master branch(sha: a31dd52ab71d2a529b55cd9111ec54acf7550ded ) and added some log lines(https://gist.github.com/<USER>42ca055720549c5ff3067b1e6c7c68b3). Built the binary and started a mesos master and 2 agent processes. Used a basic python framework that launches docker containers on these slaves. Verified that there is no existing schedule for any slaves using `curl 10.40.19.239:5050/maintenance/status`. Posted maintenance schedule for one of the slaves(https://gist.github.com/<USER>fb65170240dd32a53f27e6985c549df0) after starting the mesos framework.  *Logs:* mesos-master: https://gist.github.com/<USER>91888419fdf8284e33ebd58351131203 mesos-slave1: https://gist.github.com/<USER>3a83364b1f5ffc63902a80c728647f31 mesos-slave2: https://gist.github.com/<USER>1b341ef2271dde11d276974a27109426 Mesos framework: https://gist.github.com/<USER>bcd4b37dba03bde0a942b5b972004e8a  I think mesos should rescind offers and inverse offers only for those slaves that are marked for maintenance(draining mode).",Bug,Minor,Accepted,"2017-08-11 07:47:53","2017-08-11 06:47:53",3
"Apache Mesos","Building gRPC with CMake","gRPC manages its own third-party libraries, which overlap with Mesos' third-party library bundles. We need to write proper rules in CMake to configure gRPC's CMake properly to build it.",Improvement,Blocker,Resolved,"2017-08-11 01:15:48","2017-08-11 00:15:48",8
"Apache Mesos","Add an option to skip the Mesos style check when applying a review chain.","The following pre-commit hook would prevent us from committing a patch file for a third-party library that violates the Mesos style guide and fail {{support/apply-reviews.py}}. https://github.com/apache/mesos/blob/master/support/hooks/pre-commit#L24 As a workaround, we could add a new option to skip the pre-commit hook.",Improvement,Minor,Resolved,"2017-08-11 00:40:01","2017-08-10 23:40:01",1
"Apache Mesos","Audit test code for undefined behavior in accessing container elements","We do not always make sure we never access elements from empty containers, e.g., we use patterns like the following   While the intention here is to diagnose an empty {{offers}}, the code still exhibits undefined behavior in the element access if {{offers}} was indeed empty (compilers might aggressively exploit undefined behavior to e.g., remove impossible code). Instead one should prevent accessing any elements of an empty container, e.g.,   We should audit and fix existing test code for such incorrect checks and variations involving e.g., {{EXPECT_NE}}.",Improvement,Minor,Resolved,"2017-08-10 11:15:49","2017-08-10 10:15:49",2
"Apache Mesos","Investigate alternative malloc implementations for mesos","It is currently very hard to debug memory issues, in particular memory leaks, in mesos.  An alluring way to improve the situation would be to change the default malloc to jemalloc, which has built-in heap-tracking capabilities.  However, some care needs to be taken when considering to change such a fundamental part of mesos:    * Would such a switch have any adverse impact on performance?   * Is it available and will it compile on all our target platforms?   * Is the jemalloc-licensing compatible with bundling as third-party library?  ",Improvement,Major,Resolved,"2017-08-10 09:45:40","2017-08-10 08:45:40",5
"Apache Mesos","Scheduler hang when registration fails.","I'm finding that if framework registration fails, the mesos driver client will hang indefinitely with the following output:   I'd have expected one or both of the following: - SchedulerDriver.run() should have exited with a failed Proto.Status of some form - Scheduler.error() should have been invoked when the Got error occurred  Steps to reproduce: - Launch a scheduler instance, have it register with a known-bad framework info. In this case a role containing slashes was used - Observe that the scheduler continues in a TASK_RUNNING state despite the failed registration. From all appearances it looks like the Scheduler implementation isn't invoked at all  I'd guess that because this failure happens before framework registration, there's some error handling that isn't fully initialized at this point.",Bug,Major,Resolved,"2017-08-09 22:03:19","2017-08-09 21:03:19",5
"Apache Mesos","Agent fails assertion during request to '/state'","While processing requests to {{/state}}, the Mesos agent calls {{Framework::allocatedResources()}}, which in turn calls {{Slave::getExecutorInfo()}} on executors associated with the framework's pending tasks.  In the case of tasks launched as part of task groups, this leads to the failure of the assertion [here|https://github.com/apache/mesos/blob/a31dd52ab71d2a529b55cd9111ec54acf7550ded/src/slave/slave.cpp#L4983-L4985]. This means that the check will fail if the agent processes a request to {{/state}} at a time when it has pending tasks launched as part of a task group.  This assertion should be removed since this helper function is now used with task groups.",Bug,Major,Resolved,"2017-08-09 06:26:52","2017-08-09 05:26:52",2
"Apache Mesos","Refactor libssl and libcrypto checks for building gRPC","Refactoring library checks for OpenSSL such that they are decoupled from the `--enable-ssl` flags, due to the dependency between OpenSSL and gRPC.",Improvement,Major,Resolved,"2017-08-09 00:54:26","2017-08-08 23:54:26",1
"Apache Mesos","Build fails with `--disable-zlib` or `--with-zlib=DIR`","ZLib has been a required library for Mesos and libprocess, and {{--disable-zlib}} is not working anymore so should be removed.  Also, when {{--with-zlib=DIR}} is specified, the protobuf build will fail because it does not support specifying a customized zlib path through {{--with-zlib}}.",Bug,Major,Resolved,"2017-08-08 21:37:39","2017-08-08 20:37:39",1
"Apache Mesos","Agent may process a kill task and still launch the task.","Based on the investigation of MESOS-7744, the agent has a race in which queued tasks can still be launched after the agent has processed a kill task for them. This race was introduced when {{Slave::statusUpdate}} was made asynchronous:  (1) {{Slave::__run}} completes, task is now within {{Executor::queuedTasks}} (2) {{Slave::killTask}} locates the executor based on the task ID residing in queuedTasks, calls {{Slave::statusUpdate()}} with {{TASK_KILLED}} (3) {{Slave::___run}} assumes that killed tasks have been removed from {{Executor::queuedTasks}}, but this now occurs asynchronously in {{Slave::_statusUpdate}}. So, the executor still sees the queued task and delivers it and adds the task to {{Executor::launchedTasks}}. (3) {{Slave::_statusUpdate}} runs, removes the task from {{Executor::launchedTasks}} and adds it to {{Executor::terminatedTasks}}.",Bug,Critical,Resolved,"2017-08-08 00:44:59","2017-08-07 23:44:59",3
"Apache Mesos","Agent logs should be accesible with the `/agent/log` path.","Despite the efforts to alias all elements in Mesos from Slave to Agent, logs in the agent are still only accessible through the {{/slave/log}} path.",Improvement,Trivial,Reviewable,"2017-08-07 16:19:48","2017-08-07 15:19:48",1
"Apache Mesos","Agent may drop pending kill task status updates.","Currently there is an assumption that when a pending task is killed, the framework will still be stored in the agent. However, this assumption can be violated in two cases:  # Another pending task was killed and we removed the framework in 'Slave::run' thinking it was idle, because pending tasks were empty (we remove from pending tasks when processing the kill). (MESOS-7783 is an example instance of this). # The last executor terminated without tasks to send terminal updates for, or the last terminated executor received its last acknowledgement. At this point, we remove the framework thinking there were no pending tasks if the task was killed (removed from pending).",Bug,Critical,Resolved,"2017-08-05 03:15:35","2017-08-05 02:15:35",5
"Apache Mesos","Launching a nested container with namespace/pid isolation, with glibc < 2.25, may deadlock the LinuxLauncher and MesosContainerizer","This bug in glibc (fixed in glibc 2.25) will sometimes cause a child process of a {{fork}} to {{assert}} incorrectly, if the parent enters a new pid namespace before forking:  https://sourceware.org/bugzilla/show_bug.cgi?id=15392 https://sourceware.org/bugzilla/show_bug.cgi?id=21386  The LinuxLauncher code happens to do this when launching nested containers: * The MesosContainerizer process launches a subprocess, with a customized {{ns::clone}} function as an argument.  The thread then basically waits for the launch to succeed and return a child PID: https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/linux_launcher.cpp#L495 * A separate thread in the Mesos agent forks and then waits for the grandchild to report a PID: https://github.com/apache/mesos/blob/1.3.x/src/linux/ns.hpp#L453 * The child of the fork first enters the namespaces (including a pid namespace) and then forks a grandchild.  The child then calls {{waitpid}} on the grandchild: https://github.com/apache/mesos/blob/1.3.x/src/linux/ns.hpp#L555 * Due to the glibc bug, the grandchild sometimes never returns from the {{fork}} here: https://github.com/apache/mesos/blob/1.3.x/src/linux/ns.hpp#L540  According to the glibc bug, we can work around this by: {quote} The obvious solution is just to use clone() after setns() and never use fork() - and one can certainly patch both programs to do so. Nevertheless it would be nice to see if fork() also worked after setns(), especially since there is no inherent reason for it not to. {quote}",Bug,Major,Resolved,"2017-08-04 03:39:13","2017-08-04 02:39:13",5
"Apache Mesos","Authorize resource calls to provider manager api","The resource provider manager provides a function  which is exposed e.g., as an agent endpoint.  We need to add authorization to this function in order to e.g., stop rough callers.",Improvement,Critical,Resolved,"2017-08-03 08:58:42","2017-08-03 07:58:42",5
"Apache Mesos","Support shared PID namespace.","Currently, with the 'namespaces/pid' isolator enabled, each container will have its own pid namespace. This does not meet the need for some scenarios. For example, under the same executor container, one task wants to reach out to another task which need to share the same pid namespace.  We should support container pid namespace to be configurable. Users can choose one container to share its parent's pid namespace or not.  User facing API:   A new agent flag: --disallow_top_level_pid_ns_sharing (defaults to be: false) this is a security concern from operator's perspective. While some of the nested containers share the pid namespace from their parents, the top level containers always not share the pid ns from the agent.",Task,Major,Resolved,"2017-08-02 18:35:40","2017-08-02 17:35:40",5
"Apache Mesos","Master stores old resource format in the registry","We intend for the master to store all internal resource representations in the new, post-reservation-refinement format. However, [when persisting registered agents to the registrar|https://github.com/apache/mesos/blob/498a000ac1bb8f51dc871f22aea265424a407a17/src/master/master.cpp#L5861-L5876], the master does not convert the resources; agents provide resources in the pre-reservation-refinement format, and these resources are stored as-is. This means that after recovery, any agents in the master's {{slaves.recovered}} map will have {{SlaveInfo.resources}} in the pre-reservation-refinement format.    We should update the master to convert these resources before persisting them to the registry.",Bug,Major,Resolved,"2017-08-02 18:12:43","2017-08-02 17:12:43",3
"Apache Mesos","The rlimits and linux/capabilities isolators should support nested containers","The rlimits and linix/capabilities isolators don't support nesting. That means that the rlimits or capabilities set for tasks that launched by the DefaultExecutor are silently ignored.",Bug,Major,Resolved,"2017-08-02 17:28:09","2017-08-02 16:28:09",3
"Apache Mesos","Add Mesos CLI command to list active tasks","We need to add a command to list all the tasks running in a Mesos cluster by checking the endpoint {{/tasks}} and reporting the results.",Improvement,Major,Resolved,"2017-07-28 14:21:31","2017-07-28 13:21:31",3
"Apache Mesos","Propagate resource updates from local resource providers to master","When a resource provider registers with a resource provider manager, the manager should sent a message to its subscribers informing them on the changed resources.  For the first iteration where we add agent-specific, local resource providers, the agent would be subscribed to the manager. It should be changed to handle such a resource update by informing the master about its changed resources. In order to support master failovers, we should make sure to similarly inform the master on agent reregistration.",Improvement,Major,Resolved,"2017-07-27 15:08:57","2017-07-27 14:08:57",3
"Apache Mesos","Sandbox_path volume does not have ownership set correctly.","This issue was exposed when using sandbox_path volume to support shared volume for nested containers under one task group. Here is a scenario:  The agent process runs as 'root' user, while the framework user is set as 'nobody'. No matter the commandinfo user is set or not, any non-root user cannot access the sandbox_path volume (e.g., a PARENT sandbox_path volume is not writable from a nested container). This is because the source path at the parent sandbox level is created by the agent process (aka root in this case).   While the operator is responsible for guaranteeing a nested container should have permission to write to its sandbox path volume at its parent's sandbox, we should guarantee the source path created at parent's sandbox should be set as the same ownership as this sandbox's ownership.",Bug,Major,Resolved,"2017-07-25 20:15:22","2017-07-25 19:15:22",3
"Apache Mesos","Improve completed task/framework garbage collection","The Mesos master currently uses two flags to determine how it garbage collects completed tasks and frameworks from memory: * {{--max_completed_frameworks}} * {{--max_completed_tasks_per_framework}}  Setting these parameters correctly can be difficult, since there may be a large variance in the size of Task and Framework objects kept in memory. Launching a framework which makes use of task labels to pass data of significant size can quickly lead to performance issues if the master is retaining a large number of completed tasks.  We should explore other ways of garbage collecting completed frameworks and tasks, which could better handle the variation in the size of task metadata.",Improvement,Major,Open,"2017-07-25 19:48:19","2017-07-25 18:48:19",5
"Apache Mesos","Current approach to parse protobuf enum from JSON does not support upgrades","To use protobuf enum in a backwards compatible way, [the suggestion on the protobuf mailing list|https://groups.google.com/forum/#!msg/protobuf/NhUjBfDyGmY/pf294zMi2bIJ] is to use optional enum fields and include an UNKNOWN value as the first entry in the enum list (and/or explicitly specifying it as the default). This can handle the case of parsing protobuf message from a serialized string, but it can not handle the case of parsing protobuf message from JSON.  E.g., when I access master endpoint with an inexistent enum {{xxx}}, I will get an error:   In the {{Call}} protobuf message, the enum {{Type}} already has a default value {{UNKNOWN}} (see [here|https://github.com/apache/mesos/blob/1.3.0/include/mesos/v1/master/master.proto#L45] for details) and the field {{Call.type}} is optional, but the above curl command will still fail. The root cause is, in the code [here|https://github.com/apache/mesos/blob/1.3.0/3rdparty/stout/include/stout/protobuf.hpp#L449:L454] when we try to get the enum value for the string xxx, it will fail since there is no any enum value corresponding to xxx.",Bug,Major,Resolved,"2017-07-25 15:58:37","2017-07-25 14:58:37",3
"Apache Mesos","Provide a generic HTTP driver abstraction","There are currently at least two implementations of HTTP drivers, one for the scheduler API and one for the executor API. We are in the process of adding a third one to communicate with resource providers, MESOS-7469.  Instead of implementing the same logic again and again we should instead extract shared pieces into a generic HTTP driver abstraction which we then modify. It seems one major difference would be in the types of exchanges messages.",Improvement,Major,Open,"2017-07-25 12:44:03","2017-07-25 11:44:03",5
"Apache Mesos","Reorganize the new Mesos CLI to live under src/python",,Improvement,Major,Resolved,"2017-07-24 00:49:47","2017-07-23 23:49:47",2
"Apache Mesos","Libprocess internal state is not monitored by metrics.","Libprocess does not expose its internal state via metrics. Active sockets, number of HTTP proxies, number of running actors, number of pending messages for all active sockets, etc — may be of interest when monitoring and debugging Mesos clusters.",Improvement,Major,Resolved,"2017-07-21 00:24:13","2017-07-20 23:24:13",5
"Apache Mesos","Add more filtering options for unversioned operator API","The Mesos CLI hits {{/state}} to get the state of the Mesos cluster, which can cause performance issues in large clusters. To optimize the CLI for large clusters, we can add more filtering options to unversioned operator endpoints like {{/tasks}}, so that the CLI can request results for only those tasks which match certain criteria.",Improvement,Major,Open,"2017-07-20 18:14:07","2017-07-20 17:14:07",3
"Apache Mesos","Add HTTP connection handling to the resource provider driver","The {{resource_provider::Driver}} is responsible for establishing a connection with an agent/master resource provider API and provide calls to the API, receive events from the API. This is done using HTTP and should be implemented similar to how it's done for schedulers and executors (see {{src/executor/executor.cpp, src/scheduler/scheduler.cpp}}).",Task,Major,Resolved,"2017-07-20 16:17:27","2017-07-20 15:17:27",3
"Apache Mesos","Improve the test frameworks.","These improvements include three main points: * Adding a {{name}} flag to certain frameworks to distinguish between instances. * Cleaning up the code style of the frameworks. * For frameworks with custom executors, such as balloon framework, adding a {{executor_extra_uris}} flag containing URIs that will be passed to the {{command_info}} of the executor.",Improvement,Minor,Resolved,"2017-07-20 09:08:08","2017-07-20 08:08:08",3
"Apache Mesos","gRPC support in libprocess","We would like to introduce a grpc wrapper in libprocess. The wrapper provides a clean interface for gRPC asynchronous calls and returns a {{Future}}, so others can easily use actor-based programming with libprocess to support grpc communications.",Improvement,Major,Resolved,"2017-07-19 20:19:21","2017-07-19 19:19:21",8
"Apache Mesos","Building gRPC with Autotools","grpc does not come with an autotools script and have a hand-written makefile which assumes certain libraries pre-installed in the system. We need to write proper rules that override the default path options in grpc's Makefile in our autotools configurations to support grpc in autotools.",Improvement,Major,Resolved,"2017-07-19 20:11:46","2017-07-19 19:11:46",5
"Apache Mesos","Bundling gRPC into 3rdparty","grpc comes with a hand-written makefile and cmake file, but no autotool configuration scripts. As a first step to support grpc in mesos, we could integrate gRPC into our cmake build process under Linux, and make it a dependency of libprocess. Since it also depends on protobuf, this will create a triangular dependency between protobuf, grpc and libprocess, so the existing build configurations needs to be adjusted as well.",Improvement,Major,Resolved,"2017-07-19 19:56:35","2017-07-19 18:56:35",5
"Apache Mesos","Docker executor needs to return multiple IP addresses for the container","`Docker executor` currently returns only a single IP address for each docker container. In a world where container has a v4 and v6 address the executor needs to return all the addresses it sees for the container else we won't be able to support dual-stack containers.",Task,Major,Resolved,"2017-07-19 19:03:28","2017-07-19 18:03:28",1
"Apache Mesos","Add copy assignment operator to `net::IP::Network`","Currently, we can't extend the class `net::IP::Network` with out adding a copy assignment operator in the derived class, due to the use of `std::unique_ptr` in the base class. Hence, need to introduce a copy assignment operator into the base class.",Task,Major,Resolved,"2017-07-19 18:58:48","2017-07-19 17:58:48",1
"Apache Mesos","mesos-execute has incorrect example TaskInfo in help string","{{mesos-execute}} documents that a task can be defined via JSON as   If one actually uses that example task definition one gets   Removing the resource role field allows the task to execute.",Bug,Major,Resolved,"2017-07-19 16:36:15","2017-07-19 15:36:15",1
"Apache Mesos","fs::list drops path components on Windows","fs::list(/foo/bar/*.txt) returns a.txt, b.txt, not /foo/bar/a.txt, /foo/bar/b.txt    This breaks a ZooKeeper test on Windows.",Bug,Major,Resolved,"2017-07-18 00:24:02","2017-07-17 23:24:02",2
"Apache Mesos","Retry logic for unsuccessful `docker rm` during agent recovery","In MESOS-7777 we skip the failure when `docker rm` fails due to mount leakage during agent recovery. In order not to leave residual docker containers in the docker daemon, we could do a best-effort `docker rm` retry with an exponential backoff since we cannot control when the leakage would be terminated.",Improvement,Major,Accepted,"2017-07-17 18:32:28","2017-07-17 17:32:28",3
"Apache Mesos","LIBPROCESS_IP isn't passed on to the fetcher","{{LIBPROCESS_IP}} is not passed on to the fetcher.  The fetcher program uses libprocess, which depending on the DNS configuration might fail during initialization:  ",Bug,Critical,Resolved,"2017-07-14 22:21:24","2017-07-14 21:21:24",3
"Apache Mesos","Add support for ECDH ciphers","[Elliptic curve ciphers|https://wiki.openssl.org/index.php/Elliptic_Curve_Cryptography] are a family of ciphers supported by OpenSSL. They allow to have smaller keys, but require an extra configuration parameter, the actual curve to be used, which can't be done through libprocess as it is.",Improvement,Major,Resolved,"2017-07-13 14:27:46","2017-07-13 13:27:46",5
"Apache Mesos","subprocess' childMain using ABORT when encountering user errors","In {{process/posix/subprocess.hpp}}'s {{childMain}} we exit with {{ABORT}} when there was a user error,    We here abort instead of simply {{_exit}}'ing and letting the user know that we couldn't deal with the given arguments.  Abort can potentially dump core, and since this abort is before the {{execvpe}}, the process image can potentially be large (e.g., >300 MB) which could quickly fill up a lot of disk space.",Bug,Major,Resolved,"2017-07-13 10:43:18","2017-07-13 09:43:18",5
"Apache Mesos","Design hierarchical quota allocation.","When quota is assigned in the role hierarchy (see MESOS-6375), it's possible for there to be undelegated quota for a role. For example:        Here, the eng role has 60 of its 90 cpus of quota delegated to its children, and 30 cpus remain undelegated. We need to design how to allocate these 30 cpus undelegated cpus. Are they allocated entirely to the eng role? Are they allocated to the eng role tree? If so, how do we determine how much is allocated to each role in the eng tree (i.e. eng, eng/ads, eng/build).",Task,Major,Resolved,"2017-07-12 22:24:04","2017-07-12 21:24:04",8
"Apache Mesos","Add `SUBSCRIBE` call handling to the resource provider manager","Resource providers will use the HTTP API to subscribe to the {{ResourceProviderManager}}. Handling these calls needs to be implemented. On subscription, a unique resource provider ID will be assigned to the resource provider and a {{SUBSCRIBED}} event will be sent.",Task,Major,Resolved,"2017-07-11 13:42:20","2017-07-11 12:42:20",5
"Apache Mesos","Agent failed to recover due to mount namespace leakage in Docker 1.12/1.13","Docker changed its default mount propagation to shared since 1.12 to enable persistent volume plugins. However, Docker has a known issue (https://github.com/moby/moby/issues/25718) that it sometimes leaks its mount namespace to other processes, which could make Mesos agents fail to remove Docker containers during recovery. The following shows the logs of such a faliure:  ",Bug,Critical,Resolved,"2017-07-11 03:17:19","2017-07-11 02:17:19",3
"Apache Mesos","Eliminate extra process abort in a subprocess watchdog","`abort()` is called in `SUPERVISOR` hook when child process exits with an error code, or `waitpid()` fails, or parent process exits. All these cases shouldn't lead to abnormal program termination with coredumps.",Bug,Major,Resolved,"2017-07-10 15:39:40","2017-07-10 14:39:40",3
"Apache Mesos","Copy-n-paste error in slave/main.cpp","Coverity diagnosed a copy-n-paste error in {{slave/main.cpp}} (https://scan5.coverity.com/reports.htm#v10074/p10429/fileInstanceId=120155401&defectInstanceId=33592186&mergedDefectId=1414687+1+Comment),    We check the incorrect IP for some value here (check on {{ip6}}, but use of {{ip}}), and it seems extremely likely we intended to use {{flags.ip6}}.",Bug,Blocker,Resolved,"2017-07-08 00:01:50","2017-07-07 23:01:50",1
"Apache Mesos","Persistent volume might not be mounted if there is a sandbox volume whose source is the same as the target of the persistent volume.","This issue is only for Mesos Containerizer.  If the source of a sandbox volume is a relative path, we'll create the directory in the sandbox in Isolator::prepare method: https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L480-L485  And then, we'll try to mount persistent volumes. However, because of this TODO in the code: https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L726-L739  We'll skip mounting the persistent volume. That will cause a silent failure.  This is important because the workaround we suggest folks to solve MESOS-4016 is to use an additional sandbox volume.",Bug,Critical,Resolved,"2017-07-07 22:40:48","2017-07-07 21:40:48",3
"Apache Mesos","libprocess initializes to bind to random port if --ip is not specified","When running current [HEAD|https://github.com/apache/mesos/commit/c90bea80486c089e933bef64aca341e4cfaaef25],  {noformat:title=without --ip} ./mesos-master.sh --work_dir=/tmp/mesos-test1 ... I0707 14:14:05.927870  5820 master.cpp:438] Master db2a2d26-a9a9-4e6f-9909-b9eca47a2862 (<host>) started on <addr>:36839   It would be great this is caught by tests/CI.",Bug,Blocker,Resolved,"2017-07-07 22:16:08","2017-07-07 21:16:08",1
"Apache Mesos","Make `net::IP` fields protected to allow for inheritance","Correctly the properties of `net::IP` are `private` making it hard for classes to inherit `net::IP`. Although `net::IPv4` and `net::IPv6` already inherit `net::IP` they don't have access to the storage structures stored within `net::IP`. While this works for the current API this is not very extensible.   Hence, we should make the properties of `net::IP` protected instead of private.",Task,Minor,Resolved,"2017-07-07 17:02:23","2017-07-07 16:02:23",1
"Apache Mesos","MasterTest.KillUnknownTask is failling due to a bug in `net::IPv4::ANY()`","Seeing the following failure when running `MasterTest.KillUnknownTask`: ``` I0706 14:08:20.724071 25596 sched.cpp:1041] Scheduler::statusUpdate took 19411ns [libprotobuf FATAL google/protobuf/message_lite.cc:294] CHECK failed: IsInitialized(): Can't serialize message of type mesos.scheduler.Call because it is missing required fields: acknowledge.slave_id.value libprocess: scheduler-5cca230e-e4c9-466e-b2cd-bde7b7d7ed71@127.0.0.1:44650 terminating due to CHECK failed: IsInitialized(): Can't serialize message of type mesos.scheduler.Call because it is missing required fields: acknowledge.slave_id.valueI0706 14:08:20.724196 25570 sched.cpp:2021] Asked to stop the driver ```  Looks we introduced a bug when we create the `net::IPv4` class. The `ANY` method of this class returns `INADDR_LOOPBACK` instead of `INADDR_ANY`. This ends up causing weird issues in terms of connectivity. We need to fix `net::IPv4::ANY` to return `INADDR_ANY`.",Bug,Blocker,Resolved,"2017-07-06 22:46:48","2017-07-06 21:46:48",1
"Apache Mesos","net::IP::Network not building on Windows","Building master (well, 2c1be9ced) is currently broken on Windows. Repro:    (Build instructions here: https://github.com/apache/mesos/blob/master/docs/windows.md)  Get a bunch of compilation errors:  ",Bug,Major,Resolved,"2017-07-06 00:11:44","2017-07-05 23:11:44",1
"Apache Mesos","Website ruby deps do not bundle on macOS","When trying to bundle the ruby dependencies of the website on macOS-10.12.5 I get    It seems eventmachine-1.0.3 has known and fixed issues on macOS-10.10.1 already. I suspect there might be a similar issue for the macOS-10.12.5 I am using.",Bug,Major,Resolved,"2017-07-05 12:31:28","2017-07-05 11:31:28",1
"Apache Mesos","Stout doesn't build standalone.","Stout doesn't build in a standalone configuration:    Note that the build expects {{3rdparty/googletest-release-1.8.0/googlemock-build-stamp}}, but {{googletest}} hasn't been staged yet:  ",Bug,Major,Resolved,"2017-07-05 07:22:39","2017-07-05 06:22:39",2
"Apache Mesos","Update master to handle updates to agent total resources","With MESOS-7755 we update the allocator interface to support updating the total resources on an agent. These allocator invocations are driven by the master when it receives an update the an agent's total resources.  We could transport the updates from agents to the master either as update to {{UpdateSlaveMessage}}, e.g., by adding a {{repeated Resource total}} field; in order to distinguish updates to {{oversubscribed}} to updates to {{total}} we would need to introduce an additional tag field (an empty list of {{Resource}} has the same representation as an absent list of {{Resource}}). Alternatively we could introduce a new message transporting just the update to {{total}}; it should be possible to reuse such a message for external resource providers which we will likely add at a later point.",Task,Major,Resolved,"2017-07-04 15:56:57","2017-07-04 14:56:57",5
"Apache Mesos","Update allocator to support updating agent total resources","Agents encapsulate resource providers making their resources appear to the master as agent resources. In order to permit updates to the resources of a local resource provider (e.g., available disk expanded physically by adding another driver, resource provider resources disappeared since resource provider disappeared), we need to allow agents to change their total resources.  Expected semantics for the hierarchical allocator would be that {{total}} can shrink independently of the current {{allocated}}; should {{allocated}} exceed {{total}} no allocations can be made until {{allocated < total}}.",Task,Major,Resolved,"2017-07-04 15:37:05","2017-07-04 14:37:05",3
"Apache Mesos","Slow subscribers of streaming APIs can lead to Mesos OOMing.","For each active subscriber, Mesos master / slave maintains an event queue, which grows over time if the subscriber does not read fast enough. As the number of such slow subscribers grows, so does Mesos master / slave memory consumption, which might lead to an OOM event.  Ideas to consider: * Restrict the number of subscribers for the streaming APIs * Check (ping) for inactive or slow subscribers * Disconnect the subscriber when there are too many queued events in memory",Bug,Critical,Accepted,"2017-06-30 14:21:54","2017-06-30 13:21:54",8
"Apache Mesos","Improve metrics around active subscribers.","Active subscribers to, e.g., Mesos streaming API, may influence Mesos master performance. To improve triaging and having a better understanding of master workload, we should add metrics to track active subscribers, send queue size and so on.",Improvement,Major,Reviewable,"2017-06-30 14:14:15","2017-06-30 13:14:15",3
"Apache Mesos","Race conditions in IOSwitchboard: listening on unix socket and premature closing of the connection.","Observed this on ASF CI and internal Mesosphere CI. Affected tests:      This issue comes at least in three different flavours. Take {{AgentAPIStreamingTest.AttachInputToNestedContainerSession}} as an example.  h5. Flavour 1      h5. Flavour 2      h5. Flavour 3  ",Bug,Major,Resolved,"2017-06-29 18:50:11","2017-06-29 17:50:11",5
"Apache Mesos","RegisterSlaveValidationTest.DropInvalidReregistration is flaky.","Observed this on ASF CI.  Seems a bit different from MESOS-7441.  {code} [ RUN      ] RegisterSlaveValidationTest.DropInvalidReregistration I0629 05:23:17.367363  2252 cluster.cpp:162] Creating default 'local' authorizer I0629 05:23:17.370198  2276 master.cpp:436] Master 25091bef-3845-4bb6-ae23-e18ac0f4d174 (b3c104d65da7) started on 172.17.0.3:42034 I0629 05:23:17.370234  2276 master.cpp:438] Flags at startup: --acls= --agent_ping_timeout=15secs --agent_reregister_timeout=10mins --allocation_interval=1secs - -allocator=HierarchicalDRF --authenticate_agents=true --authenticate_frameworks=true --authenticate_http_frameworks=true --authenticate_http_readonly=true --au thenticate_http_readwrite=true --authenticators=crammd5 --authorizers=local --credentials=/tmp/V0UvSM/credentials --framework_sorter=drf --help=false --hostn ame_lookup=true --http_authenticators=basic --http_framework_authenticators=basic --initialize_driver_logging=true --log_auto_initialize=true --logbufsecs=0  --logging_level=INFO --max_agent_ping_timeouts=5 --max_completed_frameworks=50 --max_completed_tasks_per_framework=1000 --max_unreachable_tasks_per_framework=1000 --port=5050 --quiet=false --recovery_agent_removal_limit=100% --registry=in_memory --registry_fetch_timeout=1mins --registry_gc_interval=15mins --registry_max_agent_age=2weeks --registry_max_agent_count=102400 --registry_store_timeout=100secs --registry_strict=false --root_submissions=true --user_sorter=drf --version=false --webui_dir=/mesos/mesos-1.3.1/_inst/share/mesos/webui --work_dir=/tmp/V0UvSM/master --zk_session_timeout=10secs I0629 05:23:17.370513  2276 master.cpp:488] Master only allowing authenticated frameworks to register I0629 05:23:17.370525  2276 master.cpp:502] Master only allowing authenticated agents to register I0629 05:23:17.370534  2276 master.cpp:515] Master only allowing authenticated HTTP frameworks to register I0629 05:23:17.370543  2276 credentials.hpp:37] Loading credentials for authentication from '/tmp/V0UvSM/credentials' I0629 05:23:17.370806  2276 master.cpp:560] Using default 'crammd5' authenticator I0629 05:23:17.370929  2276 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly' I0629 05:23:17.371073  2276 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite' I0629 05:23:17.371193  2276 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler' I0629 05:23:17.371318  2276 master.cpp:640] Authorization enabled I0629 05:23:17.371455  2272 hierarchical.cpp:158] Initialized hierarchical allocator process I0629 05:23:17.371477  2290 whitelist_watcher.cpp:77] No whitelist given I0629 05:23:17.373731  2277 master.cpp:2161] Elected as the leading master! I0629 05:23:17.373760  2277 master.cpp:1700] Recovering from registrar I0629 05:23:17.373891  2280 registrar.cpp:345] Recovering registrar I0629 05:23:17.374527  2280 registrar.cpp:389] Successfully fetched the registry (0B) in 593152ns I0629 05:23:17.374625  2280 registrar.cpp:493] Applied 1 operations in 19216ns; attempting to update the registry I0629 05:23:17.375228  2280 registrar.cpp:550] Successfully updated the registry in 555008ns I0629 05:23:17.375336  2280 registrar.cpp:422] Successfully recovered registrar I0629 05:23:17.375826  2282 hierarchical.cpp:185] Skipping recovery of hierarchical allocator: nothing to recover I0629 05:23:17.375850  2290 master.cpp:1799] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register I0629 05:23:17.380674  2252 containerizer.cpp:221] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni W0629 05:23:17.381237  2252 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges W0629 05:23:17.381350  2252 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges I0629 05:23:17.381384  2252 provisioner.cpp:249] Using default backend 'copy' I0629 05:23:17.383884  2252 cluster.cpp:448] Creating default 'local' authorizer I0629 05:23:17.385763  2281 slave.cpp:231] Mesos agent started on (287)@172.17.0.3:42034 I0629 05:23:17.385787  2281 slave.cpp:232] Flags at startup: --acls= --appc_simple_discovery_uri_prefix=http:// --appc_store_dir=/tmp/mesos/store/appc --authenticate_http_executors=true --authenticate_http_readonly=true --authenticate_http_readwrite=true --authenticatee=crammd5 --authentication_backoff_factor=1secs --authorizer=local --cgroups_cpu_enable_pids_and_tids_count=false --cgroups_enable_cfs=false --cgroups_hierarchy=/sys/fs/cgroup --cgroups_limit_swap=false --cgroups_root=mesos --container_disk_watch_interval=15secs --containerizers=mesos --credential=/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/credential --default_role=* --disk_watch_interval=1mins --docker=docker --docker_kill_orphans=true --docker_registry=https://registry-1.docker.io --docker_remove_delay=6hrs --docker_socket=/var/run/docker.sock --docker_stop_timeout=0ns --docker_store_dir=/tmp/mesos/store/docker --docker_volume_checkpoint_dir=/var/run/mesos/is: olators/docker/volume --enforce_container_disk_quota=false --executor_registration_timeout=1mins --executor_reregistration_timeout=2secs --executor_secret_key=/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/executor_secret_key --executor_shutdown_grace_period=5secs --fetcher_cache_dir=/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/fetch --fetcher_cache_size=2GB --frameworks_home= --gc_delay=1weeks --gc_disk_headroom=0.1 --hadoop_home= --help=false --hostname_lookup=true --http_command_executor=false --http_credentials=/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/http_credentials --http_heartbeat_interval=30secs --initialize_driver_logging=true --isolation=posix/cpu,posix/mem --launcher=posix --launcher_dir=/mesos/mesos-1.3.1/_build/src --logbufsecs=0 --logging_level=INFO --max_completed_executors_per_framework=150 --oversubscribed_resources_interval=15secs --perf_duration=10secs --perf_interval=1mins --port=5051 --qos_correction_interval_min=0ns --quiet=false --recover=reconnect --recovery_timeout=15mins --registration_backoff_factor=10ms --resources=cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000] --revocable_cpu_low_priority=true --runtime_dir=/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i --sandbox_directory=/mnt/mesos/sandbox --strict=true --switch_user=true --systemd_enable_support=true --systemd_runtime_directory=/run/systemd/system --version=false --work_dir=/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_RVRQXx I0629 05:23:17.386165  2281 credentials.hpp:86] Loading credential for authentication from '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/credential' I0629 05:23:17.386319  2281 slave.cpp:264] Agent using credential for: test-principal I0629 05:23:17.386339  2281 credentials.hpp:37] Loading credentials for authentication from '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/http_credentials' I0629 05:23:17.386600  2281 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor' I0629 05:23:17.386703  2281 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor' I0629 05:23:17.386885  2281 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly' I0629 05:23:17.386973  2281 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly' I0629 05:23:17.387302  2281 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite' I0629 05:23:17.387409  2281 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite' I0629 05:23:17.388684  2281 slave.cpp:531] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0629 05:23:17.388783  2281 slave.cpp:539] Agent attributes: [  ] I0629 05:23:17.388801  2281 slave.cpp:544] Agent hostname: b3c104d65da7 I0629 05:23:17.388916  2294 status_update_manager.cpp:177] Pausing sending status updates I0629 05:23:17.390480  2288 state.cpp:62] Recovering state from '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_RVRQXx/meta' I0629 05:23:17.390751  2275 status_update_manager.cpp:203] Recovering status update manager I0629 05:23:17.391083  2286 containerizer.cpp:608] Recovering containerizer I0629 05:23:17.392966  2285 provisioner.cpp:410] Provisioner recovery complete I0629 05:23:17.393373  2272 slave.cpp:6075] Finished recovery I0629 05:23:17.393777  2272 slave.cpp:6257] Querying resource estimator for oversubscribable resources I0629 05:23:17.394049  2284 status_update_manager.cpp:177] Pausing sending status updates I0629 05:23:17.394065  2272 slave.cpp:924] New master detected at master@172.17.0.3:42034 I0629 05:23:17.394129  2272 slave.cpp:959] Detecting new master I0629 05:23:17.394268  2272 slave.cpp:6271] Received oversubscribable resources {} from the resource estimator I0629 05:23:17.399830  2292 slave.cpp:986] Authenticating with master master@172.17.0.3:42034 I0629 05:23:17.399900  2292 slave.cpp:997] Using default CRAM-MD5 authenticatee I0629 05:23:17.400095  2289 authenticatee.cpp:121] Creating new client SASL connection I0629 05:23:17.400344  2275 master.cpp:7475] Authenticating slave(287)@172.17.0.3:42034 I0629 05:23:17.400452  2282 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(624)@172.17.0.3:42034 I0629 05:23:17.400650  2271 authenticator.cpp:98] Creating new server SASL connection I0629 05:23:17.400858  2294 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5 I0629 05:23:17.400883  2294 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5' I0629 05:23:17.400981  2291 authenticator.cpp:204] Received SASL authentication start I0629 05:23:17.401043  2291 authenticator.cpp:326] Authentication requires more steps I0629 05:23:17.401151  2293 authenticatee.cpp:259] Received SASL authentication step I0629 05:23:17.401382  2283 authenticator.cpp:232] Received SASL authentication step I0629 05:23:17.401419  2283 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b3c104d65da7' server FQDN: 'b3c104d65da7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0629 05:23:17.401434  2283 auxprop.cpp:181] Looking up auxiliary property '*userPassword' I0629 05:23:17.401470  2283 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0629 05:23:17.401492  2283 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b3c104d65da7' server FQDN: 'b3c104d65da7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0629 05:23:17.401506  2283 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0629 05:23:17.401515  2283 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0629 05:23:17.401532  2283 authenticator.cpp:318] Authentication success I0629 05:23:17.401620  2289 authenticatee.cpp:299] Authentication success I0629 05:23:17.401682  2281 master.cpp:7505] Successfully authenticated principal 'test-principal' at slave(287)@172.17.0.3:42034 I0629 05:23:17.401913  2273 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(624)@172.17.0.3:42034 I0629 05:23:17.401921  2284 slave.cpp:1081] Successfully authenticated with master master@172.17.0.3:42034 I0629 05:23:17.402212  2284 slave.cpp:1509] Will retry registration in 1.256299ms if necessary I0629 05:23:17.402345  2294 master.cpp:5429] Received register agent message from slave(287)@172.17.0.3:42034 (b3c104d65da7) I0629 05:23:17.402477  2294 master.cpp:3659] Authorizing agent with principal 'test-principal' I0629 05:23:17.402930  2271 master.cpp:5564] Registering agent at slave(287)@172.17.0.3:42034 (b3c104d65da7) with id 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 I0629 05:23:17.403379  2281 registrar.cpp:493] Applied 1 operations in 62566ns; attempting to update the registry I0629 05:23:17.404207  2281 registrar.cpp:550] Successfully updated the registry in 769024ns I0629 05:23:17.404608  2286 slave.cpp:1509] Will retry registration in 25.850396ms if necessary I0629 05:23:17.405179  2276 slave.cpp:4794] Received ping from slave-observer(289)@172.17.0.3:42034 I0629 05:23:17.405144  2294 master.cpp:5639] Registered agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0629 05:23:17.405441  2276 slave.cpp:1127] Registered with master master@172.17.0.3:42034; given agent ID 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 I0629 05:23:17.405465  2276 fetcher.cpp:94] Clearing fetcher cache I0629 05:23:17.405508  2293 hierarchical.cpp:525] Added agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 (b3c104d65da7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {}) I0629 05:23:17.405594  2294 master.cpp:5429] Received register agent message from slave(287)@172.17.0.3:42034 (b3c104d65da7) I0629 05:23:17.405743  2294 master.cpp:3659] Authorizing agent with principal 'test-principal' I0629 05:23:17.405750  2288 status_update_manager.cpp:184] Resuming sending status updates I0629 05:23:17.405933  2293 hierarchical.cpp:1850] No allocations performed I0629 05:23:17.405971  2276 slave.cpp:1155] Checkpointing SlaveInfo to '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_RVRQXx/meta/slaves/25091bef-3845-4bb6-ae23-e18ac0f4d174-S0/slave.info' I0629 05:23:17.405989  2293 hierarchical.cpp:1434] Performed allocation for 1 agents in 192285ns I0629 05:23:17.406347  2276 slave.cpp:1193] Forwarding total oversubscribed resources {} I0629 05:23:17.406415  2271 master.cpp:5542] Agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) already registered, resending acknowledgement I0629 05:23:17.406491  2276 slave.cpp:924] New master detected at master@172.17.0.3:42034 I0629 05:23:17.406496  2280 status_update_manager.cpp:177] Pausing sending status updates I0629 05:23:17.406548  2276 slave.cpp:959] Detecting new master I0629 05:23:17.406570  2271 master.cpp:6324] Received update of agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) with total oversubscribed resources {} I0629 05:23:17.406694  2276 slave.cpp:1127] Registered with master master@172.17.0.3:42034; given agent ID 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 I0629 05:23:17.406720  2276 fetcher.cpp:94] Clearing fetcher cache I0629 05:23:17.406842  2275 status_update_manager.cpp:184] Resuming sending status updates I0629 05:23:17.406961  2276 slave.cpp:1155] Checkpointing SlaveInfo to '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_RVRQXx/meta/slaves/25091bef-3845-4bb6-ae23-e18ac0f4d174-S0/slave.info' I0629 05:23:17.407268  2276 slave.cpp:1193] Forwarding total oversubscribed resources {} I0629 05:23:17.407431  2278 master.cpp:6324] Received update of agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) with total oversubscribed resources {} I0629 05:23:17.413727  2276 slave.cpp:986] Authenticating with master master@172.17.0.3:42034 I0629 05:23:17.413785  2276 slave.cpp:997] Using default CRAM-MD5 authenticatee I0629 05:23:17.413978  2278 authenticatee.cpp:121] Creating new client SASL connection I0629 05:23:17.414268  2290 master.cpp:7475] Authenticating slave(287)@172.17.0.3:42034 I0629 05:23:17.414409  2273 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(625)@172.17.0.3:42034 I0629 05:23:17.414752  2292 authenticator.cpp:98] Creating new server SASL connection I0629 05:23:17.414988  2272 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5 I0629 05:23:17.415014  2272 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5' I0629 05:23:17.415158  2278 authenticator.cpp:204] Received SASL authentication start I0629 05:23:17.415225  2278 authenticator.cpp:326] Authentication requires more steps I0629 05:23:17.415345  2285 authenticatee.cpp:259] Received SASL authentication step I0629 05:23:17.415586  2271 authenticator.cpp:232] Received SASL authentication step I0629 05:23:17.415616  2271 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b3c104d65da7' server FQDN: 'b3c104d65da7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0629 05:23:17.415629  2271 auxprop.cpp:181] Looking up auxiliary property '*userPassword' I0629 05:23:17.415665  2271 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0629 05:23:17.415689  2271 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b3c104d65da7' server FQDN: 'b3c104d65da7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0629 05:23:17.415701  2271 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0629 05:23:17.415711  2271 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0629 05:23:17.415726  2271 authenticator.cpp:318] Authentication success I0629 05:23:17.415807  2278 authenticatee.cpp:299] Authentication success I0629 05:23:17.415865  2294 master.cpp:7505] Successfully authenticated principal 'test-principal' at slave(287)@172.17.0.3:42034 I0629 05:23:17.415910  2288 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(625)@172.17.0.3:42034 I0629 05:23:17.416126  2274 slave.cpp:1081] Successfully authenticated with master master@172.17.0.3:42034 I0629 05:23:18.372947  2275 hierarchical.cpp:1850] No allocations performed I0629 05:23:18.373133  2275 hierarchical.cpp:1434] Performed allocation for 1 agents in 578948ns I0629 05:23:19.374871  2289 hierarchical.cpp:1850] No allocations performed I0629 05:23:19.375056  2289 hierarchical.cpp:1434] Performed allocation for 1 agents in 520444ns I0629 05:23:20.375833  2290 hierarchical.cpp:1850] No allocations performed I0629 05:23:20.375975  2290 hierarchical.cpp:1434] Performed allocation for 1 agents in 435113ns I0629 05:23:21.377182  2279 hierarchical.cpp:1850] No allocations performed I0629 05:23:21.377399  2279 hierarchical.cpp:1434] Performed allocation for 1 agents in 639655ns I0629 05:23:22.378278  2277 hierarchical.cpp:1850] No allocations performed I0629 05:23:22.378437  2277 hierarchical.cpp:1434] Performed allocation for 1 agents in 427151ns I0629 05:23:23.380287  2292 hierarchical.cpp:1850] No allocations performed I0629 05:23:23.380481  2292 hierarchical.cpp:1434] Performed allocation for 1 agents in 578747ns I0629 05:23:24.381932  2273 hierarchical.cpp:1850] No allocations performed I0629 05:23:24.382097  2273 hierarchical.cpp:1434] Performed allocation for 1 agents in 476981ns I0629 05:23:25.383314  2275 hierarchical.cpp:1850] No allocations performed I0629 05:23:25.383499  2275 hierarchical.cpp:1434] Performed allocation for 1 agents in 500084ns I0629 05:23:26.384625  2281 hierarchical.cpp:1850] No allocations performed I0629 05:23:26.384855  2281 hierarchical.cpp:1434] Performed allocation for 1 agents in 515354ns I0629 05:23:27.385927  2291 hierarchical.cpp:1850] No allocations performed I0629 05:23:27.386139  2291 hierarchical.cpp:1434] Performed allocation for 1 agents in 562796ns I0629 05:23:28.387132  2285 hierarchical.cpp:1850] No allocations performed I0629 05:23:28.387297  2285 hierarchical.cpp:1434] Performed allocation for 1 agents in 438370ns I0629 05:23:29.388170  2286 hierarchical.cpp:1850] No allocations performed I0629 05:23:29.388309  2286 hierarchical.cpp:1434] Performed allocation for 1 agents in 317943ns I0629 05:23:30.389729  2278 hierarchical.cpp:1850] No allocations performed I0629 05:23:30.389909  2278 hierarchical.cpp:1434] Performed allocation for 1 agents in 475524ns I0629 05:23:31.390977  2280 hierarchical.cpp:1850] No allocations performed I0629 05:23:31.391084  2280 hierarchical.cpp:1434] Performed allocation for 1 agents in 266216ns I0629 05:23:32.391724  2287 hierarchical.cpp:1850] No allocations performed I0629 05:23:32.391827  2287 hierarchical.cpp:1434] Performed allocation for 1 agents in 270448ns I0629 05:23:32.394664  2276 slave.cpp:6257] Querying resource estimator for oversubscribable resources I0629 05:23:32.395167  2293 slave.cpp:6271] Received oversubscribable resources {} from the resource estimator I0629 05:23:32.406095  2288 slave.cpp:4794] Received ping from slave-observer(289)@172.17.0.3:42034 ../../src/tests/master_validation_tests.cpp:3757: Failure Failed to wait 15secs for reregisterSlaveMessage I0629 05:23:32.409525  2291 slave.cpp:796] Agent terminating I0629 05:23:32.410306  2292 master.cpp:1313] Agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) disconnected I0629 05:23:32.410360  2292 master.cpp:3197] Disconnecting agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) I0629 05:23:32.410907  2292 master.cpp:3216] Deactivating agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) I0629 05:23:32.411128  2288 hierarchical.cpp:653] Agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 deactivated I0629 05:23:32.418536  2278 master.cpp:1155] Master terminating I0629 05:23:32.419867  2289 hierarchical.cpp:558] Removed agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 ../../3rdparty/libprocess/include/process/gmock.hpp:446: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <58-BF 04-20 F5-7F 00-00>, 1, 1-byte object <E8>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] RegisterSlaveValidationTest.DropInvalidReregistration (15061 ms) {code}  Observed on macOS when testing 1.3.1:  ",Bug,Major,Resolved,"2017-06-29 18:34:54","2017-06-29 17:34:54",3
"Apache Mesos","Java HTTP adapter crashes JVM when leading master disconnects.","When a Java scheduler using HTTP v0-v1 adapter loses the leading Mesos master, {{V0ToV1AdapterProcess::disconnected()}} is invoked, which in turn invokes Java scheduler [code via JNI|https://github.com/apache/mesos/blob/87c38b9e2bc5b1030a071ddf0aab69db70d64781/src/java/jni/org_apache_mesos_v1_scheduler_V0Mesos.cpp#L446]. This call uses the wrong object, {{jmesos}} instead of {{jscheduler}}, which crashes JVM:  ",Bug,Major,Resolved,"2017-06-27 19:16:49","2017-06-27 18:16:49",3
"Apache Mesos","MasterTest.IgnoreOldAgentReregistration test is flaky","Observed this on ASF CI.    ",Bug,Major,Resolved,"2017-06-27 18:54:51","2017-06-27 17:54:51",3
"Apache Mesos","PersistentVolumeEndpointsTest.ReserveAndSlaveRemoval test is flaky","Observed this on ASF CI.    [~neilconway] Can you take a look at this? This has been terribly flaky on ASF CI.",Bug,Major,Resolved,"2017-06-27 18:53:12","2017-06-27 17:53:12",3
"Apache Mesos","distclean error due to core dump file","Observed this failure a bunch in ASF CI (Buildbot and Reviewbot).    Likely due to perf core dump failure during configure phase ",Bug,Major,Resolved,"2017-06-26 22:18:40","2017-06-26 21:18:40",3
"Apache Mesos","Master's agent removal rate limit also applies to agent unreachability.","Currently, the implementation of partition awareness re-uses the {{--agent_removal_rate_limit}} when marking agents as unreachable. This means that partition aware frameworks are exposed to the agent removal rate limit, when they rather would like to see the information immediately and impose their own rate limiting.  Rather than waiting for non-partition-aware support to be removed (that may not occur for a long time) per MESOS-5948, we should instead fix the implementation so that unreachability does not get gated behind the agent removal rate limiting.  Marking this as a bug since from the user's perspective it doesn't behave as expected, there should be a separate flag for rate limiting unreachability marking, but likely unreachability marking does not need rate limiting, since the intention was for frameworks to impose their own rate limiting for replacing tasks.",Bug,Critical,Accepted,"2017-06-26 06:17:51","2017-06-26 05:17:51",3
"Apache Mesos","Test that master rejects requests to create refined reservations on a non-capable agent.","This test was done manually for now, but we should write a test for it. Similar to {{CreateOperationValidationTest.AgentHierarchicalRoleCapability}}.",Bug,Major,Open,"2017-06-22 23:09:16","2017-06-22 22:09:16",2
"Apache Mesos","Fix agent downgrade for reservation refinement","The agent code only partially supports downgrading of an agent correctly. The checkpointed resources are done correctly, but the resources within the {{SlaveInfo}} message as well as tasks and executors also need to be downgraded correctly and converted back on recovery.",Bug,Blocker,Resolved,"2017-06-22 23:01:15","2017-06-22 22:01:15",8
"Apache Mesos","Optimize number of copies made in dispatch/defer mechanism","Profiling agents reregistration for a large cluster shows, that many CPU cycles are spent on copying protobuf objects. This is partially due to copies made by a code like this:  {{param}} could be copied 8-10 times before it reaches {{method}}. Specifically, {{reregisterSlave}} accepts vectors of rather complex objects, which are passed to {{defer}}. Currently there are some places in {{defer}}, {{dispatch}} and {{Future}} code, which could use {{std::move}} and {{std::forward}} to evade some of the copies.",Task,Major,Resolved,"2017-06-22 21:24:17","2017-06-22 20:24:17",3
"Apache Mesos","Add --default_container_dns flag to the agent.","Mesos support both CNI (through `network/cni` isolator) and CNM (through docker) specification. Both these specifications allow for DNS entries for containers to be set on a per-container, and per-network basis.   Currently, the behavior of the agent is to use the DNS nameservers set in /etc/resolv.conf when the CNI or CNM plugin that is used to attached the container to the CNI/CNM network doesnt' explicitly set the DNS for the container. This is a bit inflexible especially when we have a mix of v4 and v6 networks.   The operator should be able to specify DNS nameservers for the networks he installs either the override the ones provided by the plugin or as defaults when the plugins are not going to specify DNS name servers.  In order to achieve the above goal we need to introduce a `\--dns` flag to the agent. The `\--dns` flag should support a JSON (or a JSON file) with the following schema: ",Task,Major,Resolved,"2017-06-22 16:56:34","2017-06-22 15:56:34",5
"Apache Mesos","Prevent non-RESERVATION_REFINEMENT frameworks from refining reservations.","We output the endpoint format through the endpoints  for backward compatibility of external tooling. A framework should be  able to use the result of an endpoint and pass it back to Mesos,  since the result was produced by Mesos. This is especially applicable  to the V1 API. We also allow the pre-reservation-refinement format  because existing resources files are written in that format, and  they should still be usable without modification.    This is probably too flexible however, since a framework without  a RESERVATION_REFINEMENT capability could make refined reservations  using the post-reservation-refinement format, although they wouldn't be  offered such resources. It still seems undesirable if anyone were to  run into it, and we should consider adding sensible restrictions.",Bug,Major,Accepted,"2017-06-22 01:52:46","2017-06-22 00:52:46",3
"Apache Mesos","stdlib.h: No such file or directory when building with GCC 6 (Debian stable freshly released)","Hi,    It seems the issue comes from a workaround added a while ago:  https://reviews.apache.org/r/40326/  https://reviews.apache.org/r/40327/    When building with external libraries it turns out creating build commands line with -isystem /usr/include which is clearly stated as being wrong, according to GCC guys:  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=70129    I'll do some testing by reverting all -isystem to -I and I'll let it know if it gets built.    Regards, Adam.      ",Bug,Major,Resolved,"2017-06-20 19:54:09","2017-06-20 18:54:09",8
"Apache Mesos","Update resource provider design in the master","Some discussion around how to use the allocator result in changes to how local resource providers and external resource providers should be handled in the master. The current approach needs to be updated.",Task,Major,Resolved,"2017-06-20 15:04:17","2017-06-20 14:04:17",7
"Apache Mesos","Add heartbeats to master stream API","Just like master uses heartbeats for scheduler API to keep the connection alive, it should do the same for the streaming API.",Improvement,Major,Resolved,"2017-06-20 03:26:36","2017-06-20 02:26:36",3
"Apache Mesos","Support local enabled cgroups subsystems automatically.","Currently, each cgroup subsystem needs to be turned on as an isolator, e.g., cgroups/blkio. Ideally, mesos should be able to detect all local enabled cgroup subsystems and turn them on automatically (or we call it auto cgroups).",Improvement,Major,Resolved,"2017-06-17 17:54:51","2017-06-17 16:54:51",8
"Apache Mesos","Stop using EXIT() in master/agent initialization code","The initialization of master/agent dependencies is currently inconsistent. For some dependencies, we initialize them outside of the actor and then inject them via the constructor; for example, in {{main.cpp}} and {{cluster.cpp}}.  Some other dependencies are created/initialized within the master/slave's {{initialize()}} method. In this case, if the dependency creation fails, we use {{EXIT(EXIT_FAILURE)}} to terminate the process. In the case of tests, this is problematic. If I create multiple agents, for example, and one of their dependencies fails to initialize successfully, the entire test harness would exit :-(  During some discussion, [~<USER> proposed an alternative: instead of using {{EXIT}} when dependency creation fails, we could terminate the master/agent libprocess process. In the case of the production binaries, this would cause the executable to exit. In the case of our tests, this would allow a single test to fail, while the test harness continues running.  ",Improvement,Major,Open,"2017-06-15 21:36:03","2017-06-15 20:36:03",3
"Apache Mesos","Isolate network ports.","If a task uses network ports, there is no isolator that can enforce that it only listens on the ports that it has resources for. Implement a ports isolator that can limit tasks to listen only on allocated TCP ports.  Roughly, the algorithm for this follows what standard tools like {{lsof}} and {{ss}} do.  * Find all the listening TCP sockets (using netlink) * Index the sockets by their node (from the netlink information) * Find all the open sockets on the system (by scanning {{/proc/\*/fd/\*}} links) * For each open socket, check whether its node (given in the link target) in the set of listen sockets that we scanned * If the socket is a listening socket and the corresponding PID is in the task, send a resource limitation for the task  Matching pids to tasks depends on using cgroup isolation, otherwise we would have to build a full process tree, which would be nice to avoid.  Scanning all the open sockets can be avoided by using the {{net_cls}} isolator with kernel + libnl3 patches to publish the socket classid when we find the listening socket.  Design Doc: https://docs.google.com/document/d/1BGmANq8IW-H4-YVUlpdf6qZFTZnDe-OKAY_e7uNp7LA Kernel Patch: http://marc.info/?l=linux-kernel&m=150293015025396&w=2",Improvement,Minor,Resolved,"2017-06-15 01:15:02","2017-06-15 00:15:02",8
"Apache Mesos","Update the documentation to reflect the addition of reservation refinement.","There are a few things we need to be sure to document:    * What reservation refinement is.  * The new format for Resource, when using the RESERVATION_REFINEMENT capability.  * The filtering of resources if a framework is not RESERVATION_REFINEMENT capable.  * The current limitations that only a single reservation can be pushed / popped within a single RESERVE / UNRESERVE operation.",Documentation,Blocker,Resolved,"2017-06-14 02:42:34","2017-06-14 01:42:34",2
"Apache Mesos","Documentation regarding TASK_LOST is misleading","Our protos describe {{TASK_LOST}} as a terminal state [\[1\]|https://github.com/apache/mesos/blob/fb54d469dcadf762e9c3f8a2fed78ed7b306120a/include/mesos/mesos.proto#L1722] [\[2\]|https://github.com/apache/mesos/blob/fb54d469dcadf762e9c3f8a2fed78ed7b306120a/include/mesos/mesos.proto#L64-L73].  A task might go from {{TASK_LOST}} to {{TASK_RUNNING}} or another state if Mesos is not using a strict register, so the documentation is misleading.  Marathon used to assume that {{TASK_LOST}} was a terminal past and that resulted in production pain for some users.  We should update the documentation to make the life of frameworks developers a bit better =).",Bug,Major,Resolved,"2017-06-14 01:18:51","2017-06-14 00:18:51",2
"Apache Mesos","Libprocess timers with long durations trigger immediately","{{process::delay()}} will schedule a method to be run right ahead when called with a veeeery long {{Duration}}.  This happens because [{{Timeout}} tries to add two long durations|https://github.com/apache/mesos/blob/13cae29e7832d8bb879c68847ad0df449d227f17/3rdparty/libprocess/include/process/timeout.hpp#L33-L38], leading to an [integer overflow in {{Duration}}|https://github.com/apache/mesos/blob/13cae29e7832d8bb879c68847ad0df449d227f17/3rdparty/stout/include/stout/duration.hpp#L116].  I'd expect libprocess to either:    1. Never run the method.   2. Schedule it in the longest possible {{Duration}}.  {{Duration::operator+=()}} should probably also handle integer overflows differently. If an addition leads to an integer overflow, it might make more sense to return {{Duration::max()}} than a negative duration.",Bug,Major,Resolved,"2017-06-14 00:46:07","2017-06-13 23:46:07",3
"Apache Mesos","HierarchicalAllocator uses the default filter instead of a very long one","If a framework accepts/refuses an offer using a very long filter, [the {{HierarchicalAllocator}} will use the default {{Filter}} instead|https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/hierarchical.cpp#L1046-L1052]. Meaning that it will filter the resources for only 5 seconds.  This can happen when a framework sets {{Filter::refuse_seconds}} to a number of seconds [larger than what fits in {{Duration}}|https://github.com/apache/mesos/blob/13cae29e7832d8bb879c68847ad0df449d227f17/3rdparty/stout/include/stout/duration.hpp#L401-L405].  The following [tests are flaky|https://issues.apache.org/jira/browse/MESOS-7514] because of this: {{ReservationTest.ReserveShareWithinRole}} and {{ReservationTest.PreventUnreservingAlienResources}}.",Bug,Major,Resolved,"2017-06-14 00:32:10","2017-06-13 23:32:10",3
"Apache Mesos","Update the JSON <=> protobuf message conversion for map support","[Map|https://developers.google.com/protocol-buffers/docs/proto#maps] is a feature starting from proto2 syntax, but it can only be compiled with proto3 compiler, see the following discussion for details:  https://groups.google.com/forum/#\!topic/protobuf/p4WxcplrlA4  We have already upgraded the protobuf compiler from 2.6.1 to 3.3.0 in [MESOS-7228|https://issues.apache.org/jira/browse/MESOS-7228], however, to use protobuf map in Mesos code, we also need to add the protobuf map support to the code in Mesos for converting protobuf message to JSON object and parsing JSON object as protobuf message, that is what we plan to handle in this ticket.    With map support added, the following field in the protobuf message can be replaced by the native protobuf map field, and we do not have to manually parse it anymore.  https://github.com/apache/mesos/blob/1.3.0/include/mesos/docker/v1.proto#L68",Improvement,Major,Resolved,"2017-06-13 08:49:27","2017-06-13 07:49:27",3
"Apache Mesos","Docker image with universal containerizer does not work if WORKDIR is missing in the rootfs.","hello, used the following docker image recently  quay.io/spinnaker/front50:master https://quay.io/repository/spinnaker/front50  Here the link to the Dockerfile https://github.com/spinnaker/front50/blob/master/Dockerfile  and here the source {color:blue}FROM java:8  MAINTAINER <EMAIL>  COPY . workdir/  WORKDIR workdir  RUN GRADLE_USER_HOME=cache ./gradlew buildDeb -x test && \   dpkg -i ./front50-web/build/distributions/*.deb && \   cd .. && \   rm -rf workdir  CMD [/opt/front50/bin/front50]{color}   The image works fine with the docker containerizer, but the universal containerizer shows the following in stderr.  Failed to chdir into current working directory '/workdir': No such file or directory  The problem comes from the fact that the Dockerfile creates a workdir but then later removes the created dir as part of a RUN. The docker containerizer has no problem with it if you do  docker run -ti --rm quay.io/spinnaker/front50:master bash  you get into the working dir, but the universal containerizer fails with the error.  thanks for your help, Michael",Bug,Critical,Resolved,"2017-06-10 03:39:11","2017-06-10 02:39:11",3
"Apache Mesos","The order of isolators provided in '--isolation' flag is not preserved and instead sorted alphabetically","According to documentation and comments in code the order of the entries in the --isolation flag should specify the ordering of the isolators. Specifically, the `create` and `prepare` calls for each isolator should run serially in the order in which they appear in the --isolation flag, while the `cleanup` call should be serialized in reverse order (with exception of filesystem isolator which is always first).  But in fact, the isolators provided in '--isolation' flag are sorted alphabetically.  That happens in [this line of code|https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/containerizer.cpp#L377]. In this line use of 'set<string>' is done (apparently instead of list or vector) and set is a sorted container.",Improvement,Major,Resolved,"2017-06-08 17:42:36","2017-06-08 16:42:36",2
"Apache Mesos","Docker containerizer fails to set sandbox logs ownership correctly.","When using the docker containerizer in connection with a task that has no command user set but a framework that has nobody set as its default user, the resulting sandbox logs will be owned by the agent owning user (e.g. root) but not the framework owner (e.g. nobody).",Bug,Blocker,Resolved,"2017-06-08 00:08:48","2017-06-07 23:08:48",2
"Apache Mesos","Add simple filtering to unversioned operator API","Add filtering for the following endpoints: - {{/frameworks}} - {{/slaves}} - {{/tasks}} - {{/containers}}  We should investigate whether we should use RESTful style or query string to filter the specific resource. We should also figure out whether it's necessary to filter a list of resources.",Improvement,Major,Resolved,"2017-06-06 18:10:35","2017-06-06 17:10:35",5
"Apache Mesos","Parsing to protobuf leads to API call validation errors","The {{::protobuf::parse()}} function will [silently drop unrecognized fields|https://github.com/apache/mesos/blob/7ec3269d51d7d180aa857140097c170c469d7959/3rdparty/stout/include/stout/protobuf.hpp#L589], which makes sense in the context of maintaining backward-compatibility across different Mesos versions which may add or remove fields from protobuf messages. However, since we [rely on this protobuf parsing|https://github.com/apache/mesos/blob/7ec3269d51d7d180aa857140097c170c469d7959/src/master/http.cpp#L514-L520] in some places for validation of user-supplied JSON, this can lead to API endpoints returning successful 2XX responses, when in fact the JSON was malformed and the call has not been completed as submitted.  We should consider adding a parameter to API calls which allows users to enable/disable ignoring unrecognized fields in the call. If the default behavior for JSON requests was to return an error rather than ignore unrecognized fields, then our parsing code would catch malformed JSON submissions. The user could opt-in to the ignore unrecognized fields behavior when backwards compatibility is a concern.",Bug,Major,Open,"2017-06-06 16:22:36","2017-06-06 15:22:36",3
"Apache Mesos","Mesos slave stucks","*Description of the problem*  Sometimes all containers on mesos-slave becomes unhealthy without any reason. Then Mesos try to kill them without success. As result old containers are still running in unhealthy state and new containers have not started. You can see what happens on host machine and in docker container mesos-slave.  We have been seen this problem several times on month on different hosts and different clusters. Restart of mesos-slave solves the problem, but it is not solution.  {quote} Mesos 1.1 Marathon 1.3.6 Docker version 17.03.0-ce {quote}  stderr of container:   stdout of container   n container with mesos-slave   Container with mesos-slave   Container with mesos-slave   host-machine   Mesos slave logs    I ask you to take a look on this problem before mesos-slave is not restared and I can collect some additional information.",Bug,Critical,Resolved,"2017-06-06 12:51:37","2017-06-06 11:51:37",3
"Apache Mesos","Create a CI job to publish the website","This job periodically scans for changes to the master branch of `mesos` and publishes an updated website to `mesos-site`.",Task,Major,Resolved,"2017-06-06 00:13:00","2017-06-05 23:13:00",3
"Apache Mesos","Create script to automate publishing website","These script will be run via ASF CI and be responsible for   1) checking out the latest master branch 2) build mesos and generate endpoints help 3) generate website contents 4) publish website by doing a git commit to `mesos-site` repo",Task,Major,Resolved,"2017-06-06 00:09:21","2017-06-05 23:09:21",5
"Apache Mesos","Move website from svn to git","Move our website svn repo at https://svn.apache.org/repos/asf/mesos/site to a git repo.  Having git repo for both the main project and website allows us to deal with one version control system. Also git based projects are easy to automate via CI (e.g., git commit) because ASF CI already has required credentials. ",Task,Major,Resolved,"2017-06-06 00:05:28","2017-06-05 23:05:28",3
"Apache Mesos","Automatically publish website through CI","Currently, publishing the website is a manual process whereby a committer runs a local docker script, copies the generated `publish` folder to svn copy and does an `svn commit`. This is both cumbersome and error prone.  We should automate this process by running this as a CI job.",Epic,Major,Resolved,"2017-06-06 00:02:41","2017-06-05 23:02:41",8
"Apache Mesos","UCR cannot read docker images containing long file paths","The latest Docker uses go 1.7.5 (https://github.com/moby/moby/blob/master/CHANGELOG.md#contrib-1), in which the {{archive/tar}} package has a bug that cannot handle file paths longer than 100 characters (https://github.com/golang/go/issues/17630). As a result, Docker will generate images containing ill-formed tar files (details below) when there are long paths. Docker itself understands the ill-formed image fine, but a standard tar program will interpret the image as if all files with long paths are placed under the root directory (https://github.com/moby/moby/issues/29360).    This bug has been fixed in go 1.8, but since Docker is still using the bugged version, we might need to handle these ill-formed images created by Dcoker utilities.    NOTE: It is confirmed that the {{archive/tar}} package in go 1.8 cannot correctly extract the ill-formed tar files, but the one in go 1.7.5 could.    Details: the {{archive/tar}} package uses {{USTAR}} format to handle files with 100+-character-long paths (by only putting file name in the {{name}} field and the path in the {{prefix}} field in the tar header), but uses {{OLDGNU}}'s magic string, which does not understand the {{prefix}} field, so a standard tar program will extract such files under the current directory.",Bug,Major,Reviewable,"2017-06-03 00:34:04","2017-06-02 23:34:04",3
"Apache Mesos","Consider supporting changes to agent's domain without full drain.","In the initial review chain, any change to an agent's domain requires a full drain. This is simple and straightforward, but it makes it more difficult for operators to opt-in to using fault domains.  We should consider allowing agents to transition from no configured domain to configured domain without requiring an agent drain. This has some complications, however: e.g., without an API for communicating changes in an agent's configuration to frameworks, they might not realize that an agent's domain has changed.",Improvement,Major,Resolved,"2017-06-02 19:36:40","2017-06-02 18:36:40",12
"Apache Mesos","UCR doesn't isolate uts namespace w/ host networking","Docker's {{run}} command supports a {{--hostname}} parameter which impacts container isolation, even in {{host}} network mode: (via https://docs.docker.com/engine/reference/run/)  {quote}  Even in host network mode a container has its own UTS namespace by default. As such --hostname is allowed in host network mode and will only change the hostname inside the container. Similar to --hostname, the --add-host, --dns, --dns-search, and --dns-option options can be used in host network mode.  {quote}  I see no evidence that UCR offers a similar isolation capability.    Related: the {{ContainerInfo}} protobuf has a {{hostname}} field which was initially added to support the Docker containerizer's use of the {{--hostname}} Docker {{run}} flag.",Improvement,Major,Reviewable,"2017-06-02 13:07:14","2017-06-02 12:07:14",3
"Apache Mesos","SlaveTest.ExecutorReregistrationTimeoutFlag aborts on Windows",,Bug,Major,Resolved,"2017-06-02 01:27:12","2017-06-02 00:27:12",1
"Apache Mesos","Add filtering capabilities to the master/agent operator APIs","We would like to add filtering capabilities to both the unversioned operator HTTP endpoints and the V1 operator APIs on the master and agent.",Epic,Major,Accepted,"2017-06-01 22:42:44","2017-06-01 21:42:44",13
"Apache Mesos","Some container launch failures are mistakenly treated as errors.","I've observed a case when a scheduler stops (i.e. calls TEARDOWN) while some of its tasks are being launched. While this is a valid behaviour, the agent prints an error and increased container launch errors metrics.  Below are log excerpts for such framework, {{6dd898d6-7f3a-406c-8ead-24b4d55ed262-0018-driver-20170601113252-0092}}.  *Master log*  *Agent log* ",Bug,Major,Reviewable,"2017-06-01 18:20:35","2017-06-01 17:20:35",5
"Apache Mesos","Implement 'apply' for resource provider related operations","Resource providers provide new offer operations ({{CREATE_BLOCK}}, {{DESTROY_BLOCK}}, {{CREATE_VOLUME}}, {{DESTROY_VOLUME}}). These operations can be applied by frameworks when they accept on offer. Handling of these operations has to be added to the master's {{accept}} call. I.e. the corresponding resource provider needs be extracted from the offer's resources and a {{resource_provider::Event::OPERATION}} has to be sent to the resource provider. The resource provider will answer with a {{resource_provider::Call::Update}} which needs to be handled as well.",Task,Major,Resolved,"2017-05-31 13:36:14","2017-05-31 12:36:14",5
"Apache Mesos","Make use of cout/cerr and glog consistent.","Some parts of mesos use glog before initialization of glog, hence messages via glog might not end up in a logdir: bq. WARNING: Logging before InitGoogleLogging() is written to STDERR  The solution might be: {{cout/cerr}} should be used before logging initialization. {{glog}} should be used after logging initialization.   Usually, main function has initialization pattern like: # load = flags.load(argc, argv) // Load flags from command line. # Check if flags are correct, otherwise print error message to cerr and then exit. # Check if user passed --help flag to print help message to cout and then exit. # Parsing and setup of environment variables. If this fails, EXIT macro is used to print error message via glog. # process::initialize() # logging::initialize()   Steps 2 and 3 should use {{cout/cerr}} to eliminate any extra information generated by glog like current time, date and log level.  It would be preferable to move step 6 between steps 3 and 4 safely, because {{logging::initialize()}} doesn’t depend on {{process::initialize()}}. In addition, initialization of glog should be added, where it's necessary.",Bug,Minor,Resolved,"2017-05-30 13:48:36","2017-05-30 12:48:36",3
"Apache Mesos","Specifying an unbundled dependency can cause build to pick up wrong Boost version","Specifying an unbundled dependency can cause the build to pick up a wrong Boost version. Assuming we have e.g., both protobuf and Boost installed in {{PREFIX}}, configuring with {{--with-protobuf=PREFIX}} causes the build to pick up the Boost version from {{PREFIX}} instead of using the bundled one.  This appears to be due to how we specify Boost include paths. Boost paths are added with {{-isystem}} to suppress warnings; the protobuf include path, on the other hand, would be added with {{-I}}. GCC and for compatibility clang first search all paths specified with {{-I}} left-to-right before looking at paths given with {{-isystem}}, see [the GCC documenation|https://gcc.gnu.org/onlinedocs/gcc/Directory-Options.html].",Bug,Major,Resolved,"2017-05-29 09:34:18","2017-05-29 08:34:18",1
"Apache Mesos","Write a proposal to make the I/O Switchboards optional","Right now DEBUG containers can only be started using the LaunchNestedContainerSession API call. They will enter its parent’s namespaces, inherit environment variables, stream its I/O, and Mesos will tie their life-cycle to the lifetime of the HTTP connection.  Streaming the I/O of a container requires an I/O Switchboard and adds some overhead and complexity:  - Mesos will launch an extra process, called an I/O Switchboard for each nested container. These process aren’t free, they take some time to create/destroy and consume resources. - I/O Switchboards are managed by a complex isolator. - /O Swichboards introduce new race conditions, and have been a source of deadlocks in the past.   Some use cases require some of the features provided by DEBUG containers, but don’t need the functionality provided by the I/O switchboard. For instance, the Default Executor uses DEBUG containers to perform (health)checks, but it doesn’t need to stream anything to/from the container. ",Task,Major,Resolved,"2017-05-26 22:21:24","2017-05-26 21:21:24",5
"Apache Mesos","Support reservation refinement for hierarchical roles.","With the introduction of hierarchical roles, Mesos provides a mechanism to delegate resources down a hierarchy.  To complement this, we'll introduce a mechanism to *refine* the reservations down the hierarchy.    For example, given resources allocated to role {{foo}}, it can be further reserved for {{foo/bar}}.  When the resources allocated to {{foo/bar}} is unreserved, it goes back to where it came from. In this case, back to role {{foo}}.",Epic,Major,Accepted,"2017-05-26 22:14:22","2017-05-26 21:14:22",12
"Apache Mesos","Add `--resource_provider_config_dir` flag to the agent.","Add an agent flag `--resource_provider_config_dir` to allow operators to specify the list of local resource providers to register with the agent.",Task,Major,Resolved,"2017-05-26 02:11:57","2017-05-26 01:11:57",2
"Apache Mesos","Add a storage local resource provider.","It will interact with CSI plugins to get capacity information as well as talking to the plugin for operations like CREATE/DESTROY. ",Task,Major,Resolved,"2017-05-26 02:04:21","2017-05-26 01:04:21",5
"Apache Mesos","Introduce a heartbeat mechanism for v1 HTTP executor <-> agent communication.","Currently, we do not have heartbeats for executor <-> agent communication. This is especially problematic in scenarios when IPFilters are enabled since the default conntrack keep alive timeout is 5 days. When that timeout elapses, the executor doesn't get notified via a socket disconnection when the agent process restarts. The executor would then get killed if it doesn't re-register when the agent recovery process is completed.    Enabling application level heartbeats or TCP KeepAlive's can be a possible way for fixing this issue.    We should also update executor API documentation to explain the new behavior.",Bug,Critical,Resolved,"2017-05-25 01:24:49","2017-05-25 00:24:49",5
"Apache Mesos","Add storage resource provider specific information in ResourceProviderInfo.","For storage resource provider, there will be some specific configuration information. For instance, the most important one is the `ContainerConfig` of the CSI Plugin container.    That config information will be sent to the corresponding agent that will use the resources provided by the resource provider. For storage resource provider particularly, the agent needs to launch the CSI Node Plugin to mount the volumes.    Comparing to adding first class storage resource provider information, an alternative is to add a generic labels field in ResourceProviderInfo and let resource provider itself figure out the format of the labels. However, I believe a first class solution is better and more clear.",Task,Major,Resolved,"2017-05-24 22:28:15","2017-05-24 21:28:15",2
"Apache Mesos","Add 'type' and 'name' to ResourceProviderInfo.","The 'type' field will be used to load the corresponding implementation (either internal or via module). To avoid conflict, the naming should follow java packing naming scheme (e.g., org.apache.mesos.resource_provider.local.storage).  Since there could be multiple instances of the same resource provider type, it's important to also add a 'name' field to distinguish between instances of the same type.",Task,Major,Resolved,"2017-05-24 21:21:19","2017-05-24 20:21:19",2
"Apache Mesos","Add resource provider validation","Similar to how it's done during agent registration/re-registration, the informations provided by a resource provider need to get validation during certain operation (e.g. re-registration, while applying offer operations, ...). Some of these validations only cover the provided informations (e.g. are the resources in {{ResourceProviderInfo}} only of type {{disk}}), others take the current cluster state into account (e.g. do the resources that a task wants to use exist on the resource provider).",Task,Major,Resolved,"2017-05-24 13:32:09","2017-05-24 12:32:09",3
"Apache Mesos","Add resource provider IDs to the registry","To support resource provider re-registration following a master fail-over, the IDs of registered resource providers need to be kept in the registry. An operation to commit those IDs using the registrar needs to be added as well.",Task,Major,Resolved,"2017-05-24 13:20:25","2017-05-24 12:20:25",5
"Apache Mesos","Publish Local Resource Provider resources in the agent before container launch or update.","The agent will ask RP manager to publish the resources before container can start to use them. SLRP (storage local resource provider) will be responsible for making sure the CSI volume is made available on the host. This will involve calling `ControllerPublishVolume` and `NodePublishVolume` RPCs from the CSI Plugin.    This will happen when a workload (i.e., task/executor) are being launched on the agent that uses a CSI volume as a persistent volume. During the creation of a CSI volume, the SLRP will generate a fixed mount point under the agent's work directory based on the ID of the CSI volume, and store the mount point in the `Resource.disk.source.path.root` or `Resource.disk.source.path.mount` fields. Prior to a workload launch, SLRP will mount the CSI volume to the same path, then the Docker containerizer or the Mesos containerizer will again bind-mount the volume into the container of the workload. Since the containerizers know nothing about the resource providers, it would extract the mount point of the CSI volume from the `Resource.disk.source.path.root` or `Resource.disk.source.path.mount` fields.    For storage local resource provider, the agent's work directory is known during the creation of the CSI volume since it will be created an used on the same agent. However, in the case of a storage external resource provider, where a CSI volume might be created on one agent X and published on another agent Y, the work directory of agent Y might not be known at the creation of a CSI volume on X. To support it in the future, we introduce new semantics for `Resource.disk.source.path.root` and `Resource.disk.source.path.mount`, such that if these fields are set to relative paths, they are relative to the agent's work directory, so the containerizer can extract the mount point by prefixing the relative paths with the agent's work directory.",Task,Major,Resolved,"2017-05-24 00:12:47","2017-05-23 23:12:47",5
"Apache Mesos","WAIT_NESTED_CONTAINER sometimes returns 404","{{WAIT_NESTED_CONTAINER}} sometimes returns 404s even though the nested container has already exited and the parent task/executor is still running.  This happens when an agent uses more than one containerizer (e.g.,  {{docker,mesos}}, {{WAIT_NESTED_CONTAINER}} and the exit status of the nested container has already been checkpointed.  The root cause of this is a bug in the {{ComposingContainerizer}} in the following lines: https://github.com/apache/mesos/blob/1c7ffbeb505b3f5ab759202195f0b946a20cb803/src/slave/containerizer/composing.cpp#L620-L628  ",Bug,Critical,Resolved,"2017-05-23 00:26:38","2017-05-22 23:26:38",3
"Apache Mesos","Add executor reconnection retry logic to the agent","Currently, the agent sends a single {{ReconnectExecutorMessage}} to PID-based executors during recovery. It would be more robust to have the agent retry these messages until {{executor_reregister_timeout}} has elapsed.",Improvement,Major,Resolved,"2017-05-22 22:25:08","2017-05-22 21:25:08",3
"Apache Mesos","Add an agent flag for executor re-registration timeout.","Currently, the executor re-register timeout is hard-coded at 2 seconds. It would be beneficial to allow operators to specify this value.",Improvement,Major,Resolved,"2017-05-22 19:45:37","2017-05-22 18:45:37",1
"Apache Mesos","OversubscriptionTest.RescindRevocableOfferWithIncreasedRevocable is flaky.","  ",Bug,Major,Resolved,"2017-05-17 18:45:23","2017-05-17 17:45:23",1
"Apache Mesos","ReservationTest.PreventUnreservingAlienResources is flaky","This repros consistently for me on a many-core box:  ",Bug,Major,Resolved,"2017-05-16 19:17:46","2017-05-16 18:17:46",5
"Apache Mesos","CniIsolatorTest.ROOT_DynamicAddDelofCniConfig is flaky.",,Bug,Major,Resolved,"2017-05-16 16:23:29","2017-05-16 15:23:29",1
"Apache Mesos","Multiple tests leave orphan containers.","I've observed a number of flaky tests that leave orphan containers upon cleanup. A typical log looks like this:      All currently affected tests:  ",Bug,Major,Resolved,"2017-05-15 19:58:31","2017-05-15 18:58:31",8
"Apache Mesos","Parent's mount namespace cannot be determined when launching a nested container.","I've observed this failure twice in different Linux environments. Here is an example of such failure:  ",Bug,Major,Resolved,"2017-05-15 18:54:44","2017-05-15 17:54:44",3
"Apache Mesos","Build error on Windows when using int for a file descriptor","There is a build error for mesos-tests in src/tests/check_tests.cpp on Windows associated with the use of an int file descriptor:  C:\mesos\mesos\src\tests\check_tests.cpp(1890): error C2440: 'initializing': cannot convert from 'Try<std::array<os::WindowsFD,2>,Error>' to 'Try<std::array<int,2>,Error>' [C:\mesos\mesos\build\src\tests\mesos-tests.vcxproj]",Bug,Major,Resolved,"2017-05-11 21:29:27","2017-05-11 20:29:27",1
"Apache Mesos","Command checks via agent lead to flaky tests.","Tests that rely on command checks via agent are flaky on Apache CI. Here is an example from one of the failed run: https://pastebin.com/g2mPgYzu",Bug,Major,Resolved,"2017-05-11 11:34:46","2017-05-11 10:34:46",8
"Apache Mesos","Introduce a daemon manager in the agent.","Once we have standalone container support from the containerizer, we should consider adding a daemon manager inside the agent. It'll be like 'monit', 'upstart' or 'systemd', but with very limited functionalities. For instance, as a start, the manager will simply always restart the daemons if the daemon fails. It'll also try to cleanup unknown daemons.  This feature will be used to manage CSI plugin containers on the agent.  The daemon manager should have an interface allowing operators to register a daemon with a name and a config of the daemon. The daemon manager is responsible for restarting the daemon if it crashes until some one explicitly unregister it. Some simple backoff and health check functionality should be provided.  We probably need a small design doc for this.  ",Task,Major,Resolved,"2017-05-10 15:13:38","2017-05-10 14:13:38",5
"Apache Mesos","Build a CSI client to talk to a CSI plugin.","The abstraction should be something like the following: ",Task,Major,Resolved,"2017-05-10 14:26:04","2017-05-10 13:26:04",5
"Apache Mesos","Add `--ip6` and `--ip6_discovery_command` flag to Mesos agent","As a first step to support IPv6 containers on Mesos, we need to provide {{--ip6}} and {{--ip6_discovery_command}} flags to the agent so that the operator can specify an IPv6 address for the {{libprocess}} actor on the agent. In this ticket we will not aim to add IPv6 communication support for Mesos but will aim to use the IPv6 address provided by the operator to fill in the v6 address for any containers running on the host network in a dual stack environment.",Task,Major,Resolved,"2017-05-10 07:41:32","2017-05-10 06:41:32",5
"Apache Mesos","Mesos fetcher cache doesn't retry when missed.","Mesos Fetcher doesn't retry when a cache is missed. It needs to have the ability to pull from source when it fails.     421 15:52:53.022902 32751 fetcher.cpp:498] Fetcher Info: {cache_directory:\/tmp\/mesos\/fetch\/slaves\/<slaveid>,items:[\{action:RETRIEVE_FROM_CACHE,cache_filename:<file.name.tar.gz>),uri:\{cache:true,executable:false,extract:true,value:https:\/\/<file.path.s3.amazonaws.com>\/<file.name.tar.gz>}}],sandbox_directory:\/var\/lib\/mesos\/slave\/slaves\/<slaveid>\/frameworks\<frameworkid>\/executors\/name\/runs\/<id>}   I0421 15:52:53.024926 32751 fetcher.cpp:409] Fetching URI 'https:\/\/<file.path.s3.amazonaws.com>\/<file.name.tar.gz>   I0421 15:52:53.024942 32751 fetcher.cpp:306] Fetching from cache   I0421 15:52:53.024958 32751 fetcher.cpp:84] Extracting with command: tar -C \/var\/lib\/mesos\/slave\/slaves\/<slaveid>\/frameworks\<frameworkid>\/executors\/name\/runs\/<id>' -xf '/tmp/mesos/fetch/slaves/f3feeab8-a2fe-4ac1-afeb-ec7bd4ce7b0d-S29/c1-docker-hub.tar.gz'   tar: /https:\/\/<file.path.s3.amazonaws.com>\/<file.name.tar.gz>: Cannot open: No such file or directory   tar: Error is not recoverable: exiting now   Failed to fetch 'https:\/\/<file.path.s3.amazonaws.com>\/<file.name.tar.gz>': Failed to extract: command tar -C '\/var\/lib\/mesos\/slave\/slaves\/<slaveid>\/frameworks\<frameworkid>\/executors\/name\/runs\/<id>' -xf '/tmp/mesos/fetch/slaves/<file.name.tar.gz>' exited with status: 512  ",Bug,Major,Resolved,"2017-05-08 22:27:26","2017-05-08 21:27:26",5
"Apache Mesos","Provisioner recover should not always assume 'rootfses' dir exists.","The mesos agent would restart due to many reasons (e.g., disk full). Always assume the provisioner 'rootfses' dir exists would block the agent to recover.    This issue may occur due to the race between removing the provisioner container dir and the agent restarts:   In provisioner recover, when listing the container rootfses, it is possible that the 'rootfses' dir does not exist. Because a possible race between the provisioner destroy and the agent restart. For instance, while the provisioner is destroying the container dir the agent restarts. Due to os::rmdir() is recursive by traversing the FTS tree, it is possible that 'rootfses' dir is removed but the others (e.g., scratch dir) are not.  Currently, we are returning an error if the 'rootfses' dir does not exist, which blocks the agent from recovery. We should skip it if 'rootfses' does not exist.",Bug,Critical,Resolved,"2017-05-08 20:40:40","2017-05-08 19:40:40",2
"Apache Mesos","Add resource provider driver.","Similar to scheduler/executor driver, resource provider driver will be used to connect the resource provider and the Resource provider manager (resides in either agent for local resource providers, or master for external resource providers).",Task,Major,Resolved,"2017-05-08 13:26:39","2017-05-08 12:26:39",5
"Apache Mesos","HierarchicalAllocatorTest.NestedRoleQuota is flaky"," ",Bug,Major,Resolved,"2017-05-03 21:44:53","2017-05-03 20:44:53",2
"Apache Mesos","Expose MOUNT volumes of an agent in master's v0 HTTP API","Currently the /slaves endpoint shows scalar values of resources configured on the agent but doesn't show the whole information. Therefore important information like MOUNT volumes statically configured on the agent is missing (even though dynamic reservations are exposed). Note that this information is already present in the v1 GET_AGENT call.",Improvement,Major,Resolved,"2017-05-02 23:32:03","2017-05-02 22:32:03",2
"Apache Mesos","Refactor containerizers to not depend on TaskInfo or ExecutorInfo","The Containerizer interfaces should be refactored so that they do not depend on {{TaskInfo}} or {{ExecutorInfo}}, as a standalone container will have neither.  Currently, the {{launch}} interface depends on those fields.  Instead, we should consistently use {{ContainerInfo}} and {{CommandInfo}} in Containerizer and isolators.",Task,Major,Resolved,"2017-05-01 22:24:21","2017-05-01 21:24:21",5
"Apache Mesos","Add support for pruning the list of gone agents in the registry.","The list of gone agents in the registry can grow unbounded. We need to implement a pruning operation similar to the already existing {{PruneUnreachable}} for unreachable agents.",Task,Major,Resolved,"2017-05-01 20:50:25","2017-05-01 19:50:25",5
"Apache Mesos","Add authorization for the MARK_AGENT_GONE call.","We need to add the relevant AuthZ needed for the MARK_AGENT_GONE call. Note that AuthN would be added as part of the implementation of the API handler itself.",Task,Major,Resolved,"2017-05-01 20:44:36","2017-05-01 19:44:36",3
"Apache Mesos","Add authorization for the MARK_AGENT_GONE call.","We need to add the relevant AuthZ needed for the MARK_AGENT_GONE call. Note that AuthN would be added as part of the implementation of the API handler itself.",Task,Major,Resolved,"2017-05-01 20:44:32","2017-05-01 19:44:32",3
"Apache Mesos","Implement the API handler on the master for marking agents as gone.","We need to implement the v1 Operator API call on the master that stores the agent gone information in the registry and returns a 200 {{OK}} response if successful. The handler would also be responsible for transitioning all active tasks to {{TASK_GONE}} and sending status updates for them to the framework.",Task,Major,Resolved,"2017-05-01 20:42:43","2017-05-01 19:42:43",3
"Apache Mesos","Add support for storing gone agents to the master registry.","We need to add the {{MarkSlaveGone}} registry operation to the master allowing it to store agents that have been marked as gone. The relevant changes to {{registry.proto}} would also be done as part of this issue.",Task,Major,Resolved,"2017-05-01 20:36:20","2017-05-01 19:36:20",5
"Apache Mesos","Add the MARK_AGENT_GONE call to the Operator v1 API protos.","We need to add the relevant call to the v1 Operator API protos to mark an agent as GONE. The actual handler implementation on the master would be done in a separate ticket.",Task,Major,Resolved,"2017-05-01 20:11:14","2017-05-01 19:11:14",2
"Apache Mesos","Move some protobuf-related helpers into public headers","It recently came up during some module implementations that a couple protobuf-related helper functions would be useful if made available to module developers via public headers. Namely, * {{getRootContainerId}} found [here|https://github.com/apache/mesos/blob/2b427c788d46747cc4abfb16138548dd389c60f1/src/common/protobuf_utils.cpp#L637-L650] * {{operator<<}} for {{Labels}} found [here|https://github.com/apache/mesos/blob/2b427c788d46747cc4abfb16138548dd389c60f1/src/common/resources.cpp#L1942-L1963]  We should move these into public headers, and do an audit of other similar helpers to see what else could be useful to module developers.",Improvement,Major,Open,"2017-05-01 19:04:45","2017-05-01 18:04:45",3
"Apache Mesos","Double free or corruption when using parallel test runner","I observed the following when using the parallel test runner:    Not sure how reproducible this is, appears to occur in the authentication path of the agent.",Bug,Major,Resolved,"2017-04-28 23:42:02","2017-04-28 22:42:02",2
"Apache Mesos","SlaveTest.RestartSlaveRequireExecutorAuthentication is flaky.","This test failure has been observed on an internal CI system. It occurs on a variety of Linux distributions. It seems that using {{cat}} as the task command may be problematic; see attached log file {{SlaveTest.RestartSlaveRequireExecutorAuthentication.txt}}.",Bug,Major,Accepted,"2017-04-28 02:18:14","2017-04-28 01:18:14",5
"Apache Mesos","Set working directory in DEBUG containers.","Currently working directory is not set for DEBUG containers. The most reasonable value seems to be parent's working directory.",Task,Major,Resolved,"2017-04-27 22:53:40","2017-04-27 21:53:40",3
"Apache Mesos","Registry puller cannot fetch manifests from Google GCR: 403 Forbidden.","When the registry puller is pulling a repository from Google's GCE Container Registry, a '403 Forbidden' error occurs instead of 401 when fetching manifests.",Bug,Blocker,Resolved,"2017-04-27 03:55:38","2017-04-27 02:55:38",5
"Apache Mesos","ImageAlpine/ProvisionerDockerTest.ROOT_INTERNET_CURL_SimpleCommand/3 is flaky in some OS.","The unit test (ProvisionerDockerTest.ROOT_INTERNET_CURL_SimpleCommand) is fine, only flaky with one parameter (where GetParam() = registry.cn-hangzhou.aliyuncs.com/acs-sample/alpine). Might related this GnuTLS bug (https://bugs.launchpad.net/ubuntu/+source/gnutls26/+bug/1111882). The bug might be exposed by the slowness (due to intercontinental communication with AliCloud).  In Ubuntu 16.04 with SSL enable:   In Debian 8: ",Bug,Major,Accepted,"2017-04-25 18:03:25","2017-04-25 17:03:25",3
"Apache Mesos","Add s390x builds to Mesos CI","Hi Vinod,  We had raised an issue to add s390x support for mesos which was fixed and resolved. https://issues.apache.org/jira/browse/MESOS-6742  We also want to know about Mesos CI.   We need following details about current Mesos CI: 1. How is the current Mesos CI infrastructure? Travis/Jenkins? 2. Can Mesos CI extended to support s390x systems?  We are not sure if this is right channel to discuss this topic.  Please let us know if you want to start this discussion on some other channel.  Thanks,",Task,Major,"In Progress","2017-04-25 10:09:51","2017-04-25 09:09:51",3
"Apache Mesos","Implement Secret support for environment variables","MESOS-7009 added a new {{Secret}}-type field to the {{Environment}} protobuf message, but the agent has not yet implemented generic support for supplying environment variables as secrets. Currently, if an environment variable is specified as a secret, the variable's default-initialized {{value}} field will still be used when setting the variable, leading to an empty variable in the environment.",Task,Major,Accepted,"2017-04-24 19:39:48","2017-04-24 18:39:48",5
"Apache Mesos","Design doc for file-based secrets.",,Task,Major,Resolved,"2017-04-24 15:11:00","2017-04-24 14:11:00",8
"Apache Mesos","Filter results of `/master/slaves` and the v1 call GET_AGENTS","The results returned by both the endpoint {{/master/slaves}} and the API v1 {{GET_AGENTS}} return full information about the agent state which probably need to be filtered for certain uses, particularly in a multi-tenancy scenario.  The kind of leaked data includes specific role names and their specific allocations.",Task,Major,Resolved,"2017-04-24 14:24:24","2017-04-24 13:24:24",5
"Apache Mesos","Add authorization to master's operator maintenance API in v0 and v1","None of the maintenance primitives in either API v0 or API v1 have any kind of authorization, which allows any user with valid credentials to do things such as shutting down a machine, schedule time off on an agent, modify maintenance schedule, etc.  The authorization support needs to be added to the v0 endpoints:  * {{/master/machine/up}} * {{/master/machine/down}} * {{/master/maintenance/schedule}} * {{/master/maintenance/status}}  as well as to the v1 calls:  * {{GET_MAINTENANCE_STATUS}} * {{GET_MAINTENANCE_SCHEDULE}} * {{UPDATE_MAINTENANCE_SCHEDULE}} * {{START_MAINTENANCE}} * {{STOP_MAINTENANCE}}",Task,Major,Resolved,"2017-04-24 14:08:22","2017-04-24 13:08:22",3
"Apache Mesos","Enable authorization for master's logging API calls: GET_LOGGING_LEVEL  and SET_LOGGING_LEVEL","The Operator API calls {{GET_LOGGING_LEVEL}}  and {{SET_LOGGING_LEVEL}} lack authorization so any recognized user will be able to change the logging level of a given master.  The v0 endpoint {{/logging/toggle}} has authorization through the {{GET_ENDPOINT_WITH_PATH}} action. We need to decide whether it should also use additional authorization.  Note that there are already actions defined for authorization of these actions as they were already implemented in the agent.",Task,Major,Resolved,"2017-04-24 09:50:46","2017-04-24 08:50:46",5
"Apache Mesos","Move implicit authorization into the authorizer","The HTTP scheduler and executor APIs contain implicit authorization rules. Roughly stated, the rule is that schedulers and executors can only perform actions for/on schedulers/executors with the same principal. For example, schedulers can only launch tasks on schedulers with the same principal, and executors can only launch nested containers within an executor using the same principal.  These implicit authorization rules should be moved into the authorizer to maintain separation of authorization logic consistent with the rest of the Mesos codebase.  Note that these rules will be unnecessary in the V0 scheduler/executor APIs due to their implementation. Since V0 schedulers and executors authenticate once when their persistent TCP connection is established, the implicit authorization of subsequent actions performed on that connection is inherent to the implementation.",Improvement,Major,Open,"2017-04-18 16:04:25","2017-04-18 15:04:25",5
"Apache Mesos","Update allocator interfaces to support resource providers",,Task,Major,Resolved,"2017-04-13 17:06:10","2017-04-13 16:06:10",8
"Apache Mesos","Add authentication to the checker and health checker libraries","The health checker library in {{src/checks/health_checker.cpp}} must be updated to authenticate with the agent's HTTP operator API.",Task,Major,Resolved,"2017-04-11 16:57:33","2017-04-11 15:57:33",2
"Apache Mesos","Running DOCKER images in Mesos Container Runtime without `linux/filesystem` isolation enabled renders host unusable","If I run the pod below (using Marathon 1.4.2) against a mesos agent that has the flags (also below), then the overlay filesystem replaces the system root mount, effectively rendering the host unusable until reboot.  flags:  - {{--containerizers mesos,docker}} - {{--image_providers APPC,DOCKER}} - {{--isolation cgroups/cpu,cgroups/mem,docker/runtime}}  pod definition for Marathon:   Mesos should probably check for this and avoid replacing the system root mount point at startup or launch time.",Bug,Blocker,Resolved,"2017-04-10 03:57:00","2017-04-10 02:57:00",3
"Apache Mesos","Remove thread_local workaround on OSX","{{include/stout/thread_local.hpp}} in stout contains the following comment:    As of XCode 8, this workaround should no longer be necessary, because Apple's clang supports {{thread_local}} natively -- see http://stackoverflow.com/a/29929949/5327044",Bug,Major,Resolved,"2017-04-07 23:36:49","2017-04-07 22:36:49",2
"Apache Mesos","MasterAPITest.GetRoles is flaky on machines with non-C locale.","{{MasterAPITest.GetRoles}} test sets role weight to a real number using {{.}} as a decimal mark. This however is not correct on machines with non-standard locale, because weight parsing code relies on locale: [https://github.com/apache/mesos/blob/7f04cf886fc2ed59414bf0056a2f351959a2d1f8/src/master/master.cpp#L727-L750]. This leads to test failures: [https://pastebin.com/sQR2Tr2Q].  There are several solutions here.  h4. 1. Change parsing code to be locale-agnostic. This seems to be the most robust solution. However, the {{--weights}} flag is deprecated and will probably be removed soon, together with the parsing code.   h4. 2. Fix call sites in our tests to ensure decimal mark is locale dependent. This seems like a reasonable solution, but I'd argue we can do even better.  h4. 3. Use locale-agnostic format for doubles in tests. Instead of saying {{2.5}} we can say {{25e-1}} which is locale agnostic.",Bug,Major,Resolved,"2017-04-07 13:39:23","2017-04-07 12:39:23",1
"Apache Mesos","Compile error with recent glibc","  Observed on recent Arch Linux, GCC 6.3.1, glibc 2.25-1.",Bug,Major,Resolved,"2017-04-06 18:20:01","2017-04-06 17:20:01",1
"Apache Mesos","Upgrade vendored GMock / GTest","We currently vendor gmock 1.7.0. The latest upstream version of gmock is 1.8.0, which fixes at least one annoying warning (MESOS-6539).",Improvement,Major,Resolved,"2017-04-06 18:16:35","2017-04-06 17:16:35",3
"Apache Mesos","Command checks via agent pollute agent logs.","Command checks via agent leverage debug container API of the agent to start checks. Each such invocation triggers a bunch of logs on the agent, because the API was not originally designed with periodic invocations in mind. We should find a way to avoid excessive logging on the agent.",Improvement,Major,Resolved,"2017-04-06 15:28:13","2017-04-06 14:28:13",3
"Apache Mesos","Command checks via agent implicitly set up IOSwitchboard but do not use it.","Command checks via agent leverage launching debug containers via agent API to start check commands. This means IOSwitchboard is also set up despite not being used. To improve performance, we should bypass IOSwtichboard altogether or at least fast track its cleanup.",Improvement,Major,Open,"2017-04-06 11:16:17","2017-04-06 10:16:17",1
"Apache Mesos","Set MESOS_SANDBOX in debug containers.","Currently {{MESOS_SANDBOX}} is not set for debug containers, see [https://github.com/apache/mesos/blob/7f04cf886fc2ed59414bf0056a2f351959a2d1f8/src/slave/containerizer/mesos/containerizer.cpp#L1392-L1407]. The most reasonable value seems to be task's sandbox.",Improvement,Major,Resolved,"2017-04-06 10:42:56","2017-04-06 09:42:56",1
"Apache Mesos","Failed to pull image from Nexus Registry due to signature missing.","I’m trying to launch docker container with universal containerizer, mesos 1.2.0. But getting error “Failed to parse the image manifest: Docker v2 image manifest validation failed: ‘signatures’ field size must be at least one”. And if I switch to docker containerizer, app is starting normally.   We are working with private docker registry v2 backed by nexus repository manager  3.1.0   Here agent's log:  ",Bug,Critical,Resolved,"2017-04-05 20:21:16","2017-04-05 19:21:16",2
"Apache Mesos","Document Mesos check feature.","This should include framework authors recommendations about how and when to use general checks as well as comparison with health checks.",Documentation,Major,Resolved,"2017-04-05 16:51:19","2017-04-05 15:51:19",3
"Apache Mesos","Prototype resource offer operation handling in the master","Prototype the following workflow in the master, in accordance with the resource provider design; * Handle accept calls including resource provider related offer operations ({{CREATE_VOLUME}}, ...) * Implement internal bookkeeping of the disk resources these operations will be applied on * Implement resource bookkeeping for resource providers in the master * Send resource provider operations to resource providers",Task,Major,Resolved,"2017-04-05 08:44:20","2017-04-05 07:44:20",8
"Apache Mesos","Add authorization to agent executor API","The agent's {{/executor}} endpoint must be updated to accomplish simple implicit authorization of executor actions. This is analogous to the way the master's {{/scheduler}} endpoint handler verifies the framework's authenticated principal, effectively performing implicit authorization.",Task,Major,Resolved,"2017-04-04 18:41:02","2017-04-04 17:41:02",3
"Apache Mesos","Add resource provider API protobuf.","Resource provider API will be Event/Call based, similar to the scheduler or executor API. Resource providers will use this API to interact with the master, sending Calls to the master and receiving Event from the master.  The same API will be used for both local resource providers and external resource providers.",Task,Major,Resolved,"2017-04-03 00:56:08","2017-04-02 23:56:08",2
"Apache Mesos","Add resource provider to offer","In order to introduce external resource providers we need to add an {{optional}} resource provider field to the {{Offer}} message which can be used to unambiguously identify the provider. In addition, the existing {{slave_id}} will become {{optional}} with the requirement that either {{slave_id}} or {{resource_provider_id}} is set, ",Improvement,Minor,Resolved,"2017-03-31 10:38:46","2017-03-31 09:38:46",2
"Apache Mesos","Authorize offer operations for converting disk resources","All offer operations are authorized, hence authorization logic has to be added to new offer operations as well.",Task,Major,Resolved,"2017-03-31 10:28:09","2017-03-31 09:28:09",3
"Apache Mesos","Add a test with multiple tasks and checks for the default executor.",,Improvement,Major,Resolved,"2017-03-30 19:09:52","2017-03-30 18:09:52",2
"Apache Mesos","Remove deprecated ACL `ShutdownFramework`","The ACLs {{ShutdownFramework}}  was marked for deprecation more than six months ago, where it was replaced for the more convenient {{TeardownFramework}} ACL. The deprecation cycle for this action is finally due",Task,Major,Resolved,"2017-03-28 10:24:48","2017-03-28 09:24:48",1
"Apache Mesos","Upgrading Mesos to 1.2.0 results in some information missing from the `/flags` endpoint.","From OSS Mesos Slack: I recently tried upgrading one of our Mesos clusters from 1.1.0 to 1.2.0. After doing this, it looks like the {{zk}} field on the {{/master/flags}} endpoint is no longer present.   This looks related to the recent {{Flags}} refactoring that was done which resulted in some flags no longer being populated since they were not part of {{master::Flags}} in {{src/master/flags.hpp}}.",Bug,Critical,Resolved,"2017-03-27 21:39:48","2017-03-27 20:39:48",1
"Apache Mesos","Add offer operations for converting disk resources","One should be able to convert {{RAW}} and {{BLOCK}} disk resources into a different types by applying operations to them. The offer operations and the related validation and resource handling needs to be implemented.",Task,Major,Resolved,"2017-03-27 14:50:28","2017-03-27 13:50:28",5
"Apache Mesos","Update Resource proto for storage resource providers.","Storage resource provider support requires a number of changes to the {{Resource}} proto:  * support for {{RAW}} and {{BLOCK}} type {{Resource::DiskInfo::Source}} * {{ResourceProviderID}} in Resource * {{Resource::DiskInfo::Source::Path}} should be {{optional}}.",Bug,Major,Resolved,"2017-03-27 08:54:52","2017-03-27 07:54:52",3
"Apache Mesos","Support mount propagation for host volumes.","Currently, all mounts in a container are marked as 'slave' by default. However, for some cases, we may want mounts under certain directory in a container to be propagate back to the root mount namespace. This is useful for the case where we want the mounts to survive container failures.    See more documentation about mount propagation in:  https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt    Given mount propagation is very hard for users to understand, probably worth limiting this to just host volumes because we only see use case for that at the moment.    Some relevant discussion can be found here:  https://github.com/kubernetes/community/blob/master/contributors/design-proposals/propagation.md",Improvement,Major,Resolved,"2017-03-24 20:35:31","2017-03-24 20:35:31",8
"Apache Mesos","Adjust the recover logic of MesosContainerizer to allow standalone containers.","The current recovery logic in MesosContainerizer assumes that all top level containers are tied to some Mesos executors. Adding standalone containers will invalid this assumption. The recovery logic must be changed to adapt to that.",Task,Major,Resolved,"2017-03-24 19:56:17","2017-03-24 19:56:17",8
"Apache Mesos","Fetcher should not depend on SlaveID.","Currently, various Fetcher interfaces depends on SlaveID, which is an unnecessary coupling. For instance:   Looks like the only reason we need a SlaveID is because we need to calculate the fetcher cache directory based on that. We should calculate the fetcher cache directory in the caller and pass that directory to Fetcher.",Task,Major,Resolved,"2017-03-24 19:27:04","2017-03-24 19:27:04",3
"Apache Mesos","Implement a plugin to list container's on a given agent.","We need the CLI to support a `list` command to enumerate the containers running on a given agent. To achieve this we will need to implement a container plugin that will be implement this list method.",Task,Major,Resolved,"2017-03-22 16:09:18","2017-03-22 16:09:18",3
"Apache Mesos","Allow Mesos CLI to take masters IP","Allow the Mesos CLI to take master IPs. This will allow the CLI to send HTTP requests to one master in a cluster with one or multiple ones.",Task,Major,Resolved,"2017-03-22 16:06:24","2017-03-22 16:06:24",2
"Apache Mesos","Add ability to initialize a test cluster for Mesos CLI unit-test infrastructure","Similar to the Mesos unit-tests we need to have the ability to bring up a test cluster against which we can run the python unit-tests for Mesos CLI.",Task,Major,Resolved,"2017-03-22 15:44:07","2017-03-22 15:44:07",2
"Apache Mesos","Create a table abstraction for the Mesos CLI","Add a an abstraction for printing and formatting tables in the Mesos CLI. This will very useful for all commands that need to print their output in a table format.",Task,Major,Resolved,"2017-03-22 15:36:22","2017-03-22 15:36:22",2
"Apache Mesos","Unified containerizer provisions docker image error with COPY backend","Error occurs on some specific docker images with COPY backend, both 1.0.2 and 1.2.0. It works well with OVERLAY backend on 1.2.0.  {quote} I0321 09:36:07.308830 27613 paths.cpp:528] Trying to chown '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-S102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct:Transcoding_Test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7' to user 'root' I0321 09:36:07.319628 27613 slave.cpp:5703] Launching executor ct:Transcoding_Test_114489497_1490060156172:3 of framework 20151223-150303-2677017098-5050-30032-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-S102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct:Transcoding_Test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7' I0321 09:36:07.321436 27615 containerizer.cpp:781] Starting container '7e518538-7b56-4b14-a3c9-bee43c669bd7' for executor 'ct:Transcoding_Test_114489497_1490060156172:3' of framework '20151223-150303-2677017098-5050-30032-0000' I0321 09:36:37.902195 27600 provisioner.cpp:294] Provisioning image rootfs '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9' for container 7e518538-7b56-4b14-a3c9-bee43c669bd7 *E0321 09:36:58.707718 27606 slave.cpp:4000] Container '7e518538-7b56-4b14-a3c9-bee43c669bd7' for executor 'ct:Transcoding_Test_114489497_1490060156172:3' of framework 20151223-150303-2677017098-5050-30032-0000 failed to start: Collect failed: Failed to copy layer: cp: cannot create regular file ‘/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9/usr/bin/python’: Text file busy* I0321 09:36:58.707991 27608 containerizer.cpp:1622] Destroying container '7e518538-7b56-4b14-a3c9-bee43c669bd7' I0321 09:36:58.708468 27607 provisioner.cpp:434] Destroying container rootfs at '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9' for container 7e518538-7b56-4b14-a3c9-bee43c669bd7 {quote}  Docker image is a private one, so that i have to try to reproduce this bug with some sample Dockerfile as possible.",Bug,Critical,Resolved,"2017-03-22 02:12:03","2017-03-22 02:12:03",2
"Apache Mesos","General checker does not support command checks via agent.","Command checks via agent are necessary for executors, that launch their tasks via agent, e.g., default executor. General checker should support launching command as nested containers via agent in order to be used by such executors.",Improvement,Major,Resolved,"2017-03-21 12:51:44","2017-03-21 12:51:44",5
"Apache Mesos","General checker does not support pause / resume.","Consider a scenario, when an agent is unreachable from the executor. In this case, checks via agent cannot be performed, and result of local checks cannot be delivered. For simplicity and consistency, executors may want to pause checking in such scenarios.",Improvement,Major,Resolved,"2017-03-21 12:42:22","2017-03-21 12:42:22",1
"Apache Mesos","General checker does not support TCP checks.","The general checker introduced in MESOS-6906 does not support TCP checks. This is a disadvantage, because the health checker does support them.",Improvement,Major,Resolved,"2017-03-21 12:39:35","2017-03-21 12:39:35",3
"Apache Mesos","Health checker does not support pause / resume.","Consider a scenario, when an agent is unreachable from the executor. In this case, health checks via agent cannot be performed, and result of local health checks cannot be delivered. For simplicity and consistency, executors may want to pause health checking in such scenarios.",Improvement,Major,Resolved,"2017-03-21 12:33:01","2017-03-21 12:33:01",5
"Apache Mesos","Unified containerizer does not support docker registry version < 2.3.","in file `src/uri/fetchers/docker.cpp`  ```     Option<string> contentType = response.headers.get(Content-Type);           if (contentType.isSome() &&               !strings::startsWith(                   contentType.get(),                   application/vnd.docker.distribution.manifest.v1)) {             return Failure(                 Unsupported manifest MIME type:  + contentType.get());           }   ```  Docker fetcher check the contentType strictly, while docker registry with version < 2.3 returns manifests with contentType `application/json`, that leading failure like `E0321 13:27:27.572402 40370 slave.cpp:4650] Container 'xxx' for executor 'xxx' of framework xxx failed to start: Unsupported manifest MIME type: application/json; charset=utf-8`.",Bug,Blocker,Resolved,"2017-03-21 05:34:44","2017-03-21 05:34:44",2
"Apache Mesos","Java V1 Framwork Test failed on macOS","macOS's scheduler make ExamplesTest.JavaV1Framework terminate before the scheduler driver stopped, which results in an exception and a test failure. This failure is not seen in an Linux environment yet but there's a possibility that it would also happen.",Bug,Trivial,Resolved,"2017-03-21 00:27:37","2017-03-21 00:27:37",1
"Apache Mesos","Containerizer startup may cause sensitive data to leak into sandbox logs.","The task sandbox logging does show the callup for the containerizer launch with all of its flags. This is not safe when assuming that we may not want to leak sensitive data into the sandbox logging.  Example: ",Bug,Major,Resolved,"2017-03-20 03:13:54","2017-03-20 03:13:54",3
"Apache Mesos","Possibly duplicate environment variables should not leak values to the sandbox.","When looking into MESOS-7263, the following also came up.    Within the contents of `stdout`:   There seems no obvious need to warn the user as the value is identical.",Bug,Major,Resolved,"2017-03-20 02:49:06","2017-03-20 02:49:06",1
"Apache Mesos","User supplied task environment variables cause warnings in sandbox stdout.","The default executor causes task/command environment variables to get duplicated internally, causing warnings in the resulting sandbox {{stdout}}.    Result in {{stdout}} of the sandbox:  ",Bug,Major,Resolved,"2017-03-20 02:16:23","2017-03-20 02:16:23",3
"Apache Mesos","Provide scheduler calls to subscribe to additional roles and unsubscribe from roles.","The current support for schedulers to subscribe to additional roles or unsubscribe from some of their roles requires that the scheduler obtain a new subscription with the master which invalidates the event stream.    A more lightweight mechanism would be to provide calls for the scheduler to subscribe to additional roles or unsubscribe from some roles such that the existing event stream remains open and offers to the new roles arrive on the existing event stream. E.g.    SUBSCRIBE_TO_ROLE   UNSUBSCRIBE_FROM_ROLE    One open question pertains to the terminology here, whether we would want to avoid using subscribe in this context. An alternative would be:    UPDATE_FRAMEWORK_INFO    Which provides a generic mechanism for a framework to perform framework info updates without obtaining a new event stream.    In addition, it would be easier to use if it returned 200 on success and an error response if invalid, etc. Rather than returning 202.    *NOTE*: Not specific to this issue, but we need to figure out how to allow the framework to not leak reservations, e.g. MESOS-7651.",Improvement,Major,Resolved,"2017-03-16 19:03:36","2017-03-16 19:03:36",13
"Apache Mesos","Need to fix resource check in long-lived framework","The multi-role changes in Mesos changed the implementation of `Resources::contains`.  This results in the search for a given resource to be performed only for unallocated resources. For allocated resources the search is actually performed only for a given role.   Due to this change the resource check in both the long-lived framework are failing leading to these frameworks not launching any tasks.   The fix would be to unallocate all resources in a given offer and than do the `contains` check.",Bug,Major,Resolved,"2017-03-16 05:15:15","2017-03-16 05:15:15",2
"Apache Mesos","Support pulling images from AliCloud private registry.","The image puller via curl doesn't work when I'm specifying the image name as: registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75 400 BAD REQUEST  But the docker pulls it successfully  bq. docker pull registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75",Bug,Blocker,Resolved,"2017-03-16 03:32:03","2017-03-16 03:32:03",2
"Apache Mesos","Docker executor does not support general checks.","MESOS-6906 introduced general non-interpreting checks. Docker executor should support them.  For TCP checks, we should make sure that the docker executor has access to the agent's filesystem.",Improvement,Major,Accepted,"2017-03-15 21:38:55","2017-03-15 21:38:55",5
"Apache Mesos","Default executor does not support general checks.","MESOS-6906 introduced general non-interpreting checks. Default executor should start supporting them.",Improvement,Major,Resolved,"2017-03-15 21:36:09","2017-03-15 21:36:09",5
"Apache Mesos","Add documentation for AGENT_ADDED/AGENT_REMOVED events.","We need to add documentation to the existing Mesos Operator API docs for the newly added {{AGENT_ADDED}}/{{AGENT_REMOVED}} events. The protobuf definition for the events can be found here: https://github.com/apache/mesos/blob/master/include/mesos/v1/master/master.proto",Documentation,Minor,Resolved,"2017-03-14 21:27:44","2017-03-14 21:27:44",1
"Apache Mesos","Add support to auto-load /dev/nvidia-uvm in the GPU isolator","Loading /dev/nvidia-uvm (and installing a script to make sure it loads on reboot) is not technically part of the official Nvidia driver installation process. The rationale being that CUDA applications typically load this device on-demand if they need it. Unfortunately, it can't load it if mesos hasn't made it available to the container running the CUDA application though.  We should add support to have the mesos agent auto-load this device when running the GPU isolator.",Improvement,Major,Resolved,"2017-03-11 01:24:58","2017-03-11 01:24:58",2
"Apache Mesos","Introduce precompiled headers (on Windows)","Precompiled headers (PCHs) exist on both Windows and Linux. For Linux, you can refer to https://gcc.gnu.org/onlinedocs/gcc/Precompiled-Headers.html. Straight from the GNU CC documentation: The time the compiler takes to process these header files over and over again can account for nearly all of the time required to build the project.  PCHs are only being proposed for the CMake system.  In theory, we can introduce this change with only a few, non-intrusive code changes.  The feature will primarily be a CMake change.  See: https://github.com/sakra/cotire",Improvement,Major,Resolved,"2017-03-09 23:08:16","2017-03-09 23:08:16",5
"Apache Mesos","Tasks launched via the default executor cannot access disk resource volumes.","Currently, when a task in a task group tries to access a volume specified in disk resources (e.g., persistent volumes), it doesn't have access to them since they are mounted in the root container (executor). This happens due to there being no mechanism to specify resources for child containers yet. Hence, by default any resources (e.g., disk) are added to the root container.  A possible solution can be to set up the mapping manually by the default executor using the {{SANDBOX_PATH}} volume source type giving child containers access to the volume mounted in the parent container. This is at best a workaround and the ideal solution would be tackled as part of MESOS-7207.",Bug,Major,Resolved,"2017-03-09 17:37:31","2017-03-09 17:37:31",3
"Apache Mesos","'EXPECT_SOME' and other asserts don't work with 'Try's that have a custom error state.","MESOS-5110 introduced an additional template parameter for {{Try}} to support custom error types. Using these values with {{EXPECT_SOME}} doesn't work, i.e.    won't compile. The other assertions in {{stout/gtest.hpp}} are likely affected as well.",Bug,Minor,Resolved,"2017-03-08 11:29:35","2017-03-08 11:29:35",1
"Apache Mesos","HTTP health check doesn't work when mesos runs with --docker_mesos_image","When running mesos-slave with option docker_mesos_image like:   from the container that was started with option pid: host like:   and example marathon job, that use MESOS_HTTP checks like:   I see the errors like:   Looks like option docker_mesos_image makes, that newly started mesos job is not using pid host option same as mother container was started, but has his own PID namespace (so it doesn't matter if mother container was started with pid host or not it will never be able to find PID)",Bug,Critical,Resolved,"2017-03-06 08:04:38","2017-03-06 08:04:38",3
"Apache Mesos","Persistent volume ownership is set to root when task is running with non-root user","I’m running docker container in universal containerizer, mesos 1.1.0. switch_user=true, isolator=filesystem/linux,docker/runtime.  Container is launched with marathon, “user”:”someappuser”. I’d want to use persistent volume, but it’s exposed to container with root user permissions even if root folder is created with someppuser ownership (looks like mesos do chown to this folder).   here logs for my container:   ",Bug,Critical,Resolved,"2017-03-06 01:57:47","2017-03-06 01:57:47",3
"Apache Mesos","Add support for hierarchical roles to the local authorizer","We should update the local authorizer so that role values for role-based actions matching whole role subhierarchies are understood, e.g., given roles {{a/b/c}}, {{a/b/d}} and {{a/e}} it should be possible to specify a role value {{a/b/%}} matching actions on roles {{a/b/c}} and {{a/b/d}}, or a value {{a/%}} matching actions on all above roles.",Improvement,Major,Resolved,"2017-03-02 17:29:57","2017-03-02 17:29:57",5
"Apache Mesos","Requesting tiny amount of CPU crashes master.","If a task is submitted with a tiny CPU request e.g. 0.0004, then when it completes the master crashes due to a CHECK failure:   I can reproduce this with the following command:   If I replace 0.0004 with 0.001 the issue no longer occurs.",Bug,Critical,Resolved,"2017-03-02 09:01:53","2017-03-02 09:01:53",3
"Apache Mesos","Use of `GTEST_IS_THREADSAFE` in asserts is problematic.","Some test cases in libprocess use {{ASSERT_TRUE(GTEST_IS_THREADSAFE)}}. This is a misuse of that define, [the documentation in GTest says|https://github.com/google/googletest/blob/master/googletest/include/gtest/internal/gtest-port.h#L155-L163]:  Currently, the use of {{GTEST_IS_THREADSAFE}} works fine in the assert, because it is defined to be {{1}}. But newer upstream versions of GTest use a more complicated define, that can yield to be undefined, causing compilation errors.",Bug,Major,Resolved,"2017-03-01 16:24:42","2017-03-01 16:24:42",5
"Apache Mesos","Update endpoint handlers to use 'ObjectApprover'","The {{ObjectApprover}}-based interface for the authorizer has been introduced, but not all handlers make use of this new functionality (i.e., {{Slave::Http::flags()}}. We should consider migrating all authorization code to use {{getObjectApprover}}, and deprecating the older {{authorized()}} interface.",Improvement,Major,Open,"2017-02-28 18:58:09","2017-02-28 18:58:09",3
"Apache Mesos","Add nested container launch/wait/kill APIs to agent API docs.",,Documentation,Major,Resolved,"2017-02-28 01:34:47","2017-02-28 01:34:47",1
"Apache Mesos","Add documentation for Debug APIs to Operator API doc",,Bug,Major,Resolved,"2017-02-28 01:31:27","2017-02-28 01:31:27",1
"Apache Mesos","Agent should validate that the nested container ID does not exceed certain length.","This is related to MESOS-691.  Since nested container ID is generated by the executor, the agent should verify that the length of it does not exceed certain length.",Bug,Major,Resolved,"2017-02-24 21:24:40","2017-02-24 21:24:40",3
"Apache Mesos","Rename 'AuthenticationResult'","The naming of {{process::http::authentication::AuthenticationResult}} is redundant. It would be more concise to use {{process::http::authentication::Result}} instead.  Note that this is a breaking change for authentication modules, so it should be advertised prominently.",Improvement,Major,Resolved,"2017-02-23 22:46:14","2017-02-23 22:46:14",3
"Apache Mesos","Parsing of perf version segfaults","Parsing the perf version [fails with a segfault in ASF CI|https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-eu2)/3294/], ",Bug,Major,Resolved,"2017-02-22 21:19:16","2017-02-22 21:19:16",3
"Apache Mesos","Document provisioner auto backend support.","Document the provisioner auto backend semantic in container-image.md",Documentation,Major,Resolved,"2017-02-21 23:28:27","2017-02-21 23:28:27",2
"Apache Mesos","The new http::Headers abstraction may break some modules.","In the favor of the new http::Headers abstraction, the headers class was changed from a hashmap to a class. However, this change may potentially break some modules since functionalities like constructor using initializer or calling methods from undered_map. We should have the new class derived from the hashmap instead. ",Bug,Blocker,Resolved,"2017-02-21 22:43:28","2017-02-21 22:43:28",3
"Apache Mesos","The agent may be flapping after the machine reboots due to provisioner recover.","After the agent machine reboots, if the agent work dir survives (e.g., /var/lib/mesos) and the container runtime directory is gone (an empty SlaveState as well), the provisioner recover() would get into segfault because that case break the semantic that a child container should always be cleaned up before it parent container.  This is a particular case which only happens if the machine reboots and the provisioner directory survives.    The provisioner directory is supposed to be under the container runtime directory. However, this is not backward compatible. We can only change it after a deprecation cycle.  For now, we have to three options: 1. make provisioner::destroy() recursive. 2. sort the container during recovery to guarantee `child before parent` semantic. 3. remove the check-failure since the while provisioner dir will be removed eventually at the end (not recommended).  Recommend (1).",Bug,Blocker,Resolved,"2017-02-21 22:22:44","2017-02-21 22:22:44",5
"Apache Mesos","Wrap IOSwitchboard.connect() in a dispatch","Since the IOSwitchboard is implemented as a MesosIsolatorProcess, most of its API calls are automatically dispatched onto its underlying process by an Isolator wrapper. However, the IOSwitchboard also includes an additional connect() call which is not accessed through the Isolator wrapper. As such, we need to wrap it in a dispatch call manually.",Bug,Major,Resolved,"2017-02-18 18:36:29","2017-02-18 18:36:29",1
"Apache Mesos","ABORT checks its preconditions incorrectly and incompletely","Currently, stout's {{ABORT}} (which is mapped to {{_Abort}}) checks it precondition incompletely and incorrectly.  Its current control flow is roughly   We here check the precondition {{message != nullptr}} after we already have called {{strlen(message)}}; calling {{strlen}} on a {{nullptr}} already triggers undefined behavior.  Similarly, we never guard against a {{prefix}} which is {{nullptr}}, but unconditionally call {{strlen}} on it.  It seems it should be possible to assert that neither {{prefix}} nor {{message}} are {{nullptr}} before any use.  This was diagnosed by coverity as CID-1400833, and has been present in all releases since 0.23.0.",Bug,Minor,Open,"2017-02-18 09:22:33","2017-02-18 09:22:33",1
"Apache Mesos","Custom executors cannot use any reserved resources.","A custom executor or the built-in default executor cannot launch a task if they use reserved resources as part of {{ExecutorInfo}}. This mostly happens due to the fact that we don't unallocate the {{Resource}} when comparing it with the checkpointed resources on the agent:    The fix can be as simple as changing this to: ",Bug,Blocker,Resolved,"2017-02-16 21:47:51","2017-02-16 21:47:51",2
"Apache Mesos","Remove the default value for 'Environment.variables.type' field","Type information was added to the {{Environment}} protobuf message in Mesos 1.3, and the default type for the {{Environment.variables.type}} field was set to {{VALUE}} for backward compatibility. In Mesos 2.1, this default can be removed.  The related comment in {{validateEnvironment()}} within {{src/common/validation.cpp}} should also be removed.",Task,Major,Accepted,"2017-02-16 16:14:54","2017-02-16 16:14:54",1
"Apache Mesos","port_mapping isolator: executor hangs when running on EC2","Hi, I'm experiencing a weird issue: I'm using a CI to do testing on infrastructure automation. I recently activated the {{network/port_mapping}} isolator.  I'm able to make the changes work and pass the test for bare-metal servers and virtualbox VMs using this configuration.  But when I try on EC2 (on which my CI pipeline rely) it systematically fails to run any container.  It appears that the sandbox is created and the port_mapping isolator seems to be OK according to the logs in stdout and stderr and the {tc} output :   Then the executor never come back in REGISTERED state and hang indefinitely.  {GLOG_v=3} doesn't help here.  My skills in this area are limited, but trying to load the symbols and attach a gdb to the mesos-executor process, I'm able to print this stack:   I concluded that the underlying shell script launched by the isolator or the task itself is just .. blocked. But I don't understand why.  Here is a process tree to show that I've no task running but the executor is:   If someone has a clue about the issue I could experience on EC2, I would be interested to talk...",Bug,Major,Resolved,"2017-02-15 18:52:52","2017-02-15 18:52:52",2
"Apache Mesos","Replace monadic type get() functions with operator*","In MESOS-2757 we introduced {{T* operator->}} for {{Option}}, {{Future}} and {{Try}}. This provided a convenient short-hand for existing member functions {{T& get}} providing identical functionality.    To finalize the work of MESOS-2757 we should replace the existing {{T& get()}} member functions with functions {{T& operator*}}.    This is desirable as having both {{operator->}} and {{get}} in the code base at the same time lures developers into using the old-style {{get}} instead of {{operator->}} where it is not needed, e.g.,    instead of      We still require the functionality of {{get}} to directly access the contained value, but the current API unnecessarily conflates two (at least from a usage perspective) unrelated aspects; in these instances, we should use an {{operator*}} instead,      Using {{operator*}} in these instances makes it much less likely that users would use it in instances when they wanted to call functions of the wrapped value, i.e.,    appears more natural than          Note that this proposed change is in line with the interface of {{std::optional}}. Also, {{std::shared_ptr}}'s {{get}} is a useful function and implements an unrelated interface: it surfaces the wrapped pointer as opposed to its {{operator*}} which dereferences the wrapped pointer. Similarly, our current {{get}} also produce values, and are unrelated to {{std::shared_ptr}}'s {{get}}.",Improvement,Major,Resolved,"2017-02-14 09:09:17","2017-02-14 09:09:17",3
"Apache Mesos","Make IO Switchboard optional for debug containers","Starting a new IO switchboard for each debug container adds some overhead.  The functionality provided by the IO switchboard is not always necessary, so we should make the IO switchboard optional in order to improve the performance of launching nested containers.",Improvement,Major,Open,"2017-02-13 18:50:09","2017-02-13 18:50:09",5
"Apache Mesos","Test ContentTypeAndSSLConfig/SchedulerSSLTest.RunTaskAndTeardown/1 segfaults","{{ContentTypeAndSSLConfig/SchedulerSSLTest.RunTaskAndTeardown/1}} segfaulted in our internal CI:  ",Bug,Major,Accepted,"2017-02-10 07:00:16","2017-02-10 07:00:16",2
"Apache Mesos","Crash when sending a SIGUSR1 signal to the agent.","Looks like sending a {{SIGUSR1}} to the agent crashes it. This is a regression and used to work fine in the 1.1 release. Note that the agent does unregisters with the master and the crash happens after that.  Steps to reproduce: - Start the agent. - Send it a {{SIGUSR1}} signal.  The agent should crash with a stack trace similar to this: ",Bug,Critical,Resolved,"2017-02-10 00:35:44","2017-02-10 00:35:44",2
"Apache Mesos","Quota can be exceeded due to coarse-grained offer technique.","The current implementation of quota allocation allocates the entire available resources on an agent when trying to satisfy the quota. What this means is that quota can be exceeded by the size of an agent.    This is especially problematic for large machines, consider a 48 core, 512 GB memory server where a role is given 4 cores and 4GB of memory. Given our current approach, we will send an offer for the entire 48 cores and 512 GB of memory!    This ticket is to perform fine grained offers when the allocation will exceed the quota.",Bug,Critical,Resolved,"2017-02-09 23:31:17","2017-02-09 23:31:17",5
"Apache Mesos","Health checker duplicates a lot of checker's functionality.","With the introduction of a general check (MESOS-6906), health checker should leverage a general check plus add interpretation on top. This will avoid code duplication and increase maintainability.",Improvement,Major,Resolved,"2017-02-09 11:22:48","2017-02-09 11:22:48",5
"Apache Mesos","Support private registry credential per container.",,Epic,Major,Resolved,"2017-02-09 00:17:56","2017-02-09 00:17:56",13
"Apache Mesos","libprocess tests fail when using libevent 2.1.8","Running {{libprocess-tests}} on Mesos compiled with {{--enable-libevent --enable-ssl}} on an operating system using libevent 2.1.8, SSL related tests fail like      Tests failing are    ",Bug,Critical,Resolved,"2017-02-07 15:30:28","2017-02-07 15:30:28",8
"Apache Mesos","The linux filesystem isolator should set mode and ownership for host volumes.","If the host path is a relative path, the linux filesystem isolator should set the mode and ownership for this host volume since it allows non-root user to write to the volume. Note that this is the case of sharing the host fileysystem (without rootfs).",Bug,Major,Resolved,"2017-02-06 20:14:52","2017-02-06 20:14:52",2
"Apache Mesos","Add OnTerminationPolicy handling to the default executor.","We should support handling {{OnTerminationPolicy}} specified in {{TaskInfo}} to the default executor. Currently, the default policy for the default executor is to kill the entire task group when a task in the task group fails. This would allow framework developers to specify a custom policy e.g., keep the executor still alive when a back up task in the task group fails etc.",Task,Major,Reviewable,"2017-02-06 19:23:53","2017-02-06 19:23:53",3
"Apache Mesos","Add the `OnTerminationPolicy` to the TaskInfo protobuf.","As outlined in the [design doc | https://docs.google.com/document/d/1VxfoZ-DzMHnKY0gzoccHEhx1rvdC2-RATJfJUfiAwGY/edit?usp=sharing] , we need to introduce the {{OnTerminationPolicy}} to the {{TaskInfo}} protobuf allowing every task to specify what would an executor do upon task termination.   Note that this issue won't introduce the {{RestartPolicy}} message and those would be added via a separate issue.",Task,Major,Reviewable,"2017-02-06 19:19:52","2017-02-06 19:19:52",3
"Apache Mesos","Consider using the relink functionality of libprocess in the executor driver.","As outlined in the root cause analysis for MESOS-5332, it is possible for a iptables firewall to terminate an idle connection after a timeout. (the default is 5 days). Once this happens, the executor driver is not notified of the disconnection. It keeps on thinking that it is still connected with the agent.  When the agent process is restarted, the executor still tries to re-use the old broken connection to send the re-register message to the agent. This is when it eventually realizes that the connection is broken (due to the nature of TCP) and calls the {{exited}} callback and commits suicide in 15 minutes upon the recovery timeout.  To offset this, an executor should always {{relink}} when it receives a reconnect request from the agent.",Bug,Major,Resolved,"2017-02-03 18:55:51","2017-02-03 18:55:51",2
"Apache Mesos","Consider updating build system to allow executor authentication without SSL","Since the executor JWT module will use OpenSSL for its HMAC SHA256 routine, the simplest thing to do for the MVP is only allow executor authentication when Mesos is build with libevent/SSL enabled.  Later, we could update the build system to allow Mesos to use executor authentication when built without libevent/SSL.",Task,Major,Accepted,"2017-02-02 19:50:01","2017-02-02 19:50:01",3
"Apache Mesos","Introduce a new http::Headers abstraction.","Introduce a new http::Headers abstraction to replace the previous hashmap 'Headers'. The benefit is that it can be embedded with other header classes (e.g., WWW-Authenticate) to parse a header content, as well as doing validation.",Improvement,Major,Resolved,"2017-02-02 00:53:49","2017-02-02 00:53:49",2
"Apache Mesos","IOSwitchboard FDs leaked when containerizer launch fails -- leads to deadlock","If the containizer launch path fails before actually launching the container, the FDs allocated to the container by the IOSwitchboard isolator are leaked. This leads to deadlock in the destroy path because the IOSwitchboard does not shutdown until the FDs it allocates to the container have been closed. Since the switchboard doesn't shutdown, the future returned by its 'cleanup()' function is never satisfied.   We need a general purpose method for closing the IOSwitchboard FDs when failing in the launch path.",Bug,Critical,Resolved,"2017-02-02 00:00:08","2017-02-02 00:00:08",2
"Apache Mesos","Update agent for hierarchical roles.","Agents use the role name in the file system path for persistent volumes: a persistent volume is written to {{work_dir/volumes/roles/<role-name>/<persistence-id>}}. When using hierarchical roles, {{role-name}} might contain slashes. It seems like there are three options here:  # When converting the role name into the file system path, escape any slashes that appear. # Hash the role name before using it in the file system path. # Create a directory hierarchy that corresponds to the nesting in the role name. So a volume for role {{a/b/c/d}} would be stored in {{roles/a/b/c/d/<persistence-id>}}.  If we adopt #3, we'd probably also want to cleanup the filesystem when a volume is removed.",Task,Major,Resolved,"2017-02-01 18:00:44","2017-02-01 18:00:44",2
"Apache Mesos","Send SIGKILL after SIGTERM to IOSwitchboard after container termination.","This is follow up for MESOS-6664",Bug,Major,Resolved,"2017-01-31 20:04:58","2017-01-31 20:04:58",2
"Apache Mesos","NetSocketTest.EOFBeforeRecv is flaky.","This was observed on ASF CI:  ",Bug,Major,Resolved,"2017-01-28 16:21:07","2017-01-28 16:21:07",5
"Apache Mesos","CommandExecutor ENV overwritten by Docker Image ENV in Unified Containerizer","Using the unified containerizer, if a docker image is provisioned and has environment variables set via the ENV directive, those environment variables will be inherited by the {{mesos-executor}} process and overwrite similarly named environment variables that otherwise would have been inherited from the agent.  This causes problems (for example) in DC/OS when trying to launch tasks based off the {{nvidia/cuda}} image. The {{nvidia/cuda}} image explicitly sets {{LD_LIBRARY_PATH}} to its own value so that the proper nvidia libraries will be available to whatever command is launched inside the container.  However, DC/OS relies on {{LD_LIBRARY_PATH}} to contain a path to {{/opt/<USER>lib}} so that all of the <USER>libraries are available to the mesos binaries launched by the agent ({{mesos-containerizer}}, {{mesos-execute}}, etc.). This is necessary to make sure that any external dependencies they might have (e.g. libssl.so) can be resolved at runtime. By overwriting the executor's environment with the Docker Image environment, {{LD_LIBRARY_PATH}} will not be set properly and {{mesos-execute}} will fail.  It seems to me, the Docker Image environment should *only* actually overwrite the environment of the user process (not its executor). However, this can get complicated, because the executor actually is the user process in the case of launching a custom executor.  We need to rethink how the environment is inherited/overwritten through all the various processes that get spawned while launching a container as well as how to make it work for tasks launched by arbitrary executors.",Bug,Critical,Resolved,"2017-01-28 02:52:21","2017-01-28 02:52:21",5
"Apache Mesos","Update authorization / authorization-filtering to handle hierarchical roles.","Authorization and endpoint filtering will need to be updated in order to allow the authorization to be performed in a hierarchical manner (e.g. a user can see all beneath /eng/* vs. a user can see all beneath /eng/frontend/*).",Task,Blocker,Resolved,"2017-01-27 23:42:19","2017-01-27 23:42:19",5
"Apache Mesos","Update framework authorization to support multiple roles","Currently the master assumes that a framework is only in a single role, see {{Master::authorizeFramework}}. This code should be updated to support frameworks with multiple roles. In particular it should get authorization of the framework's principal to register in each of the framework's roles.",Bug,Major,Resolved,"2017-01-27 20:50:53","2017-01-27 20:50:53",3
"Apache Mesos","Add implicit executor authorization to local authorizer","The local authorizer should be updated to perform implicit authorization of executor actions. When executors authenticate using a default executor secret, the authorizer will receive an authorization {{Subject}} which contains claims, but no principal. In this case, implicit authorization should be performed. Implicit authorization rules should enforce that an executor can perform actions on itself; i.e., subscribe as itself, send messages as itself, launch nested containers within itself.",Task,Major,Resolved,"2017-01-26 17:48:53","2017-01-26 17:48:53",3
"Apache Mesos","Update the authorizer interface for executor authentication","The authorizer interface must be updated to accommodate changes introduced by the implementation of executor authentication: * The {{authorization::Subject}} message must be extended to include the {{claims}} from a {{Principal}} * The local authorizer must be updated to accommodate this interface change",Task,Major,Resolved,"2017-01-26 17:44:07","2017-01-26 17:44:07",2
"Apache Mesos","Add authorization actions for V1 executor calls","Authorization actions should be added for the V1 executor calls: * Subscribe * Update * Message",Task,Major,Resolved,"2017-01-26 17:34:05","2017-01-26 17:34:05",2
"Apache Mesos","Add an '--executor_secret_key' flag to the agent","A new {{\-\-executor_secret_key}} flag should be added to the agent to allow the operator to specify a secret file to be loaded into the default executor JWT authenticator and SecretGenerator modules. This secret will be used to generate default executor secrets when {{\-\-generate_executor_secrets}} is set, and will be used to verify those secrets when {{\-\-authenticate_http_executors}} is set.",Task,Major,Resolved,"2017-01-26 17:30:01","2017-01-26 17:30:01",1
"Apache Mesos","Add the HttpAuthenticatee module interface","A new {{HttpAuthenticatee}} module interface should be added to permit the default executor to authenticate with the agent.",Task,Major,Accepted,"2017-01-26 17:24:12","2017-01-26 17:24:12",2
"Apache Mesos","Add a 'secret' field to the 'Environment' message","A new field of type {{Secret}} should be added to the {{Environment}} message to enable the inclusion of secrets in executor and task environments.",Task,Major,Resolved,"2017-01-26 17:07:50","2017-01-26 17:07:50",1
"Apache Mesos","Quota not recovered from registry in empty cluster.","When a quota was set and the master is restarted, removal of the quota reliably leads to a {{CHECK}} failure for me.  Start a master:   Set a quota. This creates an implicit role.   Restart the master process using the same {{work_dir}} and attempt to delete the quota after the master is started. The {{DELETE}} succeeds with an {{OK}}.   After handling the request, the master hits a {{CHECK}} failure and is aborted. ",Bug,Critical,Resolved,"2017-01-26 12:40:50","2017-01-26 12:40:50",3
"Apache Mesos","filesystem/shared and --default_container_info broken since 1.1","I face this issue, that prevent me to upgrade to 1.1.0 (and the change was consequently introduced in this version):  I'm using default_container_info to mount a /tmp volume in the container's mount namespace from its current sandbox, meaning that each container have a dedicated /tmp, thanks to the {{filesystem/shared}} isolator.  I noticed through our automation pipeline that integration tests were failing and found that this is because /tmp (the one from the host!) contents is trashed each time a container is created.  Here is my setup:  * {{--isolation='cgroups/cpu,cgroups/mem,namespaces/pid,*disk/du,filesystem/shared,filesystem/linux*,docker/runtime'}} * {{--default_container_info='\{type:MESOS,volumes:\[\{host_path:tmp,container_path:/tmp,mode:RW\}\]\}'}}  I discovered this issue in the early days of 1.1 (end of Nov, spoke with someone on Slack), but had unfortunately no time to dig into the symptoms a bit more.  I found nothing interesting even using GLOGv=3.  Maybe it's a bad usage of isolators that trigger this issue ? If it's the case, then at least a documentation update should be done.  Let me know if more information is needed.",Bug,Major,Resolved,"2017-01-26 11:33:11","2017-01-26 11:33:11",3
"Apache Mesos","Add executor authentication documentation","Documentation should be added regarding executor authentication. This will include updating: 1) the configuration docs to include new agent flags 2) the authentication documentation 3) the authorization documentation 4) the upgrade documentation 5) the CHANGELOG",Documentation,Major,Resolved,"2017-01-26 08:21:46","2017-01-26 08:21:46",3
"Apache Mesos","Enable multiple HTTP authenticator modules","To accommodate executor authentication, we will add support for the loading of multiple authenticator modules. The {{--http_authenticators}} flag is already set up for this, but we must relax the constraint in Mesos which enforces just a single authenticator.  In order to load multiple authenticators for a realm, a new Mesos-level authenticator, the {{CombinedAuthenticator}}, will be added. This class will call multiple authenticators and combine their results if necessary.",Task,Major,Resolved,"2017-01-26 08:19:51","2017-01-26 08:19:51",5
"Apache Mesos","Introduce a 'Principal' type","We will introduce a new type to represent the identity of an authenticated entity in Mesos: the {{Principal}}. To accomplish this, the following should be done: * Add the new {{Principal}} type * Update the {{AuthenticationResult}} to use {{Principal}} * Update all authenticated endpoint handlers to handle this new type * Update the default authenticator modules to use the new type",Task,Major,Resolved,"2017-01-26 07:55:40","2017-01-26 07:55:40",5
"Apache Mesos","Implement a JWT HTTP authenticatee module","An implementation of the new {{HttpAuthenticatee}} interface should be added for executors to use when authenticating with the default JSON web token (JWT) authenticator module. This module will be loaded into the default executor by default when HTTP executor authentication is enabled.",Task,Major,Accepted,"2017-01-26 07:53:52","2017-01-26 07:53:52",3
"Apache Mesos","Implement a JWT authenticator","A JSON web token (JWT) authenticator module should be added to authenticate executors which use default credentials generated by the agent. This module will be loaded as an HTTP authenticator by default when {{--authenticate_http_executors}} is set, unless HTTP authenticators are specified explicitly.",Task,Major,Resolved,"2017-01-26 07:51:15","2017-01-26 07:51:15",5
"Apache Mesos","Implement a JWT SecretGenerator","The default {{SecretGenerator}} for the generation of default executor credentials will be a module which generates JSON web tokens. This module will be loaded by default when executor secret generation is enabled.",Task,Major,Resolved,"2017-01-26 07:47:06","2017-01-26 07:47:06",5
"Apache Mesos","Add agent support for generating and passing executor secrets","The agent must generate and pass executor secrets to all executors using the V1 API. For MVP, the agent will have this behavior by default when compiled with SSL support. To accomplish this, the agent must: * load the default {{SecretGenerator}} module * call the secret generator when launching an executor * pass the generated secret into the executor's environment",Task,Major,Resolved,"2017-01-26 07:44:15","2017-01-26 07:44:15",5
"Apache Mesos","Add authentication support to agent's '/v1/executor' endpoint","The new agent flag {{--authenticate_http_executors}} must be added. When set, it will require that requests received on the {{/v1/executor}} endpoint be authenticated, and the default JWT authenticator will be loaded. Note that this will require the addition of a new authentication realm for that endpoint.",Task,Major,Resolved,"2017-01-26 07:38:23","2017-01-26 07:38:23",5
"Apache Mesos","Add the SecretGenerator module interface","A new {{SecretGenerator}} module interface will be added to permit the agent to generate default executor credentials.",Task,Major,Resolved,"2017-01-26 07:31:04","2017-01-26 07:31:04",2
"Apache Mesos","Add a 'Secret' protobuf message","A {{Secret}} protobuf message should be added to serve as a generic message for sending credentials and other secrets throughout Mesos.",Task,Major,Resolved,"2017-01-26 07:29:24","2017-01-26 07:29:24",2
"Apache Mesos","Change `Environment.Variable.Value` from required to optional","To prepare for future work which will enable the modular fetching of secrets, we should change the {{Environment.Variable.Value}} field from {{required}} to {{optional}}. This way, the field can be left empty and filled in by a secret fetching module.",Bug,Major,Resolved,"2017-01-25 20:07:36","2017-01-25 20:07:36",2
"Apache Mesos","Docker executor segfaults in ~MesosExecutorDriver()","With the current Mesos master state (commit 42e515bc5c175a318e914d34473016feda4db6ff), the Docker executor segfaults during shutdown.   Steps to reproduce:  1) Start master:  (note that building it at 13:37 is not part of the repro)  2) Start agent:   3) Run {{mesos-execute}} with the Docker containerizer:   Relevant agent output that shows the executor segfault:   The complete task stderr:     ",Bug,Blocker,Resolved,"2017-01-25 18:59:28","2017-01-25 18:59:28",1
"Apache Mesos","Fix BOOST random generator initialization on Windows","seed_rng::seed_rng does not produced the expected result in Windows since is using `/dev/urandom` file.    0:005> k  # Child-SP          RetAddr           Call Site 00 00000049`22dfc108 00007ff6`5193822f kernel32!CreateFileW ... 0e 00000049`22dfc660 00007ff6`502228fd mesos_agent!boost::uuids::detail::seed_rng::seed_rng+0x3d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 80] 0f 00000049`22dfc690 00007ff6`502591e3 mesos_agent!boost::uuids::detail::seed<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0x4d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 246] 10 00000049`22dfc790 00007ff6`50395518 mesos_agent!boost::uuids::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0xd3 [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\random_generator.hpp @ 50] 11 00000049`22dfc800 00007ff6`500ad140 mesos_agent!id::UUID::random+0x78 [d:\repositories\mesoswin\3rdparty\stout\include\stout\uuid.hpp @ 49] 12 00000049`22dfc870 00007ff6`5007ff55 mesos_agent!mesos::internal::slave::Framework::launchExecutor+0x70 [d:\repositories\mesoswin\src\slave\slave.cpp @ 6301] 13 00000049`22dfd520 00007ff6`502a0a35 mesos_agent!mesos::internal::slave::Slave::_run+0x2455 [d:\repositories\mesoswin\src\slave\slave.cpp @ 1990] ... 0:005> du @rcx 000001d7`cc55fb60  /dev/urandom ",Bug,Major,Resolved,"2017-01-23 07:21:28","2017-01-23 07:21:28",1
"Apache Mesos","Executors don't use glog for logging.","Built-in Mesos executors use {{cout}}/{{cerr}} for logging. This is not only inconsistent with the rest of the codebase, it also complicates debugging, since, e.g., a stack trace is not printed on an abort. Having timestamps will be also a huge plus.  Consider migrating logging in all built-in executors to glog.  There have been reported issues related to glog internal state races when a process that has glog initialized {{fork-exec}}s another process that also initialize glog. We should investigate how this issue is related to this ticket, cc [~<USER>, [~<USER>, [~<USER>.",Bug,Major,Resolved,"2017-01-20 14:00:15","2017-01-20 14:00:15",1
"Apache Mesos","Separate the mesos-containerizer binary into a static binary, which only depends on stout","The {{mesos-containerizer}} binary currently has [three commands|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/main.cpp#L46-L48]:  * [MesosContainerizerLaunch|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/launch.cpp] * [MesosContainerizerMount|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/mount.cpp] * [NetworkCniIsolatorSetup|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L1776-L1997]  These commands are all heavily dependent on stout, and have no need to be linked to libprocess.  In fact, adding an erroneous call to {{process::initialize}} (either explicitly, or by accidentally using a libprocess method) will break {{mesos-containerizer}} can cause several Mesos containerizer tests to fail.  (The tasks fail to launch, saying {{Failed to synchronize with agent (it's probably exited)}}).  Because this binary only depends on stout, we can separate it from the other source files and make this a static binary. ",Task,Major,Resolved,"2017-01-20 00:47:43","2017-01-20 00:47:43",3
"Apache Mesos","Support linux filesystem type detection.","We should support detecting a linux filesystem type (e.g., xfs, extfs) and its filesystem id mapping.",Task,Critical,Resolved,"2017-01-19 23:21:52","2017-01-19 23:21:52",5
"Apache Mesos","Docker containerizer: mangled environment when env value contains LF byte.","Consider this Marathon app definition:    The JSON-encoded newline in the value of the {{TESTVAR}} environment variable leads to a corrupted task environment. What follows is a subset of the resulting task environment (as printed via {{env}}, i.e. in key=value notation):    That is, the trailing part of the intended value ended up being interpreted as variable name, and only the leading part of the intended value was used as actual value for {{TESTVAR}}.  Common application scenarios that would badly break with that involve pretty-printed JSON documents or YAML documents passed along via the environment.  Following the code and information flow led to the conclusion that Docker's {{--env-file}} command line interface is the weak point in the flow. It is currently used in Mesos' Docker containerizer for passing the environment to the container:    (Ref: [code|https://github.com/apache/mesos/blob/c0aee8cc10b1d1f4b2db5ff12b771372fdd5b1f3/src/docker/docker.cpp#L584])   Docker's {{--env-file}} argument behavior is documented via  {quote} The --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, {quote} (Ref: https://docs.docker.com/engine/reference/commandline/run/)  That is, Docker identifies individual environment variable key/value pair definitions based on newline bytes in that file which explains the observed environment variable value fragmentation. Notably, Docker does not provide a mechanism for escaping newline bytes in the values specified in this environment file.  I think it is important to understand that Docker's {{--env-file}} mechanism is ill-posed in the sense that it is not capable of transmitting the whole range of environment variable values allowed by POSIX. That's what the Single UNIX Specification, Version 3 has to say about environment variable values:  {quote} the value shall be composed of characters from the portable character set (except NUL and as indicated below).  {quote} (Ref: http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap08.html)  About The portable character set: http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap06.html#tagtcjh_3  It includes (among others) the LF byte. Understandably, the current Docker {{--env-file}} behavior will not change, so this is not an issue that can be deferred to Docker: https://github.com/docker/docker/issues/12997  Notably, the {{--env-file}} method for communicating environment variables to Docker containers was just recently introduced to Mesos as of https://issues.apache.org/jira/browse/MESOS-6566, for not leaking secrets through the process listing. Previously, we specified env key/value pairs on the command line which leaked secrets to the process list and probably also did not support the full range of valid environment variable values.  We need a solution that 1) does not leak sensitive values (i.e. is compliant with MESOS-6566). 2) allows for passing arbitrary environment variable values.  It seems that Docker's {{--env}} method can be used for that. It can be used to define _just the names of the environment variables_ to-be-passed-along, in which case the docker binary will read the corresponding values from its own environment, which we can clearly prepare appropriately when we invoke the corresponding child process. This method would still leak environment variable _names_ to the process listing, but (especially if documented) this should be fine.",Bug,Major,Resolved,"2017-01-19 01:55:56","2017-01-19 01:55:56",3
"Apache Mesos","Launching two tasks with the same Docker image simultaneously may cause a staging dir never cleaned up","If user launches two tasks with the same Docker image simultaneously (e.g., run {{mesos-executor}} twice with the same Docker image), there will be a staging directory which is for the second task never cleaned up, like this: ",Bug,Critical,Resolved,"2017-01-19 00:13:59","2017-01-19 00:13:59",2
"Apache Mesos","SchedulerTest.MasterFailover is flaky","This was observed in a CentOS 7 VM, with libevent and SSL enabled:      Find attached the entire log from a failed run.",Bug,Major,Resolved,"2017-01-19 00:08:40","2017-01-19 00:08:40",2
"Apache Mesos","Libprocess reinitialization is flaky, can segfault.","This was observed on ASF CI. Based on the placement of the stacktrace, the segfault seems to occur during libprocess reinitialization, when {{process::initialize}} is called: ",Bug,Major,Resolved,"2017-01-17 23:16:45","2017-01-17 23:16:45",3
"Apache Mesos","Add support for media types needed for streaming request/responses.","As per the design document created as part of MESOS-3601, we need to add support for the additional media types proposed to our API handlers for supporting request streaming. These headers would also be used by the server in the future for streaming responses.  The following media types needed to be added:  {{Message-Accept}}: Enables the client to perform content negotiation for the contents of the stream. The supported values for this header would be {{application/json}} and {{application/x-protobuf}}. {{Message-Content-Type}}: The content type of the RecordIO stream sent by the server. The supported values for this header would be {{application/json}} and {{application/x-protobuf}}.  The {{Content-Type}} for the response would be {{application/recordio}}. For more details/examples see the alternate proposal section of the design doc:  https://docs.google.com/document/d/1OV1D5uUmWNvTaX3qEO9fZGo4FRlCSqrx0IHq5GuLAk8/edit#",Improvement,Blocker,Resolved,"2017-01-17 17:23:20","2017-01-17 17:23:20",5
"Apache Mesos","Support pulling Docker images with V2 Schema 2 image manifest","MESOS-3505 added support for pulling Docker images by their digest to the Mesos Containerizer provisioner. However currently it only works with images that were pushed with Docker 1.9 and older or with Registry 2.2.1 and older. Newer versions use Schema 2 manifests by default. Because of CAS constraints the registry does not convert those manifests on-the-fly to Schema 1 when they are being pulled by digest.    Compatibility details are documented here: https://docs.docker.com/registry/compatibility/  Image Manifest V2, Schema 2 is documented here: https://docs.docker.com/registry/spec/manifest-v2-2/",Improvement,Major,Resolved,"2017-01-17 16:46:20","2017-01-17 16:46:20",8
"Apache Mesos","Libprocess reinit code leaks SSL server socket FD","After [this commit|https://github.com/apache/mesos/commit/789e9f7], it was discovered that tests which use {{process::reinitialize}} to switch between SSL and non-SSL modes will leak the file descriptor associated with the server socket {{\_\_s\_\_}}. This can be reproduced by running the following trivial test in repetition:  ",Bug,Major,Accepted,"2017-01-13 16:01:40","2017-01-13 16:01:40",2
"Apache Mesos","Improve health checks validation.","The general  fields should also be validated (i.e., `timeout_seconds`), similar to what's done in https://reviews.apache.org/r/55458/",Bug,Major,Resolved,"2017-01-13 12:30:48","2017-01-13 12:30:48",2
"Apache Mesos","SlaveRecoveryTest/0.RegisterDisconnectedSlave test is flaky","Observed this on ASF CI:  ",Bug,Major,Resolved,"2017-01-11 20:23:09","2017-01-11 20:23:09",2
"Apache Mesos","Zero health check timeout is interpreted literally.","Currently zero health check timeout is interpreted literally, which is not very helpful since a health check does not even get a chance to finish. We suggest to fixe this behaviour by interpreting zero as {{Duration::max()}} effectively rendering the timeout infinite.",Bug,Minor,Resolved,"2017-01-11 14:33:45","2017-01-11 14:33:45",1
"Apache Mesos","Introduce a general non-interpreting task check.","In addition to result-interpreting, killing health check, there is a requirement from Mesos framework authors for a general check that can execute an arbitrary command or send an HTTP request and pass the result to the scheduler without interpreting it.  This ticket aims to implement this functionality by introducing a new class of a check in Mesos. Design doc: https://docs.google.com/document/d/1VLdaH7i7UDT3_38aOlzTOtH7lwH-laB8dCwNzte0DkU",Improvement,Major,Resolved,"2017-01-11 11:40:33","2017-01-11 11:40:33",8
"Apache Mesos","Task status updates caused by task health update do not set appropriate reason.","When task's health changes or a health check fails, a task status update is sent with the {{healthy}} field set. However, there is no clear indication about the reason (health state updated) of the task status update.",Improvement,Major,Resolved,"2017-01-11 11:31:31","2017-01-11 11:31:31",3
"Apache Mesos","Add test for framework upgrading to multi-role capability.","Frameworks can upgrade to multi-role capability as long as the framework's role remains the same.  We consider the framework roles unchanged if  * a framework previously didn't specify a {{role}} now has {{roles=()}}, or * a framework which previously had {{role=A}} and now has {{roles=(A)}}.",Bug,Major,Resolved,"2017-01-10 14:41:13","2017-01-10 14:41:13",2
"Apache Mesos","Checkpoint 'ContainerConfig' in Mesos Containerizer.","This information can be used ford image GC in Mesos Containerizer, as well as other purposes.",Task,Major,Resolved,"2017-01-08 22:18:39","2017-01-08 22:18:39",5
"Apache Mesos","Reconsider process creation primitives on Windows","Windows does not have the same notions of process hierarchies as Unix, and so killing groups of processes requires us to make sure all processes are contained in a job object, which acts something like a cgroup. This is particularly important when we decide to kill a task, as there is no way to reliably do this unless all the processes you'd like to kill are in the job object.  This causes us a number of issues; it is a big reason we needed to fork the command executor, and it is the reason tasks are currently unkillable in the default executor.  As we clean this issue up, we need to think carefully about the process governance semantics of Mesos, and how we can map them to a reliable, simple Windows implementation.",Bug,Major,Resolved,"2017-01-07 23:27:52","2017-01-07 23:27:52",5
"Apache Mesos","Add authorization tests for debug API handlers","Should test authz of all 3 debug calls.",Task,Major,Resolved,"2017-01-07 02:15:58","2017-01-07 02:15:58",3
"Apache Mesos","Agent silently ignores FS isolation when protobuf is malformed","cc [~<USER>    I accidentally set my Mesos ContainerInfo to include a DockerInfo instead of a MesosInfo:        I would have expected a validation error before or during containerization, but instead, the agent silently decided to ignore filesystem isolation altogether, and launch my executor on the host filesystem. ",Bug,Minor,Resolved,"2017-01-06 20:08:06","2017-01-06 20:08:06",5
"Apache Mesos","Transition Windows away from `os::killtree`.","Windows does not have as robust a notion of a process hierarchy as Unix, and thus functions like `os::killtree` will always have critical limitations and semantic mismatches between Unix and Windows.  We should transition away from this function when we can, and replace it with something similar to how we kill a cgroup.",Bug,Major,Resolved,"2017-01-05 23:13:03","2017-01-05 23:13:03",3
"Apache Mesos","Container Exec should be possible with tasks belonging to a task group","{{LaunchNestedContainerSession}} currently requires the parent container to be an Executor (https://github.com/apache/mesos/blob/f89f28724f5837ff414dc6cc84e1afb63f3306e5/src/slave/http.cpp#L2189-L2211).  This works for command tasks, because the task container id is the same as the executor container id.  But it won't work for pod tasks whose container id is different from executor’s container id.  In order to resolve this ticket, we need to allow launching a child container at an arbitrary level.",Bug,Blocker,Resolved,"2017-01-05 18:40:19","2017-01-05 18:40:19",5
"Apache Mesos","Some tests use CHECK instead of ASSERT","Some tests check preconditions with {{CHECK}} instead of e.g., {{ASSERT_TRUE}}. When such a check fails it leads to a undesirable complete abort of the test run, potentially dumping core. We should make sure tests check preconditions in a proper way, e.g., with {{ASSERT_TRUE}}.",Bug,Major,Resolved,"2017-01-05 17:40:50","2017-01-05 17:40:50",5
"Apache Mesos","Tests for quota capacity heuristic.","We need more tests to ensure capacity heuristic works as expected.",Task,Major,Resolved,"2016-12-27 11:23:41","2016-12-27 11:23:41",5
"Apache Mesos","FaultToleranceTest.FrameworkReregister is flaky","Observed on internal CI:    Looks like another instance of MESOS-4695.",Bug,Major,Resolved,"2016-12-22 22:30:34","2016-12-22 22:30:34",3
"Apache Mesos","consecutive_failures 0 == 1 in HealthCheck.","When defining a HealthCheck with consecutive_failures=0 one would expect Mesos to never kill the task and only notify about the failure.  What seems to happen instead is Mesos handles consecutive_failures=0 as consecutive_failures=1 and kills the task after 1 failure.  Since 0 isn't the same as 1 this seems to be a bug and results in unexpected behaviour. ",Bug,Major,Open,"2016-12-22 14:44:07","2016-12-22 14:44:07",3
"Apache Mesos","OsTest.User fails on recent Arch Linux.","  Appeared relatively recently (last two weeks). Cause appears to be that {{getpwnam\_r}} now returns {{EINVAL}} for an invalid input, which {{os::getuid()}} and {{os::getgid()}} are not prepared to handle.",Bug,Major,Resolved,"2016-12-21 17:09:19","2016-12-21 17:09:19",3
"Apache Mesos","mesos-this-capture clang-tidy check has false positives","The {{mesos-this-capture}} clang-tidy checks incorrectly triggers on the code here,    https://github.com/apache/mesos/blob/d2117362349ab4c383045720f77d42b2d9fd6871/src/slave/containerizer/mesos/io/switchboard.cpp#L1487  We should tighten the matcher to avoid triggering on such constructs.",Bug,Major,Resolved,"2016-12-21 15:56:45","2016-12-21 15:56:45",2
"Apache Mesos","CNI reports confusing error message for failed interface setup.","Saw this today:    which is produced by this code: https://github.com/apache/mesos/blob/1e72605e9892eb4e518442ab9c1fe2a1a1696748/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L1854-L1859    Note that ssh'ing into the machine confirmed that {{ifconfig}} is available in {{PATH}}.    Full log: http://pastebin.com/hVdNz6yk",Bug,Major,Resolved,"2016-12-20 16:01:12","2016-12-20 16:01:12",2
"Apache Mesos","FaultToleranceTest.FrameworkReregister is flaky.","I just saw {{FaultToleranceTest.FrameworkReregister}} fail in internal CI on a Debian 8 system. Running the test in repetition on my OS X machine I was able to reproduce the issue on OS X as well.  ",Bug,Major,Resolved,"2016-12-20 09:35:37","2016-12-20 09:35:37",1
"Apache Mesos","Enable glog stack traces when we call things like `ABORT` on Windows","Currently in the Windows builds, if we call `ABORT` (etc.) we will simply bail out, with no stack traces.  This is highly undesirable. Stack traces are important for operating clusters in production. We should work to enable this behavior, including possibly working with glog to add this support if they currently they do not natively support it.",Bug,Critical,Resolved,"2016-12-19 19:32:52","2016-12-19 19:32:52",5
"Apache Mesos","IOSwitchboardServerTest.SendHeartbeat and IOSwitchboardServerTest.ReceiveHeartbeat broken on OS X","The tests IOSwitchboardServerTest.SendHeartbeat and IOSwitchboardServerTest.ReceiveHeartbeat are broken on OS X.   The issue is caused by the way the socket paths are constructed in the tests,   The lengths of the components are  * sandbox path: 55 characters (including directory delimiters), * {{mesos-io-switchboard}}: 20 characters, * UUID: 36 characters  which amounts to a total of 113 non-zero characters.  Since the socket is already created in the test's sandbox and only a single socket is created in the test, it appears that it might be possible to strip e.g., the UUID from the path to make the path fit.",Bug,Major,Resolved,"2016-12-19 10:26:16","2016-12-19 10:26:16",3
"Apache Mesos","Update the addition, deletion and modification logic of CNI configuration files.","We need update the CNI documentation to highlight that we can add/delete and modify CNI networks on the fly without the need for agent restart.",Documentation,Major,Resolved,"2016-12-15 22:02:34","2016-12-15 22:02:34",1
"Apache Mesos","Check unreachable task cache for task ID collisions on launch","As discussed in MESOS-6785, it is possible to crash the master by launching a task that reuses the ID of an unreachable/partitioned task. A complete solution to this problem will be quite involved, but an incremental improvement is easy: when we see a task launch operation, reject the launch attempt if the task ID collides with an ID in the per-framework {{unreachableTasks}} cache. This doesn't catch all situations in which IDs are reused, but it is better than nothing.",Bug,Major,Resolved,"2016-12-15 20:56:08","2016-12-15 20:56:08",2
"Apache Mesos","Running 'tty' inside a debug container that has a tty reports Not a tty","We need to inject `/dev/console` into the container and map it to the slave end of the TTY we are attached to.",Improvement,Major,Accepted,"2016-12-15 20:29:34","2016-12-15 20:29:34",2
"Apache Mesos","SSL socket can lose bytes in the case of EOF","During recent work on SSL-enabled tests in libprocess (MESOS-5966), we discovered a bug in {{LibeventSSLSocketImpl}}, wherein the socket can either fail to receive an EOF, or lose data when an EOF is received.  The {{LibeventSSLSocketImpl::event_callback(short events)}} method immediately sets any pending {{RecvRequest}}'s promise to zero upon receipt of an EOF. However, at the time the promise is set, there may actually be data waiting to be read by libevent. Upon receipt of an EOF, we should attempt to read the socket's bufferevent first to ensure that we aren't losing any data previously received by the socket.",Bug,Major,Resolved,"2016-12-15 17:52:12","2016-12-15 17:52:12",3
"Apache Mesos","Listening socket might get closed while the accept is still in flight.","This might result in the SocketImpl::accept to invoke network::accept on a wrong fd (or a closed fd). We discovered this while triaging a weird behavior in test (https://issues.apache.org/jira/browse/MESOS-6759).",Bug,Major,Resolved,"2016-12-14 18:00:19","2016-12-14 18:00:19",2
"Apache Mesos","CniIsolatorTest.ROOT_EnvironmentLibprocessIP fails on systems using dash as sh","On systems using {{dash}} as default shell (e.g., Debian, Ubuntu) {{CniIsolatorTest.ROOT_EnvironmentLibprocessIP}} often fails with   ",Bug,Major,Resolved,"2016-12-14 11:32:26","2016-12-14 11:32:26",2
"Apache Mesos","Allow to specific the device whitelist entries in cgroup devices subsystem",,Task,Major,Resolved,"2016-12-14 06:42:45","2016-12-14 06:42:45",1
"Apache Mesos","Wrong task started time in webui","Reported by [~<USER> {quote} Hi  When task has enabled Mesos healthcheck start time in UI can show wrong time. This happens because UI assumes that first status is task started [0]. This is not always true because Mesos keeps only recent tasks statuses [1] so when healthcheck updates tasks status it can override task start time displayed in webui.  Best Tomek  [0] https://github.com/apache/mesos/blob/master/src/webui/master/static/js/controllers.js#L140 [1] https://github.com/apache/mesos/blob/f2adc8a95afda943f6a10e771aad64300da19047/src/common/protobuf_utils.cpp#L263-L265 {quote}",Bug,Major,Resolved,"2016-12-14 04:03:48","2016-12-14 04:03:48",1
"Apache Mesos","SSL socket's 'shutdown()' method is broken","We recently uncovered two issues with the {{LibeventSSLSocketImpl::shutdown}} method: * The introduction of a shutdown method parameter with [this commit|https://reviews.apache.org/r/54113/] means that the implementation's method is no longer overriding the default implementation. In addition to fixing the implementation method's signature, we should add the {{override}} specifier to all of our socket implementations' methods to ensure that this doesn't happen in the future. * The {{LibeventSSLSocketImpl::shutdown}} function does not actually shutdown the SSL socket. The proper function to shutdown an SSL socket is {{SSL_shutdown}}, which is called in the implementation's destructor. We should move this into {{shutdown()}} so that by the time that method returns, the socket has actually been shutdown.",Bug,Major,Resolved,"2016-12-14 00:00:54","2016-12-14 00:00:54",2
"Apache Mesos","CHECK failure on duplicate task IDs","The master crashes with a CHECK failure in the following scenario:  # Framework launches task X on agent A1. The framework may or may not be partition-aware; let's assume it is not partition-aware. # A1 becomes partitioned from the master. # Framework launches task X on agent A2. # Master fails over. # Agents A1 and A2 both re-register with the master. Because the master has failed over, the task on A1 is _not_ terminated (non-strict registry semantics).  This results in two running tasks with the same ID, which causes a master {{CHECK}} failure among other badness:  ",Bug,Major,Resolved,"2016-12-12 22:18:47","2016-12-12 22:18:47",3
"Apache Mesos","IOSwitchboardTest.KillSwitchboardContainerDestroyed is flaky"," ",Bug,Critical,Resolved,"2016-12-12 21:58:23","2016-12-12 21:58:23",1
"Apache Mesos","Inherit Environment from parent container when launching DEBUG container.","Right now whenever we enter a DEBUG container we have a fresh environment. For a better user experience, we should have the DEBUG container inherit the environment set up in its parent container image spec (if there is one). ",Improvement,Major,Resolved,"2016-12-12 21:14:08","2016-12-12 21:14:08",3
"Apache Mesos","The 'http::connect(address)' always uses the DEFAULT_KIND() of socket even if SSL is undesired.","    The 'http::connect(address)' variant of 'http::connect()' doesn't     currently support SSL. However, when SSL is enabled, the default for     all 'Socket::create()' calls is to use the 'DEFAULT_KIND()' of socket     which is set to SSL. This causes problems with 'connect()' becuuse it     will create a socket of 'kind' SSL without a way to override it. ",Bug,Major,Resolved,"2016-12-10 18:27:15","2016-12-10 18:27:15",1
"Apache Mesos","Handle SSL socket read and write events separately","The SSL socket code in libprocess currently does not distinguish between events received during reading and those received during writing. However, libevent does provide event flags with this information: {{BEV_EVENT_READING}} and {{BEV_EVENT_WRITING}}. We should make use of these flags to handle read and write events differently.",Improvement,Major,Accepted,"2016-12-09 21:23:29","2016-12-09 21:23:29",2
"Apache Mesos","Reached unreachable statement at <path>/mesos/src/slave/containerizer/mesos/launch.cpp:766","This error message can pop up in unexpected places (e.g. when running a LAUNCH_NESTED_CONTAINER_SESSION and an invalid command is passed to it).  We should likely just remove the UNREACHABLE() statement here as it's obviously reachable in cases where the command we are trying to launch is not found.",Bug,Blocker,Resolved,"2016-12-09 10:07:34","2016-12-09 10:07:34",1
"Apache Mesos","Make the Resources wrapper copy-on-write to improve performance.","Resources currently directly stores the underlying resource objects:        What this means is that copying of Resources (which occurs frequently) is expensive since copying a {{Resource}} object is relatively heavy-weight.    One strategy, in MESOS-4770, is to avoid protobuf in favor of C++ types (i.e. replace {{Value::Scalar}}, {{Value::Set}}, and {{Value::Ranges}} with C++ equivalents). However, metadata like reservations, disk info, etc, is still fairly expensive to copy even if avoiding protobufs.    An approach to reduce copying would be to only copy the resource objects upon writing, when there are multiple references to the resource object. If there is a single reference to the resource object we could safely mutate it without copying. E.g.        On the other hand, this introduces a additional level of pointer chasing. So we would need to weigh the approaches.",Improvement,Critical,Resolved,"2016-12-09 02:07:17","2016-12-09 02:07:17",5
"Apache Mesos","Add heartbeats to both input/output connections in IOSwitchboard","Some networks will kill idle connections if no data is transfered over them within a set amount of time. For example, using AWS's Elastic Load Balancer (ELB), the default time to kill a connection is only 60s! Because of this, we need a way to send application level heartbeats to keep these connections alive for any long running LAUNCH_NESTED_CONTAINER_SESSION, ATTACH_CONTAINER_INPUT and ATTACH_CONTAINER_OUTPUT calls.   We should serve these heartbeats from the IOSwitchboard server rather than the agent handlers since the agent essentially acts as a proxy and the heartbeats should originate from the actual server being communicated with.",Improvement,Major,Resolved,"2016-12-09 01:17:17","2016-12-09 01:17:17",2
"Apache Mesos","IOSwitchboardServerTest.AttachOutput has CHECK failure if run it multiple times.","I can easily repo this issue on my dev centos7 box with the following command: ",Bug,Blocker,Resolved,"2016-12-08 18:49:03","2016-12-08 18:49:03",3
"Apache Mesos","Support 'Basic' auth docker private registry on Unified Containerizer.","Currently, the Unified Containerizer only supports the private docker registry with 'Bearer' authorization (token is needed from the auth server). We should support the 'Basic' auth registry as well.",Improvement,Blocker,Resolved,"2016-12-08 17:20:19","2016-12-08 17:20:19",5
"Apache Mesos","I/O switchboard should deal with the case when reaping of the server failed.","Currently, we don't deal with the reaping failure, which we should.",Bug,Major,Resolved,"2016-12-08 01:45:53","2016-12-08 01:45:53",3
"Apache Mesos","Metrics on the Agent view of the Mesos web UI flickers between empty and non-empty states","When viewing a specific agent on the Mesos WebUI, the metrics panel on the left side of the UI will alternate between having values and being empty.  This is due to two different callbacks that run: * This one sets the metrics into the {{$scope.state}} variable: https://github.com/apache/mesos/blob/1.1.x/src/webui/master/static/js/controllers.js#L564-L577 * This one blows away the {{$scope.state}} in favor of a new one: https://github.com/apache/mesos/blob/1.1.x/src/webui/master/static/js/controllers.js#L521  The metrics callback should simply assign to a different variable.",Bug,Minor,Resolved,"2016-12-07 20:35:00","2016-12-07 20:35:00",1
"Apache Mesos","Update master and agent endpoints to expose FrameworkInfo.roles.","With the addition of the FrameworkInfo.roles field, all of the endpoints that expose the framework information need to be updated to expose this additional field.  It should be the case that for the v1-style operator calls, the new field will be automatically visible thanks to the direct mapping from protobuf (we should verify this).  We can track the updates to metrics separately.",Task,Major,Resolved,"2016-12-07 19:55:40","2016-12-07 19:55:40",3
"Apache Mesos","I/O switchboard should inherit agent environment variables.","Since it is a libexec binary that owned by Mesos. Agent might have some environment variables (e.g., LD_LIBRARY_PATH) that are needed by the io switchboard server process.",Bug,Major,Resolved,"2016-12-07 18:12:17","2016-12-07 18:12:17",1
"Apache Mesos","IOSwitchboard doesn't properly flush data on ATTACH_CONTAINER_OUTPUT","Currently we are doing a close on the write end of all connection pipes when we exit the switchboard, but we don't wait until the read is flushed before exiting. This can cause some data to get dropped since the process may exit before the reader is flushed.  The current code is:   We should change it to: ",Bug,Major,Resolved,"2016-12-07 17:22:17","2016-12-07 17:22:17",1
"Apache Mesos","MesosContainerizer/DefaultExecutorTest.KillTask/0 is flaky","This repros consistently for me (< 20 test iterations), using {{master}} as of {{ab79d58c9df0ffb8ad35f6662541e7a5c3ea4a80}}. Test log:  ",Bug,Major,Resolved,"2016-12-07 16:35:46","2016-12-07 16:35:46",2
"Apache Mesos","DefaultExecutorTest.KillTaskGroupOnTaskFailure is flaky","This repros consistently for me (~10 test iterations or fewer). Test log:  ",Bug,Major,Resolved,"2016-12-07 16:30:55","2016-12-07 16:30:55",2
"Apache Mesos","Docker executor hangs forever if `docker stop` fails.","If {{docker stop}} finishes with an error status, the executor should catch this and react instead of indefinitely waiting for {{reaped}} to return.  An interesting question is _how_ to react. Here are possible solutions.  1. Retry {{docker stop}}. In this case it is unclear how many times to retry and what to do if {{docker stop}} continues to fail.  2. Unmark task as {{killed}}. This will allow frameworks to retry the kill. However, in this case it is unclear what status updates we should send: {{TASK_KILLING}} for every kill retry? an extra update when we failed to kill a task? or set a specific reason in {{TASK_KILLING}}?  3. Clean up and exit. In this case we should make sure the task container is killed or notify the framework and the operator that the container may still be running.",Bug,Critical,Resolved,"2016-12-07 15:13:17","2016-12-07 15:13:17",5
"Apache Mesos","The agent should synchronize with the IOSwitchboard to determine when it is ready to accept incoming connections.","Currently, the agent has no way of knowing when the IOSwitchboard has started up and is ready to listen for incoming connections. We should add support to synchronize between them so the agent can figure this out.  The implementation should not block the launch path of the container, but rather incoming connections through the IOSwitchboards {{connect()}} call.",Bug,Major,Resolved,"2016-12-07 02:47:06","2016-12-07 02:47:06",2
"Apache Mesos","Create a test filter for stout tests that use `symlink` on Windows, as they will fail if not run as admin",,Bug,Major,Resolved,"2016-12-06 19:02:48","2016-12-06 19:02:48",3
"Apache Mesos","Reserve operation should validate reserved resource role against resource allocationInfo role","When doing dynamic reservation validation, the current logic is make sure the reserved resources role is same as the framework role (see [src/master/validation.cpp|https://github.com/apache/mesos/blob/0228fa74c25f450478a6a5a42e1ca384c26db8bd/src/master/validation.cpp#L1539-L1544]):    With multi-role framework, we should validate reserved resource role same as resource allocation role.  Please make sure distinguish dynamic reservation with framework and http endpoint. If dynamic reservation was triggered by a framework, then we need to do such validation. If done by the http endpoint, then no need to validate the roles.",Bug,Major,Resolved,"2016-12-06 16:19:25","2016-12-06 16:19:25",3
"Apache Mesos","Check that `PreferredToolArchitecture` is set to `x64` on Windows before building","If this variable is not set before we build, it will cause the linker to occasionally hang forever, due to a MSVC toolchain bug in the linker.  We should make this easy on developers and check for them. If the variable is not set, we should display an error message explaining.",Bug,Major,Resolved,"2016-12-05 17:26:25","2016-12-05 17:26:25",2
"Apache Mesos","Unify active and state/connected fields in Master::Framework","Rather than tracking whether a framework is active separately from whether it is connected, we should consider using a single state variable to track the current state of the framework (connected-and-active, connected-and-inactive, disconnected, etc.)",Improvement,Minor,Resolved,"2016-12-05 16:54:40","2016-12-05 16:54:40",2
"Apache Mesos","Should destroy DEBUG containers on agent recovery.","We need to add support to destroy DEBUG containers on agent recovery. Right now these containers will stick around forever (or until they run to completion).",Bug,Major,Resolved,"2016-12-05 09:39:23","2016-12-05 09:39:23",3
"Apache Mesos","Port `slave_recovery_tests.cpp`",https://reviews.apache.org/r/65408/,Bug,Major,Resolved,"2016-12-05 07:50:07","2016-12-05 07:50:07",3
"Apache Mesos","Enable SSL in Mesos builds",,Task,Major,Resolved,"2016-12-05 04:20:56","2016-12-05 04:20:56",3
"Apache Mesos","Remove of unix domain socket path in IOSwitchboard::cleanup","We currently leak all of the unix domain socket files created by the switchboard in the `/tmp` directory. We need to clean them up properly.",Bug,Major,Resolved,"2016-12-05 00:35:19","2016-12-05 00:35:19",1
"Apache Mesos","IOSwitchboard should recover spawned server pid on agent restarts","We need to do proper recovery of the io switchboard server pid across agent restarts. As of now, if the agent restarts there is now way to recover this pid.",Bug,Major,Resolved,"2016-12-05 00:31:58","2016-12-05 00:31:58",2
"Apache Mesos","Update tests to use v1 Operator API","Most of the tests that verify information from operator API hit the old REST endpoints. We should update them to use the new v1 operator API instead.",Task,Major,Open,"2016-12-02 18:51:17","2016-12-02 18:51:17",8
"Apache Mesos","Class DynamicLibrary's default copy constructor can lead to inconsistent state","The class {{DynamicLibrary}} provides a RAII wrapper around a low-level handle to a loaded library. Currently it supports copy- and move-construction which would lead to two libraries holding handles to the same library. This can e.g., lead to libraries being unloaded while other wrappers still hold handles.",Bug,Major,Resolved,"2016-12-02 15:10:32","2016-12-02 15:10:32",1
"Apache Mesos","Some HTTP scheduler calls are missing from the docs","Some of the calls available to HTTP schedulers are missing from the HTTP scheduler API documentation. We should make sure that all of the calls available in the {{Master::Http::scheduler}} handler are in the documentation [here|https://github.com/apache/mesos/blob/master/docs/scheduler-http-api.md].",Bug,Major,Accepted,"2016-12-01 17:27:45","2016-12-01 17:27:45",2
"Apache Mesos","Nested containers can become unkillable","An incident occurred recently in a cluster running a build of Mesos based on commit {{757319357471227c0a1e906076eae8f9aa2fdbd6}} from master. A task group of five tasks was launched via Marathon. After the tasks were launched, one of the containers quickly exited and was successfully destroyed. A couple minutes later, the task group was killed manually via Marathon, and the agent can then be seen repeatedly attempting to kill the tasks for hours. No calls to {{WAIT_NESTED_CONTAINER}} are visible in the agent logs, and the executor logs do not indicate at any point that the nested containers were launched successfully.  Agent logs:   Executor log:   Meanwhile, the tasks show up as {{STAGING}} in the Mesos web UI, and their sandboxes are empty.",Bug,Major,Resolved,"2016-11-30 23:40:36","2016-11-30 23:40:36",0
"Apache Mesos","Duplicate image layer ids may make the backend failed to mount rootfs.","Some images (e.g., '<USER>inky') may contain duplicate layer ids in manifest, which may cause some backends unable to mount the rootfs (e.g., 'aufs' backend). We should make sure that each layer path returned in 'ImageInfo' is unique.  Here is an example manifest from '<USER>inky':   These two layer ids are totally identical:   It would make the backend (e.g., aufs) failed to mount the rootfs due to invalid arguments.   We should make sure the vector of layer paths that is passed to the backend contains only unique layer path.",Bug,Critical,Resolved,"2016-11-30 18:09:35","2016-11-30 18:09:35",3
"Apache Mesos","Overlayfs backend may fail to mount the rootfs if both container image and image volume are specified.","Depending on MESOS-6000, we use symlink to shorten the overlayfs mounting arguments. However, if more than one image need to be provisioned (e.g., a container image is specified while image volumes are specified for the same container), the symlink .../backends/overlay/links would fail to be created since it exists already.  Here is a simple log when we hard code overlayfs as our default backend:   We should differenciate the links for different provisioned images.",Bug,Critical,Resolved,"2016-11-30 17:54:16","2016-11-30 17:54:16",3
"Apache Mesos","Make IOSwitchboard an isolator.","This will greatly simplify the lifecycle management of the IO switchboard. We can leverage existing hooks from the Isolator interfaces to properly recover, cleanup and watch for the IO switchboard sidecar.",Task,Major,Resolved,"2016-11-30 01:57:26","2016-11-30 01:57:26",5
"Apache Mesos","MesosContainerizer launch helper should take ContainerLaunchInfo.","Currently, the launch helper takes various flags from MesosContainerizer to launch the container. This makes it very hard to add more parameters to the launch helper. To simplify that, MesosContainerizer can pass 'ContainerLaunchInfo' to the launch helper instead. 'ContainerLaunchInfo' is also the protobuf message returned by isolators during 'prepare()'. This makes it very easy to merge them and send it to the launch helper. More importantly, this makes it very easy to add more parameters to the launch helper in the future.",Improvement,Major,Resolved,"2016-11-27 07:44:38","2016-11-27 07:44:38",5
"Apache Mesos","StreamingRequestDecoder incompletely initializes its http_parser_settings","Coverity reports in CID1394703 at {{3rdparty/libprocess/src/decoder.hpp:767}}:   It seems like {{StreamingRequestDecoder}} should properly initialize its member {{settings}}, e.g., with {{http_parser_settings_init}}.",Bug,Major,Resolved,"2016-11-25 11:36:46","2016-11-25 11:36:46",1
"Apache Mesos","Update 'io::redirect()' to take an optional vector of callback hooks.","These callback hooks should be invoked before passing any data read from the 'from' file descriptor on to the 'to' file descriptor.",Improvement,Major,Resolved,"2016-11-24 06:55:36","2016-11-24 06:55:36",1
"Apache Mesos","Disallow frameworks from modifying FrameworkInfo.roles.","In phase 1 of the multi-role framework support, we want to preserve the existing behavior of single-role framework support in that we disallow frameworks from modifying their role.  With multi-role framework support, we will initially disallow frameworks from modifying the roles field. Note that in the case that the master has failed over but the framework hasn't re-registered yet, we will use the framework info from the agents to disallow changes to the roles field. We will treat {{FrameworkInfo.roles}} as a set rather than a list, so ordering does not matter for equality.  One difference between {{role}} and {{roles}} is that for {{role}} modification, we ignore it. But, with {{roles}} modification, since this is a new feature, we can disallow it by rejecting the framework subscription.  Later, in phase 2, we will allow frameworks to modify their roles, see MESOS-6627.",Task,Major,Resolved,"2016-11-23 02:27:53","2016-11-23 02:27:53",3
"Apache Mesos","Add some benchmark test for quota allocation","Comparing to non-quota allocation, current quota allocation involves a separate allocation stage and additional tracking such as headroom and role consumed quota. Thus quota allocation performance could be drastically different (probably slower) than non-quota allocation. A dedicated benchmark for quota allocation is necessary.",Task,Major,Resolved,"2016-11-23 02:18:46","2016-11-23 02:18:46",3
"Apache Mesos","Support `foreachpair` for LinkedHashMap","{{LinkedHashMap}} does not support iteration via {{foreachpair}}; it should.",Improvement,Major,Resolved,"2016-11-22 22:51:43","2016-11-22 22:51:43",3
"Apache Mesos","Expose container id in ContainerStatus in DockerContainerizer.","Currently, the container id is only exposed for MesosContainerizer. We should make it consistent in DockerContainerizer.",Bug,Major,Resolved,"2016-11-22 19:43:36","2016-11-22 19:43:36",2
"Apache Mesos","Re-enable tests impacted by request streaming support","We added support for HTTP request streaming in libprocess as part of MESOS-6466. However, this broke a few tests that relied on HTTP request filtering since the handlers no longer have access to the body of the request when {{visit()}} is invoked. We would need to revisit how we do HTTP request filtering and then re-enable these tests.",Improvement,Major,"In Progress","2016-11-22 03:32:21","2016-11-22 03:32:21",5
"Apache Mesos","NvidiaGpuTest.ROOT_INTERNET_CURL_CGROUPS_NVIDIA_GPU_NvidiaDockerImage is flaky","This test occasionally times out after one minute:      The test itself has a future that waits for 2 minutes for the executor to start up.",Bug,Minor,Resolved,"2016-11-22 02:48:22","2016-11-22 02:48:22",2
"Apache Mesos","SSL downgrade path will CHECK-fail when using both temporary and persistent sockets","The code path for downgrading sockets from SSL to non-SSL includes this code:  https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2311-L2321  It is possible for libprocess to hold both temporary and persistent sockets to the same address.  This can happen when a message is first sent ({{ProcessBase::send}}), and then a link is established ({{ProcessBase::link}}).  When the target of the message/link is a non-SSL socket, both temporary and persistent sockets go through the downgrade path.  If a temporary socket is present while a persistent socket is being created, the above code will remap both temporary and persistent sockets to the same address (it should only remap the persistent socket).  This leads to some CHECK failures if those sockets are used or closed later: *  https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L1942 *  https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2044",Bug,Blocker,Resolved,"2016-11-22 02:30:17","2016-11-22 02:30:17",3
"Apache Mesos","Improve task management for unreachable tasks","Scenario:  # Framework starts non-partition-aware task T on agent A # Agent A is partitioned. Task T is marked as a completed task in the {{Framework}} struct of the master, as part of {{Framework::removeTask}}. # Agent A re-registers with the master. The tasks running on A are re-added to their respective frameworks on the master as running tasks. # In {{Master::\_reregisterSlave}}, the master sends a {{ShutdownFrameworkMessage}} for all non-partition-aware frameworks running on the agent. The master then does {{removeTask}} for each task managed by one of these frameworks, which results in calling {{Framework::removeTask}}, which adds _another_ task to {{completed_tasks}}. Note that {{completed_tasks}} does not attempt to detect/suppress duplicates, so this results in two elements in the {{completed_tasks}} collection.  Similar problems occur when a partition-aware task is running on a partitioned agent that re-registers: the result is a task in the {{tasks}} list _and_ a task in the {{completed_tasks}} list.  Possible fixes/changes:  * Adding a task to the {{completed_tasks}} list when an agent becomes partitioned is debatable; certainly for partition-aware tasks, the task is not completed. We might consider adding an {{unreachable_tasks}} list to the HTTP endpoints. * Regardless of whether we continue to use {{completed_tasks}} or add a new collection, we should ensure the consistency of that data structure after agent re-registration.",Bug,Major,Resolved,"2016-11-21 17:47:08","2016-11-21 17:47:08",8
"Apache Mesos","Some tests use hardcoded port numbers.","DockerContainerizerTest.ROOT_DOCKER_NoTransitionFromKillingToRunning and many HealthCheckTests use hardcoded port numbers. This can create false failures if these tests are run in parallel on the same machine.  It appears instead we should use random port numbers.",Bug,Minor,Resolved,"2016-11-21 12:31:55","2016-11-21 12:31:55",3
"Apache Mesos","mesos-execute does not support health-checks.","In order to tests health-checks simply as it has already been done here https://gist.github.com/<USER>577e4a527e2a80e2aa1b1eb783870f8d. It would be useful to support health-checks directly in {{mesos-execute}}.",Improvement,Minor,Resolved,"2016-11-21 11:37:05","2016-11-21 11:37:05",3
"Apache Mesos","Error: dereferencing type-punned pointer will break strict-aliasing rules.","Trying to update the mesos package to 1.1.0 in Fedora.  Getting:    ",Bug,Major,Resolved,"2016-11-21 04:42:29","2016-11-21 04:42:29",3
"Apache Mesos","Reject optimized builds with libcxx before 3.9","Recent clang versions optimize more aggressively which leads to runtime errors using valid code, see e.g., MESOS-5745, due to code exposing undefined behavior in libcxx-3.8 and earlier. This was fixed with upstream libcxx-3.9. See https://reviews.llvm.org/D20786 for the patch and https://llvm.org/bugs/show_bug.cgi?id=28469 for the code example extracted from our code base.  We should consider rejecting builds if libcxx-3.8 or older is detected since not all users compiling Mesos might run the test suite. In our decision to reject we could possibly also take the used clang versions into account (which would just ensure we don't run into the known problems from the UB in libcxx).",Bug,Major,Resolved,"2016-11-18 08:34:40","2016-11-18 08:34:40",2
"Apache Mesos","Uninitialized member ObjectApprover::weight_info.","In {{include/mesos/authorizer/authorizer.hpp}} the member {{ObjectApprover::weight_info}} is a raw ptr which is not initialized (with {{nullptr}}) in the ctr.  While the member is {{public}}, requiring users to set up the {{class}} is error-prone; it is also contrary to what is done  for other members.  Instead this member should be initialized to {{nullptr}}. As a more long-term solution we should consider introducing some non-owning wrapper for raw ptrs to stout which on construction initializes the raw ptr, see MESOS-6603.  This was detected by coverity as CID 1394390.",Bug,Minor,Resolved,"2016-11-18 03:44:45","2016-11-18 03:44:45",1
"Apache Mesos","Shutdown completed frameworks when unreachable agent re-registers","We currently shutdown completed frameworks when an agent re-registers with a master that it is already registered with (MESOS-633). We should also shutdown completed frameworks when an unreachable agent re-registers.  This is distinct from the more difficult problem of shutting down completed frameworks after master failover (MESOS-4659).",Bug,Major,Resolved,"2016-11-18 02:49:04","2016-11-18 02:49:04",5
"Apache Mesos","Add `Containerizer::attach()` API call","This ticket just tracks the API change but not the actual implementation.",Task,Major,Resolved,"2016-11-15 20:14:47","2016-11-15 20:14:47",3
"Apache Mesos","MesosContainerizer/DefaultExecutorTest.KillTask/0 failing on ASF CI","{noformat:title=} [ RUN      ] MesosContainerizer/DefaultExecutorTest.KillTask/0 I1110 01:20:11.482097 29700 cluster.cpp:158] Creating default 'local' authorizer I1110 01:20:11.485241 29700 leveldb.cpp:174] Opened db in 2.774513ms I1110 01:20:11.486237 29700 leveldb.cpp:181] Compacted db in 953614ns I1110 01:20:11.486299 29700 leveldb.cpp:196] Created db iterator in 24739ns I1110 01:20:11.486325 29700 leveldb.cpp:202] Seeked to beginning of db in 2300ns I1110 01:20:11.486344 29700 leveldb.cpp:271] Iterated through 0 keys in the db in 378ns I1110 01:20:11.486399 29700 replica.cpp:776] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1110 01:20:11.486933 29733 recover.cpp:451] Starting replica recovery I1110 01:20:11.487289 29733 recover.cpp:477] Replica is in EMPTY status I1110 01:20:11.488503 29721 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from __req_res__(7318)@172.17.0.3:52462 I1110 01:20:11.488855 29727 recover.cpp:197] Received a recover response from a replica in EMPTY status I1110 01:20:11.489398 29729 recover.cpp:568] Updating replica status to STARTING I1110 01:20:11.490223 29723 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 575135ns I1110 01:20:11.490284 29732 master.cpp:380] Master d28fbae1-c3dc-45fa-8384-32ab9395a975 (3a31be8bf679) started on 172.17.0.3:52462 I1110 01:20:11.490317 29732 master.cpp:382] Flags at startup: --acls= --agent_ping_timeout=15secs --agent_reregister_timeout=10mins --allocation_interval=1secs --allocator=HierarchicalDRF --authenticate_agents=true --authenticate_frameworks=true --authenticate_http_frameworks=true --authenticate_http_readonly=true --authenticate_http_readwrite=true --authenticators=crammd5 --authorizers=local --credentials=/tmp/k50x7x/credentials --framework_sorter=drf --help=false --hostname_lookup=true --http_authenticators=basic --http_framework_authenticators=basic --initialize_driver_logging=true --log_auto_initialize=true --logbufsecs=0 --logging_level=INFO --max_agent_ping_timeouts=5 --max_completed_frameworks=50 --max_completed_tasks_per_framework=1000 --quiet=false --recovery_agent_removal_limit=100% --registry=replicated_log --registry_fetch_timeout=1mins --registry_gc_interval=15mins --registry_max_agent_age=2weeks --registry_max_agent_count=102400 --registry_store_timeout=100secs --registry_strict=false --root_submissions=true --user_sorter=drf --version=false --webui_dir=/mesos/mesos-1.2.0/_inst/share/mesos/webui --work_dir=/tmp/k50x7x/master --zk_session_timeout=10secs I1110 01:20:11.490696 29732 master.cpp:432] Master only allowing authenticated frameworks to register I1110 01:20:11.490712 29732 master.cpp:446] Master only allowing authenticated agents to register I1110 01:20:11.490720 29732 master.cpp:459] Master only allowing authenticated HTTP frameworks to register I1110 01:20:11.490730 29732 credentials.hpp:37] Loading credentials for authentication from '/tmp/k50x7x/credentials' I1110 01:20:11.490281 29723 replica.cpp:320] Persisted replica status to STARTING I1110 01:20:11.491210 29732 master.cpp:504] Using default 'crammd5' authenticator I1110 01:20:11.491225 29720 recover.cpp:477] Replica is in STARTING status I1110 01:20:11.491394 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly' I1110 01:20:11.491621 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite' I1110 01:20:11.491770 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler' I1110 01:20:11.491937 29732 master.cpp:584] Authorization enabled I1110 01:20:11.492276 29725 whitelist_watcher.cpp:77] No whitelist given I1110 01:20:11.492310 29723 hierarchical.cpp:149] Initialized hierarchical allocator process I1110 01:20:11.492569 29721 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from __req_res__(7319)@172.17.0.3:52462 I1110 01:20:11.492830 29719 recover.cpp:197] Received a recover response from a replica in STARTING status I1110 01:20:11.493371 29720 recover.cpp:568] Updating replica status to VOTING I1110 01:20:11.494002 29721 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 367673ns I1110 01:20:11.494032 29721 replica.cpp:320] Persisted replica status to VOTING I1110 01:20:11.494218 29734 recover.cpp:582] Successfully joined the Paxos group I1110 01:20:11.494469 29734 recover.cpp:466] Recover process terminated I1110 01:20:11.495633 29733 master.cpp:2033] Elected as the leading master! I1110 01:20:11.495685 29733 master.cpp:1560] Recovering from registrar I1110 01:20:11.495880 29720 registrar.cpp:329] Recovering registrar I1110 01:20:11.496842 29730 log.cpp:553] Attempting to start the writer I1110 01:20:11.498610 29725 replica.cpp:493] Replica received implicit promise request from __req_res__(7320)@172.17.0.3:52462 with proposal 1 I1110 01:20:11.499179 29725 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 524192ns I1110 01:20:11.499213 29725 replica.cpp:342] Persisted promised to 1 I1110 01:20:11.500258 29726 coordinator.cpp:238] Coordinator attempting to fill missing positions I1110 01:20:11.501874 29731 replica.cpp:388] Replica received explicit promise request from __req_res__(7321)@172.17.0.3:52462 for position 0 with proposal 2 I1110 01:20:11.502413 29731 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 484138ns I1110 01:20:11.502457 29731 replica.cpp:708] Persisted action NOP at position 0 I1110 01:20:11.503885 29720 replica.cpp:537] Replica received write request for position 0 from __req_res__(7322)@172.17.0.3:52462 I1110 01:20:11.503985 29720 leveldb.cpp:436] Reading position from leveldb took 56800ns I1110 01:20:11.504534 29720 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 467426ns I1110 01:20:11.504566 29720 replica.cpp:708] Persisted action NOP at position 0 I1110 01:20:11.505470 29721 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0 I1110 01:20:11.505988 29721 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 479078ns I1110 01:20:11.506021 29721 replica.cpp:708] Persisted action NOP at position 0 I1110 01:20:11.506706 29732 log.cpp:569] Writer started with ending position 0 I1110 01:20:11.508041 29734 leveldb.cpp:436] Reading position from leveldb took 50010ns I1110 01:20:11.509210 29733 registrar.cpp:362] Successfully fetched the registry (0B) in 13.068032ms I1110 01:20:11.509356 29733 registrar.cpp:461] Applied 1 operations in 27124ns; attempting to update the registry I1110 01:20:11.510251 29732 log.cpp:577] Attempting to append 168 bytes to the log I1110 01:20:11.510457 29724 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1 I1110 01:20:11.511355 29728 replica.cpp:537] Replica received write request for position 1 from __req_res__(7323)@172.17.0.3:52462 I1110 01:20:11.511828 29728 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 423890ns I1110 01:20:11.511859 29728 replica.cpp:708] Persisted action APPEND at position 1 I1110 01:20:11.512572 29734 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0 I1110 01:20:11.513051 29734 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 368122ns I1110 01:20:11.513087 29734 replica.cpp:708] Persisted action APPEND at position 1 I1110 01:20:11.514302 29726 registrar.cpp:506] Successfully updated the registry in 4.862976ms I1110 01:20:11.514503 29726 registrar.cpp:392] Successfully recovered registrar I1110 01:20:11.514593 29728 log.cpp:596] Attempting to truncate the log to 1 I1110 01:20:11.514760 29730 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2 I1110 01:20:11.515249 29723 master.cpp:1676] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register I1110 01:20:11.515534 29722 hierarchical.cpp:176] Skipping recovery of hierarchical allocator: nothing to recover I1110 01:20:11.516068 29722 replica.cpp:537] Replica received write request for position 2 from __req_res__(7324)@172.17.0.3:52462 I1110 01:20:11.516619 29722 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 497823ns I1110 01:20:11.516652 29722 replica.cpp:708] Persisted action TRUNCATE at position 2 I1110 01:20:11.517526 29734 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0 I1110 01:20:11.518040 29734 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 384129ns I1110 01:20:11.518111 29734 leveldb.cpp:399] Deleting ~1 keys from leveldb took 39398ns I1110 01:20:11.518138 29734 replica.cpp:708] Persisted action TRUNCATE at position 2 I1110 01:20:11.525027 29700 containerizer.cpp:201] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni W1110 01:20:11.525806 29700 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos W1110 01:20:11.526018 29700 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges I1110 01:20:11.527331 29700 cluster.cpp:435] Creating default 'local' authorizer I1110 01:20:11.528741 29725 slave.cpp:208] Mesos agent started on (571)@172.17.0.3:52462 I1110 01:20:11.528789 29725 slave.cpp:209] Flags at startup: --acls= --appc_simple_discovery_uri_prefix=http:// --appc_store_dir=/tmp/mesos/store/appc --authenticate_http_readonly=true --authenticate_http_readwrite=false --authenticatee=crammd5 --authentication_backoff_factor=1secs --authorizer=local --cgroups_cpu_enable_pids_and_tids_count=false --cgroups_enable_cfs=false --cgroups_hierarchy=/sys/fs/cgroup --cgroups_limit_swap=false --cgroups_root=mesos --container_disk_watch_interval=15secs --containerizers=mesos --credential=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/credential --default_role=* --disk_watch_interval=1mins --docker=docker --docker_kill_orphans=true --docker_registry=https://registry-1.docker.io --docker_remove_delay=6hrs --docker_socket=/var/run/docker.sock --docker_stop_timeout=0ns --docker_store_dir=/tmp/mesos/store/docker --docker_volume_checkpoint_dir=/var/run/mesos/isolators/docker/volume --enforce_container_disk_quota=false --executor_registration_timeout=1mins --executor_shutdown_grace_period=5secs --fetcher_cache_dir=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/fetch --fetcher_cache_size=2GB --frameworks_home= --gc_delay=1weeks --gc_disk_headroom=0.1 --hadoop_home= --help=false --hostname_lookup=true --http_authenticators=basic --http_command_executor=false --http_credentials=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/http_credentials --image_provisioner_backend=copy --initialize_driver_logging=true --isolation=posix/cpu,posix/mem --launcher=posix --launcher_dir=/mesos/mesos-1.2.0/_build/src --logbufsecs=0 --logging_level=INFO --max_completed_executors_per_framework=150 --oversubscribed_resources_interval=15secs --perf_duration=10secs --perf_interval=1mins --qos_correction_interval_min=0ns --quiet=false --recover=reconnect --recovery_timeout=15mins --registration_backoff_factor=10ms --resources=cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000] --revocable_cpu_low_priority=true --runtime_dir=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ --sandbox_directory=/mnt/mesos/sandbox --strict=true --switch_user=true --systemd_enable_support=true --systemd_runtime_directory=/run/systemd/system --version=false --work_dir=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD I1110 01:20:11.529228 29725 credentials.hpp:86] Loading credential for authentication from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/credential' I1110 01:20:11.529305 29700 scheduler.cpp:176] Version: 1.2.0 I1110 01:20:11.529436 29725 slave.cpp:346] Agent using credential for: test-principal I1110 01:20:11.529464 29725 credentials.hpp:37] Loading credentials for authentication from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/http_credentials' I1110 01:20:11.529747 29725 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly' I1110 01:20:11.529855 29729 scheduler.cpp:469] New master detected at master@172.17.0.3:52462 I1110 01:20:11.529884 29729 scheduler.cpp:478] Waiting for 0ns before initiating a re-(connection) attempt with the master I1110 01:20:11.531039 29725 slave.cpp:533] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I1110 01:20:11.531113 29725 slave.cpp:541] Agent attributes: [  ] I1110 01:20:11.531126 29725 slave.cpp:546] Agent hostname: 3a31be8bf679 I1110 01:20:11.532897 29723 state.cpp:57] Recovering state from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/meta' I1110 01:20:11.533222 29727 status_update_manager.cpp:203] Recovering status update manager I1110 01:20:11.533269 29721 scheduler.cpp:353] Connected with the master at http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.533627 29734 containerizer.cpp:557] Recovering containerizer I1110 01:20:11.534519 29725 scheduler.cpp:235] Sending SUBSCRIBE call to http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.535482 29732 provisioner.cpp:253] Provisioner recovery complete I1110 01:20:11.535652 29734 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I1110 01:20:11.535815 29724 slave.cpp:5411] Finished recovery I1110 01:20:11.536440 29724 slave.cpp:5585] Querying resource estimator for oversubscribable resources I1110 01:20:11.536898 29721 slave.cpp:915] New master detected at master@172.17.0.3:52462 I1110 01:20:11.536906 29731 status_update_manager.cpp:177] Pausing sending status updates I1110 01:20:11.536941 29721 slave.cpp:974] Authenticating with master master@172.17.0.3:52462 I1110 01:20:11.537076 29721 slave.cpp:985] Using default CRAM-MD5 authenticatee I1110 01:20:11.537214 29733 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54635 I1110 01:20:11.537353 29719 authenticatee.cpp:121] Creating new client SASL connection I1110 01:20:11.537256 29721 slave.cpp:947] Detecting new master I1110 01:20:11.537591 29733 master.cpp:2329] Received subscription request for HTTP framework 'default' I1110 01:20:11.537611 29721 slave.cpp:5599] Received oversubscribable resources {} from the resource estimator I1110 01:20:11.537701 29733 master.cpp:2069] Authorizing framework principal 'test-principal' to receive offers for role '*' I1110 01:20:11.538077 29733 master.cpp:6745] Authenticating slave(571)@172.17.0.3:52462 I1110 01:20:11.538208 29732 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1121)@172.17.0.3:52462 I1110 01:20:11.538291 29733 master.cpp:2427] Subscribing framework 'default' with checkpointing disabled and capabilities [  ] I1110 01:20:11.538508 29731 authenticator.cpp:98] Creating new server SASL connection I1110 01:20:11.538782 29720 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5 I1110 01:20:11.538823 29720 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5' I1110 01:20:11.539227 29730 hierarchical.cpp:275] Added framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.539317 29722 master.hpp:2161] Sending heartbeat to d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.539331 29730 hierarchical.cpp:1694] No allocations performed I1110 01:20:11.539696 29730 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:11.539818 29730 hierarchical.cpp:1286] Performed allocation for 0 agents in 554795ns I1110 01:20:11.540354 29720 authenticator.cpp:204] Received SASL authentication start I1110 01:20:11.540361 29719 scheduler.cpp:675] Enqueuing event SUBSCRIBED received from http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.540438 29720 authenticator.cpp:326] Authentication requires more steps I1110 01:20:11.540750 29720 authenticatee.cpp:259] Received SASL authentication step I1110 01:20:11.541038 29721 authenticator.cpp:232] Received SASL authentication step I1110 01:20:11.541081 29721 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a31be8bf679' server FQDN: '3a31be8bf679' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1110 01:20:11.541112 29721 auxprop.cpp:181] Looking up auxiliary property '*userPassword' I1110 01:20:11.541147 29719 scheduler.cpp:675] Enqueuing event HEARTBEAT received from http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.541178 29721 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1110 01:20:11.541260 29721 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a31be8bf679' server FQDN: '3a31be8bf679' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1110 01:20:11.541285 29721 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1110 01:20:11.541307 29721 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1110 01:20:11.541342 29721 authenticator.cpp:318] Authentication success I1110 01:20:11.541517 29733 authenticatee.cpp:299] Authentication success I1110 01:20:11.541586 29720 master.cpp:6775] Successfully authenticated principal 'test-principal' at slave(571)@172.17.0.3:52462 I1110 01:20:11.541826 29721 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1121)@172.17.0.3:52462 I1110 01:20:11.542129 29730 slave.cpp:1069] Successfully authenticated with master master@172.17.0.3:52462 I1110 01:20:11.542362 29730 slave.cpp:1483] Will retry registration in 9.532818ms if necessary I1110 01:20:11.542577 29733 master.cpp:5154] Registering agent at slave(571)@172.17.0.3:52462 (3a31be8bf679) with id d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 I1110 01:20:11.543083 29731 registrar.cpp:461] Applied 1 operations in 60476ns; attempting to update the registry I1110 01:20:11.543926 29729 log.cpp:577] Attempting to append 337 bytes to the log I1110 01:20:11.544077 29723 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3 I1110 01:20:11.545238 29731 replica.cpp:537] Replica received write request for position 3 from __req_res__(7325)@172.17.0.3:52462 I1110 01:20:11.546116 29731 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 825474ns I1110 01:20:11.546169 29731 replica.cpp:708] Persisted action APPEND at position 3 I1110 01:20:11.547427 29725 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0 I1110 01:20:11.547969 29725 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 483290ns I1110 01:20:11.548005 29725 replica.cpp:708] Persisted action APPEND at position 3 I1110 01:20:11.550129 29732 registrar.cpp:506] Successfully updated the registry in 6.962944ms I1110 01:20:11.550396 29726 log.cpp:596] Attempting to truncate the log to 3 I1110 01:20:11.550614 29720 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4 I1110 01:20:11.551375 29723 slave.cpp:4263] Received ping from slave-observer(531)@172.17.0.3:52462 I1110 01:20:11.551326 29734 master.cpp:5225] Registered agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I1110 01:20:11.551720 29730 replica.cpp:537] Replica received write request for position 4 from __req_res__(7326)@172.17.0.3:52462 I1110 01:20:11.551892 29723 slave.cpp:1115] Registered with master master@172.17.0.3:52462; given agent ID d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 I1110 01:20:11.551975 29723 fetcher.cpp:86] Clearing fetcher cache I1110 01:20:11.552170 29732 hierarchical.cpp:485] Added agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {}) I1110 01:20:11.552338 29721 status_update_manager.cpp:184] Resuming sending status updates I1110 01:20:11.552486 29730 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 709727ns I1110 01:20:11.552655 29723 slave.cpp:1138] Checkpointing SlaveInfo to '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/meta/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/slave.info' I1110 01:20:11.552609 29730 replica.cpp:708] Persisted action TRUNCATE at position 4 I1110 01:20:11.553383 29731 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0 I1110 01:20:11.553409 29723 slave.cpp:1175] Forwarding total oversubscribed resources {} I1110 01:20:11.553653 29732 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:11.553671 29727 master.cpp:5624] Received update of agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) with total oversubscribed resources {} I1110 01:20:11.553755 29732 hierarchical.cpp:1309] Performed allocation for agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 in 1.528714ms I1110 01:20:11.553975 29731 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 525057ns I1110 01:20:11.554072 29731 leveldb.cpp:399] Deleting ~2 keys from leveldb took 59750ns I1110 01:20:11.554065 29732 hierarchical.cpp:555] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) I1110 01:20:11.554105 29731 replica.cpp:708] Persisted action TRUNCATE at position 4 I1110 01:20:11.554260 29732 hierarchical.cpp:1694] No allocations performed I1110 01:20:11.554314 29732 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:11.554345 29727 master.cpp:6574] Sending 1 offers to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) I1110 01:20:11.554379 29732 hierarchical.cpp:1309] Performed allocation for agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 in 239597ns I1110 01:20:11.556370 29724 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.559177 29730 scheduler.cpp:235] Sending ACCEPT call to http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.560282 29734 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I1110 01:20:11.561323 29733 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634 I1110 01:20:11.562417 29733 master.cpp:3581] Processing ACCEPT call for offers: [ d28fbae1-c3dc-45fa-8384-32ab9395a975-O0 ] on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) I1110 01:20:11.562584 29733 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task c96eb523-0365-49b2-8b3b-78976ff28797 I1110 01:20:11.563097 29733 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba I1110 01:20:11.567248 29733 master.cpp:8337] Adding task c96eb523-0365-49b2-8b3b-78976ff28797 with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) I1110 01:20:11.567651 29733 master.cpp:8337] Adding task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) I1110 01:20:11.567845 29733 master.cpp:4438] Launching task group { 08848440-4c0e-4ad6-a0a9-b5947c5d21ba, c96eb523-0365-49b2-8b3b-78976ff28797 } of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) with resources cpus(*):0.2; mem(*):64; disk(*):64 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:11.568495 29724 slave.cpp:1547] Got assigned task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.569128 29729 hierarchical.cpp:1018] Recovered cpus(*):1.7; mem(*):928; disk(*):928; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):0.3; mem(*):96; disk(*):96) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.569211 29729 hierarchical.cpp:1055] Framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 filtered agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 for 5secs I1110 01:20:11.570461 29724 slave.cpp:1709] Launching task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.571297 29724 paths.cpp:530] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' to user 'mesos' I1110 01:20:11.580168 29724 slave.cpp:6319] Launching executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 with resources cpus(*):0.1; mem(*):32; disk(*):32 in work directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' I1110 01:20:11.580930 29734 containerizer.cpp:940] Starting container a283035b-25d3-4b48-b59a-964e5a4dfa06 for executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.581110 29724 slave.cpp:2031] Queued task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.581214 29724 slave.cpp:868] Successfully attached file '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' I1110 01:20:11.585572 29722 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command={arguments:[mesos-default-executor,--launcher_dir=\/mesos\/mesos-1.2.0\/_build\/src],shell:false,value:\/mesos\/mesos-1.2.0\/_build\/src\/mesos-default-executor} --help=false --pipe_read=60 --pipe_write=61 --pre_exec_commands=[] --runtime_directory=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06 --unshare_namespace_mnt=false --user=mesos --working_directory=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' I1110 01:20:11.588587 29722 launcher.cpp:127] Forked child with pid '10191' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06' I1110 01:20:11.592996 29734 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06 I1110 01:20:11.777189 10229 executor.cpp:189] Version: 1.2.0 I1110 01:20:11.786099 29719 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor' I1110 01:20:11.787382 29722 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54638 I1110 01:20:11.787693 29722 slave.cpp:3086] Received Subscribe request for HTTP executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.790436 29730 slave.cpp:2276] Sending queued task group task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] to executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (via HTTP) I1110 01:20:11.795663 10221 default_executor.cpp:130] Received SUBSCRIBED event I1110 01:20:11.797111 10221 default_executor.cpp:134] Subscribed executor on 3a31be8bf679 I1110 01:20:11.797611 10221 default_executor.cpp:130] Received LAUNCH_GROUP event I1110 01:20:11.801981 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1' I1110 01:20:11.802435 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1' I1110 01:20:11.803306 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54640 I1110 01:20:11.803452 29723 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER I1110 01:20:11.803827 29727 containerizer.cpp:1685] Starting nested container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d I1110 01:20:11.803865 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54640 I1110 01:20:11.803978 29723 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER I1110 01:20:11.804236 29727 containerizer.cpp:1709] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d' to user 'mesos' I1110 01:20:11.814858 29727 containerizer.cpp:1685] Starting nested container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 I1110 01:20:11.815129 29727 containerizer.cpp:1709] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611' to user 'mesos' I1110 01:20:11.824666 29727 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command={shell:true,value:sleep 1000} --help=false --pipe_read=63 --pipe_write=64 --pre_exec_commands=[] --runtime_directory=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d --unshare_namespace_mnt=false --user=mesos --working_directory=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d' I1110 01:20:11.826855 29727 launcher.cpp:127] Forked child with pid '10240' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d' I1110 01:20:11.828918 29727 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command={shell:true,value:sleep 1000} --help=false --pipe_read=65 --pipe_write=66 --pre_exec_commands=[] --runtime_directory=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611 --unshare_namespace_mnt=false --user=mesos --working_directory=/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611' I1110 01:20:11.831428 29727 launcher.cpp:127] Forked child with pid '10241' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611' I1110 01:20:11.834421 29731 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d I1110 01:20:11.837882 29731 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611 I1110 01:20:11.847651 10227 default_executor.cpp:470] Successfully launched child containers [ a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d, a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 ] for tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] I1110 01:20:11.849225 29728 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor' I1110 01:20:11.850085 10226 default_executor.cpp:546] Waiting for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d of task 'c96eb523-0365-49b2-8b3b-78976ff28797' I1110 01:20:11.850145 29734 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor' I1110 01:20:11.850405 10226 default_executor.cpp:546] Waiting for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 of task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba' I1110 01:20:11.850746 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639 I1110 01:20:11.851114 29726 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.851552 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639 I1110 01:20:11.851727 29726 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.852295 29726 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1' I1110 01:20:11.852826 29726 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1' I1110 01:20:11.853938 29724 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.854076 29724 status_update_manager.cpp:500] Creating StatusUpdate stream for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.854460 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54641 I1110 01:20:11.854559 29726 http.cpp:353] Processing call WAIT_NESTED_CONTAINER I1110 01:20:11.854610 29724 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent I1110 01:20:11.855126 29724 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.855190 29724 status_update_manager.cpp:500] Creating StatusUpdate stream for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.855200 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54642 I1110 01:20:11.855409 29726 http.cpp:353] Processing call WAIT_NESTED_CONTAINER I1110 01:20:11.855608 29724 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent I1110 01:20:11.855803 29726 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462 I1110 01:20:11.856199 29726 slave.cpp:4075] Status update manager successfully handled status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.856346 29725 master.cpp:5760] Status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:11.856439 29725 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.856598 29726 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462 I1110 01:20:11.856828 29726 slave.cpp:4075] Status update manager successfully handled status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.856998 29725 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING) I1110 01:20:11.857322 29725 master.cpp:5760] Status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:11.857386 29725 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.857787 29725 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING) I1110 01:20:11.858124 10226 default_executor.cpp:130] Received ACKNOWLEDGED event I1110 01:20:11.858530 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.859519 29732 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.859676 10233 default_executor.cpp:130] Received ACKNOWLEDGED event ../../src/tests/default_executor_tests.cpp:338: Failure Value of: runningUpdate1->status().task_id()   Actual: 08848440-4c0e-4ad6-a0a9-b5947c5d21ba Expected: taskInfo1.task_id() Which is: c96eb523-0365-49b2-8b3b-78976ff28797 ../../src/tests/default_executor_tests.cpp:342: Failure Value of: runningUpdate2->status().task_id()   Actual: c96eb523-0365-49b2-8b3b-78976ff28797 Expected: taskInfo2.task_id() Which is: 08848440-4c0e-4ad6-a0a9-b5947c5d21ba I1110 01:20:11.861587 29733 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.861948 29733 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.862280 29733 scheduler.cpp:235] Sending KILL call to http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:11.862632 29721 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I1110 01:20:11.863528 29719 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634 I1110 01:20:11.863664 29719 master.cpp:4870] Processing ACKNOWLEDGE call 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 I1110 01:20:11.864003 29732 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 W1110 01:20:11.864294 29732 status_update_manager.cpp:769] Unexpected status update acknowledgement (received 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87, expecting d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 E1110 01:20:11.864575 29726 slave.cpp:3015] Failed to handle status update acknowledgement (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000: Duplicate acknowledgement I1110 01:20:11.864804 29723 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I1110 01:20:11.865231 29723 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I1110 01:20:11.866297 29722 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634 I1110 01:20:11.866420 29722 master.cpp:4870] Processing ACKNOWLEDGE call d91c7deb-4646-4b4e-ba1a-5650a256e8d2 for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 I1110 01:20:11.867036 29726 status_update_manager.cpp:395] Received status update acknowledgement (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.867076 29722 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634 I1110 01:20:11.867291 29722 master.cpp:4762] Telling agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) to kill task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) W1110 01:20:11.867297 29726 status_update_manager.cpp:769] Unexpected status update acknowledgement (received d91c7deb-4646-4b4e-ba1a-5650a256e8d2, expecting 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.867449 29733 slave.cpp:2344] Asked to kill task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 E1110 01:20:11.867710 29733 slave.cpp:3015] Failed to handle status update acknowledgement (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000: Duplicate acknowledgement I1110 01:20:11.869391 10228 default_executor.cpp:130] Received KILL event I1110 01:20:11.869498 10228 default_executor.cpp:810] Received kill for task 'c96eb523-0365-49b2-8b3b-78976ff28797' I1110 01:20:11.869544 10228 default_executor.cpp:694] Shutting down I1110 01:20:11.870112 10221 default_executor.cpp:782] Killing child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d I1110 01:20:11.870338 10221 default_executor.cpp:782] Killing child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 I1110 01:20:11.870965 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1' I1110 01:20:11.871399 29730 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1' I1110 01:20:11.871984 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54643 I1110 01:20:11.872088 29723 http.cpp:353] Processing call KILL_NESTED_CONTAINER I1110 01:20:11.872284 29726 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d in RUNNING state I1110 01:20:11.872340 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54643 I1110 01:20:11.872416 29723 http.cpp:353] Processing call KILL_NESTED_CONTAINER I1110 01:20:11.872597 29726 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d I1110 01:20:11.877090 29726 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 in RUNNING state I1110 01:20:11.877320 29726 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 I1110 01:20:11.962539 29729 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d has exited I1110 01:20:11.963811 29733 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d I1110 01:20:11.963851 29729 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 has exited I1110 01:20:11.964437 29729 containerizer.cpp:2252] Checkpointing termination state to nested container's runtime directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d/termination' I1110 01:20:11.965940 29728 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 I1110 01:20:11.966202 29732 containerizer.cpp:2252] Checkpointing termination state to nested container's runtime directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611/termination' I1110 01:20:11.970046 10231 default_executor.cpp:663] Successfully waited for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d of task 'c96eb523-0365-49b2-8b3b-78976ff28797' in state TASK_KILLED I1110 01:20:11.970501 10231 default_executor.cpp:663] Successfully waited for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 of task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba' in state TASK_KILLED I1110 01:20:11.970559 10231 default_executor.cpp:768] Terminating after 1secs I1110 01:20:11.971288 29723 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor' I1110 01:20:11.972218 29728 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639 I1110 01:20:11.972488 29728 slave.cpp:3740] Handling status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.974179 29719 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor' I1110 01:20:11.975229 29726 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.975278 29729 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639 I1110 01:20:11.975517 29729 slave.cpp:3740] Handling status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.976048 29729 slave.cpp:4075] Status update manager successfully handled status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.978132 29720 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:11.978482 29725 slave.cpp:4075] Status update manager successfully handled status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:12.494274 29725 hierarchical.cpp:1880] Filtered offer with cpus(*):1.7; mem(*):928; disk(*):928; ports(*):[31000-32000] on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:12.494357 29725 hierarchical.cpp:1694] No allocations performed I1110 01:20:12.494402 29725 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:12.494491 29725 hierarchical.cpp:1286] Performed allocation for 1 agents in 915780ns I1110 01:20:13.071280 29734 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06 has exited I1110 01:20:13.071339 29734 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06 in RUNNING state I1110 01:20:13.071746 29734 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06 I1110 01:20:13.076637 29723 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06 I1110 01:20:13.077929 29721 slave.cpp:4672] Executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 exited with status 0 I1110 01:20:13.078433 29732 master.cpp:5884] Executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679): exited with status 0 I1110 01:20:13.078538 29732 master.cpp:7840] Removing executor 'default' with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:13.079448 29730 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):0.2; mem(*):64; disk(*):64) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:13.081049 29723 scheduler.cpp:675] Enqueuing event FAILURE received from http://172.17.0.3:52462/master/api/v1/scheduler  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: failure(0x7fff1aa9a950, @0x2ab91c02dd10 48-byte object <90-62 27-EC B8-2A 00-00 00-00 00-00 00-00 00-00 07-00 00-00 00-00 00-00 10-CE 01-1C B9-2A 00-00 70-CE 02-1C B9-2A 00-00 00-00 00-00 B8-2A 00-00>) Stack trace: I1110 01:20:13.496551 29730 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:13.496680 29730 hierarchical.cpp:1286] Performed allocation for 1 agents in 1.498625ms I1110 01:20:13.497339 29729 master.cpp:6574] Sending 1 offers to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) I1110 01:20:13.499797 29721 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:14.497707 29732 hierarchical.cpp:1694] No allocations performed I1110 01:20:14.497795 29732 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:14.497895 29732 hierarchical.cpp:1286] Performed allocation for 1 agents in 410313ns I1110 01:20:15.499423 29728 hierarchical.cpp:1694] No allocations performed I1110 01:20:15.499526 29728 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:15.499658 29728 hierarchical.cpp:1286] Performed allocation for 1 agents in 547651ns I1110 01:20:16.500463 29729 hierarchical.cpp:1694] No allocations performed I1110 01:20:16.500581 29729 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:16.500699 29729 hierarchical.cpp:1286] Performed allocation for 1 agents in 505442ns I1110 01:20:17.502176 29727 hierarchical.cpp:1694] No allocations performed I1110 01:20:17.502262 29727 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:17.502367 29727 hierarchical.cpp:1286] Performed allocation for 1 agents in 464526ns I1110 01:20:18.503680 29723 hierarchical.cpp:1694] No allocations performed I1110 01:20:18.503762 29723 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:18.503851 29723 hierarchical.cpp:1286] Performed allocation for 1 agents in 425163ns I1110 01:20:19.505476 29723 hierarchical.cpp:1694] No allocations performed I1110 01:20:19.505586 29723 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:19.505705 29723 hierarchical.cpp:1286] Performed allocation for 1 agents in 590762ns I1110 01:20:20.507310 29724 hierarchical.cpp:1694] No allocations performed I1110 01:20:20.507390 29724 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:20.507477 29724 hierarchical.cpp:1286] Performed allocation for 1 agents in 411721ns I1110 01:20:21.508368 29729 hierarchical.cpp:1694] No allocations performed I1110 01:20:21.508458 29729 hierarchical.cpp:1789] No inverse offers to send out! I1110 01:20:21.508564 29729 hierarchical.cpp:1286] Performed allocation for 1 agents in 432440ns W1110 01:20:21.855908 29728 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.856066 29728 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent I1110 01:20:21.856652 29734 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462 W1110 01:20:21.857002 29727 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.857069 29727 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent I1110 01:20:21.857378 29733 master.cpp:5760] Status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:21.857475 29733 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.857662 29722 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462 I1110 01:20:21.858206 29733 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING) I1110 01:20:21.858988 29733 master.cpp:5760] Status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:21.859259 29733 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.859647 29725 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1.9; mem(*):992; disk(*):992; ports(*):[31000-32000]) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.859845 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler I1110 01:20:21.859979 29733 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING) I1110 01:20:21.860970 29729 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1.8; mem(*):960; disk(*):960; ports(*):[31000-32000]) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.861687 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler ../../src/tests/default_executor_tests.cpp:400: Failure Value of: killedUpdate1->status().state()   Actual: TASK_RUNNING Expected: TASK_KILLED I1110 01:20:21.864666 29721 master.cpp:1297] Framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) disconnected I1110 01:20:21.864765 29721 master.cpp:2918] Disconnecting framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) I1110 01:20:21.864820 29721 master.cpp:2942] Deactivating framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) I1110 01:20:21.865016 29732 hierarchical.cpp:386] Deactivated framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 W1110 01:20:21.865586 29721 master.hpp:2264] Master attempted to send message to disconnected framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) W1110 01:20:21.865691 29721 master.hpp:2270] Unable to send event to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default): connection closed I1110 01:20:21.865777 29721 master.cpp:1310] Giving framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) 0ns to failover I1110 01:20:21.866277 29728 hierarchical.cpp:1018] Recovered cpus(*):1.8; mem(*):960; disk(*):960; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {}) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.867295 29733 master.cpp:6426] Framework failover timeout, removing framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) I1110 01:20:21.867328 29733 master.cpp:7170] Removing framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) I1110 01:20:21.867539 29733 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED) I1110 01:20:21.867559 29731 slave.cpp:2575] Asked to shut down framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 by master@172.17.0.3:52462 I1110 01:20:21.867617 29731 slave.cpp:2600] Shutting down framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.867585 29733 master.cpp:7811] Removing task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:21.867707 29731 slave.cpp:4776] Cleaning up executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (via HTTP) I1110 01:20:21.867904 29733 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED) I1110 01:20:21.868042 29729 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' for gc 6.99998999732444days in the future I1110 01:20:21.867939 29733 master.cpp:7811] Removing task c96eb523-0365-49b2-8b3b-78976ff28797 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:21.868232 29731 slave.cpp:4864] Cleaning up framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.868252 29729 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default' for gc 6.99998999732444days in the future I1110 01:20:21.868422 29725 status_update_manager.cpp:285] Closing status update streams for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.868484 29725 status_update_manager.cpp:531] Cleaning up status update stream for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.868777 29721 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000' for gc 6.99998999732444days in the future I1110 01:20:21.868926 29720 hierarchical.cpp:337] Removed framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.869235 29725 status_update_manager.cpp:531] Cleaning up status update stream for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 I1110 01:20:21.870568 29730 slave.cpp:787] Agent terminating I1110 01:20:21.870795 29732 master.cpp:1258] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) disconnected I1110 01:20:21.870825 29732 master.cpp:2977] Disconnecting agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:21.870930 29732 master.cpp:2996] Deactivating agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) I1110 01:20:21.871158 29726 hierarchical.cpp:584] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 deactivated I1110 01:20:21.876986 29733 master.cpp:1097] Master terminating I1110 01:20:21.877754 29724 hierarchical.cpp:517] Removed agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 [  FAILED  ] MesosContainerizer/DefaultExecutorTest.KillTask/0, where GetParam() = mesos (10406 ms) {noformat}",Bug,Major,Resolved,"2016-11-10 01:43:36","2016-11-10 01:43:36",2
"Apache Mesos","JSON serialization should not omit empty arrays in HTTP APIs","When using the JSON content type with the HTTP APIs, a {{repeated}} protobuf field is omitted entirely from the JSON serialization of the message. For example, this is a response to the {{GetTasks}} call:        I think it would be better to include empty arrays for the other fields of the message ({{pending_tasks}}, {{completed_tasks}}, etc.). Advantages:    # Consistency with the old HTTP endpoints, e.g., /state  # Semantically, an empty array is more accurate. The master's response should be interpreted as saying it doesn't know about any pending/completed tasks; that is more accurately conveyed by explicitly including an empty array, not by omitting the key entirely.    *NOTE: The [asV1Protobuf|https://github.com/apache/mesos/blob/d10a33acc426dda9e34db995f16450faf898bb3b/src/common/http.cpp#L172-L423] copy needs to also be updated.*",Improvement,Major,Reviewable,"2016-11-09 20:45:58","2016-11-09 20:45:58",3
"Apache Mesos","Actively Scan for CNI Configurations","Mesos-Agent currently loads the CNI configs into memory at startup. After this point, new configurations that are added will remain unknown to the Mesos Agent process until it is restarted.  This ticket is to request that the Mesos Agent process can the CNI config directory each time it is networking a task, so that modifying, adding, and removing networks will not require a slave reboot.",Improvement,Major,Resolved,"2016-11-09 18:58:52","2016-11-09 18:58:52",3
"Apache Mesos","The default stout stringify always copies its argument","The default implementation of the template {{stringify}} in stout always copies its argument,   For most types implementing a dedicated {{stringify}} we restrict {{T}} to some {{const}} ref with the exception of the specialization for {{bool}},   Copying by default is bad since it requires {{T}} to be copyable without {{stringify}} actually requiring this. It also likely leads to bad performance. It appears switching to e.g.,  and adjusting the {{bool}} specialization would be a general improvement.  This issue was first detected by Coverity in CID 727974 way back on 2012-09-21.",Bug,Major,Resolved,"2016-11-08 09:42:16","2016-11-08 09:42:16",2
"Apache Mesos","Update `MesosContainerizerProcess::_launch()` to pass `ContainerLaunchInfo` to launcher->fork()`","Currently, we receive a bunch of {{ContainerLaunchInfo}} structs from each of our isolators and extract information from them, which we pass one by one to our {{launcher->fork()}} call in separate parameters.  Instead, we should construct a new {{ContainerLaunchInfo}} which is the concatenation of the ones returned by each isolator, and pass this new one down to {{launcher->fork()}} instead of building up individual arguments.",Task,Major,Resolved,"2016-11-04 21:06:58","2016-11-04 21:06:58",1
"Apache Mesos","Add attach/exec commands to the Mesos CLI","After all of this support has landed, we need to update the Mesos CLI to implement {{attach}} and {{exec}} functionality as outlined in the Design Doc",Task,Major,Resolved,"2016-11-04 20:36:20","2016-11-04 20:36:20",5
"Apache Mesos","Update the mesos containerizer to launch per-container I/O switchboards","With the introduction of the new per-container I/O switchboard component, we need to update the mesos containerizer to actually launch one for each container as well as maintain any checkpointed {{pid}} information so it can reattach to it on {{recovery()}}.  As part of this, we will likely move the existing logger logic inside the I/O switchboard and have it own the logger going forward.",Task,Major,Resolved,"2016-11-04 01:18:26","2016-11-04 01:18:26",2
"Apache Mesos","Update the Containerizer to handle attachInput and attachOutput calls.","With the per-container I/O switchboard we are adding, the containerizer should be responsible for both launching the I/O switchboard process, as well as allowing external components to interface with it.  ",Task,Major,Resolved,"2016-11-04 01:13:15","2016-11-04 01:13:15",2
"Apache Mesos","Add special case for entering the mount namespace of a parent container","Currently, tasks launched with the command executor have a hierarchy of processes inside their container that looks as follows:    However, the only pid from this hierarchy of processes that the agent is aware of is the the pid for the top-level {{mesos-containerizer launch}} binary.  If all of these binaries were part of the same set of namespaces, then this would be sufficient to discover the namespaces of the {{task process}} (we could simply inspect the namespaces of the {{mesos-containerizer launch}} pid and know they were the same for the {{task process}}.  This is true for most of the namespaces that each of these processes exist in. However, the {{mnt}} namespace of the two may differ. That is, the {{mesos-containerizer launch}} binary is always in the same {{mnt}} namespace as the host, while the {{task process}} binary may be in its own {{mnt}} namespace if file system isolation is turned on and it has a new rootfs provisioned for it (e.g. a docker image was provided for it).  This has not been a problem until now because we never wanted to simply _enter_ the {{mnt}} namespace of a container before. Even with nested containers for pods, we always create a new {{mnt}} namespace branched off the host {{mnt}} namespace (in order to support the injection of host-mounted volumes).  However, with the new debugging support we are adding, we need a way of entering the {{mnt}} namespace of a parent container instead of cloning a new one.  Since we only have access to the {{pid}} of the container's init process, we can simply enter all namespaces associated with that pid except the {{mnt}} namespace. For the {{mnt}} namespace, we need to special case it to walk the process hierarchy until we find the first process in a different {{mnt}} namespace and enter that one instead. If none are found, simply enter the {{mnt}} namespace of the init process.  This is a dirty dirty hack, but should be sufficient for now.  Eventually we want to completely eliminate the command executor in favor of the pod (i.e. default) executor, which doesn't have this problem at all.",Task,Major,Resolved,"2016-11-03 21:04:24","2016-11-03 21:04:24",2
"Apache Mesos","Pull the current init process for a container out of the container.","Currently the mesos agent is in control of the init process launched inside of a container. However, in order to properly support things like systemd-in-a-container, we need to allow users to control the init process that ultimately gets launched.  We will still need to fork a process equivalent to the current init process, but it shouldn't be placed inside the container itself (instead, it should be the parent process of whatever init process it is directed to launch).  In order to do this properly, we will need to rework some of the logic in {{launcher->fork()}} to allow this new parent process to do the namespace entering / cloning instead of {{launcher->fork()}} itself.",Task,Major,Accepted,"2016-11-03 20:45:02","2016-11-03 20:45:02",3
"Apache Mesos","Pass the forked pid from `containerizer launch` to the agent and checkpoint it.","Right now the agent only knows about the pid of the init process forked by {{launcher->fork()}}. However, in order to properly enter the namespaces of a task for a nested container, we actually need the pid of the process that gets launched by the {{containerizer launch}} binary.  Using this pid, isolators can properly enter the namespaces of the actual *task* or *executor* launched by the {{containerizer launch}} binary instead of just the namespaces of the init process (which may be different).  In order to do this properly, we should pull the init process out of the container and update ",Task,Major,Open,"2016-11-03 17:34:04","2016-11-03 17:34:04",2
"Apache Mesos","Add support for incremental gzip compression.","We currently only support compression assuming the entire input is available at once. We can add a {{gzip::Compressor}} to support incremental compression.",Improvement,Major,Accepted,"2016-11-02 03:00:35","2016-11-02 03:00:35",3
"Apache Mesos","Add support for incremental gzip decompression.","We currently only support compressing and decompressing based on the entire input being available at once. We can add a {{gzip::Decompressor}} to support incremental decompression.",Improvement,Major,Resolved,"2016-11-02 02:59:27","2016-11-02 02:59:27",3
"Apache Mesos","Container status of a task in a pod is not correct.","Currently, the container status is for the top level executor container. This is not ideal. Ideally, we should get the container status for the corresponding nested container and report that with the task status update.",Task,Major,Resolved,"2016-11-02 00:12:02","2016-11-02 00:12:02",3
"Apache Mesos","Memory leak in the libprocess request decoder.","The libprocess decoder can leak a {{Request}} object in cases when a client disconnects while the request is in progress. In such cases, the decoder's destructor won't delete the active {{Request}} object that it had allocated on the heap.  https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/decoder.hpp#L271",Bug,Blocker,Resolved,"2016-11-01 23:13:40","2016-11-01 23:13:40",2
"Apache Mesos","Add API protos for managing debug containers","The API calls that we should add are:  LAUNCH_NESTED_CONTAINER_SESSION ATTACH_CONTAINER_INPUT ATTACH_CONTAINER_OUTPUT",Task,Major,Resolved,"2016-11-01 22:03:24","2016-11-01 22:03:24",3
"Apache Mesos","Command health checks ignore some fields from CommandInfo.","Command health are specified via {{CommandInfo}}, but ignore several fields: {{user}} and {{uris}}. We should either respect these fields or advice users not to use them, i.e. fail validation and/or print warnings.",Improvement,Minor,Open,"2016-11-01 19:59:09","2016-11-01 19:59:09",3
"Apache Mesos",MasterTest.OrphanTasksMultipleAgents,"Observed this on ASF CI.    ",Bug,Major,Resolved,"2016-10-31 21:10:54","2016-10-31 21:10:54",1
"Apache Mesos","Parallel test running does not respect GTEST_FILTER","Normally, you can use {{GTEST_FILTER}} to control which tests will be run by {{make check}}. However, this doesn't currently work if Mesos is configured with {{--enable-parallel-test-execution}}.",Bug,Major,Resolved,"2016-10-31 14:09:19","2016-10-31 14:09:19",2
"Apache Mesos","Show framework info in /state and /frameworks for frameworks that have orphan tasks","Since Mesos 1.0, the master has access to FrameworkInfo of frameworks that have orphan tasks. So we could expose this information in /state and /frameworks endpoints. Note that this information is already present in the v1 operator API.",Improvement,Major,Accepted,"2016-10-29 00:24:54","2016-10-28 23:24:54",3
"Apache Mesos","Use 'geteuid()' for the root privileges check.","Currently, parts of code in Mesos check the root privileges using os::user() to compare to root, which is not sufficient, since it compares the real user. When people change the mesos binary by 'setuid root', the process may not have the right permission to execute.  We should check the effective user id instead in our code. ",Bug,Major,Resolved,"2016-10-28 21:40:51","2016-10-28 20:40:51",3
"Apache Mesos","_version uses incorrect MESOS_{MAJOR,MINOR,PATCH}_VERSION in libmesos java binding.","When the macros were re-assigned they were not flushed fully through the codebase: https://github.com/apache/mesos/commit/6bc6a40a54491cfd733263cd3962e490b0b4bdbb",Bug,Blocker,Resolved,"2016-10-28 20:48:04","2016-10-28 19:48:04",1
"Apache Mesos","Add a test for duplicate framework ids in unregistered_frameworks","For details see MESOS-4973 and MESOS-6461.",Bug,Major,Resolved,"2016-10-28 20:23:56","2016-10-28 19:23:56",2
"Apache Mesos","Java Scheduler Adapter does not surface MasterInfo.","The HTTP adapter does not surface the {{MasterInfo}}. This makes it not compatible with the V0 API where the {{registered}} and {{reregistered}} calls provided the MasterInfo to the framework. cc [~<USER>",Bug,Blocker,Resolved,"2016-10-27 21:26:11","2016-10-27 20:26:11",2
"Apache Mesos","Add test cases for the HTTPS health checks.",,Task,Major,Resolved,"2016-10-27 17:05:44","2016-10-27 16:05:44",3
"Apache Mesos","Build a standalone python client for connecting to our Mock HTTP Server that implements the new Debug APIs","This client prototype should have a similar CLI to what we eventually want to build into the Mesos or DC/OS CLI.  ",Task,Major,Resolved,"2016-10-25 00:08:57","2016-10-24 23:08:57",3
"Apache Mesos","Build a Mock HTTP Server that implements the new Debugging API calls","The mock server should simply launch a process to run whatever command is passed to it, rather than attempt to launch an actual nested container in mesos. However, it should do everything necessary to deal with attaching a {{pty}}  / redirecting {{stdin/stdout/stderr}} properly.",Task,Major,Resolved,"2016-10-25 00:06:55","2016-10-24 23:06:55",3
"Apache Mesos","Add fine-grained ACLs for authorization with the new debugging APIs","We already have ACLs in place for determining if a user has access to see a certain task when querying {{state.json}} on the master/agent, or browse/download a task's sandbox. However, we will have to add similar ACLs for making sure they have the correct permissions to execute the new Debugging APs on behalf of those tasks.  ",Task,Major,Resolved,"2016-10-24 23:47:39","2016-10-24 22:47:39",5
"Apache Mesos","Build support for ATTACH_CONTAINER_OUTPUT into the Agent API in Mesos","Coupled with the ATTACH_CONTAINER_INPUT call, this call will attach a remote client to the the input/output of the entrypoint of a container. All input/output data will be packed into I/O messages and interleaved with control messages sent between a client and the agent. A single chunked request will be used to stream messages to the agent over the input stream, and a single chunked response will be used to stream messages to the client over the output stream.  This call will integrate with the I/O switchboard to stream data between the container and the HTTP stream.",Task,Major,Resolved,"2016-10-24 23:46:51","2016-10-24 22:46:51",5
"Apache Mesos","Build support for ATTACH_CONTAINER_INPUT into the Agent API in Mesos","Coupled with the ATTACH_CONTAINER_OUTPUT call, this call will attach a remote client to the the input/output of the entrypoint of a container. All input/output data will be packed into I/O messages and interleaved with control messages sent between a client and the agent. A single chunked request will be used to stream messages to the agent over the input stream, and a single chunked response will be used to stream messages to the  client over the output stream.  This call will integrate with the I/O switchboard to stream data between the container and the HTTP stream.",Task,Major,Resolved,"2016-10-24 23:46:12","2016-10-24 22:46:12",5
"Apache Mesos","Build support for LAUNCH_NESTED_CONTAINER_SESSION call into the Agent API in Mesos","This HTTP API call will launch a nested container whose life-cycle is tied to the lifetime of the connection used to make this call. Once the agent receives the request, it will hold onto it until the container runs to completion or there is an error. As it holds onto it, it will stream the {{stdout}} and {{stderr}} of the container over the HTTP connection in a streaming response body. This response will mimic the response body returned by a call to ATTACH_NESTED_CONTAINER.  Upon success, the error code of the response will be 200. On error, an appropriate 400 error will be returned. If the connection is ever broken by either the client or the agent, the container will be destroyed. ",Task,Major,Resolved,"2016-10-24 23:45:36","2016-10-24 22:45:36",5
"Apache Mesos","Support TTY in IOSwitchboard.","The idea is to let IOSwitchboard isolator to allocate the pty. The master end will be sent to the IOSwitchboard server while the slavePath will be sent to launch.cpp. launch.cpp will perform login_tty to set the control terminal of the process to the allocated tty. We probably don't want to use login_tty directly because it'll create a new session which is not necessary.  It is unclear if this will need to be added to {{launch.cpp}} so that a pty is allocated at the time that a task is first launched. Or if it makes sense to only allow {{pty}} allocation on-the-fly as we attach to a process (in which case it will need to be the responsibility of the logger in our new model).",Task,Major,Resolved,"2016-10-24 23:44:42","2016-10-24 22:44:42",5
"Apache Mesos","Build an Attach Container Actor","The new agent API calls for ATTACH_CONTAINER_INPUT and ATTACH_CONTAINER_OUTPUT are intimately intertwined. That is, most attach operations will likely want to call both ATTACH_CONTAINER_INPUT and ATTACH_CONTAINER_OUTPUT in order to attach all three of stdin, stdout and stderr to a local terminal.  Moreover, we plan to allow multiple ATTACH_CONTAINER_OUTPUT calls can be made for the same container (i.e. from multiple clients), while only one ATTACH_CONTAINER_INPUT call will be allowed to connect at a time.  In order to ensure that these calls are properly grouped (as well as to ensure that any state they need to share is properly confined), we will lazily launch a “per-container” actor to manage all ATTACH_CONTAINER_OUTPUT and ATTACH_CONTAINER_INPUT calls on behalf of a container.  It will be the responsibility of this actor to:  * Manage the read end of the pipe set up by the HTTP handler for the ATTACH_CONTAINER_INPUT call for a given container.  * Manage the write end of the pipes set up by the HTTP handler for all ATTACH_CONTAINER_OUTPUT calls for a given container.  * Establish a connection to a per-container “I/O switchboard” (discussed below) in order to forward data coming from the ATTACH_CONTAINER_INPUT pipe to the switchboard.  * Establish a second connection to the per-container “I/O switchboard” to stream all stdout data coming from the switchboard to all ATTACH_CONTAINER_OUTPUT pipes.  * Establish a third connection to the per-container “I/O switchboard” to stream all stderr data coming from the switchboard to all ATTACH_CONTAINER_OUTPUT pipes.",Task,Major,Resolved,"2016-10-24 23:43:48","2016-10-24 22:43:48",0
"Apache Mesos","Update Mesos logger components to handle redirection of stdin","Currently the Mesos loggers only handle redirection of {{stdout/stderr}}. We need to update them to also handle redirection of {{stdin}}.",Task,Major,Resolved,"2016-10-24 23:42:52","2016-10-24 22:42:52",3
"Apache Mesos","Build a Container I/O Switchboard","In order to facilitate attach operations for a running container, we plan to introduce a new component into Mesos known as an “I/O switchboard”. The goal of this switchboard is to allow external components to *dynamically* interpose on the {{stdin}}, {{stdout}} and {{stderr}} of the init process of a running Mesos container. It will be implemented as a per-container, stand-alone process launched by the mesos containerizer at the time a container is first launched.  Each per-container switchboard will be responsible for the following:  * Accepting a single dynamic request to register an fd for streaming data to the {{stdin}} of a container’s init process.  * Accepting *multiple* dynamic requests to register fds for streaming data from the {{stdout}} and {{stderr}} of a container’s init process to those fds.  * Allocating a pty for the new process (if requested), and directing data through the master fd of the pty as necessary.  * Passing the *actual* set of file descriptors that should be dup’d onto the {{stdin}}, {{stdout}} and {{stderr}} of a container’s init process back to the containerizer.   The idea being that the switchboard will maintain three asynchronous loops (one each for {{stdin}}, {{stdout}} and {{stderr}}) that constantly pipe data to/from a container’s init process to/from all of the file descriptors that have been dynamically registered with it.",Task,Major,Resolved,"2016-10-24 23:42:03","2016-10-24 22:42:03",3
"Apache Mesos","Add support for streaming HTTP requests in Mesos","We already have support for streaming HTTP responses in Mesos. We now also need to add support for streaming HTTP requests.",Task,Major,Resolved,"2016-10-24 23:41:15","2016-10-24 22:41:15",8
"Apache Mesos","Add a task_id -> container_id mapping in state.json","Currently, there is no way to get the {{container-id}} of a task from hitting the mesos master alone.  You must first hit the master to get the {{task_id -> agent_id}} and {{task_id -> executor_id}} mappings, then hit the corresponding agent with {{agent_id}} to get the {{executor_id -> container_id}} mapping.  It would simplify things alot if the {{container_id}} information was immediately available in the {{/tasks}} and {{/state}} endpoints of the master itself.",Task,Major,Resolved,"2016-10-24 23:40:12","2016-10-24 22:40:12",2
"Apache Mesos","Add fine grained control of which namespaces a nested container should inherit (or not).","We need finer grained control of which namespaces / cgroups a nested container should inherit or not.  Right now, there are some implicit assumptions about which cgroups we enter and which namespaces we inherit when we launch a nested container. For example, under the current semantics, a nested container will always get a new pid namespace but inherit the network namespace from its parent. Moreover, nested containers will always inherit all of the cgroups from their parent (except the freezer cgroup), with no possiblity of choosing any different configuration.  My current thinking is to pass the set of isolators to {{containerizer->launch()} that we would like to have invoked as part of launching a new container. Only if that isolator is enabled (via the agent flags) AND it is passed in via {{launch()}, will it be used to isolate the new container (note that both cgroup isolation as well as namespace membership also implemented using isolators).  This is a sort of a whitelist approach, where we have to know the full set of isolators we want our container launched with ahead of time.  Alternatively, we could consider passing in the set of isolators that we would like *disabled* instead.  This way we could blacklist certain isolators from kicking in, even if they have been enabled via the agent flags.  In both approaches, one major caveat of this is that it will have to become part of the top-level containerizer API, but it is specific only to the universal containerizer. Maybe this is OK as we phase out the docker containerizer anyway.  I am leaning towards the blacklist approach at the moment...",Task,Major,Resolved,"2016-10-24 23:39:06","2016-10-24 22:39:06",2
"Apache Mesos","Build a prototype for remote pty support","Running programs such as vim and gdb require a {{pty}} device to attach to. We need to investigate how to properly handle this when executing these programs on a remote machine. The plan is to build a standalone prototype (i.e. outside of mesos) as a proof-of-concept for to how make this work properly. Once we have a good feel for it, we will back port it into Mesos and integrate it with the rest of the system.  Development is being done here: https://github.com/<USER><USER>remote-pty",Task,Major,Resolved,"2016-10-24 23:37:09","2016-10-24 22:37:09",8
"Apache Mesos","Design Doc: Mesos Support for Container Attach and Container Exec","Here is a link to the design doc: https://docs.google.com/document/d/1nAVr0sSSpbDLrgUlAEB5hKzCl482NSVk8V0D56sFMzU  It is not yet complete, but it is filled out enough to start eliciting feedback. Please feel free to add comments (or even add content!) as you wish.",Task,Major,Resolved,"2016-10-24 23:34:32","2016-10-24 22:34:32",8
"Apache Mesos","Duplicate framework ids in /master/frameworks endpoint 'unregistered_frameworks'.","This issue was exposed from MESOS-6400. There are duplicate framework ids presented from the /master/frameworks endpoint due to:  https://github.com/apache/mesos/blob/master/src/master/http.cpp#L1338  We should use a `set` or a `hashset` instead of an array, to avoid duplicate ids.",Bug,Minor,Resolved,"2016-10-24 23:27:10","2016-10-24 22:27:10",2
"Apache Mesos","PosixRLimitsIsolatorTest.TaskExceedingLimit fails on OS X","This test consistently fails on OS X:  ",Bug,Major,Resolved,"2016-10-24 13:47:28","2016-10-24 12:47:28",2
"Apache Mesos","PosixRLimitsIsolatorTest.TaskExceedingLimit failed on OSX",,Bug,Major,Resolved,"2016-10-23 21:21:59","2016-10-23 20:21:59",2
"Apache Mesos","Display reservations in the agent page in the webui.","We currently do not display the reservations present on an agent in the webui. It would be nice to see this information.  It would also be nice to update the resource statistics tables to make the distinction between unreserved and reserved resources. E.g.  Reserved: Used, Allocated, Available and Total  Unreserved: Used, Allocated, Available and Total",Task,Major,Resolved,"2016-10-21 19:21:44","2016-10-21 18:21:44",3
"Apache Mesos","Roles with quota assigned can game the system to receive excessive resources.","The current implementation of quota allocation attempts to satisfy each resource quota for a role, but in doing so can far exceed the quota assigned to the role.  For example, if a role has quota for {{\[30,20,10\]}}, it can consume up to: {{\[∞, ∞, 10\]}} or {{\[∞, 20, ∞\]}} or {{\[30, ∞, ∞\]}} as only once each resource in the quota vector is satisfied do we stop allocating agent's resources to the role!  As a first step for preventing gaming, we could consider quota satisfied once any of the resources in the vector has quota satisfied. This approach works reasonably well for resources that are required and are present on every agent (cpus, mem, disk). However, it doesn't work well for resources that are optional / only present on some agents (e.g. gpus) (a.k.a. non-ubiquitous / scarce resources). For this we would need to determine which agents have resources that can satisfy the quota prior to performing the allocation.",Bug,Critical,Resolved,"2016-10-20 22:01:27","2016-10-20 21:01:27",5
"Apache Mesos","Add support for port-mapping in `mesos-execute`","Add support to specify port-mappings for a container in mesos-execute.",Task,Major,Resolved,"2016-10-20 19:28:25","2016-10-20 18:28:25",1
"Apache Mesos","The python linter doesn't rebuild the virtual environment before linting when pip-requirements.txt has changed","We need to detect if pip-requirements.txt changes and rebuild the virtual environment if it has.",Bug,Major,Resolved,"2016-10-20 19:10:14","2016-10-20 18:10:14",2
"Apache Mesos","Mesos containerizer helper function signalSafeWriteStatus is not AS-Safe","In {{src/slave/containerizer/mesos/launch.cpp}} a helper function {{signalSafeWriteStatus}} is defined. Its name seems to suggest that this function is safe to call in e.g., signal handlers, and it is used in this file's {{signalHandler}} for exactly that purpose.  Currently this function is not AS-Safe since it e.g., allocates memory via construction of {{string}} instances, and might destructively modify {{errno}}.  We should clean up this function to be in fact AS-Safe.",Bug,Major,Resolved,"2016-10-20 15:49:19","2016-10-20 14:49:19",3
"Apache Mesos","Add documentation for rlimit support of Mesos containerizer",,Improvement,Major,Resolved,"2016-10-20 13:52:59","2016-10-20 12:52:59",3
"Apache Mesos","Add rlimit support to Mesos containerizer","Reviews: https://reviews.apache.org/r/53061/ https://reviews.apache.org/r/53062/ https://reviews.apache.org/r/53063/ https://reviews.apache.org/r/53078/",Improvement,Major,Resolved,"2016-10-20 13:51:57","2016-10-20 12:51:57",8
"Apache Mesos","Possible nullptr dereference in flag loading","Coverity reports the following:   The {{dynamic_cast}} is needed here if the derived {{Flags}} class got intentionally sliced (e.g., to a {{FlagsBase}}). Since the base class of the hierarchy ({{FlagsBase}}) stores the flags they would not be sliced away; the {{dynamic_cast}} here effectively filters out all flags still valid for the {{Flags}} used when the {{Flag}} was {{add}}'ed.  It seems the intention here was to confirm that the {{dynamic_cast}} to {{Flags*}} succeeded like is done e.g., in {{flags.stringify}} and {{flags.validate}} just below.   AFAICT this code has existed since 2013, but was only reported by coverity recently.",Bug,Major,Resolved,"2016-10-20 09:16:35","2016-10-20 08:16:35",2
"Apache Mesos","The 'master/teardown' endpoint should support tearing down 'unregistered_frameworks'.","This issue is exposed from [MESOS-6400](https://issues.apache.org/jira/browse/MESOS-6400). When a user is trying to tear down an 'unregistered_framework' from the 'master/teardown' endpoint, a bad request will be returned: `No framework found with specified ID`.  Ideally, we should support tearing down an unregistered framework, since those frameworks may occur due to network partition, then all the orphan tasks still occupy the resources. It would be a nightmare if a user has to wait until the unregistered framework to get those resources back.  This may be the initial implementation: https://github.com/apache/mesos/commit/bb8375975e92ee722befb478ddc3b2541d1ccaa9",Bug,Critical,Resolved,"2016-10-19 20:41:54","2016-10-19 19:41:54",8
"Apache Mesos","Introduce an extra 'unknown' health check state.","There are three logical states regarding health checks: 1) no health checks; 2) a health check is defined, but no result is available yet; 3) a health check is defined, it is either healthy or not.  Currently, we do not distinguish between 1) and 2), which can be problematic for framework authors.",Improvement,Major,Accepted,"2016-10-19 11:41:21","2016-10-19 10:41:21",5
"Apache Mesos","Add documentation for CNI port-mapper plugin.","Need to add the CNI port-mapper plugin to the CNI documentation within Mesos.",Documentation,Major,Resolved,"2016-10-18 18:24:20","2016-10-18 17:24:20",1
"Apache Mesos","Benchmark call ingestion path on the Mesos master.","[~<USER> reported on the user mailing [list|http://mail-archives.apache.org/mod_mbox/mesos-user/201610.mbox/%3C6B42E374-9AB7-4444-A315-A6558753E08B%40apple.com%3E] that there seems to be a significant regression in performance on the call ingestion path on the Mesos master wrt to the scheduler driver (v0 API).   We should create a benchmark to first get a sense of the numbers and then go about fixing the performance issues. ",Improvement,Critical,Accepted,"2016-10-17 18:01:02","2016-10-17 17:01:02",3
"Apache Mesos","Draft design doc for rlimit support for Mesos containerizer",,Task,Major,Resolved,"2016-10-17 15:53:24","2016-10-17 14:53:24",8
"Apache Mesos","Add rlimit support to Mesos containerizer","We should allow containers to expose their rlimit requirements so that their environment can be set up via Mesos abstractions.",Epic,Major,Resolved,"2016-10-17 15:52:42","2016-10-17 14:52:42",15
"Apache Mesos","HealthChecker sends updates to executor via libprocess messaging.","Currently {{HealthChecker}} sends status updates via libprocess messaging to the executor's UPID. This seems unnecessary after refactoring health checker into the library: a simple callback will do. Moreover, not requiring executor's {{UPID}} will simplify creating a mocked {{HealthChecker}}.",Improvement,Major,Resolved,"2016-10-14 13:33:31","2016-10-14 12:33:31",3
"Apache Mesos","Ensure Python support scripts are linted","Currently {{support/mesos-style.py}} does not lint files under {{support/}}. This is mostly due to the fact that these scripts are too inconsistent style-wise that they wouldn't even pass the linter now.  We should clean up all Python scripts under {{support/}} so they pass the Python linter, and activate that directory in the linter for future additions. ",Improvement,Major,Resolved,"2016-10-13 21:26:15","2016-10-13 20:26:15",3
"Apache Mesos","Report new PARTITION_AWARE task statuses in HTTP endpoints","At a minimum, the {{/state-summary}} endpoint needs to be updated.",Bug,Major,Resolved,"2016-10-13 21:13:13","2016-10-13 20:13:13",1
"Apache Mesos","Reached unreachable statement in LinuxCapabilitiesIsolatorTest","  Observed running the tests as root on CentOS 7.2. Verbose test output attached.",Bug,Minor,Resolved,"2016-10-13 19:09:20","2016-10-13 18:09:20",3
"Apache Mesos","Document how containerization works in terms of entering namespaces / setting up cgroups, etc.","There is currently alot of tribal knowledge in what it means to actually setup a container and launch a process inside of it.  It would be nice to see some documentation produced which outlines the exact process of launching a new container, as well as the process involved in executing a new task inside that container (or as a nested container, which shares some portion of the container, but not all of it).",Task,Minor,Open,"2016-10-13 19:05:44","2016-10-13 18:05:44",5
"Apache Mesos","Add documentation for capabilities support of the mesos containerizer",,Task,Major,Resolved,"2016-10-12 14:33:46","2016-10-12 13:33:46",3
"Apache Mesos","Remove the 'recover()' interface in 'ContainerLogger'.","This issue arises from the nested container support in Mesos.  Currently, the container logger interface mainly contains `recover()` and `prepare()` methods. The `prepare` will be called in containerizer::launch() to launch a container, while `recover` will be called in containerizer::recover() to recover containers. Both methods rely on 2 parameters: ExecutorInfo and sandbox directory. The sandbox directory for nested containers can still be passed to the logger. However, because of nested container support, ExecutorInfo is no longer available for nested containers.  In logger prepare, the ExecutorInfo is used for deliver FrameworkID, ExecutorID, and Label for custom metadata. In containerizer launch, we can still pass the ExecutorInfo of a nested container's top level parent to the logger, so that those information will not be lost.  In logger recover, since currently the logger is stateless, and most of the logger modules are doing `noop` in logger::recover(). The recover interface should exist together with `cleanup` method if the logger become stateful in the future. To avoid adding tech debt in containerizer nested container support, we should remove the `recover` in container logger for now (can add it back together with `cleanup` in the future if the container logger become stateful).",Improvement,Major,Resolved,"2016-10-12 00:27:21","2016-10-11 23:27:21",3
"Apache Mesos","Add a column for FrameworkID when displaying tasks in the WebUI","The Mesos Web UI home page shows a list of active/completed/orphan tasks tasks like this: || ID || Name || State || Started || Host || || | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox | | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox | | 2 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |  When you start multiple frameworks, the task IDs and names show in the UI may be ambiguous, requiring extra clicks/investigation to disambiguate.    In the above case, to disambiguate between the two tasks with ID {{1}}, the user would need to navigate to each sandbox and check the associated frameworkID in the {{/browse}} view.    We could add a column showing the {{FrameworkID}} next to each task: || Framework || ID || Name || State || Started || Host || || | 179b5436-30ec-45e9-b324-fa5c5a1dd756-0000 | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox | | 179b5436-30ec-45e9-b324-fa5c5a1dd756-0001 | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox | | 179b5436-30ec-45e9-b324-fa5c5a1dd756-0001 | 2 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |  The {{FrameworkID}} s could be links to the associated framework  ----- This involves additions to three tables: https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L152-L157 https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L199-L205 https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L246-L252 ",Improvement,Minor,Resolved,"2016-10-11 23:38:13","2016-10-11 22:38:13",1
"Apache Mesos","Design doc for executor authentication",,Task,Major,Resolved,"2016-10-11 20:19:13","2016-10-11 19:19:13",5
"Apache Mesos","Enable partition-awareness in mesos-execute","Helpful for testing.",Improvement,Minor,Resolved,"2016-10-11 17:57:03","2016-10-11 16:57:03",2
"Apache Mesos","Allow `network/cni` isolator unit-tests to run with CNI plugins ","Currently, we don't have any infrastructure to allow for CNI plugins to be used in `network/cni` isolator unit-tests. This forces us to mock CNI plugins that don't use new network namespaces leading to very restricting form of unit-tests.   Especially for port-mapper plugin, in order to test its DNAT functionality it will be very useful if we run the containers in separate network namespace requiring an actual CNI plugin.  The proposal is there to introduce a test filter called CNIPLUGIN, that gets set when CNI_PATH env var is set. Tests using the CNIPLUGIN filter can then use actual CNI plugins in their tests.",Task,Major,Reviewable,"2016-10-10 06:10:26","2016-10-10 05:10:26",1
"Apache Mesos","Allow `network/cni` isolator to take a search path for CNI plugins instead of single directory","Currently the `network/cni` isolator expects a single directory with the `--network_cni_plugins_dir` . This is very limiting because this forces the operator to put all the CNI plugins in the same directory.   With Mesos port-mapper CNI plugin this would also imply that the operator would have to move this plugin from the Mesos installation directory to a directory specified in the `--network_cni_plugins_dir`.   To simplify the operators experience it would make sense for the `--network_cni_plugins_dir` flag to take in set of directories instead of single directory. The `network/cni` isolator can then search this set of directories to find the CNI plugin.",Task,Blocker,Resolved,"2016-10-08 05:30:12","2016-10-08 04:30:12",1
"Apache Mesos","Documentation Error: Default Executor does not implicitly construct resources","https://github.com/apache/mesos/blob/d16f53d5a9e15d1d9533739a8c052bc546ec3262/include/mesos/v1/mesos.proto#L544-L546  This probably got carried forward from early design discussions.",Documentation,Blocker,Resolved,"2016-10-08 03:16:42","2016-10-08 02:16:42",1
"Apache Mesos","CNI should not use `ifconfig` in executors `pre_exec_command`","Currently the `network/cni` isolator sets up the `pre_exec_command` for executors when a container needs to be launched on a non-host network. The `pre_exec_command` is `ifconfig lo up`. This is done to primarily bring loopback up in the new network namespace.  Setting up the `pre_exec_command` to bring loopback up is problematic since the executors PATH variable is generally very limited (doesn't contain all path that the agents PATH variable has due to security concerns).   Therefore instead of running `ifconfig lo up` in the `pre_exec_command` we should run it in `NetworkCniIsolatorSetup` subcommand, which runs with the same PATH variable as the agent.",Bug,Major,Resolved,"2016-10-07 02:07:29","2016-10-07 01:07:29",1
"Apache Mesos","'mesos-containerizer launch' should inherit agent environment variables.","If some dynamic libraries that agent depends on are stored in a non standard location, and the operator starts the agent using LD_LIBRARY_PATH. When we actually fork/exec the 'mesos-containerizer launch' helper, we need to make sure it inherits agent's environment variables. Otherwise, it might throw linking errors. This makes sense because it's a Mesos controlled process.  However, the the helper actually fork/exec the user container (or executor), we need to make sure to strip the agent environment variables.  The tricky case is for default executor and command executor. These two are controlled by Mesos as well, we also want them to have agent environment variables. We need to somehow distinguish this from custom executor case.",Bug,Critical,Resolved,"2016-10-07 01:54:46","2016-10-07 00:54:46",5
"Apache Mesos","Agent fails to kill empty parent container","I launched a pod using Marathon, which led to the launching of a task group on a Mesos agent. The pod spec was flawed, so this led to Marathon repeatedly re-launching multiple instances of the task group. After this went on for a few minutes, I told Marathon to scale the app to 0 instances, which sends {{TASK_KILLED}} for one task in each task group. After this, the Mesos agent reports {{TASK_KILLED}} status updates for all 3 tasks in the pod, but hitting the {{/containers}} endpoint on the agent reveals that the executor container for this task group is still running.  Here is the task group launching on the agent:  and here is the executor container starting:  and here is the output showing the {{TASK_KILLED}} updates for one task group:  however, if we grep the log for the executor's ID, the last line mentioning it is:  so it seems the executor never exited. If we hit the agent's {{/containers}} endpoint, we get output which includes this executor container:  and looking through the output of {{ps}} on the agent, indeed we can locate the executor process:  Looking at the executor's logs, we find stdout is:  and stderr is:  With this short executor log, it seems possible that the agent received the {{TASK_KILLED}} before any tasks were sent to the executor, and the agent removed those tasks from its data structures without terminating the parent container. ",Bug,Blocker,Resolved,"2016-10-07 00:31:03","2016-10-06 23:31:03",3
"Apache Mesos","CHECK failure in HierarchicalAllocatorTest.NoDoubleAccounting","Observed in internal CI:   ",Bug,Major,Resolved,"2016-10-06 19:44:32","2016-10-06 18:44:32",1
"Apache Mesos","Implement clang-tidy check to catch incorrect flags hierarchies","Classes need to always use {{virtual}} inheritance when being derived from {{FlagsBase}}. Also, in order to compose such derived flags they should be inherited virtually again.  Some examples:    We should implement a clang-tidy checker to catch such wrong inheritance issues.",Bug,Major,Resolved,"2016-10-06 16:05:56","2016-10-06 15:05:56",3
"Apache Mesos","Update CHANGELOG to mention addition of agent '--runtime_dir' flag.","We recently introduced a new agent flag for {{\-\-runtime_dir}}. Unlike the {{\-\-work_dir}}, this directory is designed to hold the state of a running agent between subsequent agent-restarts (but not across host reboots).  By default, this flag is set to {{/var/run/mesos}} since this is a {{tempfs}} on linux that gets automatically cleaned up on reboot. When running as non-root we set the default to {{os::temp()/mesos/runtime}}.  We should call this out in the CHAGNELOG",Task,Blocker,Resolved,"2016-10-04 20:56:35","2016-10-04 19:56:35",1
"Apache Mesos","Add authorization support for nested container calls","We need to authorize {LAUNCH, KILL, WAIT}_NESTED_CONTAINER API calls.",Improvement,Major,Resolved,"2016-10-03 20:03:00","2016-10-03 19:03:00",3
"Apache Mesos","Add authentication support to the default executor","The V1 executors should be updated to authenticate with the agent when HTTP executor authentication is enabled. This will be hard-coded into the executor library for the MVP, and it can be refactored into an {{HttpAuthenticatee}} module later. The executor must: * load a JWT from its environment, if present * decorate its requests with an {{Authorization}} header containing the JWT",Improvement,Major,Resolved,"2016-10-03 19:58:45","2016-10-03 18:58:45",2
"Apache Mesos","Agent recovery can fail after nested containers are launched","After launching a nested container which used a Docker image, I restarted the agent which ran that task group and saw the following in the agent logs during recovery:  and the agent continues to restart in this fashion. Attached is the Marathon app definition that I used to launch the task group.",Bug,Blocker,Resolved,"2016-10-01 02:51:36","2016-10-01 01:51:36",3
"Apache Mesos","Recursive destroy in MesosContainerizer is problematic.","    When doing recursive destroy, we should return the collected future of     nested container destroys. Intead, we should fail the corresponding     termination and return that termination if nested container destroys     failed.          In addition, we cannot remove 'Container' struct from the internal map     when the destroy of a nested container failed. This is to ensure that     the top level container do not proceed with destroy if any of its     nested container failed to destroy.",Bug,Major,Resolved,"2016-09-30 21:56:14","2016-09-30 20:56:14",3
"Apache Mesos","A destroyed nested container is not reflected in the parent container's children map.","We should update parent container's children map if it's nested container is terminated.",Bug,Major,Resolved,"2016-09-30 21:54:57","2016-09-30 20:54:57",2
"Apache Mesos","Master doesn't remove task from pending when it is invalid","In `Master::_accept()` there are cases when a task is not launched (e.g, agent disconnected, agent removed or task group is invalid). Doesn't look like we make sure the task is removed from `framework->pending` in this case. In addition to the memory leak reconciliation will result in a scheduler thinking the task is still in TASK_STAGING. Same for operators who are looking at operator endpoints.  Note that pending task not being removed when agent is disconnected/removed is a long standing bug. We should fix it in all the supported versions.   ",Improvement,Blocker,Resolved,"2016-09-30 19:52:23","2016-09-30 18:52:23",2
"Apache Mesos","Default executor should be able to launch multiple task groups","This gives more flexibility for schedulers that do not know all the tasks that they want to launch up front. For example a backup task that needs to be launched regularly next to a main task in the same executor.",Improvement,Major,Resolved,"2016-09-30 18:46:59","2016-09-30 17:46:59",5
"Apache Mesos","HealthCheckTest.HealthyTaskViaHTTPWithoutType fails on some distros.","I see consistent failures of this test in the internal CI in *some* distros, specifically CentOS 6, Ubuntu 14, 15, 16. The source of the health check failure is always the same: {{curl}} cannot connect to the target: ",Bug,Major,Resolved,"2016-09-30 09:27:40","2016-09-30 08:27:40",3
"Apache Mesos","Support nested containers for logger in Mesos Containerizer.","Currently, there are two issues in mesos containerizer using logger for nested contaienrs:  1. An empty executorinfo is passed to logger when launching a nested container, it would potentially break some logger modules if any module tries to access the required proto field (e.g., executorId).  2. The logger does not reocver the nested containers yet in MesosContainerizer::recover.",Bug,Blocker,Resolved,"2016-09-30 02:08:10","2016-09-30 01:08:10",2
"Apache Mesos","The default executor should maintain launcher_dir.","Both command and docker executors require {{launcher_dir}} is provided in a flag. This directory contains mesos binaries, e.g. a tcp checker necessary for TCP health check. The default executor should obtain somehow (a flag, env var) and maintain this directory for health checker to use.",Bug,Major,Resolved,"2016-09-29 23:12:39","2016-09-29 22:12:39",3
"Apache Mesos","Master does not remove an agent if it is responsive but not registered","As part of MESOS-6285, we observed an agent stuck in the recovery phase.  The agent would do the following in a loop: # Systemd starts the agent. # The agent detects the master, but does not connect yet.  The agent needs to recover first. # The agent responds to {{PingSlaveMessage}} from the master, but it is stalled in recovery. # The agent is OOM-killed by the kernel before recovery finishes.  Repeat (1-4).  The consequences of this: * Frameworks will never get a TASK_LOST or terminal status update for tasks on this agent. * Executors on the agent can connect to the agent, but will not be able to register.  We should consider adding some timeout/intervention in the master for responsive, but non-recoverable agents.",Bug,Blocker,Resolved,"2016-09-29 21:23:27","2016-09-29 20:23:27",5
"Apache Mesos","MesosContainerizer should skip non-nesting aware isolators for nested container.","If an isolator is not nesting aware, we should not invoke methods from them for nested containers. This ensures that an old isolator does not get surprises",Task,Blocker,Resolved,"2016-09-29 21:15:14","2016-09-29 20:15:14",3
"Apache Mesos","Task group executor should support command health checks.","Currently, the default (aka pod) executor supports only HTTP and TCP health checks. We should also support command health checks as well.",Improvement,Critical,Resolved,"2016-09-29 17:46:01","2016-09-29 16:46:01",5
"Apache Mesos","Add test cases for the TCP health check.",,Task,Major,Resolved,"2016-09-29 17:39:35","2016-09-29 16:39:35",3
"Apache Mesos","Add test cases for the HTTP health checks.",,Task,Major,Resolved,"2016-09-29 17:38:55","2016-09-29 16:38:55",3
"Apache Mesos","Agent should not allow HTTP executors to re-subscribe before containerizer recovery is done.","In the old API, agent will send a reconnect request to the executor and then the executor will register with the agent.  Now, in the new API, agent will allow an executor to re-subscribe before containerizer recovery is done. This is problematic because containerizer has no idea about the containers yet, calling containerizer->update will lead to a failure, causing the container being killed.  ",Bug,Blocker,Resolved,"2016-09-29 05:57:53","2016-09-29 04:57:53",3
"Apache Mesos","Allow WebUI/other tools to access the task sandbox for a nested container.","Currently, the agent does not expose any information about the {{ContainerID}} -> {{TaskID}} mapping. This makes it very hard for the WebUI/other tools to access construct the path to the task sandbox. As a workaround, one possible approach can be to create a symlink from {{tasks/TaskID}} -> {{containers/ContainerID}}. The WebUI can then construct the path to the task sandbox by following the symbolic link. Eventually, we would like to expose this information on the agent itself.",Task,Major,Resolved,"2016-09-29 03:11:33","2016-09-29 02:11:33",2
"Apache Mesos","Mesos containerizer should figure out the correct sandbox directory for nested launch.","Currently the mesos containerizer take the sandbox directory from the agent. Ideally, a nested sandbox dir can be figured out by the containerizer. And there is no need to pass it from the agent. We should remove the `directory` parameter in nested launch interface.",Bug,Major,Resolved,"2016-09-28 02:12:12","2016-09-28 01:12:12",3
"Apache Mesos","Default executor should kill all other tasks in a task group if any task exits with a non-zero exit status.","The default restart policy for a task group is to kill all active containers if any of the tasks terminates with a non-zero exit status code for now. The default executor needs to honor this default policy.",Bug,Major,Resolved,"2016-09-27 21:48:14","2016-09-27 20:48:14",3
"Apache Mesos","Composing containerizer needs to properly handle nested container launch","Right now if the agent is started with --containerizers=docker,mesos , nested container launches will fail because the composing containerizer doesn't implement the nested `launch` method. This results in it using the default `launch` method defined in the based class, which just returns an Error.",Bug,Blocker,Resolved,"2016-09-27 19:31:26","2016-09-27 18:31:26",3
"Apache Mesos","CNI isolator should not `CHECK` for `resolv.conf` under `rootContainerDir`",,Bug,Blocker,Resolved,"2016-09-27 18:55:07","2016-09-27 17:55:07",1
"Apache Mesos","Libprocess links will not generate an ExitedEvent if the socket creation fails","Noticed this while inspecting nearby code for potential races.  Normally, when a libprocess actor (the linkee) links to a remote process, it does the following: 1) Create a socket. 2) Connect to the remote process (asynchronous). 3) Check the connection succeeded.  If (2) or (3) fail, the linkee will receive a {{ExitedEvent}}, which indicates that the link broke.  In case (1) fails, there is no {{ExitedEvent}}: https://github.com/apache/mesos/blob/7c833abbec9c9e4eb51d67f7a8e7a8d0870825f8/3rdparty/libprocess/src/process.cpp#L1558-L1562",Bug,Major,Resolved,"2016-09-24 01:11:49","2016-09-24 00:11:49",2
"Apache Mesos","Driver based schedulers performing explicit acknowledgements cannot acknowledge updates from HTTP based executors.","It seems that the agent code sets {{StatusUpdate}}->{{slave_id}} but does not set the {{TaskStatus}}->{{slave_id}} if it's not already set. On the driver, when we receive such a status update and if it has explicit ACK enabled, it would pass the {{TaskStatus}} to the scheduler. But, the scheduler has no way of acking this update due to {{slave_id}} not being present. Note that, implicit acknowledgements still work since they use the {{slave_id}} from {{StatusUpdate}}. Hence, we never noticed this in our tests as all of them use implicit acknowledgements on the driver.",Bug,Major,Resolved,"2016-09-23 23:24:10","2016-09-23 22:24:10",3
"Apache Mesos","Launch subprocesses associated with specified namespaces.","Currently there is no standard way in Mesos to launch a child process in a different namespace (e.g. {{net}}, {{mnt}}). A user may leverage {{Subprocess}} and provide its own {{clone}} callback, but this approach is error-prone.  One possible solution is to implement a {{Subprocess}}' child hook. In [MESOS-5070|https://issues.apache.org/jira/browse/MESOS-5070], we have introduced a child hook framework in subprocess and implemented three child hooks {{CHDIR}}, {{SETSID}} and {{SUPERVISOR}}. We suggest to introduce another child hook {{SETNS}} so that other components (e.g., health check) can call it to enter the namespaces of a specific process.",Improvement,Major,Reviewable,"2016-09-23 03:59:43","2016-09-23 02:59:43",8
"Apache Mesos","Add 'argv' variant of 'os::system'","The {{os::system()}} function always spawns whatever string you pass to is a a direct argument to {{sh -c '<arg_string>'}}. However, this can be problematic if you build {{<arg_string>}} from user supplied input and they have the opportunity to inject arbitrary commands at the end of it (e.g. by adding a ; rm -rf as part of the last user supplied argument).  To counter this, we should introduce a variant of {{os::system()}} that takes a single command and a list of args (similar to the {{posix_spawn()}} function.",Task,Major,Resolved,"2016-09-23 03:02:51","2016-09-23 02:02:51",3
"Apache Mesos","Potential socket leak during Zookeeper network changes","There is a potential leak when using the version of {{link}} with {{RemoteConnection::RECONNECT}}.  This was originally implemented to refresh links during master recovery.   The leak occurs here: https://github.com/apache/mesos/blob/5e23edd513caec51ce3e94b3d785d714052525e8/3rdparty/libprocess/src/process.cpp#L1592-L1597 ^ The comment here is not correct, as that is *not* the last reference to the {{existing}} socket.  At this point, the {{existing}} socket may be a perfectly valid link.  Valid links will all have a reference inside a callback loop created here: https://github.com/apache/mesos/blob/5e23edd513caec51ce3e94b3d785d714052525e8/3rdparty/libprocess/src/process.cpp#L1503-L1509  -----  We need to stop the callback loop but prevent any resulting {{ExitedEvents}} from being sent due to stopping the callback loop.  This means discarding the callback loop's future after we have called {{swap_implementing_socket}}.",Bug,Major,Resolved,"2016-09-23 00:35:24","2016-09-22 23:35:24",3
"Apache Mesos","Master CHECK fails during recovery while relinking to other masters","Mesos Version: 1.0.1 OS: CoreOS 1068  ",Bug,Blocker,Resolved,"2016-09-22 22:16:55","2016-09-22 21:16:55",2
"Apache Mesos","Add support for health checks to the default executor.","Currently, there is no health checking mechanism for the tasks in a task group. Ideally, we would like to re-use the existing health checking infrastructure and do health checking for all the tasks in a task group. If one of them, fails we should kill all the tasks in the task group (default policy). We would add support for specifying custom policies in the future.",Task,Major,Resolved,"2016-09-22 21:30:59","2016-09-22 20:30:59",3
"Apache Mesos","Update the default executor to launch/wait/destroy child containers.","Currently, the default executor does not launch an actual task but mocks out a {{TASK_RUNNING}} status update. Eventually, we would like the default executor to send {{LAUNCH/WAIT/DESTROY_NESTED_CONTAINER}} calls to the agent via the Agent API to launch/wait/destroy child containers.",Task,Major,Resolved,"2016-09-22 15:38:43","2016-09-22 14:38:43",8
"Apache Mesos","PAGE_SIZE was not declared in PPC64LE","When compile Mesos in PPC64LE, get this error    ",Bug,Major,Resolved,"2016-09-21 17:22:53","2016-09-21 16:22:53",1
"Apache Mesos","LibeventSSLSocketImpl::create is not safe to call concurrently with os::getenv","{{LibeventSSLSocketImpl::create}} is called whenever a potentially ssl-enabled socket is created. It in turn calls {{openssl::initialize}} which calls a function {{reinitialize}} using {{os::setenv}}. Here {{os::setenv}} is used to set up SSL-related libprocess environment variables {{LIBPROCESS_SSL_*}}.  Since {{os::setenv}} is not thread-safe just like the {{::setenv}} it wraps, any calling of functions like {{os::getenv}} (or via {{os::environment}}) concurrently with the first invocation of {{LibeventSSLSocketImpl::create}} performs unsynchronized r/w access to the same data structure in the runtime.  We usually perform most setup of the environment before we start the libprocess runtime with {{process::initialize}} from a {{main}} function, see e.g., {{src/slave/main.cpp}} or {{src/master/main.cpp}} and others. It appears that we should move the setup of libprocess' SSL environment variables to a similar spot.",Bug,Major,Resolved,"2016-09-21 09:30:02","2016-09-21 08:30:02",5
"Apache Mesos","Containers that use the Mesos containerizer but don't want to provision a container image fail to validate.","Tasks using  features like volumes or CNI in their containers, have to define these in {{TaskInfo.container}}. When these tasks don't want/need to provision a container image, neither {{ContainerInfo.docker}} nor {{ContainerInfo.mesos}} will be set. Nevertheless, the container type in {{ContainerInfo.type}} needs to be set, because it is a required field. In that case, the recently introduced validation rules in {{master/validation.cpp}} ({{validateContainerInfo}} will fail, which isn't expected.",Bug,Major,Resolved,"2016-09-19 15:52:48","2016-09-19 14:52:48",1
"Apache Mesos","Introduce a runtime directory owned by the containerizer for checkpointing container information.","Currently the containerizer optionally checkpoints the container pid into the agent meta directory if requested by the agent. Moveing forward, we would like to have a containerizer-specific runtime directory where the containerizer will checkpoint any container state it would like to maintain independent of the agent container information that it checkpoints.  Currently, this will include the container pid as well as the container exit status. In conjunction with the changes to the launch helper introduced by MESOS-6088, the containerizer will now be able to better recover the container exit status if it happens to be offline at the time that a container finishes.  This includes being able to reap the exit status from a checkpointed location in addition to reading it directly from the container as it exits.  Originally we were planning on pushing some of this functionality down into the LinuxLauncher, but in the end we decided it made more sense to keep all checkpointing information in the containerizer itself.  We will likely revisit this decision in the future. ",Task,Major,Resolved,"2016-09-18 19:29:51","2016-09-18 18:29:51",5
"Apache Mesos","Make the docker/volume isolator nesting aware.",,Task,Major,Resolved,"2016-09-17 19:09:05","2016-09-17 18:09:05",5
"Apache Mesos","Add a virtual method to Isolator to indicate if it supports nesting.",,Task,Major,Resolved,"2016-09-16 23:01:30","2016-09-16 22:01:30",2
"Apache Mesos","Make the `gpu/nvidia` isolator nesting aware",,Task,Major,Resolved,"2016-09-16 19:56:27","2016-09-16 18:56:27",3
"Apache Mesos","Make the generic `cgroups` isolator nesting aware",,Task,Major,Resolved,"2016-09-16 19:55:06","2016-09-16 18:55:06",3
"Apache Mesos","Health checks should use a general mechanism to enter namespaces of the task.","To perform health checks for tasks, we need to enter the corresponding namespaces of the container. For now health check use custom clone to implement this   After the childHooks patches merged, we could change the health check to use childHooks to call {{setns}} and make {{process::defaultClone}} private again.  ",Improvement,Major,Reviewable,"2016-09-16 16:28:02","2016-09-16 15:28:02",3
"Apache Mesos","Introduce global decision policy for unhealthy tasks.","Currently, if the task is deemed unhealthy, i.e. it failed a health check a certain number of times, it is killed by both default executors: [command|https://github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/src/launcher/executor.cpp#L299] and [docker|https://github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/src/docker/executor.cpp#L315]. This is what can be called local kill policy.  While local kill policy can save some network traffic and unload the scheduler, there are cases, when a scheduler may want to decide what—and when—to do. This is what can be called global policy, i.e. the health check library reports whether a health check failed or succeeded, while the executor forwards this update to the scheduler without taking any action.",Improvement,Major,Open,"2016-09-15 15:00:01","2016-09-15 14:00:01",8
"Apache Mesos","Health check grace period covers failures happening after first success.","Currently, the health check library [ignores *all* failures|https://github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/src/health-check/health_checker.cpp#L192-L197] from the task’s start (technically from the health check library initialization) [until after the grace period ends|https://github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/include/mesos/v1/mesos.proto#L403].  This behaviour is misleading. Once the health check succeeds for the first time, grace period rule for failures should not be applied any more.  For example, if the grace period is set to 10 minutes, the task becomes healthy after 1 minute and fails after 2 minutes, the failure should be treated as a normal failure with all the consequences.",Bug,Major,Resolved,"2016-09-15 14:49:44","2016-09-15 13:49:44",5
"Apache Mesos","Add support for cgroups blkio subsystem blkio statistics.","Noted that cgroups blkio subsystem may have performance issue, refer to https://github.com/opencontainers/runc/issues/861",Task,Major,Resolved,"2016-09-15 03:13:15","2016-09-15 02:13:15",8
"Apache Mesos","Remove stout's Set type","stout provides a {{Set}} type which wraps a {{std::set}}. As only addition it provides new constructors,  which simplified creation of a {{Set}} from (up to four) known elements.  C++11 brought {{std::initializer_list}} which can be used to create a {{std::set}} from an arbitrary number of elements, so it appears that it should be possible to retire {{Set}}.",Bug,Minor,Resolved,"2016-09-15 00:06:33","2016-09-14 23:06:33",1
"Apache Mesos","ContainerInfo is not validated.","Currently Mesos does not validate {{ContainerInfo}} provided with {{TaskInfo}} or {{ExecutorInfo}}, hence invalid task configurations can be accepted.",Bug,Blocker,Resolved,"2016-09-14 12:33:24","2016-09-14 11:33:24",3
"Apache Mesos","Make the `network/cni` isolator nesting aware","In pods, child containers share the network and UTS namespace with the parent containers. This implies that during `prepare` and `isolate` the `network/cni` isolator needs to be aware the parent-child relationship between containers to make the following decisions: a) During `prepare` a container should be allocated a new network namespace and UTS namespace only if the container is a top level container. b) During `isolate` the network files (/etc/hosts, /etc/hostname, /etc/resolv.conf) should be created only for top level containers. The network files for child containers will just be symlinks to the parent containers network files.",Task,Major,Resolved,"2016-09-14 00:11:16","2016-09-13 23:11:16",3
"Apache Mesos","Clean up queued tasks if a task group is killed before launch.","We are not properly cleaning up a queued task group when one of its task is killed i.e. https://github.com/apache/mesos/blob/aaa353acc515c0435a859113c9ee236247b51169/src/slave/slave.cpp#L6554 , we clean up the queued task but don't go around cleaning up the queued task group. Also, it would be great to add a test similar to we did for exercising the {{pending}} tasks workflow i.e. {{SlaveTest.KillTaskGroupBetweenRunTaskParts}} for {{queuedTasks}}.",Bug,Major,Resolved,"2016-09-13 13:53:12","2016-09-13 12:53:12",2
"Apache Mesos","Resource leak in slave.cpp.","Coverity detected the following resource leak:   https://scan5.coverity.com/reports.htm#v39597/p10429/fileInstanceId=98881751&defectInstanceId=28450463",Bug,Major,Resolved,"2016-09-13 09:05:42","2016-09-13 08:05:42",1
"Apache Mesos","Resource leak in libevent_ssl_socket.cpp.","Coverity detected the following resource leak. IMO  should be  .     https://scan5.coverity.com/reports.htm#v39597/p10429/fileInstanceId=98881747&defectInstanceId=28450468",Bug,Major,Resolved,"2016-09-13 09:03:25","2016-09-13 08:03:25",2
"Apache Mesos","Populate `CommandInfo` correctly for default executors.","As a follow up to MESOS-6076, we need to ensure that we populate {{CommandInfo}} correctly for the default executor. There are two approaches:  - We can populate the command info and then store it. When the agent (re-)registers with the master, we need to unset it. - Call {{containerizer->launch}} with the modified {{ExecutorInfo}} for default executors and not store them. This has the advantage of getting rid of the book keeping code and getting rid of it when sending it to the master upon (re-)registration. However, we do send the {{ExecutorInfo}} to the executor as part of the {{SUBSCRIBED}} event. However, we don't see the default executor implementation trying to do something with this value.",Task,Major,Resolved,"2016-09-12 22:16:18","2016-09-12 21:16:18",2
"Apache Mesos","Introduce the new isolator recover interface for nested container support.","Currently, the isolator::recover include two parameters: 1. The list of ContainerState, which are the checkpointed conttainers. 2. The hashset of orphans, which are returned from the launcher::recover.  However, to support nested containers in Mesos Pod, this interface is not sufficient. Because unknown nested containers may exist under either the top level alive container or orphan container. We have to include a full list of unknown containers which includes containers from all hierarchy.  We could have added a 3rd parameter to the isolator::recover interface, to guarantee the backward compatibility. However, considering the potential interface changes in the future work and the old orphan hashset should be deprecated, it is the right time to introduce a new protobuf message `ContainerRecoverInfo` for isolator::recover(), which wraps all information for isolators to recover containers.",Task,Major,Resolved,"2016-09-12 18:50:13","2016-09-12 17:50:13",5
"Apache Mesos","Frameworks may RESERVE for an arbitrary role.","The master does not validate that resources from a reservation request have the same role the framework is registered with. As a result, frameworks may reserve resources for arbitrary roles.  I've modified the role in [the {{ReserveThenUnreserve}} test|https://github.com/apache/mesos/blob/bca600cf5602ed8227d91af9f73d689da14ad786/src/tests/reservation_tests.cpp#L117] to yoyo and observed the following in the test's log: ",Bug,Critical,Resolved,"2016-09-08 18:00:08","2016-09-08 17:00:08",3
"Apache Mesos","Some tests do not properly set 'flags.launcher' with the correct value","In some of our tests we manually create a 'PosixLauncher' rather than relying on the value of 'flags.launcher' to decide which type of launcher to create. Since calls to 'CreateSlaveFlags()' set 'flags.launcher' to 'linux' by default, there is a discrepency in what the flags say, and what actual launcher type we are creating.  We should fix this.",Bug,Major,Resolved,"2016-09-08 11:09:07","2016-09-08 10:09:07",2
"Apache Mesos","Add a parallel test runner","In order to allow parallelization of the test execution we should add a parallel test executor to Mesos, and subsequently activate it in the build setup.",Improvement,Major,Resolved,"2016-09-08 10:17:34","2016-09-08 09:17:34",5
"Apache Mesos","ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky","Observed in our internal CI: ",Bug,Major,Open,"2016-09-07 18:36:02","2016-09-07 17:36:02",1
"Apache Mesos","Make the disk usage isolator nesting-aware","With the addition of task groups, the disk usage isolator must be updated. Since sub-container sandboxes are nested within the parent container's sandbox, the isolator must exclude these folders from its usage calculation when examining the parent container's disk usage.",Task,Major,Resolved,"2016-09-06 23:07:12","2016-09-06 22:07:12",3
"Apache Mesos","TCP health checks are not portable.","MESOS-3567 introduced a dependency on bash for TCP health checks, which is undesirable. We should implement a portable solution for TCP health checks.",Bug,Major,Resolved,"2016-09-02 16:07:37","2016-09-02 15:07:37",3
"Apache Mesos","Consider supporting TCP half-open in checks.","A TCP half-open check does not complete the TCP handshake and hence the tested task is not notified that the someone is connecting. This is usually more performant than doing a complete TCP connection.",Bug,Major,Resolved,"2016-09-01 10:40:15","2016-09-01 09:40:15",8
"Apache Mesos","Source tree contains compiled protobuf source","Stout's {{protobuf_tests.cpp}} uses checked in, generated protobuf files {{protobuf_tests.pb.h}} and {{protobuf_tests.pb.cc}}.  These files are * not meant to be edited, * might require updates whenever protobuf is updated, and * likely do not follow Mesos coding standards.  We should try to remove them from the source tree.",Bug,Major,Resolved,"2016-09-01 08:32:34","2016-09-01 07:32:34",3
"Apache Mesos","Deprecate using health checks without setting the type","When sending a task launch using the 1.0.x protos and the legacy (non-http) API, tasks with a healthcheck defined are rejected (TASK_ERROR) because the 'type' field is not set.  This field is marked optional in the proto and is not available before 1.1.0, so it should not be required in order to keep the mesos v1 api compatibility promise.  For backwards compatibility temporarily allow the use case when command health check is set without a type.",Bug,Blocker,Resolved,"2016-08-30 23:17:41","2016-08-30 22:17:41",3
"Apache Mesos","Potential FD double close in libevent's implementation of `sendfile`.","Repro copied from: https://reviews.apache.org/r/51509/  It is possible to make the master CHECK fail by repeatedly hitting the web UI and reloading the static assets:  1) Paste lots of text (16KB or more) of text into `src/webui/master/static/home.html`.  The more text, the more reliable the repro.  2) Start the master with SSL enabled:   3) Run two instances of this python script repeatedly:   i.e.  ",Bug,Critical,Resolved,"2016-08-30 01:51:07","2016-08-30 00:51:07",3
"Apache Mesos","Add Framwork events to master's operator API","Consider the following case: 1) a subscriber connects to master; 2) a new scheduler registered as a new framework; 3) a task is launched from this framework.  In this sequence, subscriber does not have a way to know the FrameworkInfo belonging to the FrameworkId.  We should support an event (e.g. when framework info in master is added/changed).",Task,Major,Resolved,"2016-08-29 17:06:16","2016-08-29 16:06:16",5
"Apache Mesos","Docker containerizer launch command may access a Container struct after it has been destroyed","Per a comment in this review: https://reviews.apache.org/r/51391/#review146735    There are two places in the Docker containerizer where we copy a {{Container*}} by reference into a continuation:  * https://github.com/apache/mesos/blob/71179708650642e145f75e250bf6d6b3eaabb6c5/src/slave/containerizer/docker.cpp#L1192  * https://github.com/apache/mesos/blob/71179708650642e145f75e250bf6d6b3eaabb6c5/src/slave/containerizer/docker.cpp#L1283    If the container is destroyed in the mean time, then we will potentially segfault here.",Bug,Minor,Resolved,"2016-08-25 19:27:46","2016-08-25 18:27:46",2
"Apache Mesos","Update launch helper to checkpoint exit status of launched process.","    Currently the 'mesos-containerizer launch' binary simply execs     into the actual command we wanted to launch after doing some set of     preperatory work. The problem with this approach, however, is that     this gives us no opportunity to checkpoint the exit status of the     command so the agent can recover it in cases where it is offline at     the time the command completes.  We should add support for this      checkpointing.",Improvement,Major,Resolved,"2016-08-25 02:21:31","2016-08-25 01:21:31",5
"Apache Mesos","Deprecate and remove the included MPI framework","The Mesos codebase still includes code for an [MPI|http://www.mcs.anl.gov/research/projects/mpi/] framework.  This code has been untouched and probably not used since around Mesos 0.9.0.  Since we don't support this code anymore, we should deprecate and remove it.    The code is located here:  https://github.com/apache/mesos/tree/db4c8a0e9eaf27f3e2d42a620a5e612863cbf9ea/mpi",Task,Minor,Resolved,"2016-08-24 20:25:31","2016-08-24 19:25:31",1
"Apache Mesos","Added a default (task group) executor.","We would like to build a basic default pod executor that upon receiving a {{LAUNCH_GROUP}} event from the agent, sends a {{TASK_RUNNING}} status update. This would be a good building block for getting to a fully functional pod based default command executor.",Task,Major,Resolved,"2016-08-24 05:48:52","2016-08-24 04:48:52",3
"Apache Mesos","Implement RunTaskGroup handler on the agent.","We need to implement the {{RunTaskGroup}} handler on the agent. This would be similar to the {{RunTask}} handler that already exists except that this would have the relevant logic to send the task group to the executor atomically.  Ideally, we would like to re-use as much pieces of the already existing functionality from the {{runTask()}} handler. We also need to add a state {{queuedTaskGroups}} since it is needed for dispatching queued task groups to the executor upon registration. Also, we should ensure to populate {{queuedTasks}} with the task group information too thereby enabling users to query it via the `/state` endpoint/master reconciliation messages etc.",Task,Major,Resolved,"2016-08-24 05:46:55","2016-08-24 04:46:55",8
"Apache Mesos","Update the streaming function for ContainerID to be nesting aware.","We need to print the hierarchical structure of the nested container. For instance: xxxxx/yyyyy/zzzzzz",Task,Major,Resolved,"2016-08-23 22:14:01","2016-08-23 21:14:01",2
"Apache Mesos","Renamed containerizer::Termination to ContainerTermination.","`containerizer::Termination` is a legacy protobuf for external containerizer. Since we already removed the external containerizer, we should rename it to `ContainerTermination` and moved the definition to `containerizer.proto`. We should also move all definitions in `isolator.proto` to `containerizer.proto` to be more consistent.",Task,Major,Resolved,"2016-08-22 23:27:15","2016-08-22 22:27:15",3
"Apache Mesos","Refactor MesosContainerizer::launch to prepare for nesting support.","The idea is to have a common launch path for both top level executor container and nested containers. That means the parameters to the launch method should be container agnostic.  Then the original launch can just call this common launch code. When we add nesting support later, the same common launch code will be re-used.",Task,Major,Resolved,"2016-08-22 23:16:51","2016-08-22 22:16:51",5
"Apache Mesos","Support provisioner to be nested aware for Mesos Pods.","The provisioner has to be nested aware for sub-container provisioning, as well as recovery and nested container destroy. Better to support multi-level hierarchy. ",Task,Major,Resolved,"2016-08-22 22:58:11","2016-08-22 21:58:11",8
"Apache Mesos","Support provisioning image volumes in an isolator.","Currently the image volumes are provisioned in mesos containerizer. This makes the containerzer logic complicated, and hard to make containerizer launch to be nest aware.  We should implement a 'volume/image' isolator to move these part of logic away from the mesos containerizer.",Improvement,Major,Resolved,"2016-08-22 21:14:13","2016-08-22 20:14:13",5
"Apache Mesos","Combine test helpers into one single binary.","Currently, we have multiple test helper binaries setns-test-helper memory-test-helper active-user-test-helper  And we are adding more: capabilities-test-helper (https://reviews.apache.org/r/50269)  Since most of them already use `Subcommand`, we should combine them into one single binary. That will simply adding a new test helper that has to run in a separate process.",Improvement,Major,Resolved,"2016-08-17 19:28:18","2016-08-17 18:28:18",3
"Apache Mesos","Unable to launch containers on CNI networks on CoreOS","CoreOS does not have an `/etc/hosts`. Currently, in the `network/cni` isolator, if we don't see a `/etc/hosts` on the host filesystem we don't bind mount the containers `hosts` file to this target for the `command executor`. On distros such as CoreOS this fails the container launch since the `libprocess` initialization of the `command executor` fails cause it can't resolve its `hostname`.  We should be creating the `/etc/hosts` and `/etc/hostname` files when they are absent on the host filesystem since creating these files should not affect name resolution on the host network namespace, and it will allow the `/etc/hosts` file to be bind mounted correctly and allow name resolution in the containers network namespace as well. ",Bug,Major,Resolved,"2016-08-17 18:03:49","2016-08-17 17:03:49",1
"Apache Mesos","Add functions to the 'Launcher' abstraction to aid in checkpointing the exit status of containers.","Currently, the launcher does not checkpoint the exit status of a container anywhere.  This will become especially important as we start to add nested container support. In preparation for adding this support, we should add a few helper functions to the Launcher to aid in determining the path where this exit status should be stored.",Improvement,Major,Resolved,"2016-08-17 03:24:39","2016-08-17 02:24:39",3
"Apache Mesos","Add an agent flag for 'runtime_dir'","Currently, a number of agent components hard code a path under '/var/run/mesos' in order to persist runtime information across agent crashes. This path should be configurable.",Improvement,Major,Resolved,"2016-08-16 23:40:44","2016-08-16 22:40:44",2
"Apache Mesos","Document how to start/run the agent in a systemd environment","I am following the getting started guide and built Mesos. I run    {{./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=../../tmp}}  and the startup works fine, I can look at the web interface etc. Then I run    {{./bin/mesos-agent.sh --master=127.0.0.1:5050 --work_dir=../../tmp}}  and this fails with the following exception:    When I add {{--no-systemd_enable_support}} the agent runs fine.",Documentation,Minor,Accepted,"2016-08-16 05:57:35","2016-08-16 04:57:35",1
"Apache Mesos","Add interface for launching nested containers in Containerizer.","See title.",Task,Major,Resolved,"2016-08-15 19:53:47","2016-08-15 18:53:47",2
"Apache Mesos","Validate TaskGroup launch in the master","This might need some refactoring of the task validation that we currently do in master/validation.cpp.",Task,Major,Resolved,"2016-08-15 19:38:02","2016-08-15 18:38:02",5
"Apache Mesos","Update elfio to version 3.2","Some patches I worked on got merged upstream and became part of elfio-3.2. We should upgrade our version bundled with Mesos to 3.2",Improvement,Major,Resolved,"2016-08-15 06:00:04","2016-08-15 05:00:04",1
"Apache Mesos","Offer::Operation.type should be optional","As we add more offer operations, we need to make sure a scheduler sending the new operation to an old master (that doesn't know about the operation) is handled correctly.  To do this, we need to make offer type optional and have a default UNKNOWN type. See MESOS-4997 for additional details.",Improvement,Major,Resolved,"2016-08-12 22:43:06","2016-08-12 21:43:06",2
"Apache Mesos","Define the Framework API protobufs required for TaskGroups","This includes protobufs needed for both Scheduler and Executor APIs.  Note that this does not include protobufs needed by the executor to launch sub containers.",Task,Major,Resolved,"2016-08-12 18:37:32","2016-08-12 17:37:32",3
"Apache Mesos","Scheduler library's reconnection logic should be teardown-aware","The reconnection logic in the scheduler library currently is not aware of any explicit teardown calls sent by the scheduler to the master. This means that after a scheduler has explicitly torn itself down, it will attempt a reconnection to master under the covers, and if this succeeds the scheduler's {{connected}} callback will be invoked. This forces frameworks to implement their own teardown awareness logic in their callbacks.  We should add teardown-awareness to the scheduler library's reconnection code so that framework authors don't have to worry about this.",Improvement,Major,Open,"2016-08-12 00:41:17","2016-08-11 23:41:17",3
"Apache Mesos","Add infrastructure for unit tests in the new python-based CLI.",,Task,Major,Resolved,"2016-08-11 23:25:33","2016-08-11 22:25:33",2
"Apache Mesos","Validate health check protobuf in the master.","When a new task arrives, its {{HealthCheck}} should be validated. This includes: * health check type is set; * health check description corresponds to its type; * {{scheme}} is one of the known values (for http health check);",Improvement,Major,Resolved,"2016-08-10 22:26:08","2016-08-10 21:26:08",3
"Apache Mesos","Create a binary for the port-mapper plugin","The CNI port mapper plugin needs to be a separate binary that will be invoked by the `network/cni` isolator as a CNI plugin.",Task,Blocker,Resolved,"2016-08-10 19:39:24","2016-08-10 18:39:24",8
"Apache Mesos","unit-test for port-mapper CNI plugin","Write unit-tests for the port mapper plugin.",Task,Major,Resolved,"2016-08-10 19:31:21","2016-08-10 18:31:21",3
"Apache Mesos","Consolidate two `Containerizer::launch` methods into one.","Looks like keeping both of them is not necessary.   We can just make `taskInfo` optional.",Task,Major,Resolved,"2016-08-10 19:03:36","2016-08-10 18:03:36",3
"Apache Mesos","Remove `slavePid` from the Containerizer::launch API.","Looks like we only pass in `slavePid` to construct executor environment variables for both containerizers.  We should just construct the executor environment variables in the agent and pass them to Containerizer::launch.  Given that containerizer interface will have to support launching sub-containers. Sub-containers might have a different set of environment variables than the executor.",Task,Major,Resolved,"2016-08-10 18:54:35","2016-08-10 17:54:35",3
"Apache Mesos","Introduce `PortMapping` protobuf.","Currently we have a `PortMapping` message defined for `DockerInfo`. This can be used only by the `DockerContainerizer`. We need to introduce a new Protobuf message in `NetworkInfo` which will allow frameworks to specify port mapping when using CNI with the `MesosContainerizer`.",Task,Major,Resolved,"2016-08-10 00:08:51","2016-08-09 23:08:51",1
"Apache Mesos","Design for port-mapper CNI plugin","Create a design doc for port-mapper CNI plugin.",Task,Major,Resolved,"2016-08-09 22:33:59","2016-08-09 21:33:59",1
"Apache Mesos","Added port mapping CNI plugin.","Currently there is no CNI plugin that supports port mapping. Given that the unified containerizer is starting to become the de-facto container run time, having  a CNI plugin that provides port mapping is a must have. This is primarily required for support BRIDGE networking mode, similar to docker bridge networking that users expect to have when using docker containers.   While the most obvious use case is that of using the port-mapper plugin with the bridge plugin, the port-mapping functionality itself is generic and should be usable with any CNI plugin that needs it.  Keeping port-mapping as a CNI plugin gives operators the ability to use the default port-mapper (CNI plugin) that Mesos provides, or use their own plugin.",Epic,Blocker,Resolved,"2016-08-09 22:28:45","2016-08-09 21:28:45",5
"Apache Mesos","Use readdir instead of readdir_r.","{{readdir_r}} is deprecated in recent versions of glibc (https://sourceware.org/ml/libc-alpha/2016-02/msg00093.html). As a result, Mesos doesn't build on recent Arch Linux:    Seems like {{readdir_r}} is deprecated; manpage suggests using {{readdir}} instead.",Bug,Major,Resolved,"2016-08-09 18:51:01","2016-08-09 17:51:01",3
"Apache Mesos","Add the infrastructure for a new python-based CLI.",,Improvement,Major,Resolved,"2016-08-09 00:38:04","2016-08-08 23:38:04",7
"Apache Mesos","Abstract mesos-style.py to allow future linters to be added more easily.","    Currently, mesos-style.py is just a collection of functions that     check the style of relevant files in the mesos code base.  However,     the script assumes that we always wanted to run cpplint over every     file we are checking. Since we are planning on adding a python linter     to the codebase soon, it makes sense to abstract the common     functionality from this script into a class so that a cpp-based linter     and a python-based linter can inherit the same set of common     functionality.",Improvement,Major,Resolved,"2016-08-08 20:48:51","2016-08-08 19:48:51",2
"Apache Mesos","Aufs backend cannot support the image with numerous layers.","This issue was exposed in this unit test `ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller` by manually specifying the `bind` backend. Most likely mounting the aufs with specific options is limited by string length.  ",Bug,Critical,Resolved,"2016-08-05 23:59:46","2016-08-05 22:59:46",3
"Apache Mesos","Re-evaluate libevent SSL socket EOF semantics in libprocess","While debugging some issues related to libprocess finalization/reinitialization, [~<USER> pointed out that libprocess doesn't strictly adhere to the expected behavior of Unix sockets after an EOF is received. If a socket receives EOF, this means only that the writer on the other end has closed the write end of its socket. However, the other end may still be interested in reading. Libprocess currently treats a received EOF as if {{shutdown()}} has been called on the socket, and both ends have been closed for both reading and writing (see [here|https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L349-L360] and [here|https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/process.cpp#L692-L697]).  We should consider changing the EOF semantics of the {{Socket}} object to more closely match those of Unix sockets.",Bug,Major,Open,"2016-08-05 23:16:00","2016-08-05 22:16:00",3
"Apache Mesos","Protobuf JSON deserialisation does not accept numbers formated as strings","Proto2 does not specify JSON mappings but [Proto3|https://developers.google.com/protocol-buffers/docs/proto3#json] does and it recommend to map 64bit numbers as a string. Unfortunately Mesos does not accepts strings in places of uint64 and return 400 Bad  {quote} Request error Failed to convert JSON into Call protobuf: Not expecting a JSON string for field 'value'. {quote} Is this by purpose or is this a bug?",Bug,Critical,Resolved,"2016-08-05 08:48:40","2016-08-05 07:48:40",1
"Apache Mesos","PollSocketImpl can write to a stale fd.","When tracking down MESOS-5986 with [~<USER> and [~<USER>. We were curious why PollSocketImpl avoids the same issue. It seems that PollSocketImpl has a similar race, however in the case of PollSocketImpl we will simply write to a stale file descriptor.  One example is {{PollSocketImpl::send(const char*, size_t)}}:  https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/poll_socket.cpp#L241-L245   If the last reference to the {{Socket}} goes away before the {{socket_send_data}} loop completes, then we will write to a stale fd!  It turns out that we have avoided this issue because in libprocess we happen to keep a reference to the {{Socket}} around when sending:  https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/process.cpp#L1678-L1707   However, this may not be true in all call-sites going forward. Currently, it appears that http::Connection can trigger this bug.",Bug,Blocker,Resolved,"2016-08-04 19:16:43","2016-08-04 18:16:43",3
"Apache Mesos","Update health check protobuf for HTTP and TCP health check","To support HTTP and TCP health check, we need to update the existing {{HealthCheck}} protobuf message according to [~<USER> and [~gaston] commented in https://reviews.apache.org/r/36816/ and https://reviews.apache.org/r/49360/",Task,Major,Resolved,"2016-08-04 18:17:29","2016-08-04 17:17:29",3
"Apache Mesos","SSL Socket CHECK can fail after socket receives EOF","While writing a test for MESOS-3753, I encountered a bug where [this check|https://github.com/apache/mesos/blob/853821cafcca3550b9c7bdaba5262d73869e2ee1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L708] fails at the very end of the test body, while objects in the stack frame are being destroyed. After adding some debug logging output, I produced the following:   The {{in send()17}} line indicates the beginning of {{send()}} for the SSL socket using FD 17. {{in shutdown(): 17}} indicates the beginning of {{shutdown()}} for the same socket, while {{sending on socket: 17}} indicates the execution of the lambda from {{send()}} on the event loop. Since {{shutdown()}} was called in between the call to {{send()}} and the execution of its lambda, it looks like the {{Socket}} was destroyed before the lambda could run. It's unclear why this would happen, since {{send()}}'s lambda captures a shared copy of the socket's {{this}} pointer in order to keep it alive.",Bug,Blocker,Resolved,"2016-08-04 17:30:13","2016-08-04 16:30:13",3
"Apache Mesos","Number of libprocess worker threads is not configurable for log-rotation module.",,Improvement,Major,Resolved,"2016-08-03 21:40:51","2016-08-03 20:40:51",1
"Apache Mesos","NvidiaVolume errors out if any binary is missing","We currently error out if a binary we were trying to add to the volume is not found on the host filesystem. However, these are not the semantics that we want. By design, we list all the binaries that *may* exist on the filesystem that we want to put in the volume, not all of the binaries that *must* exist. We should simply skip any unfound binaries and move on to the next one instead of erroring out.",Bug,Major,Resolved,"2016-08-03 19:41:26","2016-08-03 18:41:26",2
"Apache Mesos","Remove HTTP_PARSER_VERSION_MAJOR < 2 code in decoder.",https://reviews.apache.org/r/50683,Task,Major,Resolved,"2016-08-02 22:04:44","2016-08-02 21:04:44",1
"Apache Mesos","Linux 'MountInfoTable' entries not sorted as expected","Many places in the codebase assume that the mountinfo table is sorted according to the order: {{parent mount point < child mount point}}.  However, in some cases this may not be true if (for example), a parent mount point (say {{/}}) is remounted to add some extra flags to it.  When this happens, the remounted file system will appear in the mount table at the point where it was remounted.  We actually encountered this problem in the wild for the case of {{/}} being remounted after {{/run}} was mounted -- causing problems in the {{NvidiaVolume}} which assumes the {{parent  < child}} ordering.",Bug,Major,Resolved,"2016-08-02 19:51:43","2016-08-02 18:51:43",3
"Apache Mesos","Add support for 'docker image inspect' in our docker abstraction.","Docker's command line tool for {{docker inspect}} can take either a {{container}}, an {{image}}, or a {{task}} as its argument, and return a JSON array containing low-level information about that container, image or task.   However, the current {{docker inspect}} support in our docker abstraction only supports inspecting containers (not images or tasks).  We should expand this to (at least) support images.  In particular, this additional functionality is motivated by the upcoming GPU support, which needs to inspect the labels in a docker image to decide if it should inject the required Nvidia volumes into a container.  ",Improvement,Major,Reviewable,"2016-08-02 17:20:06","2016-08-02 16:20:06",5
"Apache Mesos","Add libprocess HTTP tests with SSL support","Libprocess contains SSL unit tests which test our SSL support using simple sockets. We should add tests which also make use of libprocess's various HTTP classes and helpers in a variety of SSL configurations.",Task,Major,Resolved,"2016-08-02 17:10:29","2016-08-02 16:10:29",3
"Apache Mesos","Support health policies in health checks.","Currently, Mesos supports a single restart policy (restart policy is a poor name, alternatives are unhealthy policy, health policy): kill a task when it becomes unhealthy. This can be described as local health policy, because the decision is made locally.  In contrast, a framework may want to apply global health policy, which means health status is reported to the scheduler and no action is taken on the agent; the scheduler decides how to act and instructs executor explicitly. A third policy can be mixed: executor does kill tasks without consulting the scheduler, but it nevertheless propagates task health updates to the scheduler.",Improvement,Major,Open,"2016-08-02 13:02:29","2016-08-02 12:02:29",8
"Apache Mesos","HealthChecker should not decide when to kill tasks and when to stop performing health checks.","Currently, {{HealthChecker}} library decides when a task should be killed based on its health status. Moreover, it stops checking it health after that. This seems unfortunate, because it's up to the executor and / or framework to decide both when to kill tasks and when to health check them. ",Bug,Major,Resolved,"2016-08-02 12:58:47","2016-08-02 11:58:47",5
"Apache Mesos","Support multiple health checks per task.","Currently, only a single check and a single health check per task is supported. Consider supporting multiple checks and/or health checks. There are various approaches how to treat them: * do health aggregation in Mesos or delegate it to a frameworks, * have a single or multiple restart policies (one per health check), * introduce health check ids or not.",Improvement,Major,Open,"2016-08-02 12:53:39","2016-08-02 11:53:39",8
"Apache Mesos","HTTP and TCP health checks should support docker executor and bridged mode.","If an executor and a task, e.g. the docker executor and docker container in bridged mode, exist in different network namespaces, HTTP and TCP health checks using {{localhost}} may not work properly. One solution would be to enter the container's network in the health check binary.",Improvement,Major,Resolved,"2016-08-02 11:35:01","2016-08-02 10:35:01",8
"Apache Mesos","All non-root tests fail on GPU machine","A recent addition to ensure that {{NvidiaVolume::create()}} ran as root broke all non-root tests on GPU machines. The reason is that we unconditionally create this volume so long as we detect {{nvml.isAvailable()}} which will fail now that we are only allowed to create this volume if we have root permissions.  We should fix this by adding the proper conditions to determine when / if we should create this volume based on some combination of {{\-\-containerizer}} and {{\-\-isolation}} flags.",Bug,Major,Resolved,"2016-08-01 22:14:12","2016-08-01 21:14:12",2
"Apache Mesos","Reviewbot failing due to python files not being cleaned up after distclean","This is on ASF CI. https://builds.apache.org/job/mesos-reviewbot/14573/consoleFull  ",Bug,Critical,Resolved,"2016-08-01 19:58:44","2016-08-01 18:58:44",2
"Apache Mesos","The mesos-health-check binary is not used anymore.","MESOS-5727 and MESOS-5954 refactored the health check code into the {{HealthChecker}} library, hence the mesos-health-check binary became unused.  While the command and docker executors could just use the library to avoid the subprocess complexity, we may want to consider keeping a binary version that ships with the installation, because the intention of the binary was to allow other executors to re-use our implementation. On the other side, this binary is ill suited to this since it uses libprocess message passing, so if we do not have code that requires the binary it seems ok to remove it for now. Custom executors may use the {{HealthChecker}} library directly, it is not much more complex than using the binary.",Improvement,Major,Resolved,"2016-08-01 11:36:03","2016-08-01 10:36:03",3
"Apache Mesos","Docker executor does not use HealthChecker library.","https://github.com/apache/mesos/commit/1556d9a3a02de4e8a90b5b64d268754f95b12d77 refactored health checks into a library. Command executor uses the library instead of the mesos-health-check binary, docker executor should do the same for consistency.",Improvement,Major,Resolved,"2016-08-01 11:28:34","2016-08-01 10:28:34",3
"Apache Mesos","NvidiaVolume::create() should check for root before creating volume","Without root, we cannot create the nvidia volume in {{/var/run/mesos}} or mount a {{tmpfs}} in cases where we need to override the {{noexec}} on the current file system.",Bug,Major,Resolved,"2016-07-31 19:54:29","2016-07-31 18:54:29",2
"Apache Mesos","Remove `O_SYNC` from StatusUpdateManager logs","Currently the {{StatusUpdateManager}} uses {{O_SYNC}} to flush status updates to disk.   We don't need to use {{O_SYNC}} because we only read this file if the host did not crash. {{os::write}} success implies the kernel will have flushed our data to the page cache. This is sufficient for the recovery scenarios we use this data for.",Improvement,Major,Resolved,"2016-07-31 01:43:35","2016-07-31 00:43:35",1
"Apache Mesos","Incremental http parsing of URLs leads to decoder error","When requests arrive to the decoder in pieces (e.g. {{mes}} followed by a separate chunk of {{os.apache.org}}) the http parser is not able to handle this case if the split is within the URL component.  This causes the decoder to error out, and can lead to connection invalidation.  The scheduler driver is susceptible to this.",Bug,Blocker,Resolved,"2016-07-31 00:28:34","2016-07-30 23:28:34",3
"Apache Mesos","Add upgrade testing to the ASF CI","We should add execution of the {{support/test-upgrade.py}} script to the ASF CI runs. This will require having a build of a previous Mesos version to run against latest master; perhaps we could cache builds of the last stable release somewhere, which could be fetched and executed against CI builds.",Improvement,Major,Accepted,"2016-07-29 22:56:13","2016-07-29 21:56:13",5
"Apache Mesos","Enable the upgrade test script to run multiple masters/agents","The script designed to test upgrades between different Mesos versions, {{support/test-upgrade.py}}, should be improved to test upgrades with multiple masters and agents.",Improvement,Major,Open,"2016-07-29 22:50:55","2016-07-29 21:50:55",3
"Apache Mesos","Support auto backend in Unified Containerizer.","Currently in Unified Containerizer, copy backend will be selected by default. This is not ideal, especially for production environment. It would take a long time to prepare an huge container image to copy it from the store to provisioner.  Ideally, we should support `auto backend`, which would automatically/intelligently select the best/optimal backend for image provisioner if user does not specify one from the agent flag.  We should have a logic design first in this ticket, to determine how we want to choose the right backend (e.g., overlayfs or aufs should be preferred if available from the kernel).",Improvement,Blocker,Resolved,"2016-07-29 19:10:33","2016-07-29 18:10:33",8
"Apache Mesos","Orphan tasks can show up as running after they have finished.","On my cluster I have 111 Orphan Tasks of which some are RUNNING some are FINISHED and some are FAILED. When I open the task details for a FINISHED tasks the following page shows a state of TASK_FINISHED and likewise when I open a FAILED task the details page shows TASK_FAILED.  However when I open the details for the RUNNING tasks they all have a task state of TASK_FINISHED. None of them is in state TASK_RUNNING. ",Bug,Major,Resolved,"2016-07-29 18:30:03","2016-07-29 17:30:03",3
"Apache Mesos","Agent's '--version' flag doesn't work","With the removal of the agent's default {{work_dir}}, the {{--version}} flag no longer works. Instead, the agent complains about the lack of a {{work_dir}} and prints the usage instructions.",Bug,Major,Resolved,"2016-07-29 17:10:22","2016-07-29 16:10:22",1
"Apache Mesos","Unable to run scratch Dockerfiles with Unified Containerizer.","It is not possible to run Docker containers that are based upon the scratch container.  Setup: Mesos 1.0.0 with the following Mesos settings:  {code:none} echo 'docker' | sudo tee /etc/mesos-slave/image_providers echo 'filesystem/linux,docker/runtime' | sudo tee /etc/mesos-slave/isolation   Effect: The container will crash with messages from Mesos reporting it can't mount folder x/y/z. E.g. can't mount /tmp. This means you can't run any container that is not a fat container (i.e. one with a full OS). E.g. error:  bq. Failed to enter chroot '/var/lib/mesos/provisioner/containers/fed6add8-0126-40e6-ae81-5859a0c1a2d4/backends/copy/rootfses/4feefc8b-fd5a-4835-95db-165e675f11cd': /tmp in chroot does not existI0729 07:49:56.753474  4362 exec.cpp:413] Executor asked to shutdown  Expected: Run without issues.  Use case: We use scratch based containers with static binaries to keep the image size down. This is a common practice.",Bug,Major,Resolved,"2016-07-29 08:34:12","2016-07-29 07:34:12",3
"Apache Mesos","Fetcher may print logging error when run as unprivileged user","Now that the fetcher performs its fetching as  the framework's/task's user when one is specified, it prints an error message when its user does not have permissions to create the default glog logging file:   It seems that the fetcher binary is unable to create the default logging file due to a permissions issue. However, when I set the relevant {{GLOG_logtostderr=true}} flag, which should prevent the creation of this default file, it had no effect.  Note that the fetcher's logging output was piped to stdout/stderr as expected, and the task ran and completed successfully, so these errors do not seem to affect the execution of the task.",Bug,Major,Open,"2016-07-29 00:07:31","2016-07-28 23:07:31",2
"Apache Mesos","Ubuntu 14.04 LTS GPU Isolator /run directory is noexec","In Ubuntu 14.04 LTS the mount for /run directory is noexec.  It affect the {{/var/run/mesos/isolators/gpu/nvidia_352.63/bin}} directory which mesos GPU isolators depended on.  {{bill@billz:/var/run$ mount | grep noexec proc on /proc type proc (rw,noexec,nosuid,nodev) sysfs on /sys type sysfs (rw,noexec,nosuid,nodev) devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620) tmpfs on /run type tmpfs (rw,noexec,nosuid,size=10%,mode=0755)}}  The /var/run is link to /run: {{bill@billz:/var$ ll total 52 drwxr-xr-x 13 root root     4096 May  5 20:00 ./ drwxr-xr-x 27 root root     4096 Jul 14 17:29 ../ lrwxrwxrwx  1 root root        9 May  5 19:50 lock -> /run/lock/ drwxrwxr-x 19 root syslog   4096 Jul 28 08:00 log/ drwxr-xr-x  2 root root     4096 Aug  4  2015 opt/ lrwxrwxrwx  1 root root        4 May  5 19:50 run -> /run/}}  Current the work around is mount without noexec: {{sudo mount -o remount,exec /run}}",Bug,Major,Resolved,"2016-07-28 23:50:05","2016-07-28 22:50:05",3
"Apache Mesos","Stout OsTest.User test can fail on some systems","Libc call {{getgrouplist}} doesn't return the {{gid}} list in a sorted manner (in my case, it's returning 471 100) ... whereas {{id -G}} return a sorted list (100 471 in my case) causing the validation inside the loop to fail.  We should sort both lists before comparing the values.",Bug,Major,Resolved,"2016-07-26 21:19:58","2016-07-26 20:19:58",2
"Apache Mesos","ExamplesTest.DiskFullFramework fails on Arch","This test fails consistently on recent Arch linux, running in a VM.",Bug,Major,Resolved,"2016-07-26 17:29:50","2016-07-26 16:29:50",1
"Apache Mesos","Process routes implementation seems to drop routes on Windows.","In several libprocess tests, the routes set up by process.cpp seem to be getting mangled/dropped on Windows. Specifically:    * HTTPTest.Endpoints help has the route '/help/(14)', but '/help/(14)/body' fails; on HTTPTest.EndpointsHelpRemoval, the former can be removed, but when the latter is attempted, it fails.  * HTTPTest.NestedGet will properly generate the route '/a/b/c', but '/a/b' is missing.  * ProcessTest.FirewallDisabledPaths, FIrewallUninstall seem to fail to create firewall rules.",Bug,Major,Resolved,"2016-07-25 22:20:18","2016-07-25 21:20:18",1
"Apache Mesos","Make the command executor unversioned","Currently, the command executor in {{src/launcher/executor.cpp}} is in the {{v1}} namespace. As referenced in the versioning design doc, we had agreed to keep the mesos internal code in the unversioned namespace and use {{evolve/devolve}} helpers for requests/responses.   Following this pattern, we should bring the command executor in the {{mesos::internal}} namespace.",Improvement,Major,Resolved,"2016-07-25 19:45:02","2016-07-25 18:45:02",2
"Apache Mesos","Support Unix domain socket connections in libprocess","We should consider allowing two programs on the same host using libprocess to communicate via Unix domain sockets rather than TCP. This has a few advantages:  * Security: remote hosts cannot connect to the Unix socket. Domain sockets also offer additional support for [authentication|https://docs.fedoraproject.org/en-US/Fedora_Security_Team/1/html/Defensive_Coding/sect-Defensive_Coding-Authentication-UNIX_Domain.html]. * Performance: domain sockets are marginally faster than localhost TCP.",Improvement,Major,Resolved,"2016-07-25 14:20:20","2016-07-25 13:20:20",5
"Apache Mesos","/help endpoint does not set Content-Type to HTML","This change added a default {{Content-Type}} to all responses: https://github.com/apache/mesos/commit/b2c5d91addbae609af3791f128c53fb3a26c7d53  Unfortunately, this changed the {{/help}} endpoint from no {{Content-Type}} to {{text/plain}}.  For a browser to render this page correctly, we need an HTML content type.",Bug,Critical,Resolved,"2016-07-22 23:53:36","2016-07-22 22:53:36",1
"Apache Mesos","Enhance DispatchEvent to include demangled method name.","Currently, [{{DispatchEvent}}|https://github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/event.hpp#L148] does not include any user-friendly information about the actual method being dispatched. This can be helpful in order to simplify triaging and debugging, e.g., using {{\_\_processes\_\_}} endpoint. Now we print the [event type only|https://github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/src/process.cpp#L3198-L3203].",Improvement,Major,Open,"2016-07-22 11:22:04","2016-07-22 10:22:04",5
"Apache Mesos","FUTURE_DISPATCH may react on irrelevant dispatch.","[{{FUTURE_DISPATCH}}|https://github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/gmock.hpp#L50] uses [{{DispatchMatcher}}|https://github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/gmock.hpp#L350] to figure out whether a processed {{DispatchEvent}} is the same the user is waiting for. However, comparing {{std::type_info}} of function pointers is not enough: different class methods with same signatures will be matched. Here is the test that proves this:   The test passes:   This change was introduced in https://reviews.apache.org/r/28052/.",Bug,Major,Accepted,"2016-07-22 11:12:20","2016-07-22 10:12:20",5
"Apache Mesos","`os::cloexec` does not exist on Windows","`os::cloexec` does not work on Windows. It will never work at the OS level. Because of this, there are likely many important and hard-to-detect bugs hanging around the agent.    This is extremely important to fix. Some possible solutions to investigate (some of which are _extremely_ risky):    * Abstract out file descriptors into a class, implement cloexec in that class on Windows (since we can't rely on the OS to do it).  * Refactor all the code that relies on `os::cloexec` to not rely on it.    Of the two, the first seems less risky in the short term, because the cloexec code only affects Windows. Depending on the semantics of the implementation of the `FileDescriptor` class, it is possible that this is riskier to Windows in the longer term, as the semantics of `cloexec` may have subtle difference between Linux and Windows.",Bug,Major,Resolved,"2016-07-21 19:25:55","2016-07-21 18:25:55",2
"Apache Mesos","cgroups/net_cls isolator causing agent recovery issues","We run with 'cgroups/net_cls' in our isolator list, and when we restart any agent process in a cluster running an experimental custom isolator as well, the agents are unable to recover from checkpoint, because net_cls reports that unknown orphan containers have duplicate net_cls handles.  While this is a problem that needs to be solved (probably by fixing our custom isolator), it's also a problem that the net_cls isolator fails recovery just for duplicate handles in cgroups that it is literally about to unconditionally destroy during recovery. Can this be fixed?",Bug,Major,Resolved,"2016-07-21 17:32:32","2016-07-21 16:32:32",1
"Apache Mesos","Strict/RegistrarTest.UpdateQuota/0 is flaky","Observed on ASF CI (https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2539/consoleFull). Log file is attached. Note that this might have been uncovered due to the recent removal of {{os::sleep}} from {{Clock::settle}}.",Bug,Major,Resolved,"2016-07-21 13:14:35","2016-07-21 12:14:35",3
"Apache Mesos","Only send ShutdownFrameworkMessage to agents associated with framework.","slave.cpp:2079] Asked to shut down framework ${framework} by master@${master} slave.cpp:2094] Cannot shut down unknown framework ${framework}   For high framework/churn clusters this saturates agent logs with these messages. When a framework terminates a ShutdownFrameworkMessage is sent to every registered slave in a for loop. This patch proposes sending this message to agents with executors associated with the framework.   Also proposed is moving the logline to VLOG(1). ",Improvement,Minor,Open,"2016-07-20 23:05:08","2016-07-20 22:05:08",1
"Apache Mesos","Document MESOS_SANDBOX executor env variable.","And we should document the difference with MESOS_DIRECTORY.",Bug,Major,Resolved,"2016-07-19 22:53:25","2016-07-19 21:53:25",2
"Apache Mesos","Enabling SSL causes fetcher fail to fetch from HTTPS sites.","This is because curl (which fetcher relies on) also relies on some of the environment variables used by libprocess SSL support. For instance, `SSL_CERT_FILE`. If the operator sets `SSL_CERT_FILE` env var for Mesos agent, the fetcher will inherit this env var and cause curl to fail:    To solve this problem, we deprecated the existing `SSL_` env variables and used `LIBPROCESS_SSL_` instead. To be backward compatible, we still accept `SSL_` env variables for the time being.",Bug,Major,Resolved,"2016-07-19 18:16:04","2016-07-19 17:16:04",3
"Apache Mesos","Logrotate ContainerLogger module does not rotate logs when run as root with `--switch_user`.","The logrotate ContainerLogger module runs as the agent's user.  In most cases, this is {{root}}.  When {{logrotate}} is run as root, there is an additional check the configuration files must pass (because a root {{logrotate}} needs to be secured against non-root modifications to the configuration): https://github.com/logrotate/logrotate/blob/fe80cb51a2571ca35b1a7c8ba0695db5a68feaba/config.c#L807-L815  Log rotation will fail under the following scenario: 1) The agent is run with {{--switch_user}} (default: true) 2) A task is launched with a non-root user specified 3) The logrotate module spawns a few companion processes (as root) and this creates the {{stdout}}, {{stderr}}, {{stdout.logrotate.conf}}, and {{stderr.logrotate.conf}} files (as root).  This step races with the next step. 4) The Mesos containerizer and Fetcher will {{chown}} the task's sandbox to the non-root user.  Including the files just created. 5) When {{logrotate}} is run, it will skip any non-root configuration files.  This means the files are not rotated.  ----  Fix: The logrotate module's companion processes should call {{setuid}} and {{setgid}}.",Bug,Critical,Resolved,"2016-07-19 01:43:22","2016-07-19 00:43:22",3
"Apache Mesos","Create a 'Disk (not) full' example framework","We need example frameworks for verifying the correct behavior of posix/disk isolator when the disk quota enforcement is in place. One framework for verifying that disk quota enforcement is working and that container gets terminated when it goes beyond disk quota, and another one for verifying that container does not get killed if it stays within its disk quota bounds.  ",Task,Minor,Resolved,"2016-07-17 04:27:26","2016-07-17 03:27:26",3
"Apache Mesos","CMake build needs to generate protobufs before building libmesos","The existing CMake lists place protobufs at the same level as other Mesos sources: https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/CMakeLists.txt#L415  This is incorrect, as protobuf changes need to be regenerated before we can build against them.  Note: in the autotools build, this is done by compiling protobufs into {{libmesos}}, which then builds {{libmesos_no_3rdparty}}: https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/Makefile.am#L1304-L1305",Bug,Major,Resolved,"2016-07-15 19:14:59","2016-07-15 18:14:59",2
"Apache Mesos","Add a test that runs the 'mesos-local' binary","The balloon framework test runs the Mesos master and agent binaries, but we don't seem to have any tests which run the {{mesos-local}} binary at the moment. Such a test should be added, or one of the existing example framework tests could be modified to accomplish this.",Task,Major,Resolved,"2016-07-15 18:33:39","2016-07-15 17:33:39",2
"Apache Mesos","Docker health checks are malformed.","When wrapping the health check command into {{docker exec}}, docker executor erroneously forms the health check command itself. Here is an excerpt from an executor log: ",Bug,Major,Resolved,"2016-07-15 11:55:48","2016-07-15 10:55:48",1
"Apache Mesos","The fetcher can access any local file as root","The Mesos fetcher currently runs as root and does a blind cp+chown of any file:// URI into the task's sandbox, to be owned by the task user. Even if frameworks are restricted from running tasks as root, it seems they can still access root-protected files in this way. We should secure the fetcher so that it has the filesystem permissions of the user its associated task is being run as. One option would be to run the fetcher as the same user that the task will run as.",Bug,Major,Resolved,"2016-07-14 01:39:09","2016-07-14 00:39:09",3
"Apache Mesos","PersistentVolumeEndpointsTest.OfferCreateThenEndpointRemove test is flaky","Observed on ASF CI: https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2497/changes  ",Bug,Major,Resolved,"2016-07-14 00:05:14","2016-07-13 23:05:14",1
"Apache Mesos","Clean up `FlagsBase::add`","In the definition for {{FlagsBase}}, we currently have 20 overloads for the {{FlagsBase::add}} function. This makes both the {{FlagsBase}} class definition and the {{flags.cpp}} files in Mesos difficult to read. We should clean up {{FlagsBase::add}} so that it does not require so many overloads.",Improvement,Major,Open,"2016-07-13 19:03:35","2016-07-13 18:03:35",3
"Apache Mesos","Modularize Network in replicated_log","Currently replicated_log relies on Zookeeper for coordinator election. This is done through network abstraction _ZookeeperNetwork_. We need to modularize this part in order to enable replicated_log when using Master contender/detector modules.",Bug,Major,Reviewable,"2016-07-10 11:08:11","2016-07-10 10:08:11",8
"Apache Mesos","Add example framework for using inverse offers","We should have an example framework (in src/examples) demonstrating how to handle inverse offers. ",Task,Minor,Resolved,"2016-07-10 09:34:46","2016-07-10 08:34:46",3
"Apache Mesos","Support mounting image volume in mesos containerizer.","Mesos containerizer should be able to support mounting image volume type. Specifically, both image rootfs and default manifest should be reachable inside container's mount namespace.",Improvement,Major,Resolved,"2016-07-09 00:04:01","2016-07-08 23:04:01",5
"Apache Mesos","Include disk source information in stringification","Some frameworks (like kafka_mesos) ignore the Source field when trying to reserve an offered mount or path persistent volume; the resulting error message is bewildering:  {code:none} Task uses more resources cpus(*):4; mem(*):4096;     ports(*):[31000-31000]; disk(kafka, kafka)[kafka_0:data]:960679 than available cpus(*):32; mem(*):256819;  ports(*):[31000-32000]; disk(kafka, kafka)[kafka_0:data]:960679;   disk(*):240169; {code}  The stringification of disk resources should include source information. ",Improvement,Minor,Resolved,"2016-07-08 23:58:47","2016-07-08 22:58:47",3
"Apache Mesos","Add a build script for the Windows CI","The ASF CI for Mesos runs a script that lives inside the Mesos codebase: https://github.com/apache/mesos/blob/1cbfdc3c1e4b8498a67f8531ab264003c8c19fb1/support/docker_build.sh  ASF Infrastructure have set up a machine that we can use for building Mesos on Windows.  Considering the environment, we will need a separate script to build here.",Improvement,Major,Resolved,"2016-07-08 23:52:29","2016-07-08 22:52:29",3
"Apache Mesos","Port libprocess http_tests.cpp","The test cases:   * HTTPTest, EndpointsHelp   * HTTPTest, EndpointsHelpRemoval   * HTTPTest, NestedGet   * -HTTPTest, QueryEncodeDecode-    Are disabled in the Windows build because they fail.",Task,Major,Resolved,"2016-07-08 21:42:46","2016-07-08 20:42:46",1
"Apache Mesos","MasterAPITest.Subscribe is flaky","This test seems to be flaky, although on Mac OS X and CentOS 7 the error a bit different.  On Mac OS X:   On CentOS 7 ",Bug,Major,Resolved,"2016-07-08 19:48:57","2016-07-08 18:48:57",3
"Apache Mesos","CNI isolator should prepare network related /etc/* files for containers using host mode but specify container images.","Currently, the CNI isolator will just ignore those containers that want to join the host network (i.e., not specifying NetworkInfo). However, if the container specifies a container image, we need to make sure that it has access to host /etc/* files. We should perform the bind mount for the container. This is also what docker does when a container is running in host mode.",Bug,Major,Resolved,"2016-07-08 01:10:11","2016-07-08 00:10:11",5
"Apache Mesos","ExamplesTest.DynamicReservationFramework is flaky","Showed up on ASF CI:  https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2466/changes        Logs from a previous good run:  ",Bug,Major,Resolved,"2016-07-07 21:15:53","2016-07-07 20:15:53",3
"Apache Mesos","SlaveAuthorizerTest/0.ViewFlags is flaky.",,Bug,Major,Resolved,"2016-07-07 16:53:59","2016-07-07 15:53:59",2
"Apache Mesos","Add ability to inject Nvidia devices into a container",,Improvement,Major,Resolved,"2016-07-06 01:04:52","2016-07-06 00:04:52",3
"Apache Mesos","Add mesos tests to CMake (make check)","Provide CMakeLists.txt and configuration files to build mesos tests using CMake.",Improvement,Major,Resolved,"2016-07-05 23:54:07","2016-07-05 22:54:07",8
"Apache Mesos","Added JAVA API adapter for seamless transition to new scheduler API.","Currently, for existing JAVA based frameworks, moving to try out the new API can be cumbersome. This change intends to introduce a shim/adapter interface that makes this easier by allowing to toggle between the old/new API (driver/new scheduler library) implementation via an environment variable. This would allow framework developers to transition their older frameworks to the new API rather seamlessly.  This would look similar to the work done for the executor shim for C++ (command/docker executor). ",Task,Major,Resolved,"2016-07-05 23:01:25","2016-07-05 22:01:25",8
"Apache Mesos","Add ability to set framework capabilities in 'mesos-execute'","For now, we want to add this so that we can run {{mesos-execute}} against agents that offer GPU resources. In the future, as we add more framework capabilities, this functionality will become more generally useful.",Improvement,Major,Resolved,"2016-07-05 22:19:11","2016-07-05 21:19:11",2
"Apache Mesos","Renamed 'commands' to 'pre_exec_commands' in ContainerLaunchInfo.","Currently the 'commands' in isolator.proto ContainerLaunchInfo is somehow confusing. It is a pre-executed command (can be any script or shell command) before launch. We should renamed 'commands' to 'pre_exec_commands' in ContainerLaunchInfo and add comments.",Improvement,Major,Resolved,"2016-07-05 07:51:43","2016-07-05 06:51:43",2
"Apache Mesos","Allow Docker v1 ImageManifests to be parsed from the output of `docker inspect`","    The `docker::spec::v1::ImageManifest` protobuf implements the     official v1 image manifest specification found at:          https://github.com/docker/docker/blob/master/image/spec/v1.md          The field names in this spec are all written in snake_case as are the     field names of the JSON representing the image manifest when reading     it from disk (for example after performing a `docker save`). As such,     the protobuf for ImageManifest also provides these fields in     snake_case. Unfortunately, the `docker inspect` command also provides     a method of retrieving the JSON for an image manifest, with one major     caveat -- it represents all of its top level keys in CamelCase.          To allow both representations to be parsed in the same way, we     should intercept the incoming JSON from either source (disk or `docker     inspect`) and convert it to a canonical snake_case representation.",Improvement,Major,Reviewable,"2016-07-04 18:56:00","2016-07-04 17:56:00",3
"Apache Mesos","Add get_abi_version() to ELF abstraction in stout","This function allows us to inspect the {{.note.ABI-tag}} section of an ELF binary to determine the ABI version of the executable / library.  This is needed for checking soe of the logic in building up an NvidiaVolume for injection into a container. ",Improvement,Major,Resolved,"2016-07-02 20:37:35","2016-07-02 19:37:35",2
"Apache Mesos","Reimplement the stout ELF abstraction in terms of ELFIO","With the introduction of the new bundled ELFIO library, we need to reimplement our stout ELF abstraction in terms of it. As part of this, we need to update the tests that use it (i.e. ldcache_test.cpp)",Improvement,Major,Resolved,"2016-07-02 20:30:50","2016-07-02 19:30:50",2
"Apache Mesos","Add ELFIO as bundled Dependency to Mesos","ELFIO is a header-only replacement for parsing ELF binaries. Previously we were using libelf, which introduced both a new build-time dependency as well as a runtime dependence even though we only really needed this library when operating on machines that have GPUs.  By using this header-only library and bundling it with Mesos, we can remove this external dependence altogether.",Improvement,Major,Resolved,"2016-07-02 20:23:00","2016-07-02 19:23:00",2
"Apache Mesos","Missing License Information for Bundled NVML headers","See Summary",Bug,Major,Resolved,"2016-07-02 20:18:08","2016-07-02 19:18:08",1
"Apache Mesos","Add 'systemGetDriverVersion' to NVML abstraction.","This command returns a string representing the version of the underlying Nvidia drivers installed on a host. It will be used by the upcoming {{NvidiaVolume}} component.",Improvement,Major,Resolved,"2016-07-01 22:39:09","2016-07-01 21:39:09",2
"Apache Mesos","Improve the logic of orphan tasks","Right now, a task is called orphaned if an agent re-registers with it but the corresponding framework information is not known to the master. This happens immediately after a master failover.  It would great if the master knows the information about the framework even after a failover, irrespective of whether a framework re-registers, so that we don't have orphan tasks. Getting rid of orphan tasks will make the task authorization story easy (see MESOS-5757).",Improvement,Major,Resolved,"2016-07-01 07:28:35","2016-07-01 06:28:35",5
"Apache Mesos","ProcessRemoteLinkTest.RemoteUseStaleLink and RemoteStaleLinkRelink are flaky","{{ProcessRemoteLinkTest.RemoteUseStaleLink}} and {{ProcessRemoteLinkTest.RemoteStaleLinkRelink}} are failing occasionally with the error:   There appears to be a race between establishing a socket connection and the test calling {{::shutdown}} on the socket.  Under some circumstances, the {{::shutdown}} may actually result in failing the future in {{SocketManager::link_connect}} error and thereby trigger {{SocketManager::close}}.",Bug,Major,Resolved,"2016-07-01 02:07:17","2016-07-01 01:07:17",1
"Apache Mesos","NVML headers are not installed as part of 3rdparty install with --enable-install-module-dependencies","Review: https://reviews.apache.org/r/49480/ ",Bug,Major,Resolved,"2016-07-01 00:25:19","2016-06-30 23:25:19",2
"Apache Mesos","CommandInfo.user not honored in docker containerizer","Repro by creating a framework that starts a task with CommandInfo.user set, and observe that the dockerized executor is still running as the default (e.g. root).  cc [~<USER>",Improvement,Major,Accepted,"2016-06-30 22:44:31","2016-06-30 21:44:31",3
"Apache Mesos","Command executor should use `mesos-containerizer launch` to launch user task.","Currently, command executor and `mesos-containerizer launch` share a lot of the logic. Command executor should in fact, just use `mesos-containerizer launch` to launch the user task.  Potentially, `mesos-containerizer launch` can be also used by custom executor to launch user tasks.",Improvement,Major,Resolved,"2016-06-30 22:22:11","2016-06-30 21:22:11",8
"Apache Mesos","Potential segfault in `link` and `send` when linking to a remote process","There is a race in the SocketManager, between a remote {{link}} and disconnection of the underlying socket.  We potentially segfault here: https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1512  {{\*socket}} dereferences the shared pointer underpinning the {{Socket*}} object.  However, the code above this line actually has ownership of the pointer: https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1494-L1499  If the socket dies during the link, the {{ignore_recv_data}} may delete the Socket underneath {{link}}: https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1399-L1411  ---- The same race exists for {{send}}.  This race was discovered while running a new test in repetition: https://reviews.apache.org/r/49175/  On OSX, I hit the race consistently every 500-800 repetitions: ",Bug,Major,Resolved,"2016-06-30 00:49:54","2016-06-29 23:49:54",2
"Apache Mesos","AuthenticationTest.UnauthenticatedSlave fails with clang++3.8","With {{clang++-3.8}}, {{make check}} fails with the following message:  ",Bug,Major,Resolved,"2016-06-29 15:15:04","2016-06-29 14:15:04",3
"Apache Mesos","When start an agent with `--resources`, the GPU resource can be fractional","So far, the GPU resource is not fractional, only integer values are allowed. But when starting agents with {{\-\-resources='gpu:1.2'}}, it can also work without any warning or error. And in the webui the GPU resource is `1.2`.",Bug,Major,Resolved,"2016-06-29 10:17:28","2016-06-29 09:17:28",1
"Apache Mesos","Consider adding `relink` functionality to libprocess","Currently we don't have the {{relink}} functionality in libprocess.  i.e. A way to create a new persistent connection between actors, even if a connection already exists.   This can benefit us in a couple of ways: - The application may have more information on the state of a connection than libprocess does, as libprocess only checks if the connection is alive or not.  For example, a linkee may accept a connection, then fork, pass the connection to a child, and subsequently exit.  As the connection is still active, libprocess may not detect the exit. - Sometimes, the {{ExitedEvent}} might be delayed or might be dropped due to the remote instance being unavailable (e.g., partition, network intermediaries not sending RST's etc).  ",Improvement,Major,Resolved,"2016-06-29 03:24:43","2016-06-29 02:24:43",3
"Apache Mesos","Consider allowing the libprocess caller an option to not set CLOEXEC on libprocess sockets","Both implementations of libprocess's {{Socket}} interface will set the {{CLOEXEC}} option on all new sockets (incoming or outgoing).  This assumption is pervasive across Mesos, but since libprocess aims to be a general-purpose library, the caller should be able to *not* {{CLOEXEC}} sockets when desired.  See TODOs added here: https://reviews.apache.org/r/49281/",Improvement,Major,Open,"2016-06-28 02:08:08","2016-06-28 01:08:08",3
"Apache Mesos","Command executor health check does not work when the task specifies container image.","Since we launch the task after pivot_root, we no longer has the access to the mesos-health-check binary. The solution is to refactor health check to be a library (libprocess) so that it does not depend on the underlying filesystem.  One note here is that we should strive to keep both the command executor and the task in the same mount namespace so that Mesos CLI tooling does not need to find the mount namespace for the task. It just need to find the corresponding pid for the executor. This statement is *arguable*, see the comment below.",Bug,Major,Resolved,"2016-06-28 00:51:11","2016-06-27 23:51:11",5
"Apache Mesos","Benchmark the v1 Operator API","Just like what we did with the v1 framework API, we need to benchmark the performance of v1 operator API.  As part of this benchmarking, we should evaluate whether evolving un-versioned protos to versioned protos in some of the API handlers (e.g., getFrameworks) is expensive.",Task,Major,Resolved,"2016-06-27 22:12:05","2016-06-27 21:12:05",3
"Apache Mesos","SSL-enabled libprocess will leak incoming links to forks","Encountered two different buggy behaviors that can be tracked down to the same underlying problem.  Repro #1 (non-crashy): (1) Start a master.  Doesn't matter if SSL is enabled or not. (2) Start an agent, with SSL enabled.  Downgrade support has the same problem.  The master/agent {{link}} to one another. (3) Run a sleep task.  Keep this alive.  If you inspect FDs at this point, you'll notice the task has inherited the {{link}} FD (master -> agent). (4) Restart the agent.  Due to (3), the master's {{link}} stays open. (5) Check master's logs for the agent's re-registration message. (6) Check the agent's logs for re-registration.  The message will not appear.  The master is actually using the old {{link}} which is not connected to the agent.  ----  Repro #2 (crashy): (1) Start a master.  Doesn't matter if SSL is enabled or not. (2) Start an agent, with SSL enabled.  Downgrade support has the same problem. (3) Run ~100 sleep task one after the other, keep them all alive.  Each task links back to the agent.  Due to an FD leak, each task will inherit the incoming links from all other actors... (4) At some point, the agent will run out of FDs and kernel panic.  ----  It appears that the SSL socket {{accept}} call is missing {{os::nonblock}} and {{os::cloexec}} calls: https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L794-L806  For reference, here's {{poll}} socket's {{accept}}: https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/poll_socket.cpp#L53-L75 ",Bug,Blocker,Resolved,"2016-06-27 19:40:57","2016-06-27 18:40:57",2
"Apache Mesos","Can't autodiscovery GPU resources without '--enable-nvidia-gpu-support' and '--nvidia_gpu_devices' flags","Prerequisite: In MESOS\-5257  By default, with no '\-\-nvidia_gpu_devices' flag or `gpus` resources flag, the new auto-discovery will simply enumerate all of the GPUs on the system and in MESOS\-5630 removes this flag(\-\-enable-nvidia-gpu-support) and enables this support for all builds on Linux.  So, I '../configure' without any flag, and start agent without '\-\-resources' or '\-\-nvidia_gpu_devices' ,  but can not discovery GPU resources, and I also start agent with '\-\-resources' and '\-\-nvidia_gpu_devices' , it also does not work.  I'm sure the NVIDIA GPUs on my machines are OK, because with '\-\-enable-nvidia-gpu-support' when './configure' and with '\-\-resources', '\-\-nvidia_gpu_devices' when starting agents it works well.",Bug,Major,Resolved,"2016-06-27 08:57:59","2016-06-27 07:57:59",2
"Apache Mesos","Document docker private registry with authentication support in Unified Containerizer.","Add documentation for docker private registry with authentication support in unified containerizer. This is the basic support for docker private registry.",Improvement,Major,Resolved,"2016-06-27 08:23:45","2016-06-27 07:23:45",3
"Apache Mesos","Add a __sockets__ diagnostic endpoint to libprocess.","Libprocess exposes a endpoint {{/__processes__}}, which displays some info on the existing actors and messages queued up on each.  It would be nice to inspect the state of libprocess's {{SocketManager}} too.  This could be an endpoint like {{/__sockets__}} that exposes information like: * Inbound FDs: type and source * Outbound FDs: type and source * Temporary and persistent sockets * Linkers and linkees. * Outgoing messages and their associated socket",Wish,Major,Open,"2016-06-25 02:15:03","2016-06-25 01:15:03",3
"Apache Mesos","Document exactly what is handled by GET_ENDPOINTS_WITH_PATH acl","Users may expect that the GET_ENDPOINT_WITH_PATH acl can be used with any Mesos endpoint, but that is not (yet) the case. We should clearly document the list of applicable endpoints, in authorization.md and probably even upgrades.md.",Task,Minor,Resolved,"2016-06-24 13:40:56","2016-06-24 12:40:56",1
"Apache Mesos","Update AUTHORIZATION strings in endpoint help","The endpoint help macros support AUTHENTICATION and AUTHORIZATION sections. We added AUTHORIZATION help for some of the newer endpoints, but not the previously authenticated endpoints.  Authorization endpoints needing help string updates: Master::Http::CREATE_VOLUMES_HELP Master::Http::DESTROY_VOLUMES_HELP Master::Http::RESERVE_HELP Master::Http::STATE_HELP Master::Http::STATESUMMARY_HELP Master::Http::TEARDOWN_HELP Master::Http::TASKS_HELP Master::Http::UNRESERVE_HELP Slave::Http::STATE_HELP",Task,Minor,Resolved,"2016-06-24 13:38:37","2016-06-24 12:38:37",2
"Apache Mesos","The /logging/toggle endpoint accepts requests with any http method","Any of a GET, POST, PUT, or DELETE to `<master>/logging/toggle?level=INFO&duration=5mins` will set the log level and return 200. To be consistent with REST-like syntax, DELETE, GET, and even POST are wrong and should return a MethodNotAllowed.  Once this endpoint no longer accepts GET, it is no longer appropriate to use the GET_ENDPOINT acl here. Instead we could create a new PUT_ENDPOINT_WITH_PATH acl (which hopefully ignores query params), or add a first-class TOGGLE_LOGGING acl.",Task,Minor,Open,"2016-06-24 13:30:35","2016-06-24 12:30:35",3
"Apache Mesos","Authorization for /roles","The /roles endpoint exposes the list of all roles and their weights, as well as the list of all frameworkIds registered with each role. This is a superset of the information exposed on GET /weights, which we already protect. We should protect the data in /roles the same way. - Should we reuse VIEW_FRAMEWORK with role (from /state)? - Should we add a new VIEW_ROLE and adapt GET_WEIGHTS to use it?",Task,Blocker,Resolved,"2016-06-24 13:18:59","2016-06-24 12:18:59",3
"Apache Mesos","Add authz to /files/debug","The /files/debug endpoint exposes the attached master/agent log paths and every attached sandbox path, which includes the frameworkId and executorId. Even if sandboxes are protected, we still don't want to expose this information to unauthorized users.",Task,Minor,Resolved,"2016-06-24 13:15:22","2016-06-24 12:15:22",3
"Apache Mesos","LocalAuthorizer should error if passed a GET_ENDPOINT ACL with an unhandled path","Since GET_ENDPOINT_WITH_PATH doesn't (yet) work with any arbitrary path, we should a) validate --acls and error if GET_ENDPOINT_WITH_PATH has a path object that doesn't match an endpoint that uses this authz strategy. b) document exactly which endpoints support GET_ENDPOINT_WITH_PATH",Task,Blocker,Resolved,"2016-06-24 13:12:39","2016-06-24 12:12:39",3
"Apache Mesos","GET_ENDPOINT_WITH_PATH authz doesn't make sense for /flags","The master or agent flags are exposed in /state as well as /flags, so any user who wants to disable/control access to the flags likely intends to control access to flags no matter what endpoint exposes them. As such, /flags is a poor candidate for GET_ENDPOINT_WITH_PATH authz, since we care more about protecting the flag data than the specific endpoint path. We should remove the GET_ENDPOINT authz from master and agent /flags until we can come up with a better solution, perhaps a first-class VIEW_FLAGS acl.",Task,Major,Resolved,"2016-06-24 13:09:34","2016-06-24 12:09:34",2
"Apache Mesos","ZK credential is exposed in /flags and /state","Mesos allows zk credentials to be embedded in the zk url, but exposes these credentials in the /flags and /state endpoint. Even though /state is authorized, it only filters out frameworks/tasks, so the top-level flags are shown to any authenticated user.  zk: zk://dcos_mesos_master:my_secret_password@127.0.0.1:2181/mesos,  We need to find some way to hide this data, or even add a first-class VIEW_FLAGS acl that applies to any endpoint that exposes flags.",Task,Blocker,Resolved,"2016-06-24 13:05:42","2016-06-24 12:05:42",5
"Apache Mesos","Fine-grained authorization on /frameworks","Even if ACLs were defined for the actions VIEW_FRAMEWORKS, VIEW_EXECUTORS and VIEW_TASKS, the data these actions were supposed to protect, could still leaked through the master's /frameworks endpoint, since it didn't enable any authorization mechanism.",Task,Critical,Resolved,"2016-06-24 13:00:30","2016-06-24 12:00:30",3
"Apache Mesos","Create new documentation for Mesos networking.","With introduction of CNI and dockers support docker user-defined networks, there are quite a few options within Mesos for IP-per-container solutions for container networking.   We therefore need to re-write networking documentation for Mesos highlighting all the networking support that Mesos provides for orchestrating containers on IP networks.",Task,Major,Resolved,"2016-06-24 00:58:11","2016-06-23 23:58:11",1
"Apache Mesos","Quota sorter not updated for resource changes at agent.","Consider this sequence of events:  1. Slave connects, with 128MB of disk. 2. Master offers resources at slave to framework 3. Framework creates a dynamic reservation for 1MB and a persistent volume of the same size on the slave's resources.   => This invokes {{Master::apply}}, which invokes {{allocator->updateAllocation}}, which invokes {{Sorter::update()}} on the framework sorter and role sorter. If the framework's role has a configured quota, it also invokes {{update}} on the quota role sorter -- in this case, the framework's role has no quota, so the quota role sorter is *not* updated.   => {{DRFSorter::update}} updates the *total* resources at a given slave, among updating other state. New total resources will be 127MB of unreserved disk and 1MB of reserved disk with a volume. Note that the quota role sorter still thinks the slave has 128MB of unreserved disk. 4. The slave is removed from the cluster. {{HierarchicalAllocatorProcess::removeSlave}} invokes:    {{slaves\[slaveId\].total.nonRevocable()}} is 127MB of unreserved disk and 1MB of reserved disk with a volume. When we remove this from the quota role sorter, we're left with total resources on the reserved slave of 1MB of unreserved disk, since that is the result of subtracting <127MB unreserved, 1MB reserved+volume> from <128MB unreserved>.  The implications of this can't be good: at minimum, we're leaking resources for removed slaves in the quota role sorter. We're also introducing an inconsistency between {{total_.resources\[slaveId\]}} and {{total_.scalarQuantities}}, since the latter has already stripped-out volume/reservation information.",Bug,Blocker,Resolved,"2016-06-23 21:48:09","2016-06-23 20:48:09",5
"Apache Mesos","Support file volume in mesos containerizer.","Currently in mesos containerizer, the host_path volume (to be bind mounted from a host path) specified in ContainerInfo can only be a directory. We should also support the volume type as a file.",Improvement,Major,Resolved,"2016-06-23 19:57:26","2016-06-23 18:57:26",3
"Apache Mesos","SSL downgrade support will leak sockets in CLOSE_WAIT status","Repro steps: 1) Start a master:   2) Start an agent with SSL and downgrade enabled:   3) Start a framework that launches lots of executors, one after another:   4) Check FDs, repeatedly   The number of sockets in {{CLOSE_WAIT}} will increase linearly with the number of launched executors.",Bug,Blocker,Resolved,"2016-06-23 02:58:55","2016-06-23 01:58:55",5
"Apache Mesos","The /files/download endpoint's authorization can be compromised","If a forward slash is appended to the path of a file a user wishes to download via {{/files/download}}, the authorization logic for that path will be bypassed and the file will be downloaded regardless of permissions. This is because we store the authorization callbacks for these paths in a map which is keyed by the path name, so a request to {{/master/log/}} fails to find the callback which is installed for {{/master/log}}. When the master fails to find the callback, it assumes authorization is not required for that path and authorizes the action.  Consider the following excerpt:   We could consider disallowing paths which end in trailing slashes.",Bug,Blocker,Resolved,"2016-06-22 22:33:01","2016-06-22 21:33:01",2
"Apache Mesos","Master captures `this` when creating authorization callback","When exposing its log file, the master currently installs an authorization callback for the log file which captures the master's {{this}} pointer. Such captures have previously caused bugs (MESOS-5629), and this one should be fixed as well. The callback should be dispatched to the master process, and it should be dispatched via the {{self()}} PID.",Bug,Blocker,Resolved,"2016-06-22 19:14:00","2016-06-22 18:14:00",1
"Apache Mesos","Example frameworks should allow setting failover timeout","The example frameworks do not currently set a framework failover timeout when they register with the master. This means that when these frameworks are used in prolonged testing scenarios, small network outages can lead to flapping frameworks.  We should either set the failover timeout to a reasonable value in the example frameworks, or add command-line flags that allow the timeout to be set.",Improvement,Major,Open,"2016-06-21 21:23:57","2016-06-21 20:23:57",2
"Apache Mesos","Provide doc examples for dynamic reservation/persistent volumes","Users have found it difficult to make use of the dynamic reservation and persistent volume features. The API governing use of these features is a bit complicated, and this leads to users having trouble forming correct requests for reservations, volume creation, etc. Providing multiple examples of reserve/unreserve/create/destroy requests would make it much easier for users to get started.",Bug,Major,Open,"2016-06-21 18:38:46","2016-06-21 17:38:46",3
"Apache Mesos","Add support for master capabilities","Right now, frameworks can advertise their capabilities to the master via the {{FrameworkInfo}} they use for registration/re-registration. This allows masters to provide backward compatibility for old frameworks that don't support new functionality.  To allow new frameworks to support backward compatibility with old masters, the inverse concept would be useful: masters would tell frameworks which capabilities are supported by the master, which the frameworks could then use to decide whether to use features only supported by more recent versions of the master.  For now, frameworks can workaround this by looking at the master's version number, but that seems a bit fragile and hacky.",Improvement,Major,Resolved,"2016-06-21 12:41:25","2016-06-21 11:41:25",3
"Apache Mesos","Port mapping isolator may fail in 'isolate' method.","Port mapping isolator may return failure in isolate method, if a symlink to the network namespace handle using that ContainerId already existed. We should overwrite the symlink if it exist.  This affects a couple test failures:   Here is an example failure test log: ",Bug,Major,Resolved,"2016-06-21 03:19:04","2016-06-21 02:19:04",3
"Apache Mesos","Port mapping isolator may cause segfault if it bind mount root does not exist.","A check is needed for port mapping isolator for its bind mount root. Otherwise, non-existed port-mapping bind mount root may cause segmentation fault for some cases. Here is the test log:  ",Bug,Major,Resolved,"2016-06-21 03:02:25","2016-06-21 02:02:25",3
"Apache Mesos","MemoryPressureMesosTest.CGROUPS_ROOT_Statistics is flaky.",,Bug,Major,Resolved,"2016-06-21 02:50:40","2016-06-21 01:50:40",2
"Apache Mesos","MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky.",,Bug,Major,Resolved,"2016-06-21 02:46:47","2016-06-21 01:46:47",2
"Apache Mesos","CNI isolator should not return failure if /etc/hostname does not exist on host.","/etc/hostname may not necessarily exist on every system (e.g., CentOS 6). Currently CNI isolator just return a failure if it does not exist on host, because the isolator need to mount it into the container. This is fine for /etc/host and /etc/resolv.conf, but we should make an exception for /etc/hostname, because hostname may still be accessible even if /etc/hostname doesn't exist.  This issue relates to 3 failure tests:     ",Bug,Major,Resolved,"2016-06-21 02:36:31","2016-06-21 01:36:31",3
"Apache Mesos","Add CGROUP namespace to linux ns helper.","Since linux kernel 4.6, CGROUP namespace is added. we need to support the handle for the cgroup namespace of the process.  This also relates to two test failures on Ubuntu 16:   ",Bug,Major,Resolved,"2016-06-21 02:23:57","2016-06-21 01:23:57",3
"Apache Mesos","CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask fails on CentOS 7.",,Bug,Major,Resolved,"2016-06-21 02:03:57","2016-06-21 01:03:57",2
"Apache Mesos","Deprecate camel case proto field in isolator ContainerConfig.","Currently there are extra ExecutorInfo and TaskInfo in isolator ContaienrConfig, because a deprecation cycle is needed to deprecate camel cased proto field names. This JIRA is used for tracking this issue, which should address the TODO in isolator.proto.",Improvement,Major,Resolved,"2016-06-20 23:34:01","2016-06-20 22:34:01",2
"Apache Mesos","Invalid resources sent to '/reserve' are silently dropped","If an invalid resource is passed to the master's {{/reserve}} endpoint, it will be silently dropped and not cause an error. This can lead, for example, to a {{/reserve}} request containing a single invalid resource receiving a 200 OK response, despite the fact that no resources were reserved as a result of the request.  This is due to the fact that the {{+=}} operator for {{Resources}} silently drops invalid resources, and this operator is used when parsing the resources in the HTTP request. This could be addressed by validating the resource objects one at a time as they are parsed.",Bug,Major,Resolved,"2016-06-20 22:31:49","2016-06-20 21:31:49",1
"Apache Mesos","Remove hard dependence on libelf for Linux","    We recently added a hard dependency for `libelf` on Linux. This was in     preparation for some upcoming Nvidia GPU support for injecting volumes     into containers. Since this dependence is not actually necessary for     the upcoming release, we should remove it for now, and rethink the     best way to add it back in later (possibly as a runtime dependence     instead of a linktime one).",Task,Major,Resolved,"2016-06-20 21:17:45","2016-06-20 20:17:45",1
"Apache Mesos","Use snake casing for flag names consistently","Historically, we have always used snake casing for the flag variables e.g., {{docker_config}} etc. However, there are some instances in our .cpp code where we define the flag name in the .cpp file in camel case e.g., {{modulesDir}} but still have the flag name as {{modules_dir}} when taking arguments from the user. It would be good to audit all such occurrences and consistently uses snake casing in our .cpp/.hpp files everywhere.",Improvement,Major,Accepted,"2016-06-20 18:40:31","2016-06-20 17:40:31",1
"Apache Mesos","ContainerizerTest.ROOT_CGROUPS_BalloonFramework fails because executor environment isn't inherited","A recent change forbits the executor to inherit environment variables from the agent's environment. As a regression this break {{ContainerizerTest.ROOT_CGROUPS_BalloonFramework}}.",Bug,Major,Resolved,"2016-06-20 16:32:52","2016-06-20 15:32:52",2
"Apache Mesos","Design doc for TASK_UNREACHABLE","See MESOS-4049.",Improvement,Major,Resolved,"2016-06-20 15:16:13","2016-06-20 14:16:13",5
"Apache Mesos","Executors should not inherit environment variables from the agent.","Currently executors are inheriting environment variables form the slave in mesos containerizer. This is problematic, because of two reasons:  1. When we use docker images (such as `mongo`) in unified containerizer, duplicated environment variables inherited from the slave lead to initialization failures, because LANG and/or LC_* environment variables are not set correctly.  2. When we are looking at the environment variables from the executor tasks, there are pages of environment variables listed, which is redundant and dangerous.  Depending on the reasons above, we propose that no longer allow executors to inherit environment variables from the slave. Instead, users should specify all environment variables they need by setting the slave flag `--executor_environment_variables` as a JSON format.",Bug,Major,Resolved,"2016-06-20 07:53:03","2016-06-20 06:53:03",3
"Apache Mesos","UNRESERVE operation causes master to crash.","{{RESERVE}} operation may cause a master failure:   Possible reasons: * Recent improvements in allocator (b4d746f) * Bug in bookkeeping during the previous {{UNRESERVE}} * Network partition that happened after {{RESERVE}} and before {{UNRESERVE}}",Bug,Blocker,Resolved,"2016-06-19 10:42:05","2016-06-19 09:42:05",5
"Apache Mesos","Build an example framework to consume GPUs","This framework should show how to build a GPU capable framework that can accept offers with GPUs and launch tasks that use them.",Task,Major,Resolved,"2016-06-18 23:09:21","2016-06-18 22:09:21",3
"Apache Mesos","Expose network statistics for containers on CNI network in the `network/cni` isolator.","We need to implement the `usage` method in the `network/cni` isolator to expose metrics relating to a containers network traffic.   On receiving a request for getting `usage` for a a given container the `network/cni` isolator could use NETLINK system calls to query the kernel for interface and routing statistics for a given container's network namespace.",Task,Major,Resolved,"2016-06-18 15:33:50","2016-06-18 14:33:50",5
"Apache Mesos","Build `network/cni` isolator with `libnl` support","Currently, the `network/cni` isolator does not have the ability to collect network statistics for containers launched on a CNI network. We need to give the `network/cni` isolator the ability to query interfaces, route tables and statistics in the containers network namespace. To achieve this the `network/cni` isolator will need to talk `netlink`.  For enabling `netlink` API we need the `network/cni` isolator to be built with libnl support. ",Task,Major,Reviewable,"2016-06-18 15:29:20","2016-06-18 14:29:20",3
"Apache Mesos","Add documentation about metadata for CNI plugins.","We need to document the behavior implemented in MESOS-5592.",Task,Major,Resolved,"2016-06-18 01:51:14","2016-06-18 00:51:14",2
"Apache Mesos","Check all omissions of 'defer' for safety","When registering callbacks with {{.then}}, {{.onAny}}, etc., we sometimes omit {{defer()}} in cases where it's deemed safe; for example, when the callback uses no process state and thus could be executed in an arbitrary context. Because of recent bugs due to the unsafe omission of {{defer()}}, we should do a sweep of the codebase for all such occurrences and evaluate their safety. We should also consider using {{defer()}} consistently in all such cases, as our [documentation|https://github.com/apache/mesos/tree/master/3rdparty/libprocess#defer] recommends.",Bug,Major,Open,"2016-06-18 01:44:47","2016-06-18 00:44:47",5
"Apache Mesos","Add Framework Capability for GPU_RESOURCES","Due to the scarce resource problem described in MESOS-5377, we plan to introduce a GPU_RESOURCES Framework capability. This capability will allow the Mesos allocator to make better decisions about which frameworks should receive resources from GPU capable machines.  In essence, the allocator will ONLY allocate resources from GPU capable machines to frameworks that have this capability. This is necessary to prevent non-GPU workloads from filling up the GPU machines and preventing GPU workloads to run.",Task,Major,Resolved,"2016-06-17 20:29:54","2016-06-17 19:29:54",3
"Apache Mesos","Implement clang-tidy check for incorrect use of capturing lambdas with Futures","When one enqueues capturing lambdas to a {{Future}} with {{then}} or the {{onXXX}} variations, in general any actor might execute that callback (no constraints imposed per se).  This can lead to hard to understand dependencies or bugs if the lambda needs to access external state (i.e. anything it captures by references/pointer to instead of by value); instead such callbacks should always be constraint to a specific actor with {{dispatch}}/{{defer}} to ensure the pointed to data isn't modified in a concurrent thread.",Improvement,Major,Resolved,"2016-06-17 08:55:22","2016-06-17 07:55:22",5
"Apache Mesos","Change build to always enable Nvidia GPU support for Linux","See Summary",Improvement,Major,Resolved,"2016-06-17 07:28:48","2016-06-17 06:28:48",2
"Apache Mesos","Agent segfaults after request to '/files/browse'","We observed a number of agent segfaults today on an internal testing cluster. Here is a log excerpt:   In every case, the stack trace indicates one of the {{/files/*}} endpoints; I observed this a number of times coming from {{browse()}}, and twice from {{read()}}.  The agent was built from the 1.0.0-rc1 branch, with two cherry-picks applied: [this|https://reviews.apache.org/r/48563/] and [this|https://reviews.apache.org/r/48566/], which were done to repair a different [segfault issue|https://issues.apache.org/jira/browse/MESOS-5587] on the master and agent.  Thanks go to [~<USER> for digging into this a bit and discovering a possible cause [here|https://github.com/apache/mesos/blob/master/src/slave/slave.cpp#L5737-L5745], where use of {{defer()}} may be necessary to keep execution in the correct context.",Bug,Blocker,Resolved,"2016-06-17 00:46:43","2016-06-16 23:46:43",3
"Apache Mesos","Added a metric indicating if replicated log for the registrar has recovered or not.","This gives operator insight about the state of the replicated log for registrar. The operator needs to know when it is safe to move on to another master in the upgrade orchestration pipeline. ",Improvement,Major,Resolved,"2016-06-16 06:37:19","2016-06-16 05:37:19",3
"Apache Mesos","Put initial scaffolding in place for implementing SUBSCRIBE call on v1 Master API.","As discussed on MESOS-5498, this ticket is for tracking work to put the initial scaffolding in place for streaming task status update events to a client that has subscribed to the {{api/v1}} Operator API endpoint. Other events/support for snapshots would be done as part of MESOS-5498.",Bug,Major,Resolved,"2016-06-14 01:25:52","2016-06-14 00:25:52",5
"Apache Mesos","Improve documentation for using persistent volumes. ","When using persistent volumes at a arangoDB we ran into a few pitfalls. We should document them in order for others to avoid those issues.",Documentation,Major,Reviewable,"2016-06-12 12:26:00","2016-06-12 11:26:00",2
"Apache Mesos","Document Mesos health check feature.","We don't talk about this feature at all.",Documentation,Major,Resolved,"2016-06-10 16:45:56","2016-06-10 15:45:56",5
"Apache Mesos","Pass NetworkInfo to CNI Plugins","Mesos has adopted the Container Network Interface as a simple means of networking Mesos tasks launched by the Unified Containerizer. The CNI specification covers a minimum feature set, granting the flexibility to add customized networking functionality in the form of agreements made between the orchestrator and CNI plugin.  This proposal is to pass NetworkInfo.Labels to the CNI plugin by injecting it into the CNI network configuration json during plugin invocation.  Design Doc on this change: https://docs.google.com/document/d/1rxruCCcJqpppsQxQrzTbHFVnnW6CgQ2oTieYAmwL284/edit?usp=sharing  reviewboard: https://reviews.apache.org/r/48527/",Improvement,Major,Resolved,"2016-06-10 01:18:05","2016-06-10 00:18:05",3
"Apache Mesos","Improve authorization documentation when setting permissive flag.","A common problem for a users starting to use acls is that once they set `permisse = false` and not add acls allowing common operations (e.g., register_framework) their Mesos cluster don't behave as expected. ",Documentation,Major,Resolved,"2016-06-09 16:31:27","2016-06-09 15:31:27",1
"Apache Mesos","Create a `cgroups/devices` isolator.","Currently, all the logic for the `cgroups/devices` isolator is bundled into the Nvidia GPU Isolator. We should abstract it out into it's own component and remove the redundant logic from the Nvidia GPU Isolator. Assuming the guaranteed ordering between isolators from MESOS-5581, we can be sure that the dependency order between the `cgroups/devices` and `gpu/nvidia` isolators is met.",Improvement,Major,Resolved,"2016-06-09 08:31:29","2016-06-09 07:31:29",2
"Apache Mesos","Guarantee ordering between Isolators","Some isolators depend on other isolators. However, we currently do not have a generic method of expressing these dependencies. We special case the `filesystem/*` isolators to make sure that dependencies on them are satisfied, but no other dependencies can be expressed.      Instead, we should use a vector to represent the pairing of isolator name to isolator creator function. This way, the relative dependencies between each isolator will be implicit in the ordering of the vector. Currently, a hashmap is used to hold this pairing, but this is inadequate because hashmaps are inherently unordered. The new implementation using a vector will ensure everything is processed in the order it is listed.",Improvement,Major,Resolved,"2016-06-09 08:26:45","2016-06-09 07:26:45",3
"Apache Mesos","Implement authn/authz for the network/cni isolator","Currently any framework can launch containers on any CNI network irrespective of its role and principal. We need perform authn/authz in the network/cni isolator (or Master) to make sure that only roles/principals specified by the operator can launch containers on a given network. ",Task,Major,Open,"2016-06-09 05:47:37","2016-06-09 04:47:37",3
"Apache Mesos","Support static IP address allocation with `DockerContainerizer`","Docker run supports the `--ip` option to allocate a specific IPv4 address to the container. Also, the `NetworkInfo` protobuf has an `ipaddress` field that all frameworks to specify an IP address for the container. The docker executor should therefore invoke the `docker run` command with the --ip option whenever the `ipaddress` field of the `NetworkInfo` is set allowing frameworks to try and assign a static IP address for their services.",Task,Major,Open,"2016-06-09 05:44:11","2016-06-09 04:44:11",1
"Apache Mesos","Support static address allocation in CNI","Currently a framework can't specify a static IP address for the container when using the network/cni isolator.  The `ipaddress` field in the `NetworkInfo` protobuf was designed for this specific purpose but since the CNI spec does not specify a means to allocate an IP address to the container the `network/cni` isolator cannot honor this field even when it is filled in by the framework.  Creating this ticket to act as a place holder to track this limitation. As and when the CNI spec allows us to specify a static IP address for the container, we can resolve this ticket. ",Task,Major,Open,"2016-06-09 05:34:27","2016-06-09 04:34:27",1
"Apache Mesos","Modules using replicated log state API require zookeeper headers","The state API uses zookeeper client headers and hence the bundled zookeeper headers need to be installed during Mesos installation. ",Bug,Major,Resolved,"2016-06-09 05:16:51","2016-06-09 04:16:51",1
"Apache Mesos","Masters may drop the first message they send between masters after a network partition","We observed the following situation in a cluster of five masters: || Time || Master 1 || Master 2 || Master 3 || Master 4 || Master 5 || | 0 | Follower | Follower | Follower | Follower | Leader | | 1 | Follower | Follower | Follower | Follower || Partitioned from cluster by downing this VM's network || | 2 || Elected Leader by ZK | Voting | Voting | Voting | Suicides due to lost leadership | | 3 | Performs consensus | Replies to leader | Replies to leader | Replies to leader | Still down | | 4 | Performs writing | Acks to leader | Acks to leader | Acks to leader | Still down | | 5 | Leader | Follower | Follower | Follower | Still down | | 6 | Leader | Follower | Follower | Follower | Comes back up | | 7 | Leader | Follower | Follower | Follower | Follower | | 8 || Partitioned in the same way as Master 5 | Follower | Follower | Follower | Follower | | 9 | Suicides due to lost leadership || Elected Leader by ZK | Follower | Follower | Follower | | 10 | Still down | Performs consensus | Replies to leader | Replies to leader || Doesn't get the message! || | 11 | Still down | Performs writing | Acks to leader | Acks to leader || Acks to leader || | 12 | Still down | Leader | Follower | Follower | Follower |  Master 2 sends a series of messages to the recently-restarted Master 5.  The first message is dropped, but subsequent messages are not dropped.  This appears to be due to a stale link between the masters.  Before leader election, the replicated log actors create a network watcher, which adds links to masters that join the ZK group: https://github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/network.hpp#L157-L159  This link does not appear to break (Master 2 -> 5) when Master 5 goes down, perhaps due to how the network partition was induced (in the hypervisor layer, rather than in the VM itself).  When Master 2 tries to send an {{PromiseRequest}} to Master 5, we do not observe the [expected log message|https://github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/replica.cpp#L493-L494]  Instead, we see a log line in Master 2:   The broken link is removed by the libprocess {{socket_manager}} and the following {{WriteRequest}} from Master 2 to Master 5 succeeds via a new socket.",Improvement,Major,Resolved,"2016-06-09 03:46:20","2016-06-09 02:46:20",5
"Apache Mesos","Improve CHANGELOG and upgrades.md","Currently we have a lot of data duplication between the CHANGELOG and upgrades.md. We should try to improve this and potentially make the CHANGLOG a markdown file as well. For inspiration see the Hadoop changelog: https://github.com/apache/hadoop/blob/2e1d0ff4e901b8313c8d71869735b94ed8bc40a0/hadoop-common-project/hadoop-common/src/site/markdown/release/1.2.0/CHANGES.1.2.0.md ",Documentation,Major,"In Progress","2016-06-08 15:06:02","2016-06-08 14:06:02",3
"Apache Mesos","Document common use cases of authorization","Our authorization documentation covers the existing functionality, but it doesn't provide a practical how-to guide to help users accomplish common authorized use cases. For example, a user recently reported that to gain full use of the web UI after upgrading to Mesos 1.0, six new ACL rules needed to be added: {{get_endpoints, view_frameworks, view_tasks, view_executors, access_sandboxes, and access_mesos_logs}}. Rather than expecting users to figure this out on their own, we should document the ACLs needed to accomplish a common goal like this.  Similarly, authorizing a stateful framework to accomplish the actions it would usually be expected to perform would involve setting rules for {{register_frameworks, run_tasks, shutdown_frameworks, reserve_resources, unreserve_resources, create_volumes, and destroy_volumes}}.",Documentation,Major,Accepted,"2016-06-08 00:29:03","2016-06-07 23:29:03",1
"Apache Mesos","Rearrange Nvidia GPU files to cleanup semantics for header inclusion.","Currently, components outside of `src/slave/containerizers/mesos/isolators/gpu` have to protect their #includes for certain Nvidia header files with the ENABLE_NVIDIA_GPU_SUPPORT flag. Other headers strictly *could not* be wrapped in this flag.      We need to clean up this header madness, by creating a common nvidia.hpp header that takes care of all the dependencies. All componenents outside of `src/slave/containerizers/mesos/isolators/gpu` should only need to #include this one header instead of managing everything themselves.",Improvement,Major,Resolved,"2016-06-07 22:51:11","2016-06-07 21:51:11",1
"Apache Mesos","Add class to share Nvidia-specific components between containerizers","Once we have an `NvidiaGPUAllocator` component, we need some way to share it across multiple containerizers.  Moreover, we anticipate needing other Nvidia components to share across multiple containerizers as well (e.g. an `NvidiaVolumeManager` component). As such, we should add a wrapper class around these components to make it easily passable to each containerizer without having to continually add a bunch of parameters to the Containerizer interface.",Improvement,Major,Resolved,"2016-06-07 22:49:20","2016-06-07 21:49:20",2
"Apache Mesos","Need to remove references to messages/messages.hpp from `State` API","In order to expose the `State` API for using replicated log in Mesos modules it is necessary that the `State` API does not reference headers that are not exposed as part of the Mesos installation.   Currently include/mesos/state/protobuf.hpp references src/messages/messages.hpp making the `State` API unusable in a module.   We need to move the protobuf `serialize`/`deserialize` functions out of messages.hpp and move them to `stout/protobuf.hpp`. This will help us remove references to messages.hpp from the `State` API.",Bug,Major,Resolved,"2016-06-07 22:47:01","2016-06-07 21:47:01",2
"Apache Mesos","Integrate the `NvidiaGpuAllocator` into the `NvidiaGpuIsolator`",,Improvement,Major,Resolved,"2016-06-07 22:39:51","2016-06-07 21:39:51",3
"Apache Mesos","Update `Containerizer::resources()` to use the `NvidiaGpuAllocator`","With the introduction of the shared `NvidiaGpuAllocator` component, `Containerizer::resources()` should be updated to use it.",Improvement,Major,Resolved,"2016-06-07 22:36:48","2016-06-07 21:36:48",2
"Apache Mesos","Add `NvidiaGpuAllocator` component for cross-containerizer GPU allocation","We need some way of allocating GPUs from a centralized location to allow both the mesos containerizer and the docker containerizer to pull from central pool.  We propose to build a `NvidiaGpuAllocator` for this purpose.      This component should also be overloaded to do resource enumeration of GPUs based on the agent flags. This keeps all code for enumerating GPUs and the resources they represent in a single centralized location.",Improvement,Major,Resolved,"2016-06-07 22:35:10","2016-06-07 21:35:10",5
"Apache Mesos","Fix method of populating device entries for `/dev/nvidia-uvm`, etc.","Currently, the major/minor numbers of `/dev/nvidiactl` and `/dev/nvidia-uvm` are hard-coded. This causes problems for `/dev/nvidia-uvm` because its major number is part of the Experimental device range on Linux.  Because this range is experimental, there is no guarantee which device number will be assigned to it on a given machine.  We should use `os:stat::rdev()` to extract the major/minor numbers programatically.",Bug,Major,Resolved,"2016-06-07 22:29:49","2016-06-07 21:29:49",2
"Apache Mesos","Always provide access to NVIDIA control devices within containers (if GPU isolation is enabled).","Currently, access to `/dev/nvidiactl` and `/dev/nvidia-uvm` is only granted to / revoked from a container as GPUs are added and removed from them. On some level, this makes sense because most jobs don't need access to these devices unless they are also using a GPU. However, there are cases when access to these files is appropriate, even when not making use of a GPU. Running `nvidia-smi` to control the global state of the underlying nvidia driver, for example.      We should add `/dev/nvidiactl` and `/dev/nvidia-uvm` to the default whitelist of devices to include in every container when the `gpu/nvidia` isolator is enabled. This will allow a container to run standard nvidia driver tools (such as `nvidia-smi`) without failing with abnormal errors when no GPUs have been granted to it. As such, these tools will now report that no GPUs are installed instead of failing abnormally.",Improvement,Major,Resolved,"2016-06-07 22:27:14","2016-06-07 21:27:14",3
"Apache Mesos","Change major/minor device types for Nvidia GPUs to `unsigned int`","Currently, the GPU struct specifies the type of its `major` and `minor` fields as `dev_t`, which is actually a concatenation of both the major and minor device numbers accessible through the `major()` and `minor()` macros. These macros return an `unsigned int` when handed a `dev_t`, so it makes sense for these fields to be of that type instead.",Bug,Major,Resolved,"2016-06-07 22:22:41","2016-06-07 21:22:41",1
"Apache Mesos","Bundle NVML headers for Nvidia GPU support.","Currently, we rely on a script to install the Nvidia GDK as a build dependence for building Mesos with Nvidia GPU support.  A previous ticket removed the Mesos build dependence on `libnvidia-ml` which comes as part of the GDK. This ticket proposes bundling the NVML headers with Mesos in order to completely remove the build dependence on the GDK.  With this change it will be much simpler to configure and build with Nvidia GPU support.  All that will be required is:  ",Improvement,Major,Resolved,"2016-06-07 22:20:51","2016-06-07 21:20:51",1
"Apache Mesos","Move the Nvidia GPU isolator from `cgroups/devices/gpu/nvidia` to `gpu/nvidia`","Currently, the Nvidia GPU isolator lives in `src/slave/containerizers/mesos/isolators/cgroups/devices/gpu/nvidia`. However, in the future this isolator will do more than simply isolate GPUs using the cgroups devices subsystem (e.g. volume management for injecting machine specific Nvidia libraries into a container). For this reason, we should preemptively move this isolator up to `src/slave/containerizers/mesos/isolators/gpu/nvidia`. As part of this, we should update the string we pass to the `--isolator` agent flag to reflect this.",Improvement,Major,Resolved,"2016-06-07 22:05:53","2016-06-07 21:05:53",2
"Apache Mesos","Remove Nvidia GPU Isolator's link-time dependence on `libnvidia-ml`","The current Nvidia GPU isolator has a dependence on `libnvidia-ml`, and as such, pulls a hard dependence on this library into `libmesos`. The consequence of this is that any process that relies on `libmesos` has to have `libnvidia-ml` available as well, even on machines where no GPUs are available.  Since this library is not easily installable through standard package managers, having such a hard dependence can be burdensome.  This ticket proposes to pull in `libnvidia-ml` as a run-time dependence instead of a link-time dependence. As such, only machines that actually have GPUs installed and would like to rely on this library need to have it installed.",Improvement,Major,Resolved,"2016-06-07 21:47:21","2016-06-07 20:47:21",2
"Apache Mesos","Document aufs provisioner backend.","We should update container-image.md with the newly supported backend.",Task,Major,Resolved,"2016-06-07 17:16:21","2016-06-07 16:16:21",2
"Apache Mesos","http v1 SUBSCRIBED scheduler event always has nil http_interval_seconds","I'm writing a controller in Go to monitor heartbeats. I'd like to use the interval as communicated by the master, which should be specified in the SUBSCRIBED event. But it's not.      I *am* seeing HEARTBEAT events. Just not seeing the interval specified in the SUBSCRIBED event.",Bug,Blocker,Resolved,"2016-06-03 19:43:28","2016-06-03 18:43:28",1
"Apache Mesos","Maven build is too verbose for batch builds","During a non-interactive (without terminal) Mesos build, maven generates several thousands of log lines when downloading artifacts. This often makes several web-based log viewers unresponsive.  Further, these several thousand line long progress indicator logs don't provide any meaningful information either. From a user's point of view, just knowing that the artifact download succeeded/failed is often enough.  We should be using '--batch-mode' flag to disable these additionals log lines.",Improvement,Major,Resolved,"2016-06-01 21:14:53","2016-06-01 20:14:53",1
"Apache Mesos","Re-enable style-check for stout.","After the 3rdparty reorg, the mesos-style checker stopped checking stout.",Bug,Major,Resolved,"2016-06-01 17:28:07","2016-06-01 16:28:07",1
"Apache Mesos","Enable `Option` to handle string literals gracefully","In {{FlagsBase::add}}, MESOS-5064 begins making use of template function parameters like {{T2*}} for the default flag value rather than {{Option<T2>&}}. This is because in some places in the code base, we pass string literals for this argument. If an {{Option}} type is used, the compiler infers a {{char [x]}} type for {{T2}}, which breaks {{Option::getOrElse}}, which attempts to return that same type, since returning arrays is disallowed.  To fix this, we could employ {{std::decay}}, which would convert a return type of {{char [x]}} into {{const char *}}.",Improvement,Major,Open,"2016-05-27 10:30:57","2016-05-27 09:30:57",2
"Apache Mesos","Confirm errors in authorized persistent volume tests","The tests {{PersistentVolumeTest.BadACLDropCreateAndDestroy}} and {{PersistentVolumeTest.BadACLNoPrincipal}} check for a failed Destroy operation by confirming that the persistent volume is still contained in an offer received after the attempted operation. We should also explicitly check that the operation did not succeed due to failed authorization.",Bug,Minor,Open,"2016-05-27 06:49:43","2016-05-27 05:49:43",1
"Apache Mesos","Remove hard-coded principals in `PersistentVolumeEndpointsTest.SlavesEndpointFullResources`","In the test {{PersistentVolumeEndpointsTest.SlavesEndpointFullResources}}, the value {{test-principal}} is hard-coded into the JSON strings expected in HTTP responses. It would be more durable to use {{DEFAULT_CREDENTIAL.principal()}} instead.",Task,Major,Open,"2016-05-27 06:24:21","2016-05-27 05:24:21",1
"Apache Mesos","Update RUN_TASK_WITH_USER to use additional metadata","Currently, the `authorization::Action` `RUN_TASK_WITH_USER` will pass the user as its `Object.value` string, but some authorizers may want to make authorization decisions based on additional task attributes, like role, resources, labels, container type, etc.  We should create a new Action `RUN_TASK` that passes FrameworkInfo and TaskInfo in its Object, and the LocalAuthorizer's RunTaskWithUser ACL can be implemented using the user found in TaskInfo/FrameworkInfo. We may need to leave the old _WITH_USER action around, but it's arguable whether we should call the authorizer once for RUN_TASK and once for RUN_TASK_WITH_USER, or only use the new action and deprecate the old one?",Improvement,Blocker,Resolved,"2016-05-26 11:53:42","2016-05-26 10:53:42",5
"Apache Mesos","Master anonymous modules should initialized before any other components.","Anonymous modules on the Master are by design supposed to be independent of any Mesos components. However, there might be a dependency in the reverse direction. For e.g., Anonymous modules might want to influence the behavior of Mesos components (say by generating configuration, that might be consumed later by the components).   The Anonymous modules on the Master therefore need to be initialized before other Mesos components. ",Improvement,Major,Resolved,"2016-05-25 20:42:52","2016-05-25 19:42:52",1
"Apache Mesos","CNI should not store subnet of address in NetworkInfo","When the CNI isolator executes the CNI plugin, that CNI plugin will return an IP Address and Subnet (192.168.0.1/32). Mesos should strip the subnet before storing the address in the Task.NetworkInfo.IPAddress.  Reason being - most current mesos components are not expecting a subnet in the Task's NetworkInfo.IPAddress, and instead expect just the IP address. This can cause errors in those components, such as Mesos-DNS failing to return a NetworkInfo address (and instead defaulting to the next configured IPSource), and Marathon generating invalid links to tasks (as it includes /32 in the link)",Bug,Major,Resolved,"2016-05-25 18:35:42","2016-05-25 17:35:42",2
"Apache Mesos","Agent modules should be initialized before all components except firewall.","On Mesos Agents Anonymous modules should not have any dependencies, by design, on any other Mesos components. This implies that Anonymous modules should be initialized before all other Mesos components other than `Firewall`. The dependency on `Firewall` is primarily to enforce any policies to secure endpoints that might be owned by the Anonymous module.",Improvement,Major,Resolved,"2016-05-25 15:57:49","2016-05-25 14:57:49",1
"Apache Mesos","Make the SASL dependency optional.","Right now there is a hard dependency on SASL, which probably won't work well on Windows (at least) in the near future for our use cases.  In the future, it would be nice to have a pluggable authentication layer.",Bug,Major,Resolved,"2016-05-25 06:26:24","2016-05-25 05:26:24",2
"Apache Mesos","Allow libprocess/stout to build without first doing `make` in 3rdparty.","After the 3rdparty reorg, libprocess/stout are enable to build their dependencies and so one has to do `make` in 3rdpart/ before building libprocess/stout.",Bug,Major,Resolved,"2016-05-24 03:46:33","2016-05-24 02:46:33",2
"Apache Mesos","AppC  appc_simple_discovery_uri_prefix is lost in configuration.md","AppC  appc_simple_discovery_uri_prefix is lost in configuration.md",Bug,Major,Resolved,"2016-05-23 04:45:34","2016-05-23 03:45:34",1
"Apache Mesos","GPU resource broke framework data table in webUI","In agent_framework.html and master/static/agent.html, we add {{GPUs (Used / Allocated)}} in table header. But we didn't add the corresponding column to the table body as well.  On the other hand, we didn't provide statistics for gpus on monitor endpoints. To provide those data in webui, it requires we implement gpus statistics in monitor endpoints firstly. ",Bug,Minor,Resolved,"2016-05-23 02:36:22","2016-05-23 01:36:22",1
"Apache Mesos","Add default implementations to all Isolator virtual functions","Currently, all of the virtual functions in `mesos::slave::Isolator` are pure virtual (expect status()). For many isolators, however, it doesn't make sense to implement all of these virtual functions. Each isolator has to provide its own default implementation of these functions even if they aren't really relying on them. This adds unnecessary extra code to many isolators that don't need them.  Moreover, the `MesosIsolatorProcess` has the same problem for each of its virtual functions.  We should provide defaults for these instead of making each and every isolator implement even in cases when it doesn't make sense.",Improvement,Major,Resolved,"2016-05-23 01:34:19","2016-05-23 00:34:19",1
"Apache Mesos","Relax version compatibility requirement for some modules","Some module interfaces such as authenticatee, have not changed for a while and so we should be able to relax the version compatibility checks. This needs to be done on a case-by-case basis.  I am also hoping, this change will also provide a framework for updating the version requirement for other modules as we go towards a stable module API.  [cc: [~<USER> [~<USER> ]",Task,Major,Accepted,"2016-05-20 21:44:06","2016-05-20 20:44:06",5
"Apache Mesos","Consider using IntervalSet for Port range resource math","Follow-up JIRA for comments raised in MESOS-3051 (see comments there).  We should consider utilizing [{{IntervalSet}}|https://github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/3rdparty/stout/include/stout/interval.hpp] in [Port range resource math|https://github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/src/common/values.cpp#L143].",Improvement,Major,Resolved,"2016-05-20 18:17:30","2016-05-20 17:17:30",3
"Apache Mesos","Implement os::exists for processes","os::exists returns true if the process identified by the parameter is still running or was running and we are able to get information about it, such us the exit code. In Windows after obtaining a handle to the process it is possible perform those operations. ",Improvement,Major,Resolved,"2016-05-20 01:17:01","2016-05-20 00:17:01",1
"Apache Mesos","Document all known client libraries for the Scheduler/Executor API","Previously during various community syncs, we had decided that we would only be supporting the C++ scheduler/executor library in the Mesos code base going forward. We should however, still document the client libraries available in various languages to drive adoption/have a recommended list for users to look up.  This can be similar to the already existing frameworks doc: http://mesos.apache.org/documentation/latest/frameworks/  Other projects also seem to have been following a similar practice: https://docs.docker.com/engine/reference/api/remote_api_client_libraries/ https://github.com/kubernetes/kubernetes/blob/master/docs/devel/client-libraries.md",Documentation,Major,Resolved,"2016-05-20 00:50:22","2016-05-19 23:50:22",2
"Apache Mesos","`network/cni` isolator should skip the bind mounting of the CNI network information root directory if possible","Currently in the create() method `network/cni` isolator, for the CNI network information root directory (i.e., {{/var/run/mesos/isolators/network/cni}}), we do a self bind mount and make sure it is a shared mount of its own peer group. However, we should not do a self bind mount if the mount containing the CNI network information root directory is already a shared mount in its own share peer group, just like what we did for `filesystem/linux` isolator in [MESOS-5239 | https://issues.apache.org/jira/browse/MESOS-5239].",Bug,Major,Resolved,"2016-05-19 14:45:12","2016-05-19 13:45:12",3
"Apache Mesos","Delete the /observe HTTP endpoint","The /observe endpoint was introduced a long time ago for supporting functionality that was never implemented. We should just kill this endpoint and associated code to avoid tech debt.",Bug,Major,Resolved,"2016-05-18 02:36:43","2016-05-18 01:36:43",2
"Apache Mesos","Make fields in authorization::Request protobuf optional.","Currently {{authorization::Request}} protobuf declares {{subject}} and {{object}} as required fields. However, in the codebase we not always set them, which renders the message in the uninitialized state, for example:  * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/common/http.cpp#L603  * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/master/http.cpp#L2057  I believe that the reason why we don't see issues related to this is because we never send authz requests over the wire, i.e., never serialize/deserialize them. However, they are still invalid protobuf messages. Moreover, some external authorizers may serialize these messages.  We can either ensure all required fields are set or make both {{subject}} and {{object}} fields optional. This will also require updating local authorizer, which should properly handle the situation when these fields are absent. We may also want to notify authors of external authorizers to update their code accordingly.  It looks like no deprecation is necessary, mainly because we already—erroneously!—treat these fields as optional.",Bug,Blocker,Resolved,"2016-05-17 22:27:29","2016-05-17 21:27:29",3
"Apache Mesos","Allow `Task` to be authorized.","As we need to be able to authorize `Tasks` (e.g., for deciding whether to include them in the /state endpoint when applying authorization based filtering) we need to expose it to the authorizer. Secondly we also need to include some additional information (`user` and `Env variables`) in order to provide the authorizer  with meaning information.",Improvement,Major,Resolved,"2016-05-17 22:05:05","2016-05-17 21:05:05",3
"Apache Mesos","Introduce ObjectApprover Interface to Authorizer.","As outlined here (https://docs.google.com/document/d/1FuS79P8uj5PIBycrBlkJSBKOtmeO8ezAuiNXxwIA3qA) we plan to add the option of retrieving a FilterObject from the Authorizer with the goal of allowing for efficient authorization of a large number of (potentially large) objects. ",Bug,Major,Resolved,"2016-05-17 22:01:40","2016-05-17 21:01:40",5
"Apache Mesos","Add ability to inject a Volume of Nvidia libraries/binaries into a docker-image container in mesos containerizer.","In order to support Nvidia GPUs with docker containers in Mesos, we need to be able to consolidate all Nvidia libraries into a common volume and inject that volume into the container.  This tracks the support in the mesos containerizer. The docker containerizer support will be tracked separately.  More info on why this is necessary here: https://github.com/NVIDIA/nvidia-docker/",Improvement,Major,Resolved,"2016-05-17 20:23:52","2016-05-17 19:23:52",5
"Apache Mesos","Add preliminary support for parsing ELF files in stout.","The upcoming Nvidia GPU support for docker containers in Mesos relies on consolidating all Nvidia shared libraries into a common location for injecting a volume into a container.  As part of this, we need some preliminary parsing capabilities for ELF file to infer things about each shared library we are consolidating.",Improvement,Minor,Resolved,"2016-05-17 20:14:22","2016-05-17 19:14:22",5
"Apache Mesos","Add utility for parsing ld.so.cache on linux.","The /etc/ld.so.cache file on linux contains a mapping of dynamic library names to their fully resolved paths for use by ld when linking.  We should write a utility that knows how to parse this file so we can find the paths to these libraries as well.  This is especially important for collecting libraries into a common location for supporting Nvidia GPUs in mesos.",Improvement,Minor,Resolved,"2016-05-17 20:07:38","2016-05-17 19:07:38",5
"Apache Mesos","Rewrite os::read() to be friendlier to reading binary files","The existing read() implementation is based on calling getline() to read in chunks of data from a file. This is fine for text-based files, but is a little strange for binary files.",Improvement,Minor,Resolved,"2016-05-17 20:02:26","2016-05-17 19:02:26",3
"Apache Mesos","Slave/Agent Rename Phase 1: Update terms in the website","The following files need to be updated  site/source/index.html.md ",Bug,Major,Resolved,"2016-05-17 19:40:50","2016-05-17 18:40:50",1
"Apache Mesos","Design doc for adding resource limits support for Mesos containerizer","This will be the design doc for MESOS-5391.",Task,Major,"In Progress","2016-05-16 23:54:07","2016-05-16 22:54:07",3
"Apache Mesos","Add support for controlling resource limits in Mesos containerizer.","Currently, we dont have ability to control system resource limits. Add support for : - Frameworks to specify resource limits - Operators to override default resource limits.",Task,Major,Open,"2016-05-16 23:52:13","2016-05-16 22:52:13",5
"Apache Mesos","v1 Executor Protos not included in maven jar","According to MESOS-4793 the Executor v1 HTTP API was released in Mesos 0.28.0 however the corresponding protos are not included in the maven jar for version 0.28.0 or 0.28.1.  Script to verify ",Bug,Major,Resolved,"2016-05-16 21:58:57","2016-05-16 20:58:57",1
"Apache Mesos","docker containerizer should prefix relative volume.container_path values with the path to the sandbox","docker containerizer currently requires absolute paths for values of volume.container_path. this is inconsistent with the mesos containerizer which requires relative container_path. it makes for a confusing API. both at the Mesos level as well as at the Marathon level.  ideally the docker containerizer would allow a framework to specify a relative path for volume.container_path and in such cases automatically convert it to an absolute path by prepending the sandbox directory to it.  /cc [~<USER>",Bug,Major,Resolved,"2016-05-16 20:11:24","2016-05-16 19:11:24",3
"Apache Mesos","MesosContainerizerLaunch flags execute arbitrary commands via shell.","For example, the docker volume isolator's containerPath is appended (without sanitation) to a command that's executed in this manner. As such, it's possible to inject arbitrary shell commands to be executed by mesos.  https://github.com/apache/mesos/blob/17260204c833c643adf3d8f36ad8a1a606ece809/src/slave/containerizer/mesos/launch.cpp#L206  Perhaps instead of strings these commands could/should be sent as string arrays that could be passed as argv arguments w/o shell interpretation?",Bug,Major,Resolved,"2016-05-16 19:28:16","2016-05-16 18:28:16",5
"Apache Mesos","Add `HANDLE` overloads for functions that take a file descriptor",,Bug,Major,Resolved,"2016-05-16 04:23:39","2016-05-16 03:23:39",3
"Apache Mesos","Implement os::setHostname",,Bug,Major,Resolved,"2016-05-14 12:57:45","2016-05-14 11:57:45",1
"Apache Mesos","Implement os::fsync",,Bug,Major,Resolved,"2016-05-14 12:56:48","2016-05-14 11:56:48",1
"Apache Mesos","Killing a queued task can cause the corresponding command executor to never terminate.","We observed this in our testing environment. Sequence of events:  1) A command task is queued since the executor has not registered yet. 2) The framework issues a killTask. 3) Since executor is in REGISTERING state, agent calls `statusUpdate(TASK_KILLED, UPID())` 4) `statusUpdate` now will call `containerizer->status()` before calling `executor->terminateTask(status.task_id(), status);` which will remove the queued task. (Introduced in this patch: https://reviews.apache.org/r/43258). 5) Since the above is async, it's possible that the task is still in queued task when we trying to see if we need to kill unregistered executor in `killTask`: {code}       // TODO(<USER>: Here, we kill the executor if it no longer has       // any task to run and has not yet registered. This is a       // workaround for those single task executors that do not have a       // proper self terminating logic when they haven't received the       // task within a timeout.       if (executor->queuedTasks.empty()) {         CHECK(executor->launchedTasks.empty())             <<  Unregistered executor ' << executor->id             << ' has launched tasks;          LOG(WARNING) << Killing the unregistered executor  << *executor                      <<  because it has no tasks;          executor->state = Executor::TERMINATING;          containerizer->destroy(executor->containerId);       }     {code}  6) Consequently, the executor will never be terminated by Mesos.  Attaching the relevant agent log: ",Bug,Blocker,Resolved,"2016-05-13 20:37:05","2016-05-13 19:37:05",3
"Apache Mesos","Terminating a framework during master failover leads to orphaned tasks","Repro steps:  1) Setup:   2) Kill all three from (1), in the order they were started.  3) Restart the master and agent.  Do not restart the framework.  Result) * The agent will reconnect to an orphaned task. * The Web UI will report no memory usage * {{curl localhost:5050/metrics/snapshot}} will say:  {{master/mem_used: 128,}}  Cause)  When a framework registers with the master, it provides a {{failover_timeout}}, in case the framework disconnects.  If the framework disconnects and does not reconnect within this {{failover_timeout}}, the master will kill all tasks belonging to the framework.  However, the master does not persist this {{failover_timeout}} across master failover.  The master will forget about a framework if: 1) The master dies before {{failover_timeout}} passes. 2) The framework dies while the master is dead.  When the master comes back up, the agent will re-register.  The agent will report the orphaned task(s).  Because the master failed over, it does not know these tasks are orphans (i.e. it thinks the frameworks might re-register).  Proposed solution) The master should save the {{FrameworkID}} and {{failover_timeout}} in the registry.  Upon recovery, the master should resume the {{failover_timeout}} timers.",Bug,Major,Resolved,"2016-05-13 03:45:14","2016-05-13 02:45:14",3
"Apache Mesos","Implement stout/os/windows/kill.hpp","Implement equivalent functionality on Windows ",Improvement,Major,Resolved,"2016-05-13 00:16:17","2016-05-12 23:16:17",5
"Apache Mesos","Add support for Console Ctrl handling in `slave.cpp`","Extract supporting code to handle POSIX signals in a separate header and add support for CTRL handler when running on Windows. ",Improvement,Major,Resolved,"2016-05-12 23:30:49","2016-05-12 22:30:49",3
"Apache Mesos","Remove `Zookeeper's` NTDDI_VERSION define","Zookeeper client library defines NTDDI_VERSION to 0x0400 in winconfig.h. While this API level is suficient to compile the client library,  Mesos have to use a newer API set. After this improvement the code will compile with the latest NTDDI_VERSION.    ",Improvement,Major,Resolved,"2016-05-12 23:16:23","2016-05-12 22:16:23",2
"Apache Mesos","Add random() to os:: namespace ","The function random() is not available in Windows. After this improvement the calls to os::random() will result in calls to ::random() on POSIX and ::rand() on Windows.  ",Improvement,Major,Resolved,"2016-05-12 22:50:41","2016-05-12 21:50:41",1
"Apache Mesos","Add deprecation support for Flags","MESOS-5271 adds support for a flag name to have an alias. This ticket captures the work need to add deprecation support. The idea is for the caller to explicitly specify deprecation via `FlagsBase::add()`  and get a list of deprecation warnings when doing `FlagsBase::load()`.",Improvement,Major,Resolved,"2016-05-12 19:21:36","2016-05-12 18:21:36",5
"Apache Mesos","Introduce a timeout for docker volume driver mount/unmount operation.","'dvdcli' might hang indefinitely. We should introduce timeout for both mount/unmount operation so that launch/cleanup are not blocked forever.",Task,Major,Resolved,"2016-05-11 23:29:17","2016-05-11 22:29:17",2
"Apache Mesos","Add authentication to example frameworks","Some example frameworks do not have the ability to authenticate with the master. Adding authentication to the example frameworks that don't already have it implemented would allow us to use these frameworks for testing in authenticated/authorized scenarios.",Improvement,Major,Resolved,"2016-05-11 00:11:44","2016-05-10 23:11:44",2
"Apache Mesos","Set death signal for dvdcli subprocess in docker volume isolator.","If the slave crashes, we should kill the dvdcli subprocess. Otherwise, if the dvdcli subprocess gets stuck, it'll not be cleaned up.",Improvement,Major,Resolved,"2016-05-10 20:26:20","2016-05-10 19:26:20",2
"Apache Mesos","The scheduler library should have a delay before initiating a connection with master.","Currently, the scheduler library {{src/scheduler/scheduler.cpp}} does have an artificially induced delay when trying to initially establish a connection with the master. In the event of a master failover or ZK disconnect, a large number of frameworks can get disconnected and then thereby overwhelm the master with TCP SYN requests.   On a large cluster with many agents, the master is already overwhelmed with handling connection requests from the agents. This compounds the issue further on the master.",Bug,Major,Resolved,"2016-05-10 18:22:10","2016-05-10 17:22:10",3
"Apache Mesos","Add Windows support for StopWatch",,Improvement,Major,Resolved,"2016-05-10 16:14:15","2016-05-10 15:14:15",2
"Apache Mesos","Use `Connection` abstraction to compare stale connections in scheduler library.","Previously, we had a bug in the {{Connection}} abstraction in libprocess that hindered the ability to pass it onto {{defer}} callbacks since it could sometimes lead to deadlock (MESOS-4658). Now that it is resolved, we might consider not using {{UUID}} objects for stale connection checks but directly using the {{Connection}} abstraction in the scheduler library.",Improvement,Minor,Open,"2016-05-10 01:59:33","2016-05-10 00:59:33",3
"Apache Mesos","Add asynchronous hook for validating docker containerizer tasks","It is possible to plug in custom validation logic for the MesosContainerizer via an {{Isolator}} module, but the same is not true of the DockerContainerizer.  Basic logic can be plugged into the DockerContainerizer via {{Hooks}}, but this has some notable differences compared to isolators: * Hooks are synchronous. * Modifications to tasks via Hooks have lower priority compared to the task itself.  i.e. If both the {{TaskInfo}} and {{slaveExecutorEnvironmentDecorator}} define the same environment variable, the {{TaskInfo}} wins. * Hooks have no effect if they fail (short of segfaulting) i.e. The {{slavePreLaunchDockerHook}} has a return type of {{Try<Nothing>}}: https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/include/mesos/hook.hpp#L90 But the effect of returning an {{Error}} is a log message: https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/hook/manager.cpp#L227-L230  We should add a hook to the DockerContainerizer to narrow this gap.  This new hook would: * Be called at roughly the same place as {{slavePreLaunchDockerHook}} https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/slave/containerizer/docker.cpp#L1022 * Return a {{Future}} and require splitting up {{DockerContainerizer::launch}}. * Prevent a task from launching if it returns a {{Failure}}.",Improvement,Minor,Resolved,"2016-05-09 23:10:39","2016-05-09 22:10:39",5
"Apache Mesos","Enhance the log message when launching docker containerizer.","Log the launch flag which includes the executor command and other information when launching the docker containerizer.",Improvement,Major,Resolved,"2016-05-09 18:25:06","2016-05-09 17:25:06",2
"Apache Mesos","Enhance the log message when launching mesos containerizer.","Log the launch flag which includes the executor command, pre-launch commands and other information when launching the mesos containerizer. ",Improvement,Major,Resolved,"2016-05-09 18:21:15","2016-05-09 17:21:15",2
"Apache Mesos","Design doc for TASK_LOST_PENDING","The TASK_LOST task status describes two different situations: (a) the task was not launched because of an error (e.g., insufficient available resources), or (b) the master lost contact with a running task (e.g., due to a network partition); the master will kill the task when it can (e.g., when the network partition heals), but in the meantime the task may still be running.  This has two problems: 1. Using the same task status for two fairly different situations is confusing. 2. In the partitioned-but-still-running case, frameworks have no easy way to determine when a task has truly terminated.  To address these problems, we propose introducing a new task status, TASK_LOST_PENDING. If a framework opts into this behavior using a new capability, TASK_LOST would mean the task is definitely not running, whereas TASK_LOST_PENDING would mean the task may or may not be running (we've lost contact with the agent), but the master will try to shut it down when possible.",Task,Major,Resolved,"2016-05-09 13:31:56","2016-05-09 12:31:56",5
"Apache Mesos","Behavior of custom HTTP authenticators with disabled HTTP authentication is inconsistent between master and agent","When setting a custom authenticator with {{http_authenticators}} and also specifying {{authenticate_http=false}} currently agents refuse to start with   Masters on the other hand accept this setting.  Having differing behavior between master and agents is confusing, and we should decide on whether we want to accept these settings or not, and make the implementations consistent. ",Bug,Minor,Accepted,"2016-05-09 12:56:27","2016-05-09 11:56:27",3
"Apache Mesos","Create Tests for testing fine-grained HTTP endpoint filtering.",,Improvement,Major,Resolved,"2016-05-06 21:47:50","2016-05-06 20:47:50",3
"Apache Mesos","Add `user` to `Task` protobuf message.","The LocalAuthorizer is supposed to use the OS `user` under which tasks are running for authorization. As the master keeps track of running and completed processes we need access to this information in Task in order to authorize such tasks.",Improvement,Major,Resolved,"2016-05-06 21:44:49","2016-05-06 20:44:49",1
"Apache Mesos","Add Master Flag to enable fine-grained filtering of HTTP endpoints.","As the fine-grained filtering of endpoints can the rather expensive, we should create a master flag to enable/disable this feature.",Improvement,Major,Resolved,"2016-05-06 21:39:27","2016-05-06 20:39:27",1
"Apache Mesos","Add authorization to GET /quota.","We already authorize which http users can set/remove quota for particular roles, but even knowing of the existence of these roles (let alone their quotas) may be sensitive information. We should add authz around GET operations on /quota.",Improvement,Major,Resolved,"2016-05-06 11:01:06","2016-05-06 10:01:06",3
"Apache Mesos","Add authorization to GET /weights.","We already authorize which http users can update weights for particular roles, but even knowing of the existence of these roles (let alone their weights) may be sensitive information. We should add authz around GET operations on /weights.  Easy option: GET_ENDPOINT_WITH_PATH /weights - Pro: No new verb - Con: All or nothing  Complex option: GET_WEIGHTS_WITH_ROLE - Pro: Filters contents based on roles the user is authorized to see - Con: More authorize calls (one per role in each /weights request)",Improvement,Major,Resolved,"2016-05-06 10:52:56","2016-05-06 09:52:56",3
"Apache Mesos","GET /master/maintenance/schedule/ produces 404.","Attempts to make a GET request to /master/maintenance/schedule/ result in a 404. However, if I make a GET request to /master/maintenance/schedule (without the trailing /), it works. My current (untested) theory is that this might be related to the fact that there is also a /master/maintenance/schedule/status endpoint (an endpoint built on top of a functioning endpoint), as requests to /help and /help/ (with and without the trailing slash) produce the same functioning result.",Bug,Minor,Resolved,"2016-05-05 23:03:53","2016-05-05 22:03:53",3
"Apache Mesos","SSL related error messages can be misguiding or incomplete","I was trying to activate SSL within Mesos but had rendered an invalid certificate, it was signed with a mismatching key. Once I started the master, the error message I received was rather confusing to me:    To me, this error message hinted that the key file was not existing or had rights issues. However, a quick {{strace}} revealed  that the key-file was properly accessed, no sign of a file-not-found or alike.  The problem here is the hardcoded error-message, not taking OpenSSL's human readable error strings into account.  The code that misguided me is located at  https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/openssl.cpp#L471  We might want to change   Towards something like this   To receive a much more helpful message like this    A quick scan of the implementation within {{openssl.cpp}} to me suggests that there are more places that we might want to update with more deterministic error messages. ",Bug,Major,Resolved,"2016-05-03 13:20:21","2016-05-03 12:20:21",3
"Apache Mesos","Make `os::close` always catch structured exceptions on Windows",,Bug,Major,Resolved,"2016-05-03 06:40:04","2016-05-03 05:40:04",2
"Apache Mesos","Authorize the agent's '/containers' endpoint.","After the agent's {{/containers}} endpoint is authenticated, we should enabled authorization as well.",Improvement,Major,Resolved,"2016-05-02 20:32:16","2016-05-02 19:32:16",2
"Apache Mesos","Authenticate the agent's '/containers' endpoint.","The {{/containers}} endpoint was recently added to the agent. Authentication should be enabled on this endpoint.",Improvement,Major,Resolved,"2016-05-02 20:30:32","2016-05-02 19:30:32",2
"Apache Mesos","Failed to set quota and update weight according to document","  The right command should be adding {{@}} before the quota json file {{jsonMessageBody}}.",Documentation,Minor,Resolved,"2016-05-01 06:56:05","2016-05-01 05:56:05",1
"Apache Mesos","Env `MESOS_SANDBOX` is not set properly for command tasks that changes rootfs.","This is in the context of Mesos containerizer (a.k.a., unified containerizer).  I did a simple test:   `MESOS_SANDBOX` above should be `/mnt/mesos/sandbox`.",Bug,Major,Resolved,"2016-04-30 00:01:11","2016-04-29 23:01:11",2
"Apache Mesos","Enable `network/cni` isolator to allow modifications and deletion of CNI config","Currently the `network/cni` isolator can only load the CNI configs at startup. This makes the CNI networks immutable. From an operational standpoint this can make deployments painful for operators.   To make CNI more flexible the `network/cni` isolator should be able to load configs at run time.   The proposal is to add an endpoint to the `network/cni` isolator, to which when the operator sends a PUT request the `network/cni` isolator will reload  CNI configs. ",Task,Major,Resolved,"2016-04-29 20:28:59","2016-04-29 19:28:59",5
"Apache Mesos","Sandbox mounts should not be in the host mount namespace.","Currently, if a container uses container image, we'll do a bind mount of its sandbox (<sandbox> -> <rootfs>/mnt/mesos/sandbox) in the host mount namespace.  However, doing the mounts in the host mount table is not ideal. That complicates both the cleanup path and the recovery path.  Instead, we can do the sandbox bind mount in the container's mount namespace so that cleanup and recovery will be greatly simplified. We can setup mount propagation properly so that persistent volumes mounted at <sandbox>/xxx can be propagated into the container.  Here is a simple proof of concept:  Console 1:   Console 2:   Console 1:   Console 2: ",Improvement,Major,Resolved,"2016-04-29 02:18:30","2016-04-29 01:18:30",5
"Apache Mesos","/metrics/snapshot endpoint help disappeared on agent.","After https://github.com/apache/mesos/commit/066fc4bd0df6690a5e1a929d3836e307c1e22586 the help for the /metrics/snapshot endpoint on the agent doesn't appear anymore (Master endpoint help is unchanged).",Bug,Major,Resolved,"2016-04-28 23:07:11","2016-04-28 22:07:11",1
"Apache Mesos","Add capabilities support for mesos execute cli.","Add support for `user` and `capabilities` to execute cli. This will help in testing the `capabilities` feature for unified containerizer.",Bug,Major,Resolved,"2016-04-28 22:53:21","2016-04-28 21:53:21",3
"Apache Mesos","Consider adding an Executor Shim/Adapter for the new/old API","Currently, all the business logic for HTTP based command executor/driver based command executor lives in 2 different files. As more features are added/bugs are discovered in the executor itself, they need to be fixed in two places. It would be nice to have some kind of a shim/adapter that abstracts away the underlying library details from the executor. Hence, the executor can toggle between whether it wants to use the driver or the new API via an environment variable.",Improvement,Major,Resolved,"2016-04-28 20:04:45","2016-04-28 19:04:45",5
"Apache Mesos","Add synchronous validation for all types of Calls.","Currently, we do a best effort validation for all calls sent to the master from the scheduler by invoking {{validation::scheduler::call::validate(call, principal)}}. This is a generic validation helper for all calls. However, for more fine grained validation for a particular call, we invoke the validation as part of the call handle itself.    This in turn makes all validations asynchronous i.e. the framework gets them as {{Event::ERROR}} events later. It would be good if such validations can be handled while processing the {{Call}} message itself synchronously.",Improvement,Major,Accepted,"2016-04-28 17:29:17","2016-04-28 16:29:17",5
"Apache Mesos","Add authorization to the master's /flags endpoint.","Coarse HTTP endpoint authorization using the {{GET_ENDPOINT_WITH_PATH}} ACL rule needs to be added to the /flags endpoint of the master.",Task,Major,Resolved,"2016-04-28 08:51:45","2016-04-28 07:51:45",3
"Apache Mesos","Split Resource and Inverse offer protobufs for V1 API","The protobufs for the V1 api regarding inverse offers initially re-used the existing offer / rescind / accept / decline messages for regular offers. We should split these out the be more explicit, and provide the ability to augment the messages with particulars to either resource or inverse offers.",Improvement,Blocker,Resolved,"2016-04-28 01:50:57","2016-04-28 00:50:57",5
"Apache Mesos","Status updates after a health check are incomplete or invalid","With command health checks enabled via marathon, mesos-dns will resolve the task correctly until the task is reported as healthy. At that point, mesos-dns stops resolving the task correctly.  -Digging through src/docker/executor.cpp, I found that in the {{taskHealthUpdated()}} function is attempting to copy the taskID to the new status instance with-    -but other instances of status updates have a similar line-    -My assumption is that this difference is causing the status update after a health check to not have a proper taskID, which in turn is causing an incorrect state.json output.-  -I'll try to get a patch together soon.-  UPDATE: None of the above assumption are correct. Something else is causing the issue.",Bug,Major,Resolved,"2016-04-27 20:53:52","2016-04-27 19:53:52",1
"Apache Mesos","Add authorization to libprocess HTTP endpoints","Now that the libprocess-level HTTP endpoints have had authentication added to them in MESOS-4902, we can add authorization to them as well. As a first step, we can implement a coarse-grained approach, in which a principal is granted or denied access to a given endpoint. We will likely need to register an authorizer with libprocess.",Improvement,Major,Resolved,"2016-04-26 15:31:33","2016-04-26 14:31:33",5
"Apache Mesos","Need to add REMOVE semantics to the copy backend","Some Dockerfiles run the `rm` command to remove files from the base image using the RUN directive in the Dockerfile. An example can be found here: https://github.com/ngineered/nginx-php-fpm.git  In the final rootfs the removed files should not be present. Presence of these files in the final image can make the container misbehave. For example, the nginx-php-fpm docker image that is referenced tries to remove the default nginx config and replaces it with its own config to point to a different HTML root. If the default nginx config is still present after the building the image, nginx will start pointing to a different HTML root than the one set in the Dockerfile.   Currently the copy backend cannot handle removal of files from intermediate layers. This can cause issues with docker images built using a Dockerfile similar to the one listed here. Hence, we need to add REMOVE semantics to the copy backend.  ",Bug,Major,Resolved,"2016-04-25 22:28:10","2016-04-25 21:28:10",5
"Apache Mesos","Add capabilities support for unified containerizer.","Add capabilities support for unified containerizer.   Requirements: 1. Use the mesos capabilities API. 2. Frameworks be able to add capability requests for containers. 3. Agents be able to add maximum allowed capabilities for all containers launched.  Design document: https://docs.google.com/document/d/1YiTift8TQla2vq3upQr7K-riQ_pQ-FKOCOsysQJROGc/edit#heading=h.rgfwelqrskmd ",Task,Major,Resolved,"2016-04-25 19:05:31","2016-04-25 18:05:31",5
"Apache Mesos","Need support for Authorization information via HELP.","We should add information about authentication to the help message and thereby endpoint documentation (similarly as MESOS-4934 has done for authentication).",Improvement,Major,Resolved,"2016-04-25 09:49:38","2016-04-25 08:49:38",3
"Apache Mesos","Support docker image labels.","Docker image labels should be supported in unified containerizer, which can be used for applying custom metadata. Image labels are necessary for mesos features to support docker in unified containerizer (e.g., for mesos GPU device isolator).",Task,Major,Resolved,"2016-04-25 07:25:59","2016-04-25 06:25:59",3
"Apache Mesos","Add alias support for Flags","Currently there is no support for a flag to have an alias. Such support would be useful to rename/deprecate a flag.  For example, for MESOS-4386, we could let the flag have `--authenticate` name and a `--authenticate_frameworks` alias. The alias can be marked as deprecated (need to add support for this as well).  This support will also be useful for slave/agent flag rename. See MESOS-3781 for details. ",Improvement,Major,Resolved,"2016-04-25 06:41:28","2016-04-25 05:41:28",5
"Apache Mesos","add test cases for docker volume driver",,Bug,Major,Resolved,"2016-04-25 01:37:45","2016-04-25 00:37:45",5
"Apache Mesos","Update mesos-execute to support docker volume isolator.","The mesos-execute needs to be updated to support docker volume isolator.",Bug,Major,Resolved,"2016-04-25 01:37:08","2016-04-25 00:37:08",3
"Apache Mesos","pivot_root is not available on ARM","When compile on ARM, it will through error. The current code logic in src/linux/fs.cpp is:    Possible sollution is to add `unistd.h` header",Bug,Major,Resolved,"2016-04-24 21:21:28","2016-04-24 20:21:28",1
"Apache Mesos","Combine the internal::slave::Fetcher class and mesos-fetcher binary","After [MESOS-5259], the {{mesos-fetcher}} will no longer need to be a separate binary and can be safely folded back into the agent process.  (It was a separate binary because libcurl has synchronous/blocking calls.)    This will likely mean: * A change to the {{fetch}} continuation chain:   https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/slave/containerizer/fetcher.cpp#L315 * This protobuf can be deprecated (or just removed):   https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/include/mesos/fetcher/fetcher.proto",Task,Major,Accepted,"2016-04-23 01:16:47","2016-04-23 00:16:47",3
"Apache Mesos","Extend the uri::Fetcher::Plugin interface to include a fetchSize","In order to replace the {{mesos-fetcher}} binary with the {{uri::Fetcher}}, each plugin must be able to determine/estimate the size of a download.  This is used by the Fetcher cache when it creates cache entries and such.  The logic for each of the four {{Fetcher::Plugin}}s can be taken and refactored from the existing fetcher. https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/slave/containerizer/fetcher.cpp#L267",Task,Major,Accepted,"2016-04-23 00:42:08","2016-04-22 23:42:08",2
"Apache Mesos","Refactor the mesos-fetcher binary to use the uri::Fetcher as a backend","This is an intermediate step for combining the {{mesos-fetcher}} binary and {{uri::Fetcher}}.    The {{download}} method should be replaced with {{uri::Fetcher::fetch}}. https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/launcher/fetcher.cpp#L179  Combining the two will: * Attach the {{uri::Fetcher}} to the existing Fetcher caching logic. * Remove some code duplication for downloading URIs.",Task,Major,Accepted,"2016-04-23 00:27:48","2016-04-22 23:27:48",3
"Apache Mesos","Turn the Nvidia GPU isolator into a module","The Nvidia GPU isolator has an external dependence on `libnvidia-ml.so`. As it currently stands, this forces *all* binaries that link with `libmesos.so` to also link with `libnvidia-ml.so` (including master, agents on machines without GPUs, scheduler, exectors, etc.).  By turning the Nvidia GPU isolator into a module, it will be loaded at runtime only when an agent has explicitly including the the Nvidia GPU isolator in its `--isolation` flag.",Task,Major,Resolved,"2016-04-22 23:37:17","2016-04-22 22:37:17",5
"Apache Mesos","Add autodiscovery for GPU resources","Right now, the only way to enumerate the available GPUs on an agent is to use the `--nvidia_gpu_devices` flag and explicitly list them out.  Instead, we should leverage NVML to autodiscover the GPUs that are available and only use this flag as a way to explicitly list out the GPUs you want to make available in order to restrict access to some of them.",Task,Major,Resolved,"2016-04-22 22:58:09","2016-04-22 21:58:09",3
"Apache Mesos","Add support for per-containerizer resource enumeration","Currently the top level containerizer includes a static function for enumerating the resources available on a given agent. Ideally, this functionality should be the responsibility of individual containerizers (and specifically the responsibility of each isolator used to control access to those resources).  Adding support for this will involve making the `Containerizer::resources()` function virtual instead of static and then implementing it on a per-containerizer basis.  We should consider providing a default to make this easier in cases where there is only really one good way of enumerating a given set of resources.",Task,Major,Resolved,"2016-04-22 22:53:42","2016-04-22 21:53:42",3
"Apache Mesos","Add GPUs to container resource consumption metrics.","Currently the usage callback in the Nvidia GPU isolator is unimplemented:    It should use functionality from NVML to gather the current GPU usage and add it to a ResourceStatistics object. It is still an open question as to exactly what information we want to expose here (power, memory consumption, current load, etc.). Whatever we decide on should be standard across different GPU types, different GPU vendors, etc.",Task,Major,Accepted,"2016-04-22 22:38:07","2016-04-22 21:38:07",3
"Apache Mesos","Add URI parsing function/library","The {{uri::Fetcher}} theoretically supports all URIs, per [RFC3986|http://tools.ietf.org/html/rfc3986].  To do this, we need a spec-compliant parser from string to URI.  [uriparser|http://uriparser.sourceforge.net/] appears to fit the bill.",Task,Major,Accepted,"2016-04-22 20:17:46","2016-04-22 19:17:46",2
"Apache Mesos","Isolator cleanup should not be invoked if they are not prepared yet.","If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet.   In this case, there no need to clean up any isolator, call provisioner destroy directly.",Bug,Major,Resolved,"2016-04-22 18:58:01","2016-04-22 17:58:01",2
"Apache Mesos","Move 3rdparty/libprocess/3rdparty/* to 3rdparty/",,Task,Major,Resolved,"2016-04-21 23:32:13","2016-04-21 22:32:13",5
"Apache Mesos","Update CMake files to reflect reorganized 3rdparty",,Task,Major,Resolved,"2016-04-21 22:53:17","2016-04-21 21:53:17",2
"Apache Mesos","Remove '/system/stats.json' endpoint","The {{/system/stats.json}} endpoint was deprecated by MESOS-2058. This endpoint can now be removed.",Task,Major,Open,"2016-04-21 21:46:18","2016-04-21 20:46:18",1
"Apache Mesos","Command executor may escalate after the task is reaped.","In command executor, {{escalated()}} may be scheduled before the task has been killed, i.e. {{reaped()}}, but called after. In this case {{escalated()}} should be a no-op.",Bug,Minor,Resolved,"2016-04-21 14:57:51","2016-04-21 13:57:51",1
"Apache Mesos","Persistent volume DockerContainerizer support assumes proper mount propagation setup on the host.","We recently added persistent volume support in DockerContainerizer (MESOS-3413). To understand the problem, we first need to understand how persistent volumes are supported in DockerContainerizer.  To support persistent volumes in DockerContainerizer, we bind mount persistent volumes under a container's sandbox ('container_path' has to be relative for persistent volumes). When the Docker container is launched, since we always add a volume (-v) for the sandbox, the persistent volumes will be bind mounted into the container as well (since Docker does a 'rbind').  The assumption that the above works is that the Docker daemon should see those persistent volume mounts that Mesos mounts on the host mount table. It's not a problem if Docker daemon itself is using the host mount namespace. However, on systemd enabled systems, Docker daemon is running in a separate mount namespace and all mounts in that mount namespace will be marked as slave mounts due to this [patch|https://github.com/docker/docker/commit/eb76cb2301fc883941bc4ca2d9ebc3a486ab8e0a].  So what that means is that: in order for it to work, the parent mount of agent's work_dir should be a shared mount when docker daemon starts. This is typically true on CentOS7, CoreOS as all mounts are shared mounts by default.  However, this causes an issue with the 'filesystem/linux' isolator. To understand why, first I need to show you a typical problem when dealing with shared mounts. Let me explain that using the following commands on a CentOS7 machine:   As you can see above, there're two entries (/run/netns/test) in the mount table (unexpected). This will confuse some systems sometimes. The reason is because when we create a self bind mount (/run/netns -> /run/netns), the mount will be put into the same shared mount peer group (shared:22) as its parent (/run). Then, when you create another mount underneath that (/run/netns/test), that mount operation will be propagated to all mounts in the same peer group (shared:22), resulting an unexpected additional mount being created.  The reason we need to do a self bind mount in Mesos is that sometimes, we need to make sure some mounts are shared so that it does not get copied when a new mount namespace is created. However, on some systems, mounts are private by default (e.g., Ubuntu 14.04). In those cases, since we cannot change the system mounts, we have to do a self bind mount so that we can set mount propagation to shared. For instance, in filesytem/linux isolator, we do a self bind mount on agent's work_dir.  To avoid the self bind mount pitfall mentioned above, in filesystem/linux isolator, after we created the mount, we do a make-slave + make-shared so that the mount is its own shared mount peer group. In that way, any mounts underneath it will not be propagated back.  However, that operation will break the assumption that the persistent volume DockerContainerizer support makes. As a result, we're seeing problem with persistent volumes in DockerContainerizer when filesystem/linux isolator is turned on.",Bug,Major,Resolved,"2016-04-21 03:28:35","2016-04-21 02:28:35",3
"Apache Mesos","CHECK failure in AppcProvisionerIntegrationTest.ROOT_SimpleLinuxImageTest","Observed on the Mesosphere internal CI:    Complete test log will be attached as a file.",Bug,Major,Resolved,"2016-04-21 01:26:10","2016-04-21 00:26:10",2
"Apache Mesos","The windows version of `os::access` has differing behavior than the POSIX version.","The POSIX version of {{os::access}} looks like this:    Compare this to the Windows version of {{os::access}} which looks like this following:    As we can see, the case where {{errno}} is set to {{EACCES}} is handled differently between the 2 functions.  We can actually consolidate the 2 functions by simply using the POSIX version. The challenge is that on POSIX, we should use {{::access}} and {{::_access}} on Windows. Note however, that this problem is already solved, as we have an implementation of {{::access}} for Windows in {{3rdparty/libprocess/3rdparty/stout/include/stout/windows.hpp}} which simply defers to {{::_access}}.  Thus, I propose to simply consolidate the 2 implementations.",Bug,Major,Resolved,"2016-04-20 00:31:49","2016-04-19 23:31:49",2
"Apache Mesos","Add capability information to ContainerInfo protobuf message.","To enable support for capability as first class framework entity, we need to add capabilities related information to the ContainerInfo protobuf.",Task,Major,Resolved,"2016-04-19 07:25:09","2016-04-19 06:25:09",1
"Apache Mesos","Create Design Doc for Manage offers in allocator",,Task,Major,Accepted,"2016-04-19 04:43:01","2016-04-19 03:43:01",5
"Apache Mesos","Add tests for Capability API.","Add basic tests for the capability API.",Task,Major,Resolved,"2016-04-18 23:05:03","2016-04-18 22:05:03",3
"Apache Mesos","Implement HTTP Docker Executor that uses the Executor Library","Similar to what we did with the HTTP command executor in MESOS-3558 we should have a HTTP docker executor that can speak the v1 Executor API.",Bug,Major,Reviewable,"2016-04-18 20:03:19","2016-04-18 19:03:19",5
"Apache Mesos","Add benchmark for writing events on the persistent connection.","It would be good to add a benchmark for testing writing events on the persistent connection for HTTP frameworks wrt driver based frameworks. The benchmark can be as simple as trying to stream generated reconciliation status update events on the persistent connection between the master and the scheduler.",Task,Major,Resolved,"2016-04-14 23:46:04","2016-04-14 22:46:04",3
"Apache Mesos","Add Documentation for Nvidia GPU support",https://reviews.apache.org/r/46220/,Documentation,Minor,Resolved,"2016-04-14 21:32:14","2016-04-14 20:32:14",5
"Apache Mesos","Document docker volume driver isolator.","Should include the followings:  1. What features (driver options) are supported in docker volume driver isolator. 2. How to use docker volume driver isolator.     *related agent flags introduction and usage.     *isolator dependency clarification (e.g., filesystem/linux).     *related driver daemon preprocess.     *volumes pre-specified by users and volume cleanup.",Bug,Major,Resolved,"2016-04-14 00:00:46","2016-04-13 23:00:46",5
"Apache Mesos","Update the documentation for '/reserve' and '/create-volumes'","There are a couple issues related to the {{principal}} field in {{DiskInfo}} and {{ReservationInfo}} (see linked JIRAs) that should be better documented. We need to help users understand the purpose of these fields and how they interact with the principal provided in the HTTP authentication header. See linked tickets for background.",Documentation,Major,Resolved,"2016-04-13 23:05:02","2016-04-13 22:05:02",1
"Apache Mesos","Populate FrameworkInfo.principal for authenticated frameworks","If a framework authenticates and then does not provide a {{principal}} in its {{FrameworkInfo}}, we currently allow this and leave {{FrameworkInfo.principal}} unset. Instead, we should populate {{FrameworkInfo.principal}} for them automatically in that case to ensure that the two principals are equal.",Improvement,Major,Resolved,"2016-04-13 22:48:02","2016-04-13 21:48:02",2
"Apache Mesos","Operator endpoints should accept a principal without HTTP authentication","Mesos currently provides no way for operators to include their principal with HTTP endpoint requests when HTTP authentication is disabled. To remedy this, we should add optional {{principal}} fields to the relevant protobuf messages. When HTTP authentication is enabled, we can allow the user to leave this field empty and populate it with the principal from their HTTP Auth header.",Improvement,Major,Open,"2016-04-13 22:39:16","2016-04-13 21:39:16",3
"Apache Mesos","Allow any principal in ReservationInfo when HTTP authentication is off","Mesos currently provides no way for operators to pass their principal to HTTP endpoints when HTTP authentication is off. Since we enforce that {{ReservationInfo.principal}} be equal to the operator principal in requests to {{/reserve}}, this means that when HTTP authentication is disabled, the {{ReservationInfo.principal}} field cannot be set.  To address this in the short-term, we should allow {{ReservationInfo.principal}} to hold any value when HTTP authentication is disabled.",Improvement,Major,Resolved,"2016-04-13 22:30:37","2016-04-13 21:30:37",1
"Apache Mesos","The mesos-execute prints confusing message when launching tasks.",,Bug,Minor,Resolved,"2016-04-13 00:29:12","2016-04-12 23:29:12",1
"Apache Mesos","The filesystem/linux isolator does not set the permissions of the host_path.","The {{filesystem/linux}} isolator is not a drop in replacement for the {{filesystem/shared}} isolator. This should be considered before the latter is deprecated.  We are currently using the {{filesystem/shared}} isolator together with the following slave option. This provides us with a private {{/tmp}} and {{/var/tmp}} folder for each task.    When browsing the Mesos sandbox, one can see the following permissions:   However, when running with the new {{filesystem/linux}} isolator, the permissions are different:   This prevents user code (running as a non-root user) from writing to those folders, i.e. every write attempt fails with permission denied.   *Context*: * We are using Apache Aurora. Aurora is running its custom executor as root but then switches to a non-privileged user before running the actual user code.  * The follow code seems to have enabled our usecase in the existing {{filesystem/shared}} isolator: https://github.com/apache/mesos/blob/4d2b1b793e07a9c90b984ca330a3d7bc9e1404cc/src/slave/containerizer/mesos/isolators/filesystem/shared.cpp#L175-L198 ",Bug,Major,Resolved,"2016-04-12 10:10:58","2016-04-12 09:10:58",3
"Apache Mesos","Master should reject calls from the scheduler driver if the scheduler is not connected.","When a scheduler registers, the master will create a link from master to scheduler.  If this link breaks, the master will consider the scheduler {{inactive}} and mark it as {{disconnected}}.  This causes a couple problems: 1) Master does not send offers to {{inactive}} schedulers.  But these schedulers might consider themselves registered in a one-way network partition scenario. 2) Any calls from the {{inactive}} scheduler is still accepted, which leaves the scheduler in a starved, but semi-functional state.  See the related issue for more context: MESOS-5180  There should be an additional guard for registered, but {{inactive}} schedulers here: https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/master.cpp#L1977  The HTTP API already does this: https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/http.cpp#L459  Since the scheduler driver cannot return a 403, it may be necessary to return a {{Event::ERROR}} and force the scheduler to abort.",Bug,Major,Resolved,"2016-04-12 01:16:59","2016-04-12 00:16:59",1
"Apache Mesos","Scheduler driver does not detect disconnection with master and reregister.","The existing implementation of the scheduler driver does not re-register with the master under some network partition cases.  When a scheduler registers with the master: 1) master links to the framework 2) framework links to the master  It is possible for either of these links to break *without* the master changing.  (Currently, the scheduler driver will only re-register if the master changes).  If both links break or if just link (1) breaks, the master views the framework as {{inactive}} and {{disconnected}}.  This means the framework will not receive any more events (such as offers) from the master until it re-registers.  There is currently no way for the scheduler to detect a one-way link breakage.  if link (2) breaks, it makes (almost) no difference to the scheduler.  The scheduler usually uses the link to send messages to the master, but libprocess will create another socket if the persistent one is not available.  To fix link breakages for (1+2) and (2), the scheduler driver should implement a `::exited` event handler for the master's {{pid}} and trigger a master (re-)detection upon a disconnection. This in turn should make the driver (re)-register with the master. The scheduler library already does this: https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L395  See the related issue MESOS-5181 for link (1) breakage.",Bug,Major,Accepted,"2016-04-12 01:09:56","2016-04-12 00:09:56",3
"Apache Mesos","Enhance the error message for Duration flag.","Enhance the error message for  https://github.com/apache/mesos/blob/4dfa91fc21f80204f5125b2e2f35c489f8fb41d8/3rdparty/libprocess/3rdparty/stout/include/stout/duration.hpp#L70 to list all of the supported duration unit.",Improvement,Minor,Resolved,"2016-04-12 00:25:06","2016-04-11 23:25:06",1
"Apache Mesos","Add logic to validate for non-fractional GPU requests in the master","We should not put this logic directly into the  'Resources::validate()' function. The primary reason is that the existing 'Resources::validate()' function doesn't consider the semantics of any particular resource when performing its validation (it only makes sure that the fields in the 'Resource' protobuf message are correctly formed). Since a fractional 'gpus' resources is actually well-formed (and only semantically incorrect), we should push this validation logic up into the master.      Moreover, the existing logic to construct a 'Resources' object from a 'RepeatedPtrField<Resource>' silently drops any resources that don't pass 'Resources::validate()'. This means that if we were to push the non-fractional 'gpus' validation into 'Resources::validate()', the 'gpus' resources would just be silently dropped rather than causing a TASK_ERROR in the master. This is obviously *not* the desired behaviour.",Task,Major,Resolved,"2016-04-11 23:06:49","2016-04-11 22:06:49",2
"Apache Mesos","Update the balloon-framework to run on test clusters","There are a couple of problems with the balloon framework that prevent it from being deployed (easily) on an actual cluster:  * The framework accepts 100% of memory in an offer.  This means the expected behavior (finish or OOM) is dependent on the offer size. * The framework assumes the {{balloon-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments. * The framework does not specify CPUs with the executor.  This is required by many isolators. * The executor's {{TASK_FINISHED}} logic path was untested and is flaky. * The framework has no metrics. * The framework only launches a single task and then exits.  With this behavior, we can't have useful metrics. ",Improvement,Major,Resolved,"2016-04-11 19:22:03","2016-04-11 18:22:03",3
"Apache Mesos","Allow master/agent to take multiple modules manifest files","When loading multiple modules into master/agent, one has to merge all module metadata (library name, module name, parameters, etc.) into a single json file which is then passed on to the --modules flag. This quickly becomes cumbersome especially if the modules are coming from different vendors/developers.  An alternate would be to allow multiple invocations of --modules flag that can then be passed on to the module manager. That way, each flag corresponds to just one module library and modules from that library.  Another approach is to create a new flag (e.g., --modules-dir) that contains a path to a directory that would contain multiple json files. One can think of it as an analogous to systemd units. The operator that drops a new file into this directory and the file would automatically be picked up by the master/agent module manager. Further, the naming scheme can also be inherited to prefix the filename with an NN_ to signify oad order.",Task,Blocker,Resolved,"2016-04-11 17:27:57","2016-04-11 16:27:57",3
"Apache Mesos","Registry puller cannot fetch blobs correctly from http Redirect 3xx urls.","When the registry puller is pulling a private repository from some private registry (e.g., quay.io), errors may occur when fetching blobs, at which point fetching the manifest of the repo is finished correctly. The error message is `Unexpected HTTP response '400 Bad Request' when trying to download the blob`. This may arise from the logic of fetching blobs, or incorrect format of uri when requesting blobs.",Bug,Blocker,Resolved,"2016-04-11 17:23:59","2016-04-11 16:23:59",3
"Apache Mesos","Expose state/state.hpp to public headers","We want the Modules to be able to use replicated log along with the APIs to communicate with Zookeeper. This change would require us to expose at least the following headers state/storage.hpp, and any additional files that state.hpp depends on (e.g., zookeeper/authentication.hpp).",Task,Major,Resolved,"2016-04-11 17:16:58","2016-04-11 16:16:58",3
"Apache Mesos","Adapt json creation for authorization based endpoint filtering.","For authorization based endpoint filtering we need to adapt the json endpoint creation as discussed in MESOS-4931.",Improvement,Major,Resolved,"2016-04-11 17:08:10","2016-04-11 16:08:10",5
"Apache Mesos","Introduce new Authorizer Actions for Authorized based filtering of endpoints.","For authorization based endpoint filtering we need to introduce the authorizer actions outlined via MESOS-4932.",Improvement,Major,Resolved,"2016-04-11 17:05:29","2016-04-11 16:05:29",3
"Apache Mesos","Benchmark overhead of authorization based filtering.","When adding authorization based filtering as outlined in MESOS-4931 we need to be careful especially for performance critical endpoints such as /state.  We should ensure via a benchmark that performance does not degreade below an acceptable state.",Improvement,Major,Resolved,"2016-04-11 17:02:37","2016-04-11 16:02:37",3
"Apache Mesos","Add tests for `network/cni` isolator","We need to add tests to verify the functionality of `network/cni` isolator.",Task,Major,Resolved,"2016-04-11 14:01:47","2016-04-11 13:01:47",5
"Apache Mesos","Add authorization to agent's /monitor/statistics endpoint.","Operators may want to enforce that only specific authorized users be able to view per-executor resource usage statistics. For 0.29 MVP, we can make this coarse-grained, and assume that only the operator or a operator-privileged monitoring service will be accessing the endpoint. For a future release, we can consider fine-grained authz that filters statistics like we plan to do for /tasks.",Task,Major,Resolved,"2016-04-11 08:46:32","2016-04-11 07:46:32",5
"Apache Mesos","Commit message hook behaves incorrectly when a message includes a *.","If there is a \* in a commit message (there often is when we have bulleted lists), due to the current use of {{echo $LINE}}, the {{$LINE}} gets expanded with a * in it, which becomes a matcher in bash and therefore subsequently gets expanded into the list of files/directories in the current directory.  In order to avoid this mess, we need to wrap such variables in quotes, like so: {{echo $LINE}}.",Bug,Major,Resolved,"2016-04-10 07:38:08","2016-04-10 06:38:08",2
"Apache Mesos","Make `network/cni` enabled as the default network isolator for `MesosContainerizer`.","Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator. ",Task,Major,Resolved,"2016-04-10 04:09:28","2016-04-10 03:09:28",1
"Apache Mesos","Add test to verify error when requesting fractional GPUs","Fractional GPU requests should immediately cause a TASK_FAILED without ever launching the task.",Task,Major,Resolved,"2016-04-09 00:32:07","2016-04-08 23:32:07",1
"Apache Mesos","Update webui for GPU metrics","After adding the GPU metrics and updating the resources JSON to include GPU information, the webui should be updated accordingly.",Task,Major,Resolved,"2016-04-08 21:30:46","2016-04-08 20:30:46",1
"Apache Mesos","Run mesos builds on PowerPC platform in ASF CI","This is the last step to declare official support for PowerPC.  This is currently blocked on ASF INFRA adding PowerPC based Jenkins machines to the ASF CI. ",Bug,Major,Resolved,"2016-04-08 19:44:40","2016-04-08 18:44:40",1
"Apache Mesos","Consolidate authorization actions for quota.","We should have just a single authz action: {{UPDATE_QUOTA_WITH_ROLE}}. It was a mistake in retrospect to introduce multiple actions.  Actions that are not symmetrical are register/teardown and dynamic reservations. The way they are implemented in this way is because entities that do one action differ from entities that do the other. For example, register framework is issued by a framework, teardown by an operator. What is a good way to identify a framework? A role it runs in, which may be different each launch and makes no sense in multi-role frameworks setup or better a sort of a group id, which is its principal. For dynamic reservations and persistent volumes, they can be both issued by frameworks and operators, hence similar reasoning applies.   Now, quota is associated with a role and set only by operators. Do we need to care about principals that set it? Not that much. ",Improvement,Major,Resolved,"2016-04-08 19:14:01","2016-04-08 18:14:01",5
"Apache Mesos","Sandboxes contents should be protected from unauthorized users","MESOS-4956 introduced authentication support for the sandboxes. However, authentication can only go as far as to tell whether an user is known to mesos or not. An extra additional step is necessary to verify whether the known user is allowed to executed the requested operation on the sandbox (browse, read, download, debug).",Bug,Major,Resolved,"2016-04-08 11:35:00","2016-04-08 10:35:00",8
"Apache Mesos","Add authentication to agent's /monitor/statistics endpoint","Operators may want to enforce that only authenticated users (and subsequently only specific authorized users) be able to view per-executor resource usage statistics. Since this endpoint is handled by the ResourceMonitorProcess, I would expect the work necessary to be similar to what was done for /files or /registry endpoint authn.",Task,Major,Resolved,"2016-04-08 10:48:28","2016-04-08 09:48:28",2
"Apache Mesos","MasterAllocatorTest/1.RebalancedForUpdatedWeights is flaky.","Observed on the ASF CI:  ",Bug,Major,Resolved,"2016-04-08 00:54:43","2016-04-07 23:54:43",1
"Apache Mesos","Cleanup memory leaks in libprocess finalize()","libprocess's {{finalize}} function currently leaks memory for a few different reasons. Cleaning up the {{SocketManager}} will be somewhat involved (MESOS-3910), but the remaining memory leaks should be fairly easy to address.",Task,Major,Resolved,"2016-04-07 22:22:58","2016-04-07 21:22:58",2
"Apache Mesos","Add agent flags for HTTP authorization.","Flags should be added to the agent to: 1. Enable authorization ({{--authorizers}}) 2. Provide ACLs ({{--acls}})",Bug,Major,Resolved,"2016-04-07 09:39:17","2016-04-07 08:39:17",2
"Apache Mesos","Some ProvisionerDockerLocalStoreTest.* are flaky due to tar issue.","These tests are still occasionally fail as of Mesos 1.5.0-wip:      Found this on ASF CI while testing 0.28.1-rc2    ",Bug,Major,Accepted,"2016-04-07 04:45:43","2016-04-07 03:45:43",2
"Apache Mesos","Fix Nvidia GPU test build for namespace change of MasterDetector","An update to master the day after all of the Nvidia GPU stuff landed has a build error in the Nvidia GPU tests. The namespace that MasterDetector lives in has changed and the test needs to be updated to pull in the class from the proper namespace now.",Bug,Major,Resolved,"2016-04-07 02:40:58","2016-04-07 01:40:58",1
"Apache Mesos","Remove 'dashboard.js' from the webui.","This file is no longer in use anywhere.",Task,Major,Resolved,"2016-04-07 02:25:01","2016-04-07 01:25:01",1
"Apache Mesos","Update the default JSON representation of a Resource to include GPUs","The default JSON representation of a Resource currently lists a value of 0 if no value is set on a first class SCALAR resource (i.e. cpus, mem, disk).  We should add GPUs in here as well.  ",Task,Major,Resolved,"2016-04-07 02:21:15","2016-04-07 01:21:15",1
"Apache Mesos","Update existing documentation to Include references to GPUs as a first class resource.","Specifically, the documentation in the following files should be udated:  ",Task,Major,Resolved,"2016-04-07 02:16:51","2016-04-07 01:16:51",1
"Apache Mesos","Expose TaskStatus source & reason in master's '/state' output","It would be helpful if the TaskStatus lists provided by the master's {{/state}} endpoint included the {{source}} and {{reason}} associated with the status message. The JSON modeling function for TaskStatus should be extended to include these fields.",Improvement,Major,Open,"2016-04-06 23:20:52","2016-04-06 22:20:52",1
"Apache Mesos","Commit message hook lints the diff in verbose mode.","In verbose mode (i.e., {{git commit --verbose}}), the commit message includes the diff of the commit at the bottom, delimited by the following lines:    We should {{break}} once we encounter such a line.",Bug,Major,Resolved,"2016-04-06 22:40:22","2016-04-06 21:40:22",2
"Apache Mesos","Enable `newtork/cni` isolator in `MesosContainerizer` as the default `network` isolator.","Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator.",Task,Major,Resolved,"2016-04-06 16:48:11","2016-04-06 15:48:11",1
"Apache Mesos","PersistentVolumeTest.AccessPersistentVolume is flaky","Observed on ASF CI:  ",Bug,Major,Resolved,"2016-04-06 06:19:18","2016-04-06 05:19:18",3
"Apache Mesos","Reset `LIBPROCESS_IP` in `network\cni` isolator.","Currently the `LIBPROCESS_IP` environment variable was being set to     the Agent IP if the environment variable has not be defined by the     `Framework`. For containers having their own IP address (as with     containers on CNI networks) this becomes a problem since the command     executor tries to bind to the `LIBPROCESS_IP` that does not exist in     its network namespace, and fails. Thus, for containers launched on CNI     networks the `LIBPROCESS_IP` should not be set, or rather is set to     0.0.0.0, allowing the container to bind to the IP address provided     by the CNI network.",Bug,Major,Resolved,"2016-04-06 05:10:24","2016-04-06 04:10:24",1
"Apache Mesos","Commit message hook iterates over the commented lines.","Currently, the commit message hook iterates over the commented lines. For example, if there is a modified file for which its path is longer than 72 characters, the commit hook errors out. We should skip over the commented lines.",Bug,Major,Resolved,"2016-04-05 20:47:03","2016-04-05 19:47:03",2
"Apache Mesos","Commit message hook iterates over words, rather than lines.","{{for LINE in $COMMIT_MESSAGE}} iterates over one word at a time, rather than one line at a time. We should use the following pattern instead: ",Bug,Major,Resolved,"2016-04-05 20:44:18","2016-04-05 19:44:18",2
"Apache Mesos","TASK_KILLING is not supported by mesos-execute.","Recently {{TASK_KILLING}} state (MESOS-4547) have been introduced to Mesos. We should add support for this feature to {{mesos-execute}}.",Improvement,Major,Resolved,"2016-04-05 16:36:25","2016-04-05 15:36:25",3
"Apache Mesos","pivot_root is not available on PowerPC","When compile on ppc64le, it will through error message: src/linux/fs.cpp:443:2: error: #error pivot_root is not available  The current code logic in src/linux/fs.cpp is:    There is no old glib version and the new kernel version, it will never run code in *#ifdef __NR_pivot_root* condition, and when I build on Ubuntu 16.04(It has the latest linux kernel and glibc), it still can't step into the *#ifdef __NR_pivot_root* condition.  For powerpc case, I added another condition:  ",Bug,Major,Resolved,"2016-04-05 08:00:17","2016-04-05 07:00:17",1
"Apache Mesos","Grant access to /dev/nvidiactl and /dev/nvidia-uvm in the Nvidia GPU isolator."," Calls to 'nvidia-smi'  fail inside a container even if access to a GPU has been granted. Moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a GPU even if it has access to it.      We should grant/revoke access to /dev/nvidiactl and /dev/nvidia-uvm as GPUs are added and removed from a container in the Nvidia GPU isolator.",Bug,Major,Resolved,"2016-04-04 23:17:23","2016-04-04 22:17:23",2
"Apache Mesos","Flags::parse does not handle empty string correctly.","A missing default for quorum size has generated the following master config    This was causing each elected leader to attempt replica recovery.  E.g. {{group.cpp:700] Trying to get '/mesos/log_replicas/0000000012' in ZooKeeper}}  And eventually: {{master.cpp:1458] Recovery failed: Failed to recover registrar: Failed to perform fetch within 1mins}}  Full log on one of the masters https://gist.github.com/<USER>09a9ddfe49b92a5deb4c1b421f63479e  All masters and zk nodes were reachable over the network.  Also once the quorum was configured the master recovery protocol finished gracefully.  ",Bug,Major,Resolved,"2016-04-04 22:01:34","2016-04-04 21:01:34",2
"Apache Mesos","`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag","If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump: 0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56 56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory. (gdb) bt #0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56 #1  0x00007ffff23280d8 in __GI_abort () at abort.c:89 #2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 %s%s%s:%u: %s%sAssertion `%s' failed.\n%n, assertion=assertion@entry=0x451f5c isSome(),     file=file@entry=0x451f65 ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp, line=line@entry=111,     function=function@entry=0x45294a const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]) at assert.c:92 #3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c isSome(), file=0x451f65 ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp, line=111,     function=0x45294a const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]) at assert.c:101 #4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111 Python Exception <class 'IndexError'> list index out of range: #5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331 #6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239 #7  0x00007ffff60cd972 in std::_Function_handler<void (process::ProcessBase*), process::Future<Nothing> process::dispatch<Nothing, mesos::internal::slave::MesosIsolatorProcess, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > >(process::PID<mesos::internal::slave::MesosIsolatorProcess> const&, process::Future<Nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&), std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> >)::{lambda(process::ProcessBase*)#1}>::_M_invoke(std::_Any_data const&, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071 #8  0x00007ffff6a6bf38 in std::function<void (process::ProcessBase*)>::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471 #9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130 #10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161 #11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82 #12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570 #13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218 #14 0x00007ffff6a5bf33 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x697d70,     __args=<unknown type in /home/vagrant/<USER>mesos/build/src/.libs/libmesos-0.29.0.so, CU 0x45bb552, DIE 0x469efe5>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295 #15 0x00007ffff6a5bee6 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::operator()<, void>() (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353 #16 0x00007ffff6a5be95 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731 #17 0x00007ffff6a5be65 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::operator()() (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720 #18 0x00007ffff6a5be3c in std::thread::_Impl<std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()> >::_M_run() (this=0x697d58)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115 #19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6 #20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312 #21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111 (gdb) frame 4 #4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",Bug,Major,Resolved,"2016-04-04 21:19:20","2016-04-04 20:19:20",1
"Apache Mesos","Introduce `WindowsSocketError`.","{{WindowsError}} invokes {{::GetLastError}} to retrieve the error code. Windows has a {{::WSAGetLastError}} function which at the interface level, is intended for failed socket operations. We should introduce a {{WindowsSocketError}} which invokes {{::WSAGetLastError}} and use them accordingly.",Task,Major,Resolved,"2016-04-04 21:19:04","2016-04-04 20:19:04",2
"Apache Mesos","Update `network::connect` to use the typed error state of `Try`.","{{network::connect}} function returns a {{Try<int>}} currently and the caller is required to inspect the state of {{errno}} out-of-band. {{network::connect}} should really return something like a {{Try<int, ErrnoError>}}.",Task,Major,Resolved,"2016-04-04 21:12:51","2016-04-04 20:12:51",2
"Apache Mesos","Introduce an additional template parameter to `Try` for typed error.","Add an additional template parameter {{E}} to the {{Try}} class template.  ",Task,Major,Resolved,"2016-04-04 21:09:22","2016-04-04 20:09:22",3
"Apache Mesos","Capture the error code in `ErrnoError` and `WindowsError`.","The {{ErrnoError}} and {{WindowsError}} classes simply construct the error string via a mechanism such as {{strerror}}. They should also capture the error code, as it is an essential piece of information for such an error type.",Task,Major,Resolved,"2016-04-04 21:08:26","2016-04-04 20:08:26",2
"Apache Mesos","Design a short-term solution for a typed error handling mechanism.",,Task,Major,Resolved,"2016-04-04 21:05:39","2016-04-04 20:05:39",2
"Apache Mesos","Add CMake build to docker_build.sh","Add the CMake build system to docker_build.sh to automatically test the build on Jenkins alongside gcc and clang.",Improvement,Major,Resolved,"2016-04-03 17:14:33","2016-04-03 16:14:33",2
"Apache Mesos","Fix a bug in the Nvidia GPU device isolator that exposes a discrepancy between clang and gcc in 'using' declarations","There appears to be a discrepancy between clang and gcc, which allows clang to accept `using` declarations of the form `using ns_name::name;` that contain nested classes, structs, and enums after the `name` field in the declaration (e.g. `using ns_name::name::enum;`).  The language for describing this functionality is ambiguous in the C++11 specification as referenced here: http://en.cppreference.com/w/cpp/language/namespace#Using-declarations",Bug,Major,Resolved,"2016-04-02 22:32:41","2016-04-02 21:32:41",1
"Apache Mesos","Document TaskStatus reasons","We should document the possible {{reason}} values that can be found in the {{TaskStatus}} message.",Documentation,Major,Resolved,"2016-04-01 17:47:30","2016-04-01 16:47:30",5
"Apache Mesos","Mesos allocator leaks role sorter and quota role sorters.","The Mesos allocator {{internal::HierarchicalAllocatorProcess}} owns two raw pointer members {{roleSorter}} and {{quotaRoleSorter}}, but fails to properly manage their lifetime; they are e.g., not cleaned up in the allocator process destructor.  Since currently we do not recreate an existing allocator in production code they seem to be unaffected by these leaks; they do affect tests though where we create allocators multiple times.",Improvement,Major,Resolved,"2016-03-31 15:30:40","2016-03-31 14:30:40",1
"Apache Mesos","Refactor the clone option to subprocess.","The clone option in subprocess is only used (at least in the Mesos codebase) to specify custom namespace flags to clone. It feels having the clone function in the subprocess interface is too explicit for this functionality. ",Improvement,Major,Accepted,"2016-03-31 11:11:30","2016-03-31 10:11:30",2
"Apache Mesos","Introduce more flexible subprocess interface for child options.","We introduced a number of parameters to the subprocess interface with MESOS-5049. Adding all options explicitly to the subprocess interface makes it inflexible.  We should investigate a flexible options, which still prevents arbitrary code to be executed.",Improvement,Major,Resolved,"2016-03-31 10:54:17","2016-03-31 09:54:17",2
"Apache Mesos","Upgrade http-parser to v2.6.2",,Improvement,Major,Resolved,"2016-03-31 10:18:44","2016-03-31 09:18:44",3
"Apache Mesos","Support docker private registry default docker config.","For docker private registry with authentication, docker containerizer should support using a default .docker/config.json file (or the old .dockercfg file) locally, which is pre-handled by operators. The default docker config file should be exposed by a new agent flag `--docker_config`. ",Task,Major,Resolved,"2016-03-30 16:40:29","2016-03-30 15:40:29",3
"Apache Mesos","Remove default value for the agent `work_dir`","Following a crash report from the user we need to be more explicit about the dangers of using {{/tmp}} as agent {{work_dir}}. In addition, we can remove the default value for the {{\-\-work_dir}} flag, forcing users to explicitly set the work directory for the agent.",Bug,Blocker,Resolved,"2016-03-30 00:28:48","2016-03-29 23:28:48",2
"Apache Mesos","Update the long-lived-framework example to run on test clusters","There are a couple of problems with the long-lived framework that prevent it from being deployed (easily) on an actual cluster:  * The framework will greedily accept all offers; it runs one executor per agent in the cluster. * The framework assumes the {{long-lived-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments. * The framework does not specify an resources with the executor.  This is required by many isolators. * The framework has no metrics.",Improvement,Major,Resolved,"2016-03-29 19:46:46","2016-03-29 18:46:46",3
"Apache Mesos","Expose per-role dominant share","A client's dominant share is crucial measure for how likely it is to receive offers in the future. We should expose it in a dedicated allocator metric.  As currently the {{HierarchicalAllocatorProcess}} does work with generic {{Sorters}} which have no notion of DRF share we need to decide whether and where we would need to limit generality in order to expose the innards of the currently used {{DRFSorter}}. ",Bug,Major,Resolved,"2016-03-29 09:32:18","2016-03-29 08:32:18",2
"Apache Mesos","Slave/Agent Rename Phase I - Update strings in error messages and other strings","This is a sub ticket of MESOS-3780. In this ticket, we will update all the slave to agent in the error messages and other strings in the code",Task,Major,Resolved,"2016-03-29 06:50:48","2016-03-29 05:50:48",3
"Apache Mesos","Replace Master/Slave Terminology Phase I - Update strings in the shell scripts outputs","This is a sub ticket of MESOS-3780. In this ticket, we will rename slave to agent in the shell script outputs",Task,Major,Resolved,"2016-03-29 06:49:21","2016-03-29 05:49:21",1
"Apache Mesos","Slave/Agent Rename Phase I - Update strings in the log message and standard output","This is a sub ticket of MESOS-3780. In this ticket, we will rename all the slave to agent in the log messages and standard output.",Task,Major,Resolved,"2016-03-29 06:46:57","2016-03-29 05:46:57",2
"Apache Mesos","Namespace the stout flags","A recent name collision occurred when updating the 3rdparty http-parser library: https://github.com/apache/mesos/commit/94df63f72146501872a06c6487e94bdfd0f23025  We should put stout's {{flags}} namespace within another suitable namespace (perhaps {{stout::flags}}) to avoid such collisions.",Improvement,Major,Accepted,"2016-03-29 00:12:57","2016-03-28 23:12:57",2
"Apache Mesos","Create helpers for manipulating Linux capabilities.","These helpers can either based on some existing library (e.g. libcap), or use system calls directly.",Task,Major,Resolved,"2016-03-28 18:13:35","2016-03-28 17:13:35",5
"Apache Mesos","Design Linux capability support for Mesos containerizer","We should at least support the following cases: 1) A root user has reduced capability 2) A non-root user has the capability of CAP_NET_ADMIN (to do e.g., tcpdump)",Task,Major,Resolved,"2016-03-28 18:08:05","2016-03-28 17:08:05",5
"Apache Mesos","Refactore subproces setup functions.","Executing arbitrary setup functions while creating new processes is dangerous as all functions called have to be async safe. As setup functions are used for only very few purposes (setsid, chdir, monitoring and killing a process (see upcoming review) it makes sense to support them safely via parameters to subprocess.  Another common use of child setup are is to block the child while doing some work in the parent. This pattern can be more cleanly expressed with parentHooks. ",Improvement,Major,Resolved,"2016-03-28 17:49:02","2016-03-28 16:49:02",3
"Apache Mesos","Temporary directories created by environment->mkdtemp cleanup can be problematic.","Currently in mesos test, we have the temporary directories created by `environment->mkdtemp()` cleaned up until the end of the test suite, which can be problematic. For instance, if we have many tests in a test suite, each of those tests is performing large size disk read/write in its temp dir, which may lead to out of disk issue on some resource limited machines.   We should have these temp dir created by `environment->mkdtemp` cleaned up during each test teardown. Currently we only clean up the sandbox for each test.",Improvement,Major,Resolved,"2016-03-26 20:08:54","2016-03-26 20:08:54",1
"Apache Mesos","Add cgroups unified isolator","Implement the cgroups unified isolator for Mesos containerizer.",Task,Major,Resolved,"2016-03-26 13:44:01","2016-03-26 13:44:01",8
"Apache Mesos","Design doc for ordered message delivery in libprocess",,Task,Major,Resolved,"2016-03-25 16:45:07","2016-03-25 16:45:07",3
"Apache Mesos","Remove plain text Credential format (after deprecation cycle)","Currently two formats of credentials are supported: JSON    And a deprecated new line file:   We deprecated the new line format in 0.29, and should remove it after the deprecation cycle ends.",Improvement,Minor,Accepted,"2016-03-25 09:03:08","2016-03-25 09:03:08",3
"Apache Mesos","Authorization Action enum does not support upgrades.","We need to make the Action enum optional in authorization::Request, and add an `UNKNOWN = 0;` enum value. See MESOS-4997 for details.",Bug,Major,Resolved,"2016-03-25 07:57:14","2016-03-25 07:57:14",2
"Apache Mesos","Copy provisioner cannot replace directory with symlink","I'm trying to play with the new image provisioner on our custom docker images, but one of layer failed to get copied, possibly due to a dangling symlink.  Error log with Glog_v=1:  {quote} I0324 05:42:48.926678 15067 copy.cpp:127] Copying layer path '/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs' to rootfs '/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6' E0324 05:42:49.028506 15062 slave.cpp:3773] Container '5f05be6c-c970-4539-aa64-fd0eef2ec7ae' for executor 'test' of framework 75932a89-1514-4011-bafe-beb6a208bb2d-0004 failed to start: Collect failed: Collect failed: Failed to copy layer: cp: cannot overwrite directory ‘/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6/etc/apt’ with non-directory {quote}  Content of _/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs/etc/apt_ points to a non-existing absolute path (cannot provide exact path but it's a result of us trying to mount apt keys into docker container at build time).  I believe what happened is that we executed a script at build time, which contains equivalent of: {quote} rm -rf /etc/apt/* && ln -sf /build-mount-point/ /etc/apt {quote} ",Bug,Major,Resolved,"2016-03-24 20:19:51","2016-03-24 20:19:51",3
"Apache Mesos","Enable authenticated login in the webui","The webui hits a number of endpoints to get the data that it displays: {{/state}}, {{/metrics/snapshot}}, {{/files/browse}}, {{/files/read}}, and maybe others? Once authentication is enabled on these endpoints, we need to add a login prompt to the webui so that users can provide credentials.",Improvement,Major,Resolved,"2016-03-24 18:58:55","2016-03-24 18:58:55",2
"Apache Mesos","MesosContainerizerProvisionerTest.DestroyWhileProvisioning is flaky.","Observed on the Apache Jenkins.  ",Bug,Critical,Resolved,"2016-03-24 14:33:43","2016-03-24 14:33:43",2
"Apache Mesos","Add a reconnect() method to the C++ scheduler library","A reconnect() method on the library would allow the scheduler to force a reconnection (disconnect and reconnect) by the library. This might be used by the scheduler to react to lack of HEARTBEATs.",Improvement,Major,Resolved,"2016-03-23 18:04:37","2016-03-23 18:04:37",3
"Apache Mesos","Call and Event Type enums in executor.proto should be optional","Having a 'required' Type enum has backwards compatibility issues when adding new enum types. See MESOS-4997 for details.",Improvement,Major,Resolved,"2016-03-23 18:01:17","2016-03-23 18:01:17",2
"Apache Mesos","Call and Event Type enums in scheduler.proto should be optional","Having a 'required' Type enum has backwards compatibility issues when adding new enum types. See MESOS-4997 for details.",Improvement,Major,Resolved,"2016-03-23 18:00:57","2016-03-23 18:00:57",2
"Apache Mesos","Add docker volume driver isolator for Mesos containerizer.","The isolator will interact with Docker Volume Driver Plugins to mount and unmount external volumes to container. ",Bug,Major,Resolved,"2016-03-23 16:00:04","2016-03-23 16:00:04",8
"Apache Mesos","Installation of mesos python package is incomplete","The installation of mesos python package is incomplete, i.e., the files {{cli.py}}, {{futures.py}}, and {{http.py}} are not installed.    This appears to be first broken with {{d1d70b9}} (MESOS-3969, [Upgraded bundled pip to 7.1.2.|https://reviews.apache.org/r/40630]). Bisecting in {{pip}}-land shows that our install becomes broken for {{pip-6.0.1}} and later (we are using {{pip-7.1.2}}). ",Bug,Major,Resolved,"2016-03-22 22:43:47","2016-03-22 22:43:47",2
"Apache Mesos","Add example for mesos-execute usage of Appc images in container-image.md.","Example usage for Appc flags and images needs to be added to container-image.md.",Documentation,Major,Open,"2016-03-22 18:46:32","2016-03-22 18:46:32",3
"Apache Mesos","Enforce that DiskInfo principal is equal to framework/operator principal","Currently, we require that {{ReservationInfo.principal}} be equal to the principal provided for authentication, which means that when HTTP authentication is disabled this field cannot be set. Based on comments in 'mesos.proto', the original intention was to enforce this same constraint for {{Persistence.principal}}, but it seems that we don't enforce it. This should be changed to make the two fields equivalent, with one exception: when the framework/operator principal is {{None}}, we should allow the principal in {{DiskInfo}} to take any value, along the same lines as MESOS-5212.",Bug,Major,Resolved,"2016-03-22 18:35:45","2016-03-22 18:35:45",3
"Apache Mesos","Clarify docs on '/reserve' and '/create-volumes' without authentication","For both reservations and persistent volume creation, the behavior of the HTTP endpoints differs slightly from that of the framework operations. Due to the implementation of HTTP authentication, it is not possible for a framework/operator to provide a principal when HTTP authentication is disabled. This means that when HTTP authentication is disabled, the endpoint handlers will _always_ receive {{None()}} as the principal associated with the request, and thus if authorization is enabled, the request will only succeed if the NONE principal is authorized to do stuff.  The docs should be updated to explain this behavior explicitly.",Documentation,Major,Resolved,"2016-03-22 18:29:20","2016-03-22 18:29:20",1
"Apache Mesos","Prefix allocator metrics with mesos/ to better support custom allocator metrics.","There currently exists only a single allocator metric named   In order to support different allocator implementations (the mesos allocator being the default one included in the project currently), it would be better to rename the metric so that allocator metrics are prefixed with the allocator implementation name:    This consistent with the approach taken for containerizer metrics, where the mesos containerizer exposes its metrics under a mesos/ prefix.",Improvement,Major,Resolved,"2016-03-22 13:41:08","2016-03-22 13:41:08",1
"Apache Mesos","MasterTest.MasterLost is flaky","The test {{MasterTest.MasterLost}} and {{ExceptionTest.DisallowSchedulerActionsOnAbort}} fail at least half the time under OS X (clang, not optimized, {{30efac7}}), e.g.,   Sometimes also {{FaultToleranceTest.SchedulerFailover}} fails with the same stack trace.  I could trace this to the recent refactoring of the test helpers (MESOS-4633, MESOS-4634),   It appears the lifetimes of some objects are still not ordered correctly. ",Bug,Major,Resolved,"2016-03-22 12:48:12","2016-03-22 12:48:12",3
"Apache Mesos","Problematic fork/clone performance at high load.","Creating a new subprocess in mesos involves forking/cloning a new process. In most cases (executors, perf, ..) the parent of the new process is the agent/slave process. This can lead to problematic behavior especially when creating several new processes at the same time.  The problem here is that the normal fork() (or clone syscall used by libprocess) provides a copy-on-write (cow) view of the parents address space until the child execs its new binary. Note that during the time between fork and exec Mesos does several setup actions such as placing the new processes in systemd units or assigning them to the freezer cgroup. This cow property of the address space implies that existing memory is marked as read-only and any write will trigger a page-fault and a newly created page. Note this behavior also extends to the parent process and hence any write will be very costly.  We simulated the number of pagefaults when forking/cloning new processes by this benchmark: https://github.com/joerg84/forking-benchmark  Results can be seen here:  https://docs.google.com/presentation/d/1SUjKAVHdrutLPpFJy3Q1yhinG5FOMw3HbbEdzuhZ7A8",Epic,Major,"In Progress","2016-03-22 08:20:34","2016-03-22 08:20:34",8
"Apache Mesos","sandbox uri does not work outisde mesos http server","The SandBox uri of a framework does not work if i just copy paste it to the browser.  For example the following sandbox uri: http://172.17.0.1:5050/#/slaves/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0/frameworks/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-0009/executors/driver-20160321155016-0001/browse  should redirect to: http://172.17.0.1:5050/#/slaves/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0/browse?path=%2Ftmp%2Fmesos%2Fslaves%2F50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0%2Fframeworks%2F50f87c73-79ef-4f2a-95f0-b2b4062b2de6-0009%2Fexecutors%2Fdriver-20160321155016-0001%2Fruns%2F60533483-31fb-4353-987d-f3393911cc80  yet it fails with the message: Failed to find slaves. Navigate to the slave's sandbox via the Mesos UI. and redirects to: http://172.17.0.1:5050/#/  It is an issue for me because im working on expanding the mesos spark ui with sandbox uri, The other option is to get the slave info and parse the json file there and get executor paths not so straightforward or elegant though.  Moreover i dont see the runs/container_id in the Mesos Proto Api. I guess this is hidden info, this is the needed piece of info to re-write the uri without redirection. ",Bug,Major,Resolved,"2016-03-21 16:50:48","2016-03-21 16:50:48",3
"Apache Mesos","Destroy a container while it's provisioning can lead to leaked provisioned directories.","Here is the possible sequence of events: 1) containerizer->launch 2) provisioner->provision is called. it is fetching the image 3) executor registration timed out 4) containerizer->destroy is called 5) container->state is still in PREPARING 6) provisioner->destroy is called  So we can be calling provisioner->destory while provisioner->provision hasn't finished yet. provisioner->destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes.",Bug,Critical,Resolved,"2016-03-18 23:21:02","2016-03-18 23:21:02",3
"Apache Mesos","MasterTest.SlavesEndpointTwoSlaves is flaky","Observed on Arch Linux with GCC 6, running in a virtualbox VM:  [ RUN      ] MasterTest.SlavesEndpointTwoSlaves /mesos-2/src/tests/master_tests.cpp:1710: Failure Value of: array.get().values.size()   Actual: 1 Expected: 2u Which is: 2 [  FAILED  ] MasterTest.SlavesEndpointTwoSlaves (86 ms)  Seems to fail non-deterministically, perhaps more often when there is concurrent CPU load on the machine.",Bug,Major,Resolved,"2016-03-18 22:45:24","2016-03-18 22:45:24",2
"Apache Mesos","Update example long running to use v1 API.","We need to modify the long running test framework similar to {{src/examples/long_lived_framework.cpp}} to use the v1 API.  This would allow us to vet the v1 API and the scheduler library in test clusters.",Task,Major,Resolved,"2016-03-18 20:04:05","2016-03-18 20:04:05",5
"Apache Mesos","Update mesos-execute with Appc changes.","mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier.",Bug,Major,Resolved,"2016-03-18 17:36:44","2016-03-18 17:36:44",3
"Apache Mesos","Add more examples of JSON resources to docs","The configuration documentation currently only shows examples of scalar resource types in JSON format. The structures of JSON resources are a bit complicated, so it would be very helpful to include examples of ranges, sets, and text resource types as well.",Documentation,Major,Accepted,"2016-03-17 20:19:47","2016-03-17 20:19:47",1
"Apache Mesos","Support resizing of an existing persistent volume","We need a mechanism to update the size of a persistent volume.    The increase case is generally more interesting to us (as long as there still available disk resource on the same disk).",Improvement,Blocker,Resolved,"2016-03-16 22:56:43","2016-03-16 22:56:43",8
"Apache Mesos","Support for Mesos releases","As part of Mesos reaching 1.0, we need to formalize the policy of supporting Mesos releases.  Some specific questions we need to answer:  --> What fixes should we backports to older releases.  --> How many old releases are supported.  --> Should we have a LTS version?  --> What is the cadence of major, minor and patch releases?",Task,Major,Resolved,"2016-03-16 20:19:31","2016-03-16 20:19:31",8
"Apache Mesos","ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky","The logger subprocesses may exit before we reach the {{waitpid}} in the test.  If this happens, {{waitpid}} will return a {{-1}} as the process no longer exists.  Verbose logs: ",Bug,Major,Resolved,"2016-03-16 15:16:17","2016-03-16 15:16:17",1
"Apache Mesos","Enable support for mesos-style assertion macros in clang-tidy core analyzers","clang-tidy has a number of core analyzers that analyze control flow to make sure that e.g., dereferenced pointers are not null. The clang control flow analysis framework uses e.g., the presence of {{assert}} to prune certain edges from the control flow graph.  Mesos uses a number of custom assertion macros from glog which are not understood by these analyzers. We should find a way to add support for these macros, either by redefining these macros in ways clang static analysis can understand, or by extending the framework.",Improvement,Major,Resolved,"2016-03-16 12:07:06","2016-03-16 12:07:06",3
"Apache Mesos","Add authentication to /files endpoints","To protect access (authz) to master/agent logs as well as executor sandboxes, we need authentication on the /files endpoints.  Adding HTTP authentication to these endpoints is a bit complicated since they are defined in code that is shared by the master and agent.  While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.  We should establish a mechanism for making an endpoint authenticated that allows us to: 1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent 2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess  Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other.",Improvement,Major,Resolved,"2016-03-16 09:58:34","2016-03-16 09:58:34",5
"Apache Mesos","Enable actors to pass an authentication realm to libprocess","To prepare for MESOS-4902, the Mesos master and agent need a way to pass the desired authentication realm to libprocess. Since some endpoints (like {{/profiler/*}}) get installed in libprocess, the master/agent should be able to specify during initialization what authentication realm the libprocess-level endpoints will be authenticated under.",Improvement,Major,Resolved,"2016-03-15 20:53:18","2016-03-15 20:53:18",2
"Apache Mesos","Implement reconnect funtionality in the scheduler library.","Currently, there is no way for the schedulers to force a reconnection attempt with the master using the scheduler library {{src/scheduler/scheduler.cpp}}. It is specifically useful in scenarios where there is a one way network partition with the master. Due to this, the scheduler has not received any {{HEARTBEAT}} events from the master. In this case, the scheduler might want to force a reconnection attempt with the master instead of relying on the {{disconnected}} callback.",Bug,Major,Resolved,"2016-03-15 20:29:43","2016-03-15 20:29:43",3
"Apache Mesos","Executor shutdown grace period should be configurable.","Currently, executor shutdown grace period is specified by an agent flag, which is propagated to executors via the {{MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD}} environment variable. There is no way to adjust this timeout for the needs of a particular executor.  To tackle this problem, we propose to introduce an optional {{shutdown_grace_period}} field in {{ExecutorInfo}}.",Improvement,Major,Resolved,"2016-03-15 14:20:22","2016-03-15 14:20:22",3
"Apache Mesos","Improve overlay backend so that it's writable","Currently, the overlay backend will provision a read-only FS. We can use an empty directory from the container sandbox to act as the upper layer so that it's writable.",Task,Major,Resolved,"2016-03-15 01:25:40","2016-03-15 01:25:40",5
"Apache Mesos","Reduce the size of LinuxRootfs in tests.","Right now, LinuxRootfs copies files from the host filesystem to construct a chroot-able rootfs. We copy a lot of unnecessary files, making it very large. We can potentially strip a lot files.",Improvement,Major,Resolved,"2016-03-15 01:23:35","2016-03-15 01:23:35",13
"Apache Mesos","Docker runtime isolator tests may cause disk issue.","Currently slave working directory is used as docker store dir and archive dir, which is problematic. Because slave work dir is exactly `environment->mkdtemp()`, it will get cleaned up until the end of the whole test. But the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. Cleanup has to be done by each test tear down. ",Bug,Major,Resolved,"2016-03-15 00:15:24","2016-03-15 00:15:24",2
"Apache Mesos","Support update existing quota.","We want to support updating an existing quota without the cycle of delete and recreate. This avoids the possible starvation risk of losing the quota between delete and recreate, and also makes the interface friendly.  Design doc: https://docs.google.com/document/d/1c8fJY9_N0W04FtUQ_b_kZM6S0eePU7eYVyfUP14dSys",Improvement,Major,Resolved,"2016-03-14 21:01:22","2016-03-14 21:01:22",8
"Apache Mesos","Support specifying per-container docker registry.","Currently, we only support a per agent flag to specify the docker registry. We should instead, allow people to specify the registry as part of the docker image name (like `docker pull` does).",Task,Major,Resolved,"2016-03-14 17:31:37","2016-03-14 17:31:37",3
"Apache Mesos","Support docker registry authentication",,Task,Major,Resolved,"2016-03-14 17:26:25","2016-03-14 17:26:25",5
"Apache Mesos","Investigate container security options for Mesos containerizer","We should investigate the following to improve the container security for Mesos containerizer and come up with a list of features that we want to support in MVP.  1) Capabilities 2) User namespace 3) Seccomp 4) SELinux 5) AppArmor  We should investigate what other container systems are doing regarding security: 1) [k8s| https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/types.go#L2905] 2) [docker|https://docs.docker.com/engine/security/security/] 3) [oci|https://github.com/opencontainers/specs/blob/master/config.md]",Task,Major,Resolved,"2016-03-14 17:17:47","2016-03-14 17:17:47",5
"Apache Mesos","Enable HELP to include authentication status of endpoint.","As we enable authentication for more and more endpoints we should document which endpoints support authentication and which ones don't.",Task,Major,Resolved,"2016-03-14 12:20:35","2016-03-14 12:20:35",2
"Apache Mesos","Registrar HTTP Authentication.","Now that the master (and agents in progress) provide http authentication the registrar should do the same.   See http://mesos.apache.org/documentation/latest/endpoints/registrar/registry/",Task,Major,Resolved,"2016-03-14 11:07:04","2016-03-14 11:07:04",3
"Apache Mesos","Propose Design for Authorization based filtering for endpoints.","The design doc can be found here: https://docs.google.com/document/d/1M27S7OTSfJ8afZCklOz00g_wcVrL32i9Lyl6g22GWeY",Task,Major,Resolved,"2016-03-14 11:01:11","2016-03-14 11:01:11",5
"Apache Mesos","Remove all '.get().' calls on Option / Try variables in the resources abstraction.","When possible, {{.get()}} calls should be replaced by {{->}} for {{Option}} / {{Try}} variables.  This ticket only proposes a blanket change for this in the resource abstraction files, not the code base as a whole.  This is in preparation for introducing the new GPU resource.  Without this change, I would need to use the old {{.get()}} calls.  Instead, I propose to fix the old code surrounding it so that consistency has me doing it the right way.  ",Improvement,Major,Resolved,"2016-03-14 07:33:43","2016-03-14 07:33:43",1
"Apache Mesos","The flag parser for `hashmap<string, string>` should live in stout, not mesos.","The title says it all.",Improvement,Major,Resolved,"2016-03-14 07:27:53","2016-03-14 07:27:53",1
"Apache Mesos","Add a list parser for comma separated integers in flags.","Some flags require lists of integers to be passed in.  We should have an explicit parser for this instead of relying on ad hoc solutions.",Improvement,Major,Resolved,"2016-03-14 07:26:08","2016-03-14 07:26:08",2
"Apache Mesos","Setup proper /etc/hostname, /etc/hosts and /etc/resolv.conf for containers in network/cni isolator.","The network/cni isolator needs to properly setup /etc/hostname and /etc/hosts for the container with a hostname (e.g., randomly generated) and the assigned IP returned by CNI plugin. We should consider the following cases: 1) container is using host filesystem 2) container is using a different filesystem 3) custom executor and command executor",Bug,Major,Resolved,"2016-03-13 00:58:12","2016-03-13 00:58:12",5
"Apache Mesos","Cache module manifests while loading in ModuleManager.","Since the module managers are allowed to load the same module multiple times, we should be caching the module manifests to avoid cases where the module tries to trick the module manager by changing `ModuleBase` fields before the next call to `ModuleManager::load`.",Task,Major,Open,"2016-03-11 07:09:52","2016-03-11 07:09:52",3
"Apache Mesos","Replace non-pod static variables in module/manager.[ch]pp with pod eqivalents.",,Task,Major,Open,"2016-03-11 06:55:46","2016-03-11 06:55:46",3
"Apache Mesos","Allow modules to express if they are multi-instantiable and thread safe.","A module might be instantiated multiple time (e.g., multiple schedulers in the same Java process instantiating an authenticator module) within the same process. The current mechanism doesn't provide a way through the module API to forbid multiple instantiations. It is up to the module to check and return error on prior instantiation.  Along similar lines, a module should be able to express thread-safety concerns. Typically, a module running in Master/Agent doesn't have to be concerned about thread safety if it uses libprocess API. However, we should investigate how it plays in the scheduler environment.",Task,Major,Open,"2016-03-11 06:49:20","2016-03-11 06:49:20",8
"Apache Mesos","ProcessorManager delegate should be an Option<string>, not just a string.","Currently, the delegate field in the ProcessManager is just a string type. We check for 'existence' of a delegate by comparing (delegate != ). Using an Option is the preferred method for things like this.",Improvement,Minor,Resolved,"2016-03-11 01:30:13","2016-03-11 01:30:13",1
"Apache Mesos","LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails.","Observed on our CI: ",Bug,Major,Resolved,"2016-03-10 15:37:09","2016-03-10 15:37:09",3
"Apache Mesos","Executor driver does not respect executor shutdown grace period.","Executor shutdown grace period, configured on the agent, is propagated to executors via the `MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD` environment variable. The executor driver must use this timeout to delay the hard shutdown of the related executor.",Bug,Major,Resolved,"2016-03-10 15:22:02","2016-03-10 15:22:02",1
"Apache Mesos","Deprecate the --docker_stop_timeout agent flag.","Instead, a combination of {{executor_shutdown_grace_period}} agent flag and optionally task kill policies should be used.",Improvement,Major,Resolved,"2016-03-10 14:55:21","2016-03-10 14:55:21",1
"Apache Mesos","Introduce kill policy for tasks.","A task may require some time to clean up or even a special mechanism to issue a kill request (currently it's a SIGTERM followed by SIGKILL). Introducing kill policies per task will help address these issue.",Improvement,Major,Resolved,"2016-03-10 14:53:05","2016-03-10 14:53:05",5
"Apache Mesos","Tasks cannot be killed forcefully.","Currently there is no way for a scheduler to instruct the executor to kill a certain task immediately, skipping any possible timeouts and / or kill policies. This may be desirable in cases like, e.g., the kill policy is 10 minutes but something went wrong, so the scheduler decides to issue a forceful kill.",Improvement,Major,Resolved,"2016-03-10 12:40:08","2016-03-10 12:40:08",5
"Apache Mesos","Allow multiple loads of module manifests","The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure.",Bug,Blocker,Resolved,"2016-03-09 20:46:19","2016-03-09 20:46:19",3
"Apache Mesos","Add authentication to libprocess endpoints","In addition to the endpoints addressed by MESOS-4850 and MESOS-5152, the following endpoints would also benefit from HTTP authentication: * {{/profiler/*}} * {{/logging/toggle}} * {{/metrics/snapshot}}  Adding HTTP authentication to these endpoints is a bit more complicated because they are defined at the libprocess level.  While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.  We should establish a mechanism for making an endpoint authenticated that allows us to: 1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent 2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess  Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other.",Improvement,Major,Resolved,"2016-03-09 18:48:49","2016-03-09 18:48:49",5
"Apache Mesos","Add a '/containers' endpoint to the agent to list all the active containers.","This endpoint will be similar to /monitor/statistics.json endpoint, but it'll also contain the 'container_status' about the container (see ContainerStatus in mesos.proto). We'll eventually deprecate the /monitor/statistics.json endpoint.",Improvement,Major,Resolved,"2016-03-07 22:48:01","2016-03-07 22:48:01",8
"Apache Mesos","Implement runtime isolator tests.","There different cases in docker runtime isolator. Some special cases should be tested with unique test case, to verify the docker runtime isolator logic is correct.",Task,Major,Resolved,"2016-03-07 21:28:12","2016-03-07 21:28:12",5
"Apache Mesos","Default cmd is executed as an incorrect command.","When mesos containerizer launch a container using a docker image, which only container default Cmd. The executable command is is a incorrect sequence. For example:  If an image default entrypoint is null, cmd is sh, user defines shell=false, value is none, and arguments as [-c, echo 'hello world']. The executable command is `[sh, -c, echo 'hello world', sh]`, which is incorrect. It should be `[sh, sh, -c, echo 'hello world']` instead.  This problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. ",Bug,Major,Resolved,"2016-03-07 21:16:52","2016-03-07 21:16:52",2
"Apache Mesos","Support mesos containerizer force_pull_image option.","Currently for unified containerizer, images that are already cached by metadata manager cannot be updated. User has to delete corresponding images in store if an update is need. We should support `force_pull_image` option for unified containerizer, to provide override option if existed.",Improvement,Major,Resolved,"2016-03-07 20:42:02","2016-03-07 20:42:02",3
"Apache Mesos","Add support for command and arguments to mesos-execute.","{{CommandInfo}} protobuf support two kinds of command:   The mesos-execute cannot handle 2) now, enabling 2) can help with testing and running one off tasks.",Improvement,Major,"In Progress","2016-03-07 08:04:43","2016-03-07 08:04:43",5
"Apache Mesos","Rescind all outstanding offers after changing some weights.",,Task,Major,Resolved,"2016-03-07 07:18:00","2016-03-07 07:18:00",2
"Apache Mesos","Update glog patch to support PowerPC LE","This is a part of PowerPC LE porting",Improvement,Major,Resolved,"2016-03-07 01:58:15","2016-03-07 01:58:15",1
"Apache Mesos","Mesos containerizer can't handle top level docker image like alpine (must use library/alpine)","This can be demonstrated with the {{mesos-execute}} command:  # Docker containerizer with image {{alpine}}: success  # Mesos containerizer with image {{alpine}}: failure  # Mesos containerizer with image {{library/alpine}}: success   In the slave logs:    curl command executed:    Also got the same result with {{ubuntu}} docker image.",Bug,Major,Resolved,"2016-03-06 16:44:06","2016-03-06 16:44:06",3
"Apache Mesos","Add documentation about container image support.",,Documentation,Major,Resolved,"2016-03-05 02:18:48","2016-03-05 02:18:48",5
"Apache Mesos","Dump the contents of the sandbox when a test fails","[~<USER> added this logic for extra info about a rare flaky test: https://github.com/apache/mesos/blob/d26baee1f377aedb148ad04cc004bb38b85ee4f6/src/tests/fetcher_cache_tests.cpp#L249-L259  This information is useful regardless of the test type and should be generalized for {{cluster::Slave}}.  i.e.  # When a {{cluster::Slave}} is destructed, it can detect if the test has failed.   # If so, navigate through its own {{work_dir}} and print sandboxes and/or other useful debugging info. Also see the refactor in [MESOS-4634].",Improvement,Major,Open,"2016-03-05 00:37:43","2016-03-05 00:37:43",3
"Apache Mesos","PersistentVolumeTests do not need to set up ACLs.","The {{PersistentVolumeTest}} s have a custom helper for setting up ACLs in the {{master::Flags}}:   This is no longer necessary with implicit roles.",Improvement,Major,Resolved,"2016-03-04 19:39:28","2016-03-04 19:39:28",1
"Apache Mesos","Add GPUs as an explicit resource.","We will add gpus as an explicitly recognized resource in Mesos, akin to cpus, memory, ports, and disk.  In the containerizer, we will verify that the number of GPU resources passed in via the --resources flag matches the list of GPUs passed in via the --nvidia_gpus flag.  In the future we will add autodiscovery so this matching is unnecessary.  However, we will always have to pass gpus as a resource to make any GPU available on the system (unlike for cpus and memory, where the default is probed).",Task,Major,Resolved,"2016-03-04 01:04:29","2016-03-04 01:04:29",3
"Apache Mesos","Add flag to specify available Nvidia GPUs on an agent's command line.","In the initial GPU support we will not do auto-discovery of GPUs on an agent.  As such, an operator will need to specify a flag on the command line, listing all of the GPUs available on the system.",Task,Major,Resolved,"2016-03-04 00:58:26","2016-03-04 00:58:26",3
"Apache Mesos","Add Nvidia GPU isolator tests.","We need to be able to run unit tests that verify GPU isolation, as well as run full blown tests that actually exercise the GPUs.  These tests should only build when the proper configure flags are set for enabling nvidia GPU support.",Task,Major,Resolved,"2016-03-04 00:55:29","2016-03-04 00:55:29",2
"Apache Mesos","Add configure flags to build with Nvidia GPU support.","The configure flags can be used to enable Nvidia GPU support, as well as specify the installation directories of the nvml header and library files if not already installed in standard include/library paths on the system.  They will also be used to conditionally build support for Nvidia GPUs into Mesos.",Task,Major,Resolved,"2016-03-04 00:51:48","2016-03-04 00:51:48",2
"Apache Mesos","Add a script to install the Nvidia GDK on a host.","This script can be used to install the Nvidia GDK for Cuda 7.5 on a mesos development machine. The purpose of the Nvidia GDK is to provide all the necessary header files (nvml.h) and library files (libnvidia-ml.so) necessary to build mesos with Nvidia GPU support.  If the machine on which Mesos is being compiled doesn't have any GPUs, then libnvidia-ml.so consists only of stubs, allowing Mesos to build and run, but not actually do anything useful under the hood. This enables us to build a GPU-enabled mesos on a development machine without GPUs and then deploy it to a production machine with GPUs and be reasonably sure it will work.",Task,Minor,Resolved,"2016-03-04 00:48:40","2016-03-04 00:48:40",2
"Apache Mesos","Add explicit upgrade instructions to the docs","The documentation currently contains per-version upgrade guidelines, which for recent releases only outlines the upgrade concerns for that version, without detailing explicit upgrade instructions.  We should add explicit upgrade instructions to the top of the upgrades documentation, which can be supplemented by the per-version concerns.  This is done within the upgrade docs for some early versions, with text like:    Instructions to this effect should be featured prominently in the doc.",Improvement,Major,Open,"2016-03-03 23:39:59","2016-03-03 23:39:59",1
"Apache Mesos","Make changes to executor v1 library around managing connections.","While implementing pipelining changes for the scheduler library (MESOS-3570), we noticed a couple of small bugs that we would like to fix in the executor library:  - Don't pass {{Connection}} objects to {{defer}} callbacks as they can sometimes lead to deadlocks. - Minor cleanups around not accepting {{SUBSCRIBE}} call if one is currently in progress. - Create a random UUID (connectionId) before we initiate a connection to the agent, as in some scenarios, we can accept connection attempts from stale connections.",Task,Major,Resolved,"2016-03-03 21:24:46","2016-03-03 21:24:46",3
"Apache Mesos","Update CHANGELOG with net_cls isolator","Need to update the CHANGELOG for 0.28 release.",Documentation,Blocker,Resolved,"2016-03-03 19:13:54","2016-03-03 19:13:54",1
"Apache Mesos","Add authentication to agent endpoints /state and /flags","The {{/state}} and {{/flags}} endpoints are installed in {{src/slave/slave.cpp}}, and thus are straightforward to make authenticated. Other agent endpoints require a bit more consideration, and are tracked in MESOS-4902.  For more information on agent endpoints, see http://mesos.apache.org/documentation/latest/endpoints/ or search for `route(` in the source code: ",Task,Major,Resolved,"2016-03-03 09:30:24","2016-03-03 09:30:24",3
"Apache Mesos","Add agent flags for HTTP authentication","Flags should be added to the agent to: 1. Enable HTTP authentication ({{--authenticate_http}}) 2. Specify credentials ({{--http_credentials}}) 3. Specify HTTP authenticators ({{--authenticators}})",Task,Major,Resolved,"2016-03-03 09:21:20","2016-03-03 09:21:20",2
"Apache Mesos","Agent Authn Research Spike","Research the master authentication flags to see what changes will be necessary for agent http authentication. Write up a 1-2 page summary/design doc.",Task,Major,Resolved,"2016-03-03 09:16:20","2016-03-03 09:16:20",2
"Apache Mesos","Add authentication to master endpoints","Before we can add authorization around operator endpoints, we need to add authentication support, so that unauthenticated requests are denied when --authenticate_http is enabled, and so that the principal is passed into `route()`.",Task,Major,Resolved,"2016-03-03 06:19:51","2016-03-03 06:19:51",2
"Apache Mesos","Remove internal usage of deprecated ShutdownFramework ACL","{{ShutdownFramework}} acl was deprecated a couple of versions ago in favor of the {{TeardownFramework}} message. Its deprecation cycle came with 0.27. That means we should remove the message and its references in the code base.",Task,Minor,Resolved,"2016-03-02 17:46:45","2016-03-02 17:46:45",2
"Apache Mesos","Move placement new processes into the freezer cgroup into a parent hook.","The Linux Launcher places new processes into the freezer cgroup. This is currently done by a combination of childSetup function (blocking the new process until parent is done) and the parent (placing child process into the cgroup and then signaling child to continue). ParentHooks support this behavior (blocking child until some work is done in the parent) in a much cleaner way.  ",Improvement,Major,Resolved,"2016-03-02 16:15:22","2016-03-02 16:15:22",3
"Apache Mesos","Fix rmdir for windows","This is due to a bug in MESOS-4415 that landed for 0.27.0.",Bug,Major,Resolved,"2016-03-02 01:45:34","2016-03-02 01:45:34",1
"Apache Mesos","CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess is flaky","Verbose logs:  ",Bug,Major,Resolved,"2016-03-02 01:22:57","2016-03-02 01:22:57",2
"Apache Mesos","Add 'file' fetcher plugin.","Add support for file based URI fetcher. This could be useful for container image provisioning from local file system.",Task,Major,Resolved,"2016-03-02 00:11:54","2016-03-02 00:11:54",2
"Apache Mesos","Poor allocator performance with labeled resources and/or persistent volumes","Modifying the {{HierarchicalAllocator_BENCHMARK_Test.ResourceLabels}} benchmark from https://reviews.apache.org/r/43686/ to use distinct labels between different slaves, performance regresses from ~2 seconds to ~3 minutes. The culprit seems to be the way in which the allocator merges together resources; reserved resource labels (or persistent volume IDs) inhibit merging, which causes performance to be much worse.",Bug,Blocker,Resolved,"2016-03-01 21:34:04","2016-03-01 21:34:04",5
"Apache Mesos","DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes exits when the /tmp directory is bind-mounted","If the {{/tmp}} directory (where Mesos tests create temporary directories) is a bind mount, the test suite will exit here:   There appear to be two problems: 1) The docker containerizer should not exit on failure to clean up orphans.  The MesosContainerizer does not do this (see [MESOS-2367]). 2) Unmounting the orphan persistent volume fails for some reason.",Bug,Major,Resolved,"2016-03-01 21:22:16","2016-03-01 21:22:16",2
"Apache Mesos","Bind docker runtime isolator with docker image provider.","If image provider is specified as `docker` but docker/runtime is not set, it would be not meaningful, because of no executables. A check should be added to make sure docker runtime isolator is on if using docker as image provider.",Bug,Major,Resolved,"2016-03-01 19:43:47","2016-03-01 19:43:47",1
"Apache Mesos","Remove `grace_period_seconds` field from Shutdown event v1 protobuf.","There are two ways in which a shutdown of executor can be triggered: 1. If it receives an explicit `Shutdown` message from the agent. 2. If the recovery timeout period has elapsed, and the executor still hasn’t been able to (re-)connect with the agent.  Currently, the executor library relies on the field `grace_period_seconds` having a default value of 5 seconds to handle the second scenario. https://github.com/apache/mesos/blob/master/src/executor/executor.cpp#L608  The driver used to trigger the grace period via a constant defined in src/slave/constants.cpp. https://github.com/apache/mesos/blob/master/src/exec/exec.cpp#L92  The agent may want to force a shorter shutdown grace period (e.g. oversubscription eviction may have shorter deadline) in the future. For now, we can just read the value via an environment variable.",Task,Major,Resolved,"2016-03-01 18:48:31","2016-03-01 18:48:31",3
"Apache Mesos","Master's slave reregister logic does not update version field","The master's logic for reregistering a slave does not update the version field if the slave re-registers with a new version.",Bug,Blocker,Resolved,"2016-03-01 02:03:30","2016-03-01 02:03:30",1
"Apache Mesos","filesystem/linux isolator does not unmount orphaned persistent volumes","A persistent volume can be orphaned when: # A framework registers with checkpointing enabled. # The framework starts a task + a persistent volume. # The agent exits.  The task continues to run. # Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent. # The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.  The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}})    Test implemented here: https://reviews.apache.org/r/44122/",Bug,Blocker,Resolved,"2016-03-01 00:28:17","2016-03-01 00:28:17",2
"Apache Mesos","Implement port forwarding in `network/cni` isolator","Most docker and appc images wish to expose ports that micro-services are listening on, to the outside world. When containers are running on bridged (or ptp) networking this can be achieved by installing port forwarding rules on the agent (using iptables). This can be done in the `network/cni` isolator.   The reason we would like this functionality to be implemented in the `network/cni` isolator, and not a CNI plugin, is that the specifications currently do not support specifying port forwarding rules. Further, to install these rules the isolator needs two pieces of information, the exposed ports and the IP address associated with the container. Bother are available to the isolator.",Task,Critical,Resolved,"2016-02-29 20:12:21","2016-02-29 20:12:21",2
"Apache Mesos","Add support for local image fetching in Appc provisioner.","Currently Appc image provisioner supports http(s) fetching. It would be valuable to add support for local file path(URI) based  fetching.",Task,Major,Resolved,"2016-02-29 20:07:37","2016-02-29 20:07:37",2
"Apache Mesos","Introduce a port field in `ImageManifest` in order to set exposed ports for a container.","Networking isolators such as `network/cni` need to learn about ports that a container wishes to be exposed to the outside world. This can be achieved by adding a field to the `ImageManifest` protobuf and allowing the `ImageProvisioner` to set these fields to inform the isolator of the ports that the container wishes to be exposed. ",Task,Major,Resolved,"2016-02-29 20:06:19","2016-02-29 20:06:19",1
"Apache Mesos","Need to set `EXPOSED` ports from docker images into `ContainerConfig`","Most docker images have an `EXPOSE` command associated with them. This tells the container run-time the TCP ports that the micro-service wishes to expose to the outside world.   With the `Unified containerizer` project since `MesosContainerizer` is going to natively support docker images it is imperative that the Mesos container run time have a mechanism to expose ports listed in a Docker image. The first step to achieve this is to extract this information from the `Docker` image and set in the `ContainerConfig` . The `ContainerConfig` can then be used to pass this information to any isolator (for e.g. `network/cni` isolator) that will install port forwarding rules to expose the desired ports.",Task,Critical,Resolved,"2016-02-29 20:02:55","2016-02-29 20:02:55",1
"Apache Mesos","Add documentation for Appc image discovery.","Add documentation for the Appc image discovery feature that covers:  - Use case - Implementation detail (Simple discovery).",Documentation,Major,Accepted,"2016-02-29 19:56:41","2016-02-29 19:56:41",3
"Apache Mesos","Add end to end testing for Appc images.","Add tests that covers integration test of the Appc provisioner feature with mesos containerizer.  ",Task,Major,Resolved,"2016-02-29 19:52:59","2016-02-29 19:52:59",3
"Apache Mesos","Remove internal usage of deprecated *.json endpoints.","We still use the deprecated *.json internally (UI, tests, documentation). ",Task,Major,Resolved,"2016-02-29 19:34:49","2016-02-29 19:34:49",3
"Apache Mesos","Implement base tests for unified container using local puller.","Using command line executor to test shell commands with local docker images.",Task,Major,Resolved,"2016-02-29 18:33:30","2016-02-29 18:33:30",2
"Apache Mesos","Mesos fails to escape command health checks","As described in https://github.com/<USER>marathon/issues/3333 I would like to run a command health check   The health check fails because Mesos, while running the command inside double quotes of a sh -c  doesn't escape the double quotes in the command.  If I escape the double quotes myself the command health check succeeds. But this would mean that the user needs intimate knowledge of how Mesos executes his commands which can't be right.  I was told this is not a Marathon but a Mesos issue so am opening this JIRA. I don't know if this only affects the command health check. ",Bug,Major,Resolved,"2016-02-29 17:46:36","2016-02-29 17:46:36",5
"Apache Mesos","ProvisionerDockerPullerTest.ROOT_INTERNET_CURL_ShellCommand fails."," ",Bug,Major,Resolved,"2016-02-29 10:42:42","2016-02-29 10:42:42",3
"Apache Mesos","IOTest.BufferedRead writes to the current directory","libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,  * should the test fail data might be leaked to random locations, * the test cannot be executed from a write-only directory, or * executing the same test in parallel would race on the existence of the created file, and show bogus behavior.  The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",Bug,Minor,Resolved,"2016-02-29 08:38:18","2016-02-29 08:38:18",1
"Apache Mesos","LevelDBStateTests write to the current directory","All {{LevelDBStateTest}} tests write to the current directory. This is bad for a number of reasons, e.g.,  * should the test fail data might be leaked to random locations, * the test cannot be executed from a write-only directory, or * executing tests from the same suite in parallel (e.g., with {{gtest-parallel}} would race on the existence of the created files, and show bogus behavior.  The tests should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",Bug,Major,Resolved,"2016-02-29 08:36:11","2016-02-29 08:36:11",2
"Apache Mesos","Update ry-http-parser-1c3624a to nodejs/http-parser 2.6.1","See https://github.com/nodejs/http-parser/releases/tag/v2.6.1. The motivation is that nodejs/http-parser 2.6.1 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].",Improvement,Major,Resolved,"2016-02-29 08:16:52","2016-02-29 08:16:52",3
"Apache Mesos","Update vendored libev to 4.22","The motivation is that libev 4.22 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].",Improvement,Major,Resolved,"2016-02-29 08:07:12","2016-02-29 08:07:12",3
"Apache Mesos","Update leveldb patch file to suport PowerPC LE","See: https://github.com/google/leveldb/releases/tag/v1.18 for improvements / bug fixes. The motivation is that leveldb 1.18 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].  Update: Since someone updated leveldb to 1.4, so I only update the patch file to support PowerPC LE. Because I don't think upgrade 3rdparty library frequently is a good thing.",Improvement,Major,Resolved,"2016-02-29 07:54:41","2016-02-29 07:54:41",3
"Apache Mesos","Updated `createFrameworkInfo` for hierarchical_allocator_tests.cpp.","The function of {{createFrameworkInfo}} in hierarchical_allocator_tests.cpp should be updated by enabling caller can set a framework capability to create a framework which can use revocable resources.",Improvement,Minor,Resolved,"2016-02-29 07:01:48","2016-02-29 07:01:48",1
"Apache Mesos","Make existing scheduler library tests use the callback interface.","We need to migrate the existing tests in {{src/tests/scheduler_tests.cpp}} to use the new callback interface introduced in {{MESOS-3339}}. The changes to {{src/tests/master_maintenance_tests.cpp}} would be done when MESOS-4831 is resolved.  For an example see {{SchedulerTest.SchedulerFailover}} which already uses this new interface.",Task,Major,Resolved,"2016-02-29 01:47:49","2016-02-29 01:47:49",5
"Apache Mesos","Add a couple of registrar tests for /weights endpoint",,Task,Minor,Resolved,"2016-02-28 10:21:29","2016-02-28 10:21:29",2
"Apache Mesos","Add documentation around using the docker containerizer on CentOS 6.","Support for persistent volumes was added to the docker containerizer in [MESOS-3413].  However, this does not work on CentOS 6.  On CentOS 6, the same {{docker run -v ...}} operation does not perform a recursive bind, whereas on every other OS supported by Mesos, docker does a recursive bind.  Docker has already [dropped support for CentOS 6|https://github.com/docker/docker/issues/14365], so we should add precautionary documentation in case anyone tries to use the docker containerizer on CentOS 6.",Documentation,Major,Accepted,"2016-02-26 23:34:40","2016-02-26 23:34:40",1
"Apache Mesos","Revert external linkage of symbols in master/constants.hpp","src/master/constants.hpp contains:    From commit 232a23b2a2e11f4e905b834aa2a11afe5bf6438a. We should investigate whether this is still necessary on supported compilers; it likely is not.",Improvement,Trivial,Resolved,"2016-02-26 21:31:41","2016-02-26 21:31:41",1
"Apache Mesos","HTTP endpoint docs should use shorter paths","My understanding is that the recommended path for the v1 scheduler API is {{/api/v1/scheduler}}, but the HTTP endpoint [docs|http://mesos.apache.org/documentation/latest/endpoints/] for this endpoint list the path as {{/master/api/v1/scheduler}}; the filename of the doc page is also in the {{master}} subdirectory.  Similarly, we document the master state endpoint as {{/master/state}}, whereas the preferred name is now just {{/state}}, and so on for most of the other endpoints. Unlike we the V1 API, we might want to consider backward compatibility and document both forms -- not sure. But certainly it seems like we should encourage people to use the shorter paths, not the longer ones.",Documentation,Minor,Resolved,"2016-02-26 18:50:02","2016-02-26 18:50:02",2
"Apache Mesos","Example in C++ style guide uses wrong indention for wrapped line","  Here the second line should be indented by two spaces since it is a wrapped assignment; the corresponding rule is laid out in the preceeding paragraph.",Documentation,Trivial,Resolved,"2016-02-26 18:48:13","2016-02-26 18:48:13",1
"Apache Mesos","Reorganize ACL subject/object descriptions.","The authorization documentation would benefit from a reorganization of the ACL subject/object descriptions. Instead of simple lists of the available subjects and objects, it would be nice to see a table showing which subject and object is used with each action.",Documentation,Blocker,Resolved,"2016-02-26 17:41:51","2016-02-26 17:41:51",5
"Apache Mesos","SlaveTest.MetricsSlaveLaunchErrors test relies on implicit blocking behavior hitting the global metrics endpoint","The test attempts to observe a change in the {{slave/container_launch_errors}} metric, but does not wait for the triggering action to take place. Currently the test passes since hitting the endpoint blocks for some rate limit-related time which provides under many circumstances enough wait time for the action to take place. ",Bug,Major,Resolved,"2016-02-26 09:17:06","2016-02-26 09:17:06",1
"Apache Mesos","Disable rate limiting of the global metrics endpoint for mesos-tests execution","Once we can optionally disable rate limiting in the global metrics endpoint with MESOS-4776 we should disable the rate limiting during the execution of mesos-tests.  * rate limiting makes it cumbersome to repeatedly hit the endpoint since one would not want to interfere with the rate limiting * rate limiting might incur additional wait time which might slown down tests",Improvement,Major,Resolved,"2016-02-26 09:13:16","2016-02-26 09:13:16",3
"Apache Mesos","Executor env variables should not be leaked to the command task.","Currently, command task inherits the env variables of the command executor. This is less ideal because the command executor environment variables include some Mesos internal env variables like MESOS_XXX and LIBPROCESS_XXX. Also, this behavior does not match what Docker containerizer does. We should construct the env variables from scratch for the command task, rather than relying on inheriting the env variables from the command executor.",Bug,Major,Reviewable,"2016-02-25 23:03:45","2016-02-25 23:03:45",3
"Apache Mesos","Remove `user` and `rootfs` flags in Windows launcher.",,Task,Major,Resolved,"2016-02-25 19:16:34","2016-02-25 19:16:34",2
"Apache Mesos","Add appc/runtime isolator for runtime isolation for appc images.","Appc image also contains runtime information like 'exec', 'env', 'workingDirectory' etc. https://github.com/appc/spec/blob/master/spec/aci.md  Similar to docker images, we need to support a subset of them (mainly 'exec', 'env' and 'workingDirectory').",Task,Major,Reviewable,"2016-02-25 17:58:22","2016-02-25 17:58:22",13
"Apache Mesos","Libprocess metrics/snapshot endpoint rate limiting should be configurable.","Currently the {{/metrics/snapshot}} endpoint in libprocess has a [hard-coded|https://github.com/apache/mesos/blob/0.27.1/3rdparty/libprocess/include/process/metrics/metrics.hpp#L52] rate limit of 2 requests per second:    This should be configurable via a libprocess environment variable so that users can control this when initializing libprocess.",Improvement,Major,Resolved,"2016-02-25 16:27:13","2016-02-25 16:27:13",2
"Apache Mesos","TaskInfo/ExecutorInfo should include fine-grained ownership/namespacing","We need a way to assign fine-grained ownership to tasks/executors so that multi-user frameworks can tell Mesos to associate the task with a user identity (rather than just the framework principal+role). Then, when an HTTP user requests to view the task's sandbox contents, or kill the task, or list all tasks, the authorizer can determine whether to allow/deny/filter the request based on finer-grained, user-level ownership. Some systems may want TaskInfo.owner to represent a group rather than an individual user. That's fine as long as the framework sets the field to the group ID in such a way that a group-aware authorizer can interpret it.",Improvement,Major,Resolved,"2016-02-25 07:00:29","2016-02-25 07:00:29",2
"Apache Mesos","Document the network/cni isolator.","We need to document this isolator in mesos-containerizer.md (e.g., how to configure it, what's the pre-requisite, etc.)",Task,Major,Resolved,"2016-02-25 00:23:05","2016-02-25 00:23:05",3
"Apache Mesos","MasterMaintenanceTest.InverseOffers is flaky","[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].  Verbose logs from ASF Centos7 build: ",Bug,Major,Resolved,"2016-02-24 22:57:24","2016-02-24 22:57:24",1
"Apache Mesos","The network/cni isolator should report assigned IP address. ","In order for service discovery to work in some cases, the network/cni isolator needs to report the assigned IP address through the isolator->status() interface.",Task,Major,Resolved,"2016-02-24 19:22:47","2016-02-24 19:22:47",3
"Apache Mesos","Add test mock for CNI plugins.","In order to test the network/cni isolator, we need to mock the behavior of an CNI plugin. One option is to write a mock script which acts as a CNI plugin. The isolator will talk to the mock script the same way it talks to an actual CNI plugin.  The mock script can just join the host network?",Task,Major,Resolved,"2016-02-24 19:20:14","2016-02-24 19:20:14",5
"Apache Mesos","Setup proper DNS resolver for containers in network/cni isolator.","Please get more context from the design doc (MESOS-4742).  The CNI plugin will return the DNS information about the network. The network/cni isolator needs to properly setup /etc/resolv.conf for the container. We should consider the following cases: 1) container is using host filesystem 2) container is using a different filesystem 3) custom executor and command executor",Task,Major,Resolved,"2016-02-24 19:15:58","2016-02-24 19:15:58",5
"Apache Mesos","Add agent flags to allow operators to specify CNI plugin and config directories.","According to design doc, we plan to add the following flags:  “--network_cni_plugins_dir” Location of the CNI plugin binaries. The “network/cni” isolator will find CNI plugins under this directory so that it can execute the plugins to add/delete container from the CNI networks. It is the operator’s responsibility to install the CNI plugin binaries in the specified directory.  “--network_cni_config_dir” Location of the CNI network configuration files. For each network that containers launched in Mesos agent can connect to, the operator should install a network configuration file in JSON format in the specified directory.",Task,Major,Resolved,"2016-02-24 19:11:17","2016-02-24 19:11:17",2
"Apache Mesos","Expose metrics and gauges for fetcher cache usage and hit rate","To evaluate the fetcher cache and calibrate the value of the fetcher_cache_size flag, it would be useful to have metrics and gauges on agents that expose operational statistics like cache hit rate, occupied cache size, and time spent downloading resources that were not present.",Improvement,Minor,Open,"2016-02-24 19:08:13","2016-02-24 19:08:13",2
"Apache Mesos","Add network/cni isolator for Mesos containerizer.","See the design doc for more context (MESOS-4742).  The isolator will interact with CNI plugins to create the network for the container to join.",Task,Major,Resolved,"2016-02-24 19:07:53","2016-02-24 19:07:53",8
"Apache Mesos","Add a 'name' field into NetworkInfo.","This allows the framework writer to specify the name of the network they want their container to join.  Why not using 'groups'? That's because there might be multiple groups under a single network (e.g., admin vs. user, public vs. private, etc.).",Task,Major,Resolved,"2016-02-24 19:05:07","2016-02-24 19:05:07",1
"Apache Mesos","Mesos containerizer should get uid/gids before pivot_root.","Currently, we call os::su(user) after pivot_root. This is problematic because /etc/passwd and /etc/group might be missing in container's root filesystem. We should instead, get the uid/gids before pivot_root, and call setuid/setgroups after pivot_root.",Bug,Major,Resolved,"2016-02-24 18:47:27","2016-02-24 18:47:27",3
"Apache Mesos","The executors field is exposed under a backwards incompatible schema.","In 0.26.0, the master's {{/state}} endpoint generated the following:    In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:    This is a backwards incompatible API change.",Bug,Major,Resolved,"2016-02-24 08:49:33","2016-02-24 08:49:33",2
"Apache Mesos","Document: Mesos Executor expects all SSL_* environment variables to be set","I was trying to run Docker containers in a fully SSL-ized Mesos cluster but ran into problems because the executor was failing with a Failed to shutdown socket with fd 10: Transport endpoint is not connected.  My understanding of why this is happening is because the executor was trying to report its status to Mesos slave over HTTPS, but doesnt have the appropriate certs/env setup inside the executor.  (Thanks to mslackbot/joseph for helping me figure this out on #mesos)  It turns out, the executor expects all SSL_* variables to be set inside `CommandInfo.environment` which gets picked up by the executor to successfully reports its status to the slave.  This part of __executor needing all the SSL_* variables to be set in its environment__ is missing in the Mesos SSL transitioning guide. I request you to please add this vital information to the doc.",Documentation,Major,Resolved,"2016-02-23 21:04:52","2016-02-23 21:04:52",2
"Apache Mesos","Move HTB out of containers","Currently we set a fixed HTB bandwidth in each of the container, which makes it impossible to share the link if idle. As the first step, we should move it out of the containers, into the qdisc hierarchy of the physical interface.",Task,Minor,Open,"2016-02-23 19:52:08","2016-02-23 19:52:08",3
"Apache Mesos","Add Appc image fetcher tests.","Mesos now has support for fetching Appc images. Add tests that verifies the new component.",Task,Major,Resolved,"2016-02-23 19:50:24","2016-02-23 19:50:24",3
"Apache Mesos","ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation","Some cleanup of spawned processes is missing in {{ContainerLoggerTest.MesosContainerizerRecover}} so that when the test is run in isolation the global teardown might find lingering processes.   Observered on OS X with clang-trunk and an unoptimized build. ",Bug,Major,Resolved,"2016-02-23 19:40:34","2016-02-23 19:40:34",1
"Apache Mesos","CMake: Add leveldb library to 3rdparty external builds.",,Task,Major,Resolved,"2016-02-23 15:10:55","2016-02-23 15:10:55",3
"Apache Mesos","DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes fails on CentOS 6","This test passes consistently on other OS's, but fails consistently on CentOS 6.  Verbose logs from test failure: ",Bug,Major,Resolved,"2016-02-22 19:16:15","2016-02-22 19:16:15",2
"Apache Mesos","Update /frameworks to use jsonify","This should let us remove the duplicated code in {{http.cpp}} between {{model(Framework)}} and {{json(Full<Framework>)}}.",Improvement,Major,Resolved,"2016-02-22 01:32:14","2016-02-22 01:32:14",3
"Apache Mesos","Document scheduler driver calls in framework development guide.","The interface examples are slightly out of sync with scheduler.hpp, most notably missing the new acceptOffers call.",Documentation,Major,Resolved,"2016-02-19 19:22:14","2016-02-19 19:22:14",2
"Apache Mesos","Add allocator metric for currrent dominant shares of frameworks and roles",,Improvement,Major,Resolved,"2016-02-19 12:10:07","2016-02-19 12:10:07",5
"Apache Mesos","Add allocator metric for currently satisfied quotas","We currently expose information on set quotas via dedicated quota endpoints. To diagnose allocator problems one additionally needs information about used quotas.",Improvement,Major,Resolved,"2016-02-19 12:06:23","2016-02-19 12:06:23",2
"Apache Mesos","Add allocator metric for number of active offer filters","To diagnose scenarios where frameworks unexpectedly do not receive offers information on currently active filters are needed.",Improvement,Major,Resolved,"2016-02-19 12:05:33","2016-02-19 12:05:33",1
"Apache Mesos","Expose allocation algorithm latency via a metric.","The allocation algorithm has grown to become fairly expensive, gaining visibility into its latency enables monitoring and alerting.  Similar allocator timing-related information is already exposed in the log, but should also be exposed via an endpoint.",Improvement,Major,Resolved,"2016-02-19 12:04:46","2016-02-19 12:04:46",1
"Apache Mesos","Add allocator metrics for total vs offered/allocated resources.","Exposing the current allocation breakdown as seen by the allocator will allow us to correlated the corresponding metrics in the master with what the allocator sees. We should expose at least allocated or available, and total.",Improvement,Major,Resolved,"2016-02-19 12:03:48","2016-02-19 12:03:48",2
"Apache Mesos","Add allocator metric for number of offers each role / framework received.","A counter for the number of allocations to a framework can be used to monitor allocation progress, e.g., when agents are added to a cluster, and as other frameworks are added or removed.  Currently, an offer by the hierarchical allocator to a framework consists of a list of resources on possibly many agents. Resources might be offered in order to satisfy outstanding quota or for fairness. To capture allocations on fine granularity we should not count the number of offers, but instead the pieces making up that offer, as such a metric would better resolve the effect of changes (e.g., adding/removing a framework). ",Improvement,Major,Accepted,"2016-02-19 12:00:18","2016-02-19 12:00:18",2
"Apache Mesos","Add allocator metric for number of completed allocation runs",,Improvement,Major,Resolved,"2016-02-19 11:59:15","2016-02-19 11:59:15",1
"Apache Mesos","make DESTDIR=<path> install broken","There is a missing '$(DESTDIR)' prefix in the install-data-hook that causes DESTDIR builds to be broken.",Bug,Major,Resolved,"2016-02-18 22:39:03","2016-02-18 22:39:03",2
"Apache Mesos","ReviewBot should not fail hard if there are circular dependencies in a review chain","Instead of failing hard, ReviewBot should post an error to the review that a circular dependency is detected.",Task,Major,Resolved,"2016-02-18 22:19:29","2016-02-18 22:19:29",2
"Apache Mesos","Remove 'force' field from the Subscribe Call in v1 Scheduler API","We/I introduced the `force` field in SUBSCRIBE call to deal with scheduler partition cases. Having thought a bit more and discussing with few other folks ([~<USER>, [~<USER>), I think we can get away from not having that field in the v1 API. The obvious advantage of removing the field is that framework devs don't have to think about how/when to set the field (the current semantics are a bit confusing).  The new workflow when a master receives a SUBSCRIBE call is that master always accepts this call and closes any existing connection (after sending ERROR event) from the same scheduler (identified by framework id).    The expectation from schedulers is that they must close the old subscribe connection before resending a new SUBSCRIBE call.  Lets look at some tricky scenarios and see how this works and why it is safe.  1) Connection disconnection @ the scheduler but not @ the master     Scheduler sees the disconnection and sends a new SUBSCRIBE call. Master sends ERROR on the old connection (won't be received by the scheduler because the connection is already closed) and closes it.  2) Connection disconnection @ master but not @ scheduler  Scheduler realizes this from lack of HEARTBEAT events. It then closes its existing connection and sends a new SUBSCRIBE call. Master accepts the new SUBSCRIBE call. There is no old connection to close on the master as it is already closed.  3) Scheduler failover but no disconnection @ master  Newly elected scheduler sends a SUBSCRIBE call. Master sends ERROR event and closes the old connection (won't be received because the old scheduler failed over).  4) If Scheduler A got partitioned (but is alive and connected with master) and Scheduler B got elected as new leader.  When Scheduler B sends SUBSCRIBE, master sends ERROR and closes the connection from Scheduler A. Master accepts Scheduler B's connection. Typically Scheduler A aborts after receiving ERROR and gets restarted. After restart it won't become the leader because Scheduler B is already elected.  5) Scheduler sends SUBSCRIBE, times out, closes the SUBSCRIBE connection (A) and sends a new SUBSCRIBE (B). Master receives SUBSCRIBE (B) and then receives SUBSCRIBE (A) but doesn't see A's disconnection yet.  Master first accepts SUBSCRIBE (B). After it receives SUBSCRIBE (A), it sends ERROR to SUBSCRIBE (B) and closes that connection. When it accepts SUBSCRIBE (A) and tries to send SUBSCRIBED event the connection closure is detected. Scheduler retries the SUBSCRIBE connection after a backoff. I think this is a rare enough race for it to happen continuously in a loop.  ",Task,Major,Resolved,"2016-02-18 22:12:51","2016-02-18 22:12:51",5
"Apache Mesos","Enable zlib on Windows.",,Task,Major,Resolved,"2016-02-18 05:25:05","2016-02-18 05:25:05",1
"Apache Mesos","Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)","Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.  We should make a StoutConfigure.cmake that can be included by any package downstream.",Bug,Major,Resolved,"2016-02-18 03:50:36","2016-02-18 03:50:36",1
"Apache Mesos","Document default value of offer_timeout","There isn't a default value (i.e., offers do not timeout by default), but we should clarify this in {{flags.cpp}} and {{configuration.md}}.",Documentation,Minor,Resolved,"2016-02-18 01:28:15","2016-02-18 01:28:15",1
"Apache Mesos","Allow Reserve operations by a principal without `ReservationInfo.principal`","Currently, we require a framework or operator to specify `ReservationInfo.principal` when they reserve resources. This isn't necessary, however; we already know the principal and can fill in the field if it isn't set already.",Improvement,Major,Resolved,"2016-02-17 20:01:44","2016-02-17 20:01:44",2
"Apache Mesos","SlaveTest.StateEndpoint is flaky","  Even though this test does {{Clock::pause()}} before starting the agent, there's a possibility that a numified-stringified double to not equal itself, even after rounding to the nearest int.",Bug,Major,Resolved,"2016-02-17 19:55:29","2016-02-17 19:55:29",1
"Apache Mesos","Add a HierarchicalAllocator benchmark with reservation labels.","With {{Labels}} being part of the {{ReservationInfo}}, we should ensure that we don't observe a significant performance degradation in the allocator.",Task,Major,Resolved,"2016-02-17 08:20:49","2016-02-17 08:20:49",3
"Apache Mesos","Reorganize 3rdparty directory","This issues is currently being discussed in the dev mailing list: http://www.mail-archive.com/<EMAIL>/msg34349.html",Epic,Major,"In Progress","2016-02-16 21:28:59","2016-02-16 21:28:59",5
"Apache Mesos","Design doc for v1 Operator API","We need to design how the v1 operator API (all the HTTP endpoints exposed by master/agent that are not for scheduler/executor interactions) looks and works.",Documentation,Major,Resolved,"2016-02-16 18:37:53","2016-02-16 18:37:53",8
"Apache Mesos","Implement reliable floating point for scalar resources","Design doc: https://docs.google.com/document/d/14qLxjZsfIpfynbx0USLJR0GELSq8hdZJUWw6kaY_DXc/edit?usp=sharing",Improvement,Major,Resolved,"2016-02-16 18:13:37","2016-02-16 18:13:37",5
"Apache Mesos","Implement master failover tests for the scheduler library.","Currently, the scheduler library creates its own {{MasterDetector}} object internally. We would need to create a standalone detector and create new tests for testing that callbacks are invoked correctly in the event of a master failover.",Task,Major,Resolved,"2016-02-16 18:13:36","2016-02-16 18:13:36",3
"Apache Mesos","Create base docker image for test suite.","This should be widely used for unified containerizer testing. Should basically include:  *at least one layer. *repositories.  For each layer: *root file system as a layer tar ball. *docker image json (manifest). *docker version.",Bug,Major,Resolved,"2016-02-16 07:15:15","2016-02-16 07:15:15",3
"Apache Mesos","Document docker runtime isolator.","Should include the following information:  *What features are currently supported in docker runtime isolator. *How to use the docker runtime isolator (user manual). *Compare the different semantics v.s. docker containerizer, and explain why.",Bug,Major,Resolved,"2016-02-16 07:07:19","2016-02-16 07:07:19",2
"Apache Mesos","Upgrade vendored Protobuf to 2.6.1","We currently vendor Protobuf 2.5.0. We should upgrade to Protobuf 2.6.1. This introduces various bugfixes, performance improvements, and at least one new feature we might want to eventually take advantage of ({{map}} data type). AFAIK there should be no backward compatibility concerns.",Improvement,Major,Resolved,"2016-02-15 19:43:41","2016-02-15 19:43:41",3
"Apache Mesos","ROOT_DOCKER_Logs is flaky.",,Bug,Major,Resolved,"2016-02-15 17:36:37","2016-02-15 17:36:37",2
"Apache Mesos","Cannot disable systemd support","On certain platforms the systemd init system is available, but not used. Not being able to disable the mesos systemd integration on these platforms makes it hard to operate using a different init / monit system.",Bug,Major,Resolved,"2016-02-15 17:35:04","2016-02-15 17:35:04",1
"Apache Mesos","Linux filesystem isolator tests are flaky.","LinuxFilesystemIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem sometimes fails on CentOS 7 with this kind of output:   LinuxFilesystemIsolatorTest.ROOT_MultipleContainers often has this output:   Whether SSL is configured makes no difference.  This test may also fail on other platforms, but more rarely.  ",Bug,Major,Resolved,"2016-02-15 16:37:06","2016-02-15 16:37:06",3
"Apache Mesos","Status updates from executor can be forwarded out of order by the Agent.","Previously, all status update messages from the executor were forwarded by the agent to the master in the order that they had been received.   However, that seems to be no longer valid due to a recently introduced change in the agent:    This can sometimes lead to status updates being sent out of order depending on the order the {{Future}} is fulfilled from the call to {{status(...)}}.",Bug,Blocker,Resolved,"2016-02-13 01:59:47","2016-02-13 01:59:47",1
"Apache Mesos","`cgroup_info` not being exposed in state.json when ComposingContainerizer is used.","The ComposingContainerizer currently does not have a `status` method. This results in no `ContainerStatus` being updated in the agent, when uses `ComposingContainerizer` to launch containers. This would specifically happen when the agent is launched with `--containerizer=docker,mesos`",Bug,Major,Resolved,"2016-02-12 23:15:35","2016-02-12 23:15:35",1
"Apache Mesos","Add common compression utility","We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.",Bug,Major,Resolved,"2016-02-12 22:10:23","2016-02-12 22:10:23",2
"Apache Mesos","Expose persistent volume information in HTTP endpoints","The per-slave {{reserved_resources}} information returned by {{/state}} does not seem to include information about persistent volumes. This makes it hard for operators to use the {{/destroy-volumes}} endpoint.",Improvement,Major,Resolved,"2016-02-12 19:26:52","2016-02-12 19:26:52",3
"Apache Mesos","Document net_cls isolator in docs/mesos-containerizer.md.","We need to add a section in the doc to describe how to use cgroups/net_cls isolator.",Task,Major,Resolved,"2016-02-11 19:50:05","2016-02-11 19:50:05",1
"Apache Mesos","Add LOG(INFO) in `cgroups/net_cls` for debugging allocation of net_cls handles.","We need to add LOG(INFO) during the prepare phase of `cgroups/net_cls` for debugging management of `net_cls` handles within the isolator. ",Improvement,Minor,Resolved,"2016-02-11 15:32:51","2016-02-11 15:32:51",1
"Apache Mesos","Logrotate container logger can die with agent unit on systemd.",,Bug,Major,Resolved,"2016-02-10 16:47:38","2016-02-10 16:47:38",1
"Apache Mesos","Posix process executor can die with agent unit on systemd.",,Bug,Major,Resolved,"2016-02-10 16:46:29","2016-02-10 16:46:29",1
"Apache Mesos","Docker process executor can die with agent unit on systemd.",,Bug,Major,Resolved,"2016-02-10 16:45:07","2016-02-10 16:45:07",1
"Apache Mesos","Add parent hook to subprocess.",,Improvement,Major,Resolved,"2016-02-10 16:39:53","2016-02-10 16:39:53",3
"Apache Mesos","Tests will dereference stack allocated master objects upon assertion/expectation failure.","Tests that use the {{StartMaster}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartMaster}} helper takes raw pointer arguments, which may be stack-allocated.  In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:   The {{StartMaster}} helper should take {{shared_ptr}} arguments instead. This also means that we can remove the {{Shutdown}} helper from most of these tests.",Bug,Major,Resolved,"2016-02-09 23:04:09","2016-02-09 23:04:09",5
"Apache Mesos","Tests will dereference stack allocated agent objects upon assertion/expectation failure.","Tests that use the {{StartSlave}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartSlave}} helper takes raw pointer arguments, which may be stack-allocated.  In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:   The {{StartSlave}} helper should take {{shared_ptr}} arguments instead. This also means that we can remove the {{Shutdown}} helper from most of these tests.",Bug,Major,Resolved,"2016-02-09 22:28:20","2016-02-09 22:28:20",5
"Apache Mesos","Implement partition tests for the HTTP Scheduler API.","Currently, the HTTP V1 API does not have partition tests similar to the one in src/tests/partition_tests.cpp.  For more information see MESOS-3355.",Task,Major,Accepted,"2016-02-09 19:06:34","2016-02-09 19:06:34",5
"Apache Mesos","Implement fault tolerance tests for the HTTP Scheduler API.","Currently, the HTTP V1 API does not have fault tolerance tests similar to the one in {{src/tests/fault_tolerance_tests.cpp}}.   For more information see MESOS-3355.",Task,Major,Resolved,"2016-02-09 19:03:42","2016-02-09 19:03:42",5
"Apache Mesos","Support Nvidia GPUs with filesystem isolation enabled in mesos containerizer.","When filesystem isolation is enabled in the mesos containerizer, containers that use Nvidia GPU resources need access to GPU libraries residing on the host.  We'll need to provide a means for operators to inject the necessary volumes into *all* containers that use gpus resources.  See the nvidia-docker project for more details: [nvidia-docker/tools/src/nvidia/volumes.go|https://github.com/NVIDIA/nvidia-docker/blob/fda10b2d27bf5578cc5337c23877f827e4d1ed77/tools/src/nvidia/volumes.go#L50-L103]",Task,Major,Resolved,"2016-02-09 09:59:45","2016-02-09 09:59:45",13
"Apache Mesos","Implement Nvidia GPU isolation w/o filesystem isolation enabled.","The Nvidia GPU isolator will need to use the device cgroup to restrict access to GPU resources, and will need to recover this information after agent failover. For now this will require that the operator specifies the GPU devices via a flag.  To handle filesystem isolation requires that we provide mechanisms for operators to inject volumes with the necessary libraries into all containers using GPU resources, we'll tackle this in a separate ticket.",Task,Major,Resolved,"2016-02-09 09:52:28","2016-02-09 09:52:28",5
"Apache Mesos","Add allocation metrics for gpus resources.","Allocation metrics are currently hard-coded to include only {{\[cpus, mem, disk\]}} resources. We'll need to add gpus to the list to start, possibly following up on the TODO to remove the hard-coding.  See: https://github.com/apache/mesos/blob/0.27.0/src/master/metrics.cpp#L266-L269 https://github.com/apache/mesos/blob/0.27.0/src/slave/metrics.cpp#L123-L126 ",Task,Major,Resolved,"2016-02-09 09:47:56","2016-02-09 09:47:56",1
"Apache Mesos","Add a stub Nvidia GPU isolator.","We'll first wire up a skeleton Nvidia GPU isolator, which needs to be guarded by a configure flag due to the dependency on NVML.",Task,Major,Resolved,"2016-02-09 09:43:14","2016-02-09 09:43:14",3
"Apache Mesos","Update configuration.md with `--cgroups_net_cls_primary_handle` agent flag.","As part of the net_cls epic, we introduce an agent flag called `--cgroup_net_cls_primary_handle` . We need to update configuration.md with the corresponding help string. ",Documentation,Major,Resolved,"2016-02-08 22:39:30","2016-02-08 22:39:30",1
"Apache Mesos","Remove markdown files from doxygen pages","The doxygen html pages corresponding to doc/* markdown files are redundant and have broken links. They don't serve any reasonable purpose in doxygen site.",Bug,Major,Resolved,"2016-02-08 00:39:37","2016-02-08 00:39:37",1
"Apache Mesos","ContainerLoggerTest.DefaultToSandbox is flaky","Just saw this failure on the ASF CI:  ",Bug,Major,Resolved,"2016-02-06 01:43:12","2016-02-06 01:43:12",1
"Apache Mesos","SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky","Just saw this failure on the ASF CI:  ",Bug,Major,Resolved,"2016-02-06 00:56:24","2016-02-06 00:56:24",3
"Apache Mesos","Update vendored ZooKeeper to 3.4.8","See: http://zookeeper.apache.org/doc/r3.4.8/releasenotes.html for improvements / bug fixes  Added a new patch that solved [ZOOKEEPER-1643](https://issues.apache.org/jira/browse/ZOOKEEPER-1643)  The original patch: <https://github.com/apache/zookeeper/commit/46b565e6abd8423c43f1bb8da782d76bac7c392c>",Improvement,Major,Resolved,"2016-02-05 22:59:01","2016-02-05 22:59:01",3
"Apache Mesos","Passing a lambda to dispatch() always matches the template returning void","The following idiom does not currently compile:    This seems non-intuitive because the following template exists for dispatch:    However, lambdas cannot be implicitly cast to a corresponding std::function<R()> type. To make this work, you have to explicitly type the lambda before passing it to dispatch.    We should add template support to allow lambdas to be passed to dispatch() without explicit typing.  ",Bug,Major,Resolved,"2016-02-05 20:55:37","2016-02-05 20:55:37",5
"Apache Mesos","Subprocess should be more intelligent about setting/inheriting libprocess environment variables ","Mostly copied from [this comment|https://issues.apache.org/jira/browse/MESOS-4598?focusedCommentId=15133497&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15133497]  A subprocess inheriting the environment variables {{LIBPROCESS_*}} may run into some accidental fatalities:  | || Subprocess uses libprocess || Subprocess is something else || || Subprocess sets/inherits the same {{PORT}} by accident | Bind failure -> exit | Nothing happens (?) | || Subprocess sets a different {{PORT}} on purpose | Bind success (?) | Nothing happens (?) |  (?) = means this is usually the case, but not 100%.  A complete fix would look something like: * If the {{subprocess}} call gets {{environment = None()}}, we should automatically remove {{LIBPROCESS_PORT}} from the inherited environment.   * The parts of [{{executorEnvironment}}|https://github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp#L265] dealing with libprocess & libmesos should be refactored into libprocess as a helper.  We would use this helper for the Containerizer, Fetcher, and ContainerLogger module. * If the {{subprocess}} call is given {{LIBPROCESS_PORT == os::getenv(LIBPROCESS_PORT)}}, we can LOG(WARN) and unset the env var locally.",Bug,Major,Accepted,"2016-02-05 18:21:47","2016-02-05 18:21:47",2
"Apache Mesos","ROOT_DOCKER_DockerHealthyTask is flaky.","Log from Teamcity that is running {{sudo ./bin/mesos-tests.sh}} on AWS EC2 instances:  Happens with Ubuntu 15.04, CentOS 6, CentOS 7 _quite_ often. ",Bug,Major,Resolved,"2016-02-05 09:24:18","2016-02-05 09:24:18",2
"Apache Mesos","Use `std::quoted` for strings in error messages","We'd like to have a consistent format for error strings through the code base.  As per this comment:  [MESOS-3772|https://issues.apache.org/jira/browse/MESOS-3772?focusedCommentId=14965652&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14965652]  We can then overload the stream operator to make sur strings are quoted as needed.  Note: We need to first require compilers that support C++14. For now we have to wait for MSVC to be part of that list.",Improvement,Trivial,Open,"2016-02-04 20:03:00","2016-02-04 20:03:00",3
"Apache Mesos","Logrotate ContainerLogger should not remove IP from environment.","The {{LogrotateContainerLogger}} starts libprocess-using subprocesses.  Libprocess initialization will attempt to resolve the IP from the hostname.  If a DNS service is not available, this step will fail, which terminates the logger subprocess prematurely.  Since the logger subprocesses live on the agent, they should use the same {{LIBPROCESS_IP}} supplied to the agent.",Bug,Major,Resolved,"2016-02-04 19:41:41","2016-02-04 19:41:41",1
"Apache Mesos","Add common Appc spec utilities."," Add common utility functions such as :       - validating image information against actual data in the image directory.       - getting list of dependencies at depth 1 for an image.       - getting image path simple image discovery. ",Task,Major,Resolved,"2016-02-04 18:32:45","2016-02-04 18:32:45",2
"Apache Mesos","`/reserve` and `/create-volumes` endpoints allow operations for any role","When frameworks reserve resources, the validation of the operation ensures that the {{role}} of the reservation matches the {{role}} of the framework. For the case of the {{/reserve}} operator endpoint, however, the operator has no role to validate, so this check isn't performed.  This means that if an ACL exists which authorizes a framework's principal to reserve resources, that same principal can be used to reserve resources for _any_ role through the operator endpoint.  We should restrict reservations made through the operator endpoint to specified roles. A few possibilities: * The {{object}} of the {{reserve_resources}} ACL could be changed from {{resources}} to {{roles}} * A second ACL could be added for authorization of {{reserve}} operations, with an {{object}} of {{role}} * Our conception of the {{resources}} object in the {{reserve_resources}} ACL could be expanded to include role information, i.e., {{disk(role1);mem(role1)}}",Bug,Major,Resolved,"2016-02-03 21:18:25","2016-02-03 21:18:25",3
"Apache Mesos","Add test case for reservations with same role, different principals","We don't have a test case that covers $SUBJECT; we probably should.",Task,Major,Resolved,"2016-02-03 20:10:18","2016-02-03 20:10:18",2
"Apache Mesos","Update Rakefile for mesos site generation","The stuff in site/ directory needs some updates to make it easier to generate updates for mesos.apache.org site.",Bug,Major,Resolved,"2016-02-03 00:12:51","2016-02-03 00:12:51",2
"Apache Mesos","Rename `examples/event_call_framework.cpp` to `examples/test_http_framework.cpp`","We already have {{examples/test_framework.cpp}} for testing {{PID}} based frameworks. We would ideally want to rename {{event_call_framework}} to correctly reflect that it's an example for HTTP based framework.",Bug,Major,Resolved,"2016-02-02 22:41:56","2016-02-02 22:41:56",1
"Apache Mesos","state.json serving duplicate active fields","state.json is serving duplicate active fields in frameworks.  See the framework 47df96c2-3f85-4bc5-b781-709b2c30c752-0000 In the attached file",Bug,Blocker,Resolved,"2016-02-02 21:05:04","2016-02-02 21:05:04",1
"Apache Mesos","Introduce a stout helper for which","We may want to add a helper to {{stout/os.hpp}} that will natively emulate the functionality of the Linux utility {{which}}.  i.e.   This helper may be useful: * for test filters in {{src/tests/environment.cpp}} * a few tests in {{src/tests/containerizer/port_mapping_tests.cpp}} * the {{sha512}} utility in {{src/common/command_utils.cpp}} * as runtime checks in the {{LogrotateContainerLogger}} * etc.",Improvement,Major,Resolved,"2016-02-01 21:12:13","2016-02-01 21:12:13",2
"Apache Mesos","Fix Appc image caching to share with image fetcher","As Appc image fetcher is being developed, Image cache needs to be shared between store and the image fetcher.",Improvement,Major,Resolved,"2016-02-01 19:51:07","2016-02-01 19:51:07",3
"Apache Mesos","Design doc for scheduler HTTP Stream IDs","This ticket is for the design of HTTP stream IDs, for use with HTTP schedulers. These IDs allow Mesos to distinguish between different instances of HTTP framework schedulers.",Bug,Major,Resolved,"2016-02-01 16:28:19","2016-02-01 16:28:19",5
"Apache Mesos","DockerFetcherPluginTest.INTERNET_CURL_FetchImage seems flaky.","    Failed at the 22nd run.   ",Bug,Major,Resolved,"2016-01-31 02:39:15","2016-01-31 02:39:15",1
"Apache Mesos","Deprecate TASK_STARTING state","We currently have the following task stages:  * TASK_STAGING -> set by slave * TASK_STARTING -> set by the executor (?) * TASK_RUNNING -> set by the executor when the task is running  * TASK_XXX -> task termination statuses  The confusion here is about TASK_STARTING. This is the state between TASK_STAGING and TASK_RUNNING and is somewhat non-intuitive for the reader. Further, looks like no where in the source code, we are setting the TASK_STARTING state.  Why shouldn't we just deprecate/remove it?",Task,Major,Resolved,"2016-01-31 00:12:00","2016-01-31 00:12:00",2
"Apache Mesos","Avoid unnecessary temporary `std::string` constructions and copies in `jsonify`.","A few of the critical code paths in {{jsonify}} involve unnecessary temporary string construction and copies (inherited from the {{JSON::*}}). For example, {{strings::trim}} is used to remove trailing 0s from printing {{double}}. We print {{double}} a lot, and therefore constructing a temporary {{std::string}} on printing of every double is extremely costly. This ticket captures the work involved in avoiding them.",Improvement,Major,Resolved,"2016-01-31 00:05:33","2016-01-31 00:05:33",1
"Apache Mesos","Separate Appc protobuf messages to its own file.","It would be cleaner to keep the Appc protobuf messages separate from other mesos messages.",Improvement,Major,Resolved,"2016-01-30 02:11:12","2016-01-30 02:11:12",2
"Apache Mesos","Mesos UI shows wrong count for started tasks","The task started field shows the number of tasks in state TASKS_STARTING as opposed to those in TASK_RUNNING state.",Bug,Critical,Resolved,"2016-01-29 23:20:22","2016-01-29 23:20:22",2
"Apache Mesos","Run benchmark tests in ASF CI","The build job is already created on ASF CI (https://builds.apache.org/job/Mesos-Benchmarks/) but is currently disabled due to MESOS-4558.",Task,Major,Open,"2016-01-29 22:21:15","2016-01-29 22:21:15",2
"Apache Mesos","Reduce the running time of benchmark tests.","Currently benchmark tests take a long time (>5 hours). It would be nice to reduce the total time taken by the benchmark tests to enable us to run them on ASF CI.  Command to run only benchmark tests ",Task,Major,Open,"2016-01-29 22:19:50","2016-01-29 22:19:50",2
"Apache Mesos","Automatically generate command-line flag documentation","To ensure that the command-line flag documentation in {{configuration.md}} stays in sync with the help strings in the various {{flags.cpp}} files, it could be beneficial to automate the generation of those docs. Such a script could be run as part of the build process, ensuring that changes to the help strings would show up in the documentation as well.  In addition to parsing and formatting the help strings for display as HTML, this could also involve specifying collections of flags to be grouped together in order to provide logical structure to the {{configuration.md}} documentation.",Improvement,Major,Open,"2016-01-29 20:57:34","2016-01-29 20:57:34",3
"Apache Mesos","Investigate test suite crashes after ZK socket disconnections.","Showed up on ASF CI: https://builds.apache.org/job/Mesos/COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/1579/console  The test crashed with the following logs:   There was another test {{MasterMaintenanceTest.InverseOffersFilters}} which failed right after completion with a similar stack trace:   ",Bug,Major,Resolved,"2016-01-29 17:48:26","2016-01-29 17:48:26",3
"Apache Mesos","Mesos Agents needs to re-resolve hosts in zk string on leader change / failure to connect","Sample Mesos Agent log: https://gist.github.com/brndnmtthws/fb846fa988487250a809  Note, zookeeper has a function to change the list of servers at runtime: https://github.com/apache/zookeeper/blob/735ea78909e67c648a4978c8d31d63964986af73/src/c/src/zookeeper.c#L1207-L1232  This comes up when using an AWS AutoScalingGroup for managing the set of masters.   The agent when it comes up the first time, resolves the zk:// string. Once all the hosts that were in the original string fail (Each fails, is replaced by a new machine, which has the same DNS name), the agent just keeps spinning in an internal loop, never re-resolving the DNS names.  Two solutions I see are  1. Update the list of servers / re-resolve 2. Have the agent detect it hasn't connected recently, and kill itself (Which will force a re-resolution when the agent starts back up)",Bug,Blocker,Resolved,"2016-01-28 21:24:55","2016-01-28 21:24:55",3
"Apache Mesos","Propose design doc for reliable floating point behavior",,Task,Major,Resolved,"2016-01-28 19:14:44","2016-01-28 19:14:44",3
"Apache Mesos","Propose design doc for agent partitioning behavior",,Task,Major,Resolved,"2016-01-28 18:16:07","2016-01-28 18:16:07",8
"Apache Mesos","MasterQuotaTest.AvailableResourcesAfterRescinding is flaky.","Can be reproduced by running {{GLOG_v=1 GTEST_FILTER=MasterQuotaTest.AvailableResourcesAfterRescinding ./bin/mesos-tests.sh --gtest_shuffle --gtest_break_on_failure --gtest_repeat=1000 --verbose}}.  h5. Verbose log from a bad run:   h5. Verbose log from a good run: ",Bug,Major,Resolved,"2016-01-28 11:14:00","2016-01-28 11:14:00",3
"Apache Mesos","NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate fails on CentOS 6","This test fails in my CentOS 6 VM due to a cgroups issue:  ",Bug,Major,Resolved,"2016-01-28 01:40:23","2016-01-28 01:40:23",1
"Apache Mesos","Exclude paths in Posix disk isolator should be absolute paths.","Since du --exclude uses pattern matching. A relative path might accidentally matches an irrelevant directory/file. For instance,  ",Task,Major,Resolved,"2016-01-28 01:13:11","2016-01-28 01:13:11",2
"Apache Mesos","Add abstractions of owned and shared file descriptors to libprocess.","Libprocess currently manages file descriptors as plain {{int}} s.  This leads to some easily missed bugs regarding duplicated or closed FDs.  We should introduce an abstraction (like {{unique_ptr}} and {{shared_ptr}}) so that FD ownership can be expressed alongside the affected code.",Improvement,Major,Open,"2016-01-27 23:07:58","2016-01-27 23:07:58",3
"Apache Mesos","Logrotate ContainerLogger may not handle FD ownership correctly","One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.  The way the logrotate module uses this is slightly incorrect: # The module starts a subprocess with an output {{Subprocess::PIPE()}}. # That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}. # When the second subprocess starts, the pipe's FD is closed in the parent. # When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD.",Bug,Blocker,Resolved,"2016-01-27 20:12:33","2016-01-27 20:12:33",1
"Apache Mesos","Resources object can be mutated through the public API","The {{Resources}} object current allows mutation of it's internal state through the public mutable iterator interface. This can cause issues when the mutation involved stripping certain qualifiers on a {{Resource}}, as they will not be summed together at the end of the mutation (even though they should be).  The {{contains()}} math will not work correctly if two {{addable}} resources are not summed together on the {{lhs}} of the contains check.",Bug,Blocker,Resolved,"2016-01-27 19:54:07","2016-01-27 19:54:07",3
"Apache Mesos","Document multi-disk support.",,Task,Major,Resolved,"2016-01-27 18:07:41","2016-01-27 18:07:41",2
"Apache Mesos","NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate is flaky","While running the command  One eventually gets the following output: ",Bug,Major,Resolved,"2016-01-27 14:40:14","2016-01-27 14:40:14",1
"Apache Mesos","Update the allocator to not offer unreserved resources beyond quota.","Eventually, we will want to offer unreserved resources as revocable beyond the role's quota. Rather than offering non-revocable resources beyond the role's quota's guarantee, in the short term, we choose to not offer resources beyond a role's quota.",Task,Major,Resolved,"2016-01-27 05:15:14","2016-01-27 05:15:14",2
"Apache Mesos","Account for reserved resources in the quota guarantee check.","Reserved resources should be accounted for in the quota guarantee check so that frameworks cannot continually reserve resources to pull them out of the quota pool.",Task,Major,Resolved,"2016-01-27 05:07:28","2016-01-27 05:07:28",2
"Apache Mesos","Roles can exceed limit allocation via reservations.","Since unallocated reservations are not accounted towards the guarantee (which today is also a limit), we might exceed the limit.",Task,Major,Resolved,"2016-01-27 05:03:30","2016-01-27 05:03:30",5
"Apache Mesos","Include the allocated portion of reserved resources in the role sorter for DRF.","Reserved resources should be accounted for fairness calculation whether they are allocated or not, since they model a long or forever running task. That is, the effect of reserving resources is equivalent to launching a task in that the resources that make up the reservation are not available to other roles as non-revocable.  In the short-term, we should at least account for the allocated portion of the reservation.",Task,Major,Resolved,"2016-01-27 04:55:27","2016-01-27 04:55:27",1
"Apache Mesos","Enable benchmark tests in ASF CI","It would be nice to enable benchmark tests in the ASF CI so that we can catch performance regressions (esp. during releases).",Improvement,Major,Resolved,"2016-01-27 01:16:10","2016-01-27 01:16:10",3
"Apache Mesos","Introduce a status() interface for isolators","While launching a container mesos isolators end up configuring/modifying various properties of the container. For e.g., cgroup isolators (mem, cpu, net_cls) configure/change the properties associated with their respective subsystems before launching a container. Similary network isolator (net-modules, port mapping) configure the IP address and ports associated with a container.   Currently, there are not interface in the isolator to extract the run time state of these properties for a given container. Therefore a status() method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator. ",Improvement,Major,Resolved,"2016-01-26 21:53:03","2016-01-26 21:53:03",1
"Apache Mesos","Introduce docker runtime isolator.","Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc.",Bug,Major,Resolved,"2016-01-26 21:35:49","2016-01-26 21:35:49",3
"Apache Mesos","ContainerLoggerTest.LOGROTATE_RotateInSandbox breaks when running on Centos6.",,Bug,Blocker,Resolved,"2016-01-26 17:39:05","2016-01-26 17:39:05",1
"Apache Mesos","Build failure when using gcc-4.9 - signed/unsigned mismatch.","When building the current master, the following happens when using gcc-4.9:  ",Bug,Major,Resolved,"2016-01-26 16:41:47","2016-01-26 16:41:47",1
"Apache Mesos","Render quota status consistently with other endpoints.","Currently quota status endpoint returns a collection of {{QuotaInfo}} protos converted to JSON. An example response looks like this:   Presence of some fields, e.g. role, is misleading. To address this issue and make the output more informative, we should probably introduce a  {{model()}} function for {{QuotaStatus}}.",Bug,Major,Resolved,"2016-01-26 16:22:50","2016-01-26 16:22:50",3
"Apache Mesos","Remove deprecated .json endpoints.","We deprecated the *.json endpoints with MESOS-2058 and MESOS-2984, we should remove them after the deprecation cycle.",Task,Major,Resolved,"2016-01-26 10:37:35","2016-01-26 10:37:35",1
"Apache Mesos","Posix disk isolator should ignore disk quota enforcement for MOUNT type disk resources.","We assume MOUNT type disk is exclusive and the underlying filesystem will enforce the quota (i.e., the application won't be able to exceed the quota, and will get a write error it the disk is full).  Therefore, there's no need to enforce it's quota in posix disk isolator.",Task,Major,Resolved,"2016-01-26 00:09:59","2016-01-26 00:09:59",2
"Apache Mesos","Hierarchical allocator performance is slow due to Quota","Since we do not strip the non-scalar resources during the resource arithmetic for quota, the performance can degrade significantly, as currently resource arithmetic is expensive.  One approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars. This is valid as quota can currently only be set for scalar resource types.",Improvement,Blocker,Resolved,"2016-01-25 23:50:53","2016-01-25 23:50:53",3
"Apache Mesos","Expose ExecutorInfo and TaskInfo for isolators.","Currently we do not have these info for isolator. Image once we have docker runtime isolator, CommandInfo is necessary to support either custom executor or command executor. ",Bug,Major,Resolved,"2016-01-25 17:19:20","2016-01-25 17:19:20",2
"Apache Mesos","Docker provisioner store should reuse existing layers in the cache.","Currently, the docker provisioner store will download all the layers associated with an image if the image is not found locally, even though some layers of it might already exist in the cache.  This is problematic because anytime a user deploys a new image, Mesos will fetch all layers of that new image, even though most of the layers are already cached locally.  ",Bug,Major,Resolved,"2016-01-25 17:04:41","2016-01-25 17:04:41",5
"Apache Mesos","Refactor os.hpp to be less monolithic, and more cross-platform compatible",,Improvement,Major,Resolved,"2016-01-25 10:13:53","2016-01-25 10:13:53",1
"Apache Mesos","Delete `os::chown` on Windows",,Bug,Major,Resolved,"2016-01-25 10:02:20","2016-01-25 10:02:20",1
"Apache Mesos","Implement `size`, `usage`, and other disk metrics reporting on Windows.",,Improvement,Major,Resolved,"2016-01-25 10:00:43","2016-01-25 10:00:43",3
"Apache Mesos","Add ability to create symlink on Windows",,Improvement,Major,Resolved,"2016-01-25 09:57:41","2016-01-25 09:57:41",3
"Apache Mesos","Get container status information in slave. ","As part of MESOS-4487 an interface will be introduce into the `Containerizer` to allow agents to retrieve container state information. The agent needs to use this interface to retrieve container state information during status updates from the executor. The container state information can be then use by the agent to expose various isolator specific configuration (for e.g., IP address allocated by network isolators, net_cls handles allocated by `cgroups/net_cls` isolator), that has been applied to the container, in the state.json endpoint.  ",Improvement,Major,Resolved,"2016-01-24 19:42:21","2016-01-24 19:42:21",3
"Apache Mesos","The `cgroups/net_cls` isolator needs to expose handles in the ContainerStatus","The `cgroup/net_cls` isolator is responsible for allocating network handles to containers launched within a net_cls cgroup. The `cgroup/net_cls` isolator needs to expose these handles to the containerizer as part of the `ContainerStatus` when the containerizer queries the status() method of the isolator. The information itself will go as part of a `CgroupInfo` protobuf that will be defined as part of MESOS-4488 .  ",Improvement,Major,Resolved,"2016-01-24 19:33:09","2016-01-24 19:33:09",1
"Apache Mesos","Define a CgroupInfo protobuf to expose cgroup isolator configuration.","Within `MesosContainerizer` we have an isolator associated with each linux cgroup subsystem. The isolators apply subsystem specific configuration on the containers before launching the containers. For e.g cgroup/net_cls isolator applies net_cls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu-share isolator configures cpu shares.   Currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container. We therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container. This protobuf will be filled in by the cgroup isolator and will be stored as part of `ContainerConfig` in the containerizer. ",Improvement,Major,Resolved,"2016-01-24 19:23:18","2016-01-24 19:23:18",1
"Apache Mesos","Introduce status() interface in `Containerizer`","In the Containerizer, during container isolation, the isolators end up modifying the state of the containers. Examples would be IP address allocation to a container by the 'network isolator, or net_cls handle allocation by the cgroup/net_cls isolator.   Often times the state of the container, needs to be exposed to operators through the state.json end-point. For e.g. operators or frameworks might want to know the IP-address configured on a particular container, or the net_cls handle associated with a container to configure the right TC rules. However, at present, there is no clean interface for the slave to retrieve the state of a container from the Containerizer for any of the launched containers. Thus, we need to introduce a `status` interface in the `Containerizer` base class, in order for the slave to expose container state information in its state.json.   ",Improvement,Major,Resolved,"2016-01-24 19:07:08","2016-01-24 19:07:08",2
"Apache Mesos","Implement reservation labels",,Improvement,Major,Resolved,"2016-01-22 23:42:57","2016-01-22 23:42:57",5
"Apache Mesos","ReviewBot seemed to be crashing ReviewBoard server when posting large reviews","The bot is currently tripping on this review  https://reviews.apache.org/r/42506/ (see builds #10973 to #10978).  [~<USER> looked at the server logs and said he saw 'MySQL going away' message when the mesos bot was making these requests. I think that error is a bit misleading because it happens only for this review (which has a huge error log due to bad patch). The bot has successfully posted reviews for other review requests which had no error log (good patch).  One way to fix this would be to just post a tail of the error log (and perhaps link to Jenkins Console or some other service for the longer error text).",Bug,Major,Resolved,"2016-01-22 22:57:05","2016-01-22 22:57:05",2
"Apache Mesos","Implement process querying/counting in Windows",,Bug,Major,Resolved,"2016-01-22 22:11:19","2016-01-22 22:11:19",2
"Apache Mesos","Implement `waitpid` in Windows",,Bug,Major,Resolved,"2016-01-22 22:07:58","2016-01-22 22:07:58",5
"Apache Mesos","Enable Executor->Framework message optimization for HTTP API","Currently, we support sending executor->framework messages directly as an optimization. This is not currently possible with using the Scheduler HTTP API. We should think about exploring possible alternatives for supporting this optimization.",Task,Major,Open,"2016-01-22 20:09:33","2016-01-22 20:09:33",13
"Apache Mesos","Enable Framework->Executor message optimization for HTTP API","Currently, we support sending framework->executor messages directly as an optimization. This is not currently possible with using the Scheduler HTTP API.  We should think about exploring possible alternatives for supporting this optimization.",Task,Major,Open,"2016-01-22 20:07:47","2016-01-22 20:07:47",13
"Apache Mesos","Implement AuthN handling on the scheduler library","Currently, we do not have the ability of passing {{Credentials}} via the scheduler library. Once the master supports AuthN handling for the {{/scheduler}} endpoint, we would need to add this support to the library.",Task,Major,Resolved,"2016-01-22 19:59:20","2016-01-22 19:59:20",3
"Apache Mesos","Implement tests for the new Executor library","We need to add tests for the executor library {{src/executor/executor.cpp}}. One possible approach would be to use the existing tests in {{src/tests/scheduler_tests.cpp}} and make them use the new executor library.",Task,Major,Resolved,"2016-01-22 19:44:34","2016-01-22 19:44:34",3
"Apache Mesos","Create common sha512 compute utility function.","Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. ",Bug,Major,Resolved,"2016-01-22 17:51:59","2016-01-22 17:51:59",2
"Apache Mesos","Improve documentation around roles, principals, authz, and reservations","* What is the difference between a role and a principal? * Why do some ACL entities reference roles but others reference principals? In a typical organization, what real-world entities would my roles vs. principals map to? The ACL documentation could use more information about the motivation of ACLs and examples of configuring ACLs to meet real-world security policies. * We should give some examples of making reservations when the role and principal are different, and why you would want to do that * We should add an example to the ACL page that includes setting ACLs for reservations and/or persistent volumes",Documentation,Minor,Resolved,"2016-01-21 22:07:25","2016-01-21 22:07:25",2
"Apache Mesos","SegFault on agent during executor startup","When repeatedly performing our system tests we have found that we get a segfault on one of the agents. It probably occurs about one time in ten. I have attached the full log from that agent. I've attached the log from the agent that failed and the master (although I think this is less helpful).  To reproduce - I have no idea. It seems to occur at certain times. E.g. like if a packet is created right on a minute boundary or something. But I don't think it's something caused by our code because the timestamps are stamped by mesos. I was surprised not to find a bug already open.",Bug,Blocker,Resolved,"2016-01-21 16:16:09","2016-01-21 16:16:09",1
"Apache Mesos","Labels equality behavior is wrong","  Output:   This behavior seems pretty problematic.",Bug,Minor,Open,"2016-01-20 21:18:27","2016-01-20 21:18:27",5
"Apache Mesos","Design doc for reservation labels",,Bug,Major,Resolved,"2016-01-20 18:19:32","2016-01-20 18:19:32",3
"Apache Mesos","Refactor allocator recovery.","Allocator recovery code can be improved for readability. [~<USER> left some thoughts about it in https://reviews.apache.org/r/42222/.",Improvement,Major,Open,"2016-01-20 13:36:29","2016-01-20 13:36:29",3
"Apache Mesos","Allocate revocable resources beyond quota guarantee.","h4. Status Quo Currently resources allocated to frameworks in a role with quota (aka quota'ed role) beyond quota guarantee are marked non-revocable. This impacts our flexibility for revoking them if we decide so in the future.  h4. Proposal Once quota guarantee is satisfied we must not necessarily further allocate resources as non-revocable. Instead we can mark all offers resources beyond guarantee as revocable. When in the future {{RevocableInfo}} evolves frameworks will get additional information about revocability of the resource (i.e. allocation slack)  h4. Caveats Though it seems like a simple change, it has several implications.  h6. Fairness Currently the hierarchical allocator considers revocable resources as regular resources when doing fairness calculations. This may prevent frameworks getting non-revocable resources as part of their role's quota guarantee if they accept some revocable resources as well.  Consider the following scenario. A single framework in a role with quota set to {{10}} CPUs is allocated {{10}} CPUs as non-revocable resources as part of its quota and additionally {{2}} revocable CPUs. Now a task using {{2}} non-revocable CPUs finishes and its resources are returned. Total allocation for the role is {{8}} non-revocable + {{2}} revocable. However, the role may not be offered additional {{2}} non-revocable since its total allocation satisfies quota.  h6. Resource math If we allocate non-revocable resources as revocable, we should make sure we do accounting right: either we should update total agent resources and mark them as revocable as well, or bookkeep resources as non-revocable and convert them to revocable when necessary.  h6. Coarse-grained nature of allocation The hierarchical allocator performs coarse-grained allocation, meaning it always allocates the entire remaining agent resources to a single framework. This may lead to over-allocating some resources as non-revocable beyond quota guarantee.  h6. Quotas smaller than fair share If a quota set for a role is smaller than its fair share, it may reduce the amount of resources offered to this role, if frameworks in it do not accept revocable resources. This is probably the most important consequence of the proposed change. Operators may set quota to get guarantees, but may observe a decrease in amount of resources a role gets, which is not intuitive.",Improvement,Major,Accepted,"2016-01-20 11:15:28","2016-01-20 11:15:28",8
"Apache Mesos","Fix appc CachedImage image validation","Currently image validation is done assuming that the image's filename will have  digest (SHA-512) information. This is not part of the spec     (https://github.com/appc/spec/blob/master/spec/discovery.md).          The spec specifies the tuple <image name, labels> as unique identifier for  discovering an image. ",Task,Major,Resolved,"2016-01-20 01:10:56","2016-01-20 01:10:56",1
"Apache Mesos","Add 'dependency' message to 'AppcImageManifest' protobuf.","AppcImageManifest protobuf currently lacks 'dependencies' which is necessary for image discovery.",Task,Major,Resolved,"2016-01-20 01:06:44","2016-01-20 01:06:44",1
"Apache Mesos","Disable the test RegistryClientTest.BadTokenServerAddress.","As we are retiring registry client, disable this test which looks flaky.",Task,Major,Resolved,"2016-01-20 00:44:45","2016-01-20 00:44:45",1
"Apache Mesos","Update `Master::Http::stateSummary` to use `jsonify`.","Update {{state-summary}} to use {{jsonify}} to stay consistent with {{state}} HTTP endpoint.",Task,Major,Resolved,"2016-01-20 00:05:31","2016-01-20 00:05:31",3
"Apache Mesos","Install 3rdparty package boost, glog, protobuf and picojson when installing Mesos","Mesos modules depend on having these packages installed with the exact version as Mesos was compiled with.",Bug,Major,Resolved,"2016-01-19 22:01:09","2016-01-19 22:01:09",3
"Apache Mesos","Implement a callback testing interface for the Executor Library","Currently, we do not have a mocking based callback interface for the executor library. This should look similar to the ongoing work for MESOS-3339 i.e. the corresponding issue for the scheduler library.  The interface should allow us to set expectations like we do for the driver. An example:  ",Task,Major,Resolved,"2016-01-19 20:04:00","2016-01-19 20:04:00",3
"Apache Mesos","Introduce filtering test abstractions for HTTP events to libprocess","We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.  The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}.",Bug,Major,Resolved,"2016-01-19 02:07:26","2016-01-19 02:07:26",3
"Apache Mesos","Document that /reserve, /create-volumes endpoints can return misleading success","The docs for the {{/reserve}} endpoint say:    This is not true: the master returns {{200}} when the request has been validated and a {{CheckpointResourcesMessage}} has been sent to the agent, but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed. Same behavior applies to {{/unreserve}}, {{/create-volumes}}, and {{/destroy-volumes}}.  We should _either_:  1. Accurately document what {{200}} return code means. 2. Change the implementation to wait for the agent's next checkpoint to succeed (and to include the effect of the operation) before returning success to the HTTP client.",Task,Major,Resolved,"2016-01-18 22:34:19","2016-01-18 22:34:19",3
"Apache Mesos","Prevent allocator from crashing on successful recovery.","There might be a bug that may crash the master as pointed out by [~<USER> in https://reviews.apache.org/r/42222/: ",Bug,Blocker,Resolved,"2016-01-17 08:43:45","2016-01-17 08:43:45",3
"Apache Mesos","Implement stout/os/windows/rmdir.hpp",,Task,Major,Resolved,"2016-01-16 20:41:03","2016-01-16 20:41:03",5
"Apache Mesos","Traverse all roles for quota allocation.","There might be a bug in how resources are allocated to multiple quota'ed roles if one role's quota is met. We need to investigate this behavior.",Bug,Blocker,Resolved,"2016-01-15 23:43:54","2016-01-15 23:43:54",3
"Apache Mesos","Introduce protobuf for quota set request.","To document quota request JSON schema and simplify request processing, introduce a {{QuotaRequest}} protobuf wrapper.",Improvement,Blocker,Resolved,"2016-01-15 23:34:29","2016-01-15 23:34:29",3
"Apache Mesos","Check paths in DiskInfo.Source.Path exist during slave initialization.","We have two options here. We can either check and fail if it does not exists. Or we can create if it does not exist like we did for slave.work_dir.",Task,Major,Resolved,"2016-01-15 18:50:58","2016-01-15 18:50:58",2
"Apache Mesos","Update filesystem isolators to look for persistent volume directories from the correct location.","This is related to MESOS-4400.  Since persistent volume directories can be created from non root disk now. We need to adjust both posix and linux filesystem isolator to look for volumes from the correct location based on the information in DiskInfo.Source.  See relevant code in: ",Task,Major,Resolved,"2016-01-15 18:40:25","2016-01-15 18:40:25",2
"Apache Mesos","Create persistent volume directories based on DiskInfo.Source.","Currently, we always create persistent volumes from root disk, and the persistent volumes are directories. With DiskInfo.Source being added, we should create the persistent volume accordingly based on the information in DiskInfo.Source.  This ticket handles the case where DiskInfo.Source.type is PATH. In that case, we should create sub-directories and use the same layout as slave.work_dir.  See the relevant code here: ",Task,Major,Resolved,"2016-01-15 18:33:10","2016-01-15 18:33:10",2
"Apache Mesos","Synchronously handle AuthZ errors for the Scheduler endpoint.","Currently, any AuthZ errors for the {{/scheduler}} endpoint are handled asynchronously as {{FrameworkErrorMessage}}. Here is an example:    We would like to handle such errors synchronously when the request is received similar to what other endpoints like {{/reserve}}/{{/quota}} do. We already have the relevant functions {{authorizeXXX}} etc in {{master.cpp}}. We should just make the requests pass through once the relevant {{Future}} from the {{authorizeXXX}} function is fulfilled.",Bug,Major,Open,"2016-01-15 18:14:10","2016-01-15 18:14:10",5
"Apache Mesos","Rename ContainerPrepareInfo to ContainerLaunchInfo for isolators.","The name ContainerPrepareInfo does not really capture the purpose of this struct. ContainerLaunchInfo better captures the purpose of this struct. ContainerLaunchInfo is returned by the isolator 'prepare' function. It contains information about how a container should be launched (e.g., environment variables, namespaces, commands, etc.). The information will be used by the Mesos Containerizer when launching the container.",Task,Major,Resolved,"2016-01-15 17:32:23","2016-01-15 17:32:23",2
"Apache Mesos","Add persistent volume endpoint tests with no principal","There are currently no persistent volume endpoint tests that do not use a principal; they should be added.",Bug,Major,Resolved,"2016-01-15 16:28:11","2016-01-15 16:28:11",1
"Apache Mesos","Draft design document for resource revocability by default.","Create a design document for setting offered resources as revocable by default. Greedy frameworks can then temporarily use resources set aside to satisfy quota. ",Task,Major,Accepted,"2016-01-15 13:50:11","2016-01-15 13:50:11",8
"Apache Mesos","Shared Volumes Design Doc","Review & Approve design doc",Task,Major,Resolved,"2016-01-15 10:16:13","2016-01-15 10:16:13",3
"Apache Mesos","Deprecate 'authenticate' master flag in favor of 'authenticate_frameworks' flag","To be consistent with `authenticate_slaves` and `authenticate_http` flags, we should rename `authenticate` to `authenticate_frameworks` flag.  This should be done via deprecation cycle.   1) Release X supports both `authenticate` and `authenticate_frameworks` flags  2)  Release X + n supports only `authenticate_frameworks` flag.  ",Improvement,Major,Resolved,"2016-01-15 00:58:20","2016-01-15 00:58:20",1
"Apache Mesos","Offers and InverseOffers cannot be accepted in the same ACCEPT call","*Problem* * In {{Master::accept}}, {{validation::offer::validate}} returns an error when an {{InverseOffer}} is included in the list of {{OfferIDs}} in an {{ACCEPT}} call. * If an {{Offer}} is part of the same {{ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)  Here's a regression test: https://reviews.apache.org/r/42092/  *Proprosal* The question is whether we want to allow the mixing of {{Offers}} and {{InverseOffers}}.  Arguments for mixing: * The design/structure of the maintenance originally intended to overload {{ACCEPT}} and {{DECLINE}} to take inverse offers. * Enforcing non-mixing may require breaking changes to {{scheduler.proto}}.  Arguments against mixing: * Some semantics are difficult to explain.  What does it mean to supply {{InverseOffers}} with {{Offer::Operations}}?  What about {{DECLINE}} with {{Offers}} and {{InverseOffers}}, including a reason? * What happens if we presumably add a third type of offer? * Does it make sense to {{TASK_LOST}} valid normal offers if {{InverseOffers}} are invalid?",Bug,Major,Resolved,"2016-01-14 21:32:04","2016-01-14 21:32:04",2
"Apache Mesos","Support docker runtime configuration env var from image.","We need to support env var configuration returned from docker image in mesos containerizer.",Bug,Major,Resolved,"2016-01-14 21:09:16","2016-01-14 21:09:16",2
"Apache Mesos","Change the `principal` in `ReservationInfo` to optional","With the addition of HTTP endpoints for {{/reserve}} and {{/unreserve}}, it is now desirable to allow dynamic reservations without a principal, in the case where HTTP authentication is disabled. To allow for this, we will change the {{principal}} field in {{ReservationInfo}} from required to optional. For backwards-compatibility, however, the master should currently invalidate any {{ReservationInfo}} messages that do not have this field set.",Improvement,Major,Resolved,"2016-01-14 20:48:07","2016-01-14 20:48:07",1
"Apache Mesos","Improve upgrade compatibility documentation.","Investigate and document upgrade compatibility for 0.27 release.",Documentation,Major,Resolved,"2016-01-14 19:37:20","2016-01-14 19:37:20",3
"Apache Mesos","Adjust Resource arithmetics for DiskInfo.Source.","Since we added the Source for DiskInfo, we need to adjust the Resource arithmetics for that. That includes equality check, addable check, subtractable check, etc.",Task,Major,Resolved,"2016-01-14 19:05:14","2016-01-14 19:05:14",2
"Apache Mesos","Design doc for reservation IDs",,Task,Major,Resolved,"2016-01-14 18:51:28","2016-01-14 18:51:28",3
"Apache Mesos","Add Source to Resource.DiskInfo.","Source is used to describe the extra information about the source of a Disk resource. We will support 'PATH' type first and then 'BLOCK' later.  ",Task,Major,Resolved,"2016-01-14 18:29:11","2016-01-14 18:29:11",1
"Apache Mesos","Document units associated with resource types","We should document the units associated with memory and disk resources.",Documentation,Minor,Resolved,"2016-01-14 18:21:22","2016-01-14 18:21:22",1
"Apache Mesos","Document semantics of `slaveLost`","We should clarify the semantics of this callback:  * Is it always invoked, or just a hint? * Can a slave ever come back from `slaveLost`? * What happens to persistent resources on a lost slave?  The new HA framework development guide might be a good place to put (some of?) this information. ",Documentation,Major,Resolved,"2016-01-14 18:19:56","2016-01-14 18:19:56",2
"Apache Mesos","Make HierarchicalAllocatorProcess set a Resource's active role during allocation","The concrete implementation here depends on the implementation strategy used to solve MESOS-4367.",Improvement,Major,Resolved,"2016-01-14 12:35:07","2016-01-14 12:35:07",3
"Apache Mesos","Add tracking of the role a Resource was offered for","If a framework can have multiple roles, we need a way to identify for which of the framework's role a resource was offered for (e.g., for resource recovery and reconciliation).",Improvement,Major,Resolved,"2016-01-14 12:33:57","2016-01-14 12:33:57",5
"Apache Mesos","Migrate all existing uses of FrameworkInfo.role to FrameworkInfo.roles",,Improvement,Major,Resolved,"2016-01-14 12:28:59","2016-01-14 12:28:59",3
"Apache Mesos","Add internal migration from role to roles to master","If only the {{role}} field is given, add it as single entry to {{roles}}. Add a note to {{CHANGELOG}}/release notes on deprecation of the existing {{role}} field. File a JIRA issue for removal of that migration code once the deprecation cycle is over. ",Improvement,Major,Resolved,"2016-01-14 12:27:35","2016-01-14 12:27:35",3
"Apache Mesos","Add roles validation code to master","A {{FrameworkInfo}} can only have one of role or roles. A natural location for this appears to be under {{validation::operation::validate}}.",Improvement,Major,Resolved,"2016-01-14 12:25:43","2016-01-14 12:25:43",5
"Apache Mesos","Add a roles field to FrameworkInfo","To represent multiple roles per framework a new repeated string field for roles is needed.",Improvement,Major,Resolved,"2016-01-14 12:24:21","2016-01-14 12:24:21",1
"Apache Mesos","Formating issues and broken links in documentation.","The online documentation has a number of bad formatting issues and broken links (e.g., mesos-provider.md).",Improvement,Major,Resolved,"2016-01-14 10:45:47","2016-01-14 10:45:47",1
"Apache Mesos","Create common tar/untar utility function.","As part of refactoring and creating a common place to add all command utilities, add *tar* and *untar* as the first POC.",Bug,Major,Resolved,"2016-01-13 23:18:16","2016-01-13 23:18:16",3
"Apache Mesos","GMock warning in DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard","The following GMock warning was seen on CentOS 7.1:  ",Bug,Minor,Accepted,"2016-01-13 22:33:26","2016-01-13 22:33:26",2
"Apache Mesos","Expose net_cls network handles in agent's state endpoint","We need to expose net_cls network handles, associated with containers, to operators and network utilities that would use these network handles to enforce network policy.   In order to achieve the above we need to add a new field in the `NetworkInfo` protobuf (say NetHandles) and update this field when a container gets assigned to a net_cls cgroup. The `ContainerStatus` protobuf already has the `NetworkInfo` protobuf as a nested message, and the `ContainerStatus` itself is exposed to operators as part of TaskInfo (for tasks associated with the container) in an agent's state.json. ",Task,Major,Resolved,"2016-01-13 20:04:08","2016-01-13 20:04:08",2
"Apache Mesos","GMock warning in RoleTest.ImplicitRoleStaticReservation",,Bug,Trivial,Resolved,"2016-01-13 19:07:37","2016-01-13 19:07:37",1
"Apache Mesos","Limit the number of processes created by libprocess","Currently libprocess will create {{max(8, number of CPU cores)}} processes during the initialization, see https://github.com/apache/mesos/blob/0.26.0/3rdparty/libprocess/src/process.cpp#L2146 for details. This should be OK for a normal machine which has no much cores (e.g., 16, 32), but for a powerful machine which may have a large number of cores (e.g., an IBM Power machine may have 192 cores), this will cause too much worker threads which are not necessary.  And since libprocess is widely used in Mesos (master, agent, scheduler, executor), it may also cause some performance issue. For example, when user creates a Docker container via Mesos in a Mesos agent which is running on a powerful machine with 192 cores, the DockerContainerizer in Mesos agent will create a dedicated executor for the container, and there will be 192 worker threads in that executor. And if user creates 1000 Docker containers in that machine, then there will be 1000 executors, i.e., 1000 * 192 worker threads which is a large number and may thrash the OS.  ",Improvement,Major,Resolved,"2016-01-13 13:17:14","2016-01-13 13:17:14",1
"Apache Mesos","GMock warning on `offerRescinded` in `ReservationTest` fixture","Several tests involving checkpointing of resources in the {{ReservationTest}} fixture are throwing GMock warnings occasionally. Here is the output of {{GTEST_FILTER=ReservationTest.* bin/mesos-tests.sh --gtest_repeat=10000 --gtest_break_on_failure=1 | grep -B 3 -A 6 WARNING}}:  ",Bug,Major,Accepted,"2016-01-12 22:26:31","2016-01-12 22:26:31",2
"Apache Mesos","GMock warning in SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor","  Occurs non-deterministically for me on OSX 10.10, perhaps one run in ten.",Bug,Major,Resolved,"2016-01-12 21:16:31","2016-01-12 21:16:31",1
"Apache Mesos","GMock warning in HookTest.VerifySlaveRunTaskHook, HookTest.VerifySlaveTaskStatusDecorator","  Occurs non-deterministically for me. OSX 10.10.",Bug,Minor,Resolved,"2016-01-12 21:13:19","2016-01-12 21:13:19",1
"Apache Mesos","GMock warning in ReservationTest.ACLMultipleOperations","  Seems to occur non-deterministically for me, maybe once per 50 runs or so. OSX 10.10",Bug,Minor,Resolved,"2016-01-12 21:08:11","2016-01-12 21:08:11",1
"Apache Mesos","Implement a network-handle manager for net_cls cgroup subsystem","As part of implementing the net_cls cgroup isolator we need a mechanism to manage the minor handles that will be allocated to containers when they are associated with a net_cls cgroup. The network-handle manager needs to provide the following functionality:  a) During normal operation keep track of the free and allocated network handles. There can be a total of 64K such network handles. b) On startup, learn the allocated network handle by walking the net_cls cgroup tree for mesos and build a map of free network handles available to the agent. ",Task,Major,Resolved,"2016-01-12 17:09:57","2016-01-12 17:09:57",3
"Apache Mesos","Allow operators to assign net_cls major handles to mesos agents","The net_cls cgroup associates a 16-bit major and 16-bit minor network handle to packets originating from tasks associated with a specific net_cls cgroup. In mesos we need to give the operator the ability to fix the 16-bit major handle used in an agent (the minor handle will be allocated by the agent. See MESOS-4345). Fixing the parent handle on the agent allows operators to install default firewall rules using the parent handle to enforce a default policy (say DENY ALL) for all container traffic till the container is allocated a minor handle.   A simple way to achieve this requirement is to pass the major handle as a flag to the agent at startup. ",Improvement,Major,Resolved,"2016-01-12 17:00:32","2016-01-12 17:00:32",1
"Apache Mesos","Add parameters to apply patches quiet","Added a parameters to apply the patches quiet; so it's easy for contributor to apply patches with -c.",Bug,Minor,Resolved,"2016-01-12 12:45:02","2016-01-12 12:45:02",1
"Apache Mesos","Create utilities for common shell commands used.","We spawn shell for command line utilities like tar, untar, sha256 etc. Would be great for resuse if we can create a common utilities class/file for all these utilities.  ",Bug,Major,Resolved,"2016-01-12 01:37:50","2016-01-12 01:37:50",5
"Apache Mesos","Implement a simple Windows version of dirent.hpp, for compatibility.",,Improvement,Major,Resolved,"2016-01-12 00:23:50","2016-01-12 00:23:50",5
"Apache Mesos","Document supported file types for archive extraction by fetcher","The Mesos fetcher extracts specified URIs if requested to do so by the scheduler. However, the documentation at http://mesos.apache.org/documentation/latest/fetcher/ doesn't list the file types /extensions that will be extracted by the fetcher.  [The relevant code|https://github.com/apache/mesos/blob/master/src/launcher/fetcher.cpp#L63] specifies an exhaustive list of extensions that will be extracted, the documentation should be updated to match.",Documentation,Trivial,Resolved,"2016-01-11 23:46:50","2016-01-11 23:46:50",1
"Apache Mesos","Refactor Appc provisioner tests  ","Current tests can be refactored so that we can reuse some common tasks like test image creation. This will benefit future tests like appc image puller tests.",Improvement,Major,Resolved,"2016-01-11 20:08:06","2016-01-11 20:08:06",2
"Apache Mesos","git commit-msg hook completely breaks fixup commits.","https://reviews.apache.org/r/41586/ added a git hook to check the commit message format. This completely breaks fixup commits which can be created with  The resulting commit message will then be the one of {{$SHA1}}, but prefixed with {{fixup!}} (followed by a literal space). Tools like {{git rebase}} can automatically use these to e.g., squash matching commits like  Here all commits for e.g., {{$SHA1}} would be grouped together and squash automatically which is valuable when working on reviews.  We should find a way to reenable such functionality; otherwise we risk that developers completely disable this hook.",Bug,Major,Resolved,"2016-01-11 13:30:37","2016-01-11 13:30:37",1
"Apache Mesos","SlaveTest.LaunchTaskInfoWithContainerInfo cannot be execute in isolation","Executing {{SlaveTest.LaunchTaskInfoWithContainerInfo}} from {{468b8ec}} under OS X 10.10.5 in isolation fails due to missing cleanup,  ",Bug,Major,Resolved,"2016-01-11 12:38:16","2016-01-11 12:38:16",1
"Apache Mesos","PersistentVolumeTest.BadACLNoPrincipal is flaky","https://builds.apache.org/job/Mesos/1457/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull  ",Bug,Major,Resolved,"2016-01-08 21:27:04","2016-01-08 21:27:04",1
"Apache Mesos","Support get non-default weights by /weights","Like /quota, we should also add query logic for /weights to keep consistent. Then /roles no longer needs to show weight information.",Task,Minor,Resolved,"2016-01-08 14:46:55","2016-01-08 14:46:55",5
"Apache Mesos","Publish Quota Documentation","Publish and finish the operator guide draft  for quota which describes basic usage of the endpoints and few basic and advanced usage cases.",Documentation,Major,Resolved,"2016-01-08 13:06:39","2016-01-08 13:06:39",3
"Apache Mesos","Protobuf parse should pass error messages when parsing nested JSON.","Currently when protobuf::parse handles nested JSON objects, it cannot pass any error message out. We should enable showing those error messages.",Bug,Major,Resolved,"2016-01-08 02:04:11","2016-01-08 02:04:11",1
"Apache Mesos","Reliably report executor terminations to framework schedulers.","Now that executor terminations are reported (unreliably), we should investigate queuing up these messages (on the agent?) and resending them periodically until we get an acknowledgement, much like status updates do.  From MESOS-313: The Scheduler interface has a callback for executorLost, but currently it is never called.",Improvement,Major,Accepted,"2016-01-07 18:47:49","2016-01-07 18:47:49",5
"Apache Mesos","Expand the Getting Started installation instructions","The Getting Started documentation currently contains basic instructions to prepare several platforms for compilation and installation of Mesos. However, these instructions are not sufficient to run and pass all tests in the test suite, using all configuration options. The installation instructions should be made comprehensive in this respect.  It may also be desirable to provide scripts that have been verified to prepare a particular base OS to build, install, and test Mesos. This would be very useful for both developers and users of Mesos.  Note that using some features on some platforms requires the installation of software packages from sources that may not be completely reliable in the long-term; for example, packages which are maintained as personal projects of individuals. This should be noted in the instructions accordingly.",Improvement,Major,Accepted,"2016-01-07 17:34:31","2016-01-07 17:34:31",5
"Apache Mesos","hdfs operations fail due to prepended / on path for non-hdfs hadoop clients.","This bug was resolved for the hdfs protocol for MESOS-3602 but since the process checks for the hdfs protocol at the beginning of the URI, the fix does not extend itself to non-hdfs hadoop clients.   After a brief chat with [~<USER>, it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it.",Bug,Blocker,Resolved,"2016-01-07 02:23:23","2016-01-07 02:23:23",1
"Apache Mesos","Accepting an inverse offer prints misleading logs","Whenever a scheduler accepts an inverse offer, Mesos will print a line like this in the master logs:   Inverse offers should not trigger this warning.",Bug,Major,Resolved,"2016-01-06 16:21:14","2016-01-06 16:21:14",1
"Apache Mesos","Add AuthN and AuthZ to maintenance endpoints.","Maintenance endpoints are currently only restricted by firewall settings.  They should also support authentication/authorization like other HTTP endpoints.",Task,Major,Resolved,"2016-01-06 16:12:49","2016-01-06 16:12:49",3
"Apache Mesos","Sync up configuration.md and flags.cpp","The https://reviews.apache.org/r/39923/ made some clean up for configuration.md but the related flags.cpp was not updated, we should update those files as well.",Bug,Major,Resolved,"2016-01-06 14:11:19","2016-01-06 14:11:19",1
"Apache Mesos","Add docker URI fetcher plugin based on curl.","The existing registry client for docker assumes that Mesos is built using SSL support and SSL is enabled. That means Mesos built with libev (or if SSL is disabled) won't be able to use docker registry client to provision docker images.  Given the new URI fetcher (MESOS-3918) work has been committed, we can add a new URI fetcher plugin for docker. The plugin will be based on curl so that https and 3xx redirects will be handled automatically. The docker registry puller will just use the URI fetcher to get docker images.",Task,Major,Resolved,"2016-01-06 01:39:50","2016-01-06 01:39:50",8
"Apache Mesos","Change documentation links to *.md","Right now, links either use the form  or . We should probably switch to using the latter form consistently -- it previews better on Github, and it will make it easier to have multiple versions of the docs on the website at once in the future.",Task,Minor,Resolved,"2016-01-05 20:05:12","2016-01-05 20:05:12",3
"Apache Mesos","Protobuf parse should support parsing JSON object containing JSON Null.","(This bug was exposed by MESOS-4184, when serializing docker v1 image manifest as protobuf).  Currently protobuf::parse returns failures when parsing any JSON containing JSON::Null. If we have any protobuf field set as `JSON::Null`, any other non-repeated field cannot capture their value. For example, assuming we have a protobuf message:   If there exists any field containing JSON::Null, like below:  When we do protobuf::parse, it would return the following failure: ",Bug,Major,Resolved,"2016-01-05 19:48:35","2016-01-05 19:48:35",1
"Apache Mesos","Tests for quota with implicit roles.","With the introduction of implicit roles (MESOS-3988), we should make sure quota can be set for an inactive role (unknown to the master) and maybe transition it to the active state.",Task,Major,Resolved,"2016-01-05 11:29:34","2016-01-05 11:29:34",3
"Apache Mesos","fs::enter(rootfs) does not work if 'rootfs' is read only.","I noticed this when I was testing the unified containerizer with the bind mount backend and no volumes.  The current implementation of fs::enter will put the old root under /tmp/._old_root_.XXXXXX in the new rootfs. It assumes that /tmp is writable in the new rootfs, but this might not be true, especially if the bind mount backend is used.  To solve the problem, what we can do is to mount tmpfs to /tmp in the new rootfs and umount it after pivot_root.",Bug,Major,Resolved,"2016-01-05 01:06:02","2016-01-05 01:06:02",2
"Apache Mesos","Design doc for simple appc image discovery","Create a design document describing the following:  - Model and abstraction of the Discoverer - Workflow of the discovery process ",Task,Major,Resolved,"2016-01-04 22:42:04","2016-01-04 22:42:04",5
"Apache Mesos","Mesos command task doesn't support volumes with image","Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. ",Bug,Major,Resolved,"2016-01-04 20:13:27","2016-01-04 20:13:27",3
"Apache Mesos","Draft design doc for multi-role frameworks","Create a document that describes the problems with having only single-role frameworks and proposes an MVP solution and implementation approach.",Story,Major,Resolved,"2016-01-04 19:35:14","2016-01-04 19:35:14",8
"Apache Mesos","Update isolator prepare function to use ContainerLaunchInfo","Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers.   By doing this (ContainerPrepareInfo -> ContainerLaunchInfo), we can select any necessary information and passing then to launcher.",Bug,Blocker,Resolved,"2016-01-04 17:16:20","2016-01-04 17:16:20",2
"Apache Mesos","Correctly handle disk quota usage when volumes are bind mounted into the container.","In its current implementation disk quota enforcement on the task sandbox will not work correctly when disk volumes are bind mounted into the task sandbox (this happens when Linux filesystem isolator is used).",Bug,Major,Resolved,"2016-01-04 16:59:46","2016-01-04 16:59:46",3
"Apache Mesos","Docker executor truncates task's output when the task is killed.","I'm implementing a graceful restarts of our mesos-marathon-docker setup and I came to a following issue:  (it was already discussed on https://github.com/<USER>marathon/issues/2876 and guys form <USER>got to a point that its probably a docker containerizer problem...) To sum it up:  When i deploy simple python script to all mesos-slaves: {code} #!/usr/bin/python  from time import sleep import signal import sys import datetime  def sigterm_handler(_signo, _stack_frame):     print got %i % _signo     print datetime.datetime.now().time()     sys.stdout.flush()     sleep(2)     print datetime.datetime.now().time()     print ending     sys.stdout.flush()     sys.exit(0)  signal.signal(signal.SIGTERM, sigterm_handler) signal.signal(signal.SIGINT, sigterm_handler)  try:     print Hello     i = 0     while True:         i += 1         print datetime.datetime.now().time()         print Iteration #%i % i         sys.stdout.flush()         sleep(1) finally:     print Goodbye {code}  and I run it through Marathon like {code:javascript} data = {  args: [/tmp/script.py],  instances: 1,  cpus: 0.1,  mem: 256,  id: marathon-test-api } {code}  During the app restart I get expected result - the task receives sigterm and dies peacefully (during my script-specified 2 seconds period)  But when i wrap this python script in a docker: {code} FROM node:4.2  RUN mkdir /app ADD . /app WORKDIR /app ENTRYPOINT [] {code} and run appropriate application by Marathon: {code:javascript} data = {  args: [./script.py],  container: {   type: DOCKER,   docker: {    image: <USER>marathon-test-api   },   forcePullImage: yes  },  cpus: 0.1,  mem: 256,  instances: 1,  id: marathon-test-api } {code}  The task during restart (issued from marathon) dies immediately without having a chance to do any cleanup. ",Bug,Critical,Resolved,"2016-01-04 15:57:28","2016-01-04 15:57:28",5
"Apache Mesos","Constrain types used to instantiate Flags objects","stout's {{Flags}} can be instantiated with a number of base flags provided by the caller as template arguments; these are then inherited from by the created {{Flags}} instance.  To ensure the expected semantics we could constrain the template arguments to ones derived from {{FlagsBase}}.  This addresses an existing {{TODO}}.",Improvement,Major,Resolved,"2016-01-04 10:58:01","2016-01-04 10:58:01",2
"Apache Mesos","Provide constexpr Duration::min() and max()","{{Duration}} could be implemented so that it can provide {{constexpr}} {{min}} and {{max}} functions.  This addresses an existing {{TODO}}.",Improvement,Minor,Resolved,"2016-01-04 10:42:54","2016-01-04 10:42:54",2
"Apache Mesos","Remove dupicate Mesos constructor","{{Mesos}} offers two almost-identical constructors   Here invocations of the first constructor can replaced trivially with invocations of the second one with {{contentType = ContentType::PROTOBUF}}. ",Improvement,Major,Resolved,"2016-01-04 10:36:10","2016-01-04 10:36:10",1
"Apache Mesos","Duration uses fixed-width types inconsistently","The implementation of the {{Duration}} class correctly uses fixed-width types (here {{int64_t}}) for portability internally, but uses {{long}} types in a few places (in particular {{LLONG_MIN}} and {{LLONG_MAX}}). This is inconsistent on 64-bit platforms, and probably incorrect on 32-bit as there {{long}} is 32 bit wide.  Additionally, the longer {{Duration}} types ({{Minutes}}, {{Hours}}, {{Days}}, and {{Weeks}}) construct from {{int32_t}}, while shorter ones take {{int64_t}}. Probably as a left-over this is matched with a redundant {{Duration}} constructor taking an {{int32_t}} value where the other one taking an {{int64_t}} value would be sufficient. It should be safe to just construct from {{int64_t}} in all places.",Bug,Major,Resolved,"2016-01-04 10:15:59","2016-01-04 10:15:59",2
"Apache Mesos","Replace variadic List constructor with one taking a initializer_list","{{List}} provides a variadic constructor currently implemented with some preprocessor magic. Given that we already require C++11 we can replace that one with a much simpler one just taking a {{std::initializer_list}}. This would change the invocations,   This addresses an existing {{TODO}}. ",Improvement,Minor,Resolved,"2016-01-04 09:38:03","2016-01-04 09:38:03",1
"Apache Mesos","DurationTest.Arithmetic performs inexact float calculation in test","{{DurationTest.Arithmetic}} does a calculation with not exactly representable floating point values and also performs an equality check,  Here neither the value {{3.3}} nor {{0.33}} cannot be represented exactly as a floating point number so the check might fail incorrectly (as it does e.g. when compiling and executing the test under 32-bit on Debian8).  Instead we should just use exactly representable values to make sure the test will succeed as long as the implementation behaves as expected.",Bug,Minor,Resolved,"2016-01-04 09:20:48","2016-01-04 09:20:48",1
"Apache Mesos","Report volume usage through ResourceStatistics.","POSIX disk isolator does not currently report volume usage through ResourceStatistics. {{PosixDiskIsolatorProcess::usage()}} should be amended to take into account volume usage as well. ",Bug,Major,Resolved,"2015-12-30 23:14:38","2015-12-30 23:14:38",3
"Apache Mesos","Enable net_cls subsytem in cgroup infrastructure","Currently the control group infrastructure within mesos supports only the memory and CPU subsystems. We need to enhance this infrastructure to support the net_cls subsystem as well. Details of the net_cls subsystem and its use-cases can be found here: https://www.kernel.org/doc/Documentation/cgroups/net_cls.txt  Enabling the net_cls will allow us to provide operators to, potentially, regulate framework traffic on a per-container basis.  ",Improvement,Major,Resolved,"2015-12-30 16:07:23","2015-12-30 16:07:23",5
"Apache Mesos","Remove docker auth server flag","We currently use a configured docker auth server from a slave flag to get token auth for docker registry. However this doesn't work for private registries as docker registry supports sending down the correct auth server to contact.  We should remove docker auth server flag completely and ask the docker registry for auth server.",Improvement,Major,Resolved,"2015-12-30 08:26:02","2015-12-30 08:26:02",3
"Apache Mesos","ExamplesTest.NoExecutorFramework runs forever.","{noformat: title=Good Run}  [ RUN      ] ExamplesTest.NoExecutorFramework  I1221 23:10:02.721617 32528 exec.cpp:444] Ignoring exited event because the driver is aborted!  Using temporary directory '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn'  I1221 23:10:02.721675 32539 exec.cpp:444] Ignoring exited event because the driver is aborted!  I1221 23:10:02.722024 32554 exec.cpp:444] Ignoring exited event because the driver is aborted!  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 23:10:05.179466 32569 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32;disk:32  Trying semicolon-delimited string format instead  I1221 23:10:05.180269 32569 logging.cpp:172] Logging to STDERR  I1221 23:10:05.185768 32569 process.cpp:998] libprocess is initialized on 172.17.0.2:40874 for 16 cpus  I1221 23:10:05.200728 32569 leveldb.cpp:174] Opened db in 4.184362ms  I1221 23:10:05.202234 32569 leveldb.cpp:181] Compacted db in 1.459268ms  I1221 23:10:05.202353 32569 leveldb.cpp:196] Created db iterator in 73761ns  I1221 23:10:05.202383 32569 leveldb.cpp:202] Seeked to beginning of db in 3382ns  I1221 23:10:05.202405 32569 leveldb.cpp:271] Iterated through 0 keys in the db in 633ns  I1221 23:10:05.202674 32569 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I1221 23:10:05.205301 32604 recover.cpp:447] Starting replica recovery  I1221 23:10:05.206414 32569 local.cpp:239] Using 'local' authorizer  I1221 23:10:05.206405 32604 recover.cpp:473] Replica is in EMPTY status  I1221 23:10:05.209595 32594 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.2:40874  I1221 23:10:05.210916 32596 recover.cpp:193] Received a recover response from a replica in EMPTY status  I1221 23:10:05.211515 32597 master.cpp:365] Master 3931c1a8-1cd6-49eb-94c8-d01b33bb008e (6ccf2ee56b13) started on 172.17.0.2:40874  I1221 23:10:05.211699 32605 recover.cpp:564] Updating replica status to STARTING  I1221 23:10:05.211539 32597 master.cpp:367] Flags at startup: --acls=permissive: false  register_frameworks {    principals {      type: SOME      values: test-principal    }    roles {      type: SOME      values: *    }  }  run_tasks {    principals {      type: SOME      values: test-principal    }    users {      type: SOME      values: mesos    }  }   --allocation_interval=1secs --allocator=HierarchicalDRF --authenticate=true --authenticate_slaves=false --authenticators=crammd5 --authorizers=local --credentials=/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials --framework_sorter=drf --help=true --hostname_lookup=true --initialize_driver_logging=true --log_auto_initialize=true --logbufsecs=0 --logging_level=INFO --max_slave_ping_timeouts=5 --quiet=false --recovery_slave_removal_limit=100% --registry=replicated_log --registry_fetch_timeout=1mins --registry_store_timeout=5secs --registry_strict=false --root_submissions=true --slave_ping_timeout=15secs --slave_reregister_timeout=10mins --user_sorter=drf --version=false --webui_dir=/mesos/mesos-0.27.0/src/webui --work_dir=/tmp/mesos-otpdch --zk_session_timeout=10secs  I1221 23:10:05.212323 32597 master.cpp:412] Master only allowing authenticated frameworks to register  I1221 23:10:05.212337 32597 master.cpp:419] Master allowing unauthenticated slaves to register  I1221 23:10:05.212347 32597 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials'  W1221 23:10:05.212442 32597 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.  I1221 23:10:05.212606 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 656857ns  I1221 23:10:05.212620 32597 master.cpp:456] Using default 'crammd5' authenticator  I1221 23:10:05.212631 32600 replica.cpp:320] Persisted replica status to STARTING  I1221 23:10:05.212893 32597 authenticator.cpp:518] Initializing server SASL  I1221 23:10:05.213091 32608 recover.cpp:473] Replica is in STARTING status  I1221 23:10:05.213958 32595 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.2:40874  I1221 23:10:05.214323 32594 recover.cpp:193] Received a recover response from a replica in STARTING status  I1221 23:10:05.214689 32595 recover.cpp:564] Updating replica status to VOTING  I1221 23:10:05.215353 32596 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 487419ns  I1221 23:10:05.215384 32596 replica.cpp:320] Persisted replica status to VOTING  I1221 23:10:05.215481 32605 recover.cpp:578] Successfully joined the Paxos group  I1221 23:10:05.215867 32605 recover.cpp:462] Recover process terminated  I1221 23:10:05.216111 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  W1221 23:10:05.217021 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.221482 32608 slave.cpp:191] Slave started on 1)@172.17.0.2:40874  I1221 23:10:05.221521 32608 slave.cpp:192] Flags at startup: --appc_store_dir=/tmp/mesos/store/appc --authenticatee=crammd5 --cgroups_cpu_enable_pids_and_tids_count=false --cgroups_enable_cfs=false --cgroups_hierarchy=/sys/fs/cgroup --cgroups_limit_swap=false --cgroups_root=mesos --container_disk_watch_interval=15secs --containerizers=mesos --default_role=* --disk_watch_interval=1mins --docker=docker --docker_auth_server=auth.docker.io --docker_auth_server_port=443 --docker_kill_orphans=true --docker_local_archives_dir=/tmp/mesos/images/docker --docker_puller=local --docker_puller_timeout=60 --docker_registry=registry-1.docker.io --docker_registry_port=443 --docker_remove_delay=6hrs --docker_socket=/var/run/docker.sock --docker_stop_timeout=0ns --docker_store_dir=/tmp/mesos/store/docker --enforce_container_disk_quota=false --executor_registration_timeout=1mins --executor_shutdown_grace_period=5secs --fetcher_cache_dir=/tmp/mesos/fetch --fetcher_cache_size=2GB --frameworks_home= --gc_delay=1weeks --gc_disk_headroom=0.1 --hadoop_home= --help=false --hostname_lookup=true --image_provisioner_backend=copy --initialize_driver_logging=true --isolation=filesystem/posix,posix/cpu,posix/mem --launcher=posix --launcher_dir=/mesos/mesos-0.27.0/_build/src --logbufsecs=0 --logging_level=INFO --oversubscribed_resources_interval=15secs --perf_duration=10secs --perf_interval=1mins --qos_correction_interval_min=0ns --quiet=false --recover=reconnect --recovery_timeout=15mins --registration_backoff_factor=1secs --resources=cpus:2;mem:10240 --revocable_cpu_low_priority=true --sandbox_directory=/mnt/mesos/sandbox --strict=true --switch_user=true --systemd_runtime_directory=/run/systemd/system --version=false --work_dir=/tmp/mesos-otpdch/0  I1221 23:10:05.222578 32608 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.223465 32608 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.223621 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 23:10:05.223610 32608 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.223677 32608 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.223697 32608 slave.cpp:410] Slave checkpoint: true  W1221 23:10:05.224143 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.226668 32604 slave.cpp:191] Slave started on 2)@172.17.0.2:40874  I1221 23:10:05.226692 32604 slave.cpp:192] Flags at startup: --appc_store_dir=/tmp/mesos/store/appc --authenticatee=crammd5 --cgroups_cpu_enable_pids_and_tids_count=false --cgroups_enable_cfs=false --cgroups_hierarchy=/sys/fs/cgroup --cgroups_limit_swap=false --cgroups_root=mesos --container_disk_watch_interval=15secs --containerizers=mesos --default_role=* --disk_watch_interval=1mins --docker=docker --docker_auth_server=auth.docker.io --docker_auth_server_port=443 --docker_kill_orphans=true --docker_local_archives_dir=/tmp/mesos/images/docker --docker_puller=local --docker_puller_timeout=60 --docker_registry=registry-1.docker.io --docker_registry_port=443 --docker_remove_delay=6hrs --docker_socket=/var/run/docker.sock --docker_stop_timeout=0ns --docker_store_dir=/tmp/mesos/store/docker --enforce_container_disk_quota=false --executor_registration_timeout=1mins --executor_shutdown_grace_period=5secs --fetcher_cache_dir=/tmp/mesos/fetch --fetcher_cache_size=2GB --frameworks_home= --gc_delay=1weeks --gc_disk_headroom=0.1 --hadoop_home= --help=false --hostname_lookup=true --image_provisioner_backend=copy --initialize_driver_logging=true --isolation=filesystem/posix,posix/cpu,posix/mem --launcher=posix --launcher_dir=/mesos/mesos-0.27.0/_build/src --logbufsecs=0 --logging_level=INFO --oversubscribed_resources_interval=15secs --perf_duration=10secs --perf_interval=1mins --qos_correction_interval_min=0ns --quiet=false --recover=reconnect --recovery_timeout=15mins --registration_backoff_factor=1secs --resources=cpus:2;mem:10240 --revocable_cpu_low_priority=true --sandbox_directory=/mnt/mesos/sandbox --strict=true --switch_user=true --systemd_runtime_directory=/run/systemd/system --version=false --work_dir=/tmp/mesos-otpdch/1  I1221 23:10:05.227520 32604 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.228037 32604 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.228148 32604 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.228169 32604 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.228184 32604 slave.cpp:410] Slave checkpoint: true  I1221 23:10:05.229123 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 23:10:05.229641 32605 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/0/meta'  W1221 23:10:05.229645 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.229636 32595 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/1/meta'  I1221 23:10:05.230242 32605 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.230254 32598 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.230515 32601 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.230562 32602 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.232681 32597 auxprop.cpp:71] Initialized in-memory auxiliary property plugin  I1221 23:10:05.232803 32597 master.cpp:493] Authorization enabled  I1221 23:10:05.232867 32600 slave.cpp:4427] Finished recovery  I1221 23:10:05.232980 32598 slave.cpp:191] Slave started on 3)@172.17.0.2:40874  I1221 23:10:05.233039 32594 slave.cpp:4427] Finished recovery  I1221 23:10:05.233376 32599 whitelist_watcher.cpp:77] No whitelist given  I1221 23:10:05.233428 32601 hierarchical.cpp:147] Initialized hierarchical allocator process  I1221 23:10:05.233003 32598 slave.cpp:192] Flags at startup: --appc_store_dir=/tmp/mesos/store/appc --authenticatee=crammd5 --cgroups_cpu_enable_pids_and_tids_count=false --cgroups_enable_cfs=false --cgroups_hierarchy=/sys/fs/cgroup --cgroups_limit_swap=false --cgroups_root=mesos --container_disk_watch_interval=15secs --containerizers=mesos --default_role=* --disk_watch_interval=1mins --docker=docker --docker_auth_server=auth.docker.io --docker_auth_server_port=443 --docker_kill_orphans=true --docker_local_archives_dir=/tmp/mesos/images/docker --docker_puller=local --docker_puller_timeout=60 --docker_registry=registry-1.docker.io --docker_registry_port=443 --docker_remove_delay=6hrs --docker_socket=/var/run/docker.sock --docker_stop_timeout=0ns --docker_store_dir=/tmp/mesos/store/docker --enforce_container_disk_quota=false --executor_registration_timeout=1mins --executor_shutdown_grace_period=5secs --fetcher_cache_dir=/tmp/mesos/fetch --fetcher_cache_size=2GB --frameworks_home= --gc_delay=1weeks --gc_disk_headroom=0.1 --hadoop_home= --help=false --hostname_lookup=true --image_provisioner_backend=copy --initialize_driver_logging=true --isolation=filesystem/posix,posix/cpu,posix/mem --launcher=posix --launcher_dir=/mesos/mesos-0.27.0/_build/src --logbufsecs=0 --logging_level=INFO --oversubscribed_resources_interval=15secs --perf_duration=10secs --perf_interval=1mins --qos_correction_interval_min=0ns --quiet=false --recover=reconnect --recovery_timeout=15mins --registration_backoff_factor=1secs --resources=cpus:2;mem:10240 --revocable_cpu_low_priority=true --sandbox_directory=/mnt/mesos/sandbox --strict=true --switch_user=true --systemd_runtime_directory=/run/systemd/system --version=false --work_dir=/tmp/mesos-otpdch/2  I1221 23:10:05.233744 32600 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.233749 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.234222 32598 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.234284 32598 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.234299 32598 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.234311 32598 slave.cpp:410] Slave checkpoint: true  I1221 23:10:05.234338 32600 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.234376 32604 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.234424 32600 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.234522 32600 slave.cpp:765] Detecting new master  I1221 23:10:05.234616 32569 sched.cpp:164] Version: 0.27.0  I1221 23:10:05.234658 32600 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.234671 32594 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.234884 32606 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.235038 32595 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.235043 32606 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.235111 32606 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.235147 32606 slave.cpp:765] Detecting new master  I1221 23:10:05.235240 32594 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/2/meta'  I1221 23:10:05.235443 32608 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.235625 32594 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.236549 32599 slave.cpp:4427] Finished recovery  I1221 23:10:05.236984 32593 sched.cpp:262] New master detected at master@172.17.0.2:40874  I1221 23:10:05.237004 32599 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.237221 32593 sched.cpp:318] Authenticating with master master@172.17.0.2:40874  I1221 23:10:05.237277 32593 sched.cpp:325] Using default CRAM-MD5 authenticatee  I1221 23:10:05.237285 32604 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.237288 32599 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.237361 32599 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.237433 32599 slave.cpp:765] Detecting new master  I1221 23:10:05.237565 32599 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.238154 32605 authenticatee.cpp:97] Initializing client SASL  I1221 23:10:05.238315 32605 authenticatee.cpp:121] Creating new client SASL connection  I1221 23:10:05.239640 32597 master.cpp:1200] Dropping 'mesos.internal.AuthenticateMessage' message since not elected yet  I1221 23:10:05.239765 32597 master.cpp:1629] The newly elected leader is master@172.17.0.2:40874 with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e  I1221 23:10:05.239794 32597 master.cpp:1642] Elected as the leading master!  I1221 23:10:05.239843 32597 master.cpp:1387] Recovering from registrar  I1221 23:10:05.240056 32600 registrar.cpp:307] Recovering registrar  I1221 23:10:05.241477 32608 log.cpp:659] Attempting to start the writer  I1221 23:10:05.244540 32600 replica.cpp:493] Replica received implicit promise request from (39)@172.17.0.2:40874 with proposal 1  I1221 23:10:05.245358 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 776937ns  I1221 23:10:05.245393 32600 replica.cpp:342] Persisted promised to 1  I1221 23:10:05.246625 32601 coordinator.cpp:238] Coordinator attempting to fill missing positions  I1221 23:10:05.248757 32605 replica.cpp:388] Replica received explicit promise request from (40)@172.17.0.2:40874 for position 0 with proposal 2  I1221 23:10:05.249214 32605 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 366567ns  I1221 23:10:05.249246 32605 replica.cpp:712] Persisted action at 0  I1221 23:10:05.250998 32599 replica.cpp:537] Replica received write request for position 0 from (41)@172.17.0.2:40874  I1221 23:10:05.251111 32599 leveldb.cpp:436] Reading position from leveldb took 66773ns  I1221 23:10:05.251734 32599 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 379612ns  I1221 23:10:05.251759 32599 replica.cpp:712] Persisted action at 0  I1221 23:10:05.252555 32601 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I1221 23:10:05.253010 32601 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 381858ns  I1221 23:10:05.253036 32601 replica.cpp:712] Persisted action at 0  I1221 23:10:05.253068 32601 replica.cpp:697] Replica learned NOP action at position 0  I1221 23:10:05.254043 32595 log.cpp:675] Writer started with ending position 0  I1221 23:10:05.256741 32595 leveldb.cpp:436] Reading position from leveldb took 48607ns  I1221 23:10:05.260617 32601 registrar.cpp:340] Successfully fetched the registry (0B) in 20.47616ms  I1221 23:10:05.260988 32601 registrar.cpp:439] Applied 1 operations in 103123ns; attempting to update the 'registry'  I1221 23:10:05.264700 32604 log.cpp:683] Attempting to append 170 bytes to the log  I1221 23:10:05.265138 32601 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I1221 23:10:05.266208 32603 replica.cpp:537] Replica received write request for position 1 from (42)@172.17.0.2:40874  I1221 23:10:05.266829 32603 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 551087ns  I1221 23:10:05.266861 32603 replica.cpp:712] Persisted action at 1  I1221 23:10:05.267918 32605 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I1221 23:10:05.268442 32605 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 453416ns  I1221 23:10:05.268470 32605 replica.cpp:712] Persisted action at 1  I1221 23:10:05.268506 32605 replica.cpp:697] Replica learned APPEND action at position 1  I1221 23:10:05.270512 32606 registrar.cpp:484] Successfully updated the 'registry' in 9.375232ms  I1221 23:10:05.270705 32606 registrar.cpp:370] Successfully recovered registrar  I1221 23:10:05.271045 32602 log.cpp:702] Attempting to truncate the log to 1  I1221 23:10:05.271178 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I1221 23:10:05.271695 32605 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I1221 23:10:05.271751 32603 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover  I1221 23:10:05.272274 32596 replica.cpp:537] Replica received write request for position 2 from (43)@172.17.0.2:40874  I1221 23:10:05.272838 32596 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 451122ns  I1221 23:10:05.272867 32596 replica.cpp:712] Persisted action at 2  I1221 23:10:05.273483 32607 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I1221 23:10:05.273919 32607 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 405444ns  I1221 23:10:05.273977 32607 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33953ns  I1221 23:10:05.273998 32607 replica.cpp:712] Persisted action at 2  I1221 23:10:05.274024 32607 replica.cpp:697] Replica learned TRUNCATE action at position 2  I1221 23:10:05.288020 32593 slave.cpp:1254] Will retry registration in 1.091568855secs if necessary  I1221 23:10:05.288321 32605 master.cpp:4132] Registering slave at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0  I1221 23:10:05.289105 32607 registrar.cpp:439] Applied 1 operations in 104776ns; attempting to update the 'registry'  I1221 23:10:05.291494 32605 log.cpp:683] Attempting to append 339 bytes to the log  I1221 23:10:05.291594 32595 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I1221 23:10:05.292273 32602 replica.cpp:537] Replica received write request for position 3 from (44)@172.17.0.2:40874  I1221 23:10:05.292811 32602 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 499449ns  I1221 23:10:05.292845 32602 replica.cpp:712] Persisted action at 3  I1221 23:10:05.293373 32594 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I1221 23:10:05.293776 32594 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 376184ns  I1221 23:10:05.293799 32594 replica.cpp:712] Persisted action at 3  I1221 23:10:05.293833 32594 replica.cpp:697] Replica learned APPEND action at position 3  I1221 23:10:05.295142 32603 registrar.cpp:484] Successfully updated the 'registry' in 5.945088ms  I1221 23:10:05.295403 32602 log.cpp:702] Attempting to truncate the log to 3  I1221 23:10:05.295513 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I1221 23:10:05.296146 32598 replica.cpp:537] Replica received write request for position 4 from (45)@172.17.0.2:40874  I1221 23:10:05.296443 32608 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:40874  I1221 23:10:05.296552 32598 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 368428ns  I1221 23:10:05.296604 32598 replica.cpp:712] Persisted action at 4  I1221 23:10:05.296744 32594 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.297025 32597 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0  I1221 23:10:05.297056 32597 fetcher.cpp:81] Clearing fetcher cache  I1221 23:10:05.297175 32599 status_update_manager.cpp:181] Resuming sending status updates  I1221 23:10:05.297184 32600 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )  I1221 23:10:05.297473 32597 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/2/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0/slave.info'  I1221 23:10:05.297618 32600 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.297688 32600 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 in 454887ns  I1221 23:10:05.298058 32597 slave.cpp:963] Forwarding total oversubscribed resources   I1221 23:10:05.298235 32597 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources   I1221 23:10:05.298396 32604 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0  I1221 23:10:05.298765 32597 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )  I1221 23:10:05.298907 32597 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.298933 32597 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 in 134328ns  I1221 23:10:05.298965 32604 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 505213ns  I1221 23:10:05.299031 32604 leveldb.cpp:399] Deleting ~2 keys from leveldb took 37007ns  I1221 23:10:05.299054 32604 replica.cpp:712] Persisted action at 4  I1221 23:10:05.299103 32604 replica.cpp:697] Replica learned TRUNCATE action at position 4  I1221 23:10:05.350281 32598 slave.cpp:1254] Will retry registration in 1.181298785secs if necessary  I1221 23:10:05.350510 32608 master.cpp:4132] Registering slave at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:05.351222 32607 registrar.cpp:439] Applied 1 operations in 118623ns; attempting to update the 'registry'  I1221 23:10:05.352174 32604 log.cpp:683] Attempting to append 505 bytes to the log  I1221 23:10:05.352375 32595 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5  I1221 23:10:05.353365 32601 replica.cpp:537] Replica received write request for position 5 from (46)@172.17.0.2:40874  I1221 23:10:05.353960 32601 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 552132ns  I1221 23:10:05.353986 32601 replica.cpp:712] Persisted action at 5  I1221 23:10:05.354867 32606 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0  I1221 23:10:05.355370 32606 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 456354ns  I1221 23:10:05.355399 32606 replica.cpp:712] Persisted action at 5  I1221 23:10:05.355432 32606 replica.cpp:697] Replica learned APPEND action at position 5  I1221 23:10:05.357318 32595 registrar.cpp:484] Successfully updated the 'registry' in 6.016768ms  I1221 23:10:05.357708 32595 log.cpp:702] Attempting to truncate the log to 5  I1221 23:10:05.357805 32606 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6  I1221 23:10:05.358273 32602 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:40874  I1221 23:10:05.358331 32599 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.358405 32602 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:05.358428 32602 fetcher.cpp:81] Clearing fetcher cache  I1221 23:10:05.358722 32599 status_update_manager.cpp:181] Resuming sending status updates  I1221 23:10:05.358736 32606 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )  I1221 23:10:05.358813 32599 replica.cpp:537] Replica received write request for position 6 from (47)@172.17.0.2:40874  I1221 23:10:05.358952 32602 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/slave.info'  I1221 23:10:05.358969 32606 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.358996 32606 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 in 217083ns  I1221 23:10:05.359350 32599 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 464454ns  I1221 23:10:05.359374 32599 replica.cpp:712] Persisted action at 6  I1221 23:10:05.359591 32602 slave.cpp:963] Forwarding total oversubscribed resources   I1221 23:10:05.359740 32606 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources   I1221 23:10:05.360227 32605 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0  I1221 23:10:05.360288 32606 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )  I1221 23:10:05.360539 32606 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.360591 32606 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 in 256841ns  I1221 23:10:05.360702 32605 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 444736ns  I1221 23:10:05.360759 32605 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30869ns  I1221 23:10:05.360777 32605 replica.cpp:712] Persisted action at 6  I1221 23:10:05.360800 32605 replica.cpp:697] Replica learned TRUNCATE action at position 6  I1221 23:10:05.957257 32601 slave.cpp:1254] Will retry registration in 627.120173ms if necessary  I1221 23:10:05.957504 32597 master.cpp:4132] Registering slave at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2  I1221 23:10:05.958284 32607 registrar.cpp:439] Applied 1 operations in 174321ns; attempting to update the 'registry'  I1221 23:10:05.959336 32594 log.cpp:683] Attempting to append 671 bytes to the log  I1221 23:10:05.959477 32606 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7  I1221 23:10:05.960484 32604 replica.cpp:537] Replica received write request for position 7 from (48)@172.17.0.2:40874  I1221 23:10:05.960891 32604 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 362101ns  I1221 23:10:05.960917 32604 replica.cpp:712] Persisted action at 7  I1221 23:10:05.961642 32597 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0  I1221 23:10:05.962502 32597 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 828414ns  I1221 23:10:05.962532 32597 replica.cpp:712] Persisted action at 7  I1221 23:10:05.962563 32597 replica.cpp:697] Replica learned APPEND action at position 7  I1221 23:10:05.964241 32598 registrar.cpp:484] Successfully updated the 'registry' in 5.87392ms  I1221 23:10:05.964552 32593 log.cpp:702] Attempting to truncate the log to 7  I1221 23:10:05.964643 32606 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8  I1221 23:10:05.964964 32593 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:40874  I1221 23:10:05.965152 32602 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2  I1221 23:10:05.965111 32600 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.965178 32602 fetcher.cpp:81] Clearing fetcher cache  I1221 23:10:05.965293 32607 status_update_manager.cpp:181] Resuming sending status updates  I1221 23:10:05.965293 32593 replica.cpp:537] Replica received write request for position 8 from (49)@172.17.0.2:40874  I1221 23:10:05.965637 32603 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )  I1221 23:10:05.965734 32602 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/1/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2/slave.info'  I1221 23:10:05.965751 32593 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 424617ns  I1221 23:10:05.965772 32593 replica.cpp:712] Persisted action at 8  I1221 23:10:05.965847 32603 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.965880 32603 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 in 209313ns  I1221 23:10:05.966145 32602 slave.cpp:963] Forwarding total oversubscribed resources   I1221 23:10:05.966325 32601 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0  I1221 23:10:05.966342 32596 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources   I1221 23:10:05.966703 32601 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 347667ns  I1221 23:10:05.966753 32601 leveldb.cpp:399] Deleting ~2 keys from leveldb took 29042ns  I1221 23:10:05.966770 32601 replica.cpp:712] Persisted action at 8  I1221 23:10:05.966791 32601 replica.cpp:697] Replica learned TRUNCATE action at position 8  I1221 23:10:05.966792 32602 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )  I1221 23:10:05.966943 32602 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.966969 32602 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 in 137680ns  I1221 23:10:06.235074 32595 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:06.235142 32595 hierarchical.cpp:1079] Performed allocation for 3 slaves in 594477ns  I1221 23:10:07.237238 32599 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:07.237309 32599 hierarchical.cpp:1079] Performed allocation for 3 slaves in 506418ns  I1221 23:10:07.702738 32564 exec.cpp:88] Committing suicide by killing the process group  II1221 23:10:07.703501 32546 exec.cpp:88] Committing suicide by killing the process group  1221 23:10:07.703501 32525 exec.cpp:88] Committing suicide by killing the process group  I1221 23:10:08.239282 32608 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:08.239356 32608 hierarchical.cpp:1079] Performed allocation for 3 slaves in 671976ns  I1221 23:10:09.241236 32600 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:09.241303 32600 hierarchical.cpp:1079] Performed allocation for 3 slaves in 520741ns  W1221 23:10:10.239298 32601 sched.cpp:429] Authentication timed out  I1221 23:10:10.239653 32596 sched.cpp:387] Failed to authenticate with master master@172.17.0.2:40874: Authentication discarded  I1221 23:10:10.239714 32596 sched.cpp:318] Authenticating with master master@172.17.0.2:40874    ...  ...  ...  I1221 23:10:10.432864 32597 slave.cpp:1796] Sending queued task '0' to executor '0' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:60995  I1221 23:10:10.433215 32765 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.435856 32765 exec.cpp:220] Executor::registered took 343507ns  Registered executor on 6ccf2ee56b13  I1221 23:10:10.436198 32765 exec.cpp:295] Executor asked to run task '0'  I1221 23:10:10.436317 32765 exec.cpp:304] Executor::launchTask took 86293ns  Starting task 0  Forked command at 361  sh -c 'echo hello'  I1221 23:10:10.437671   335 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:33894 with pid 32696  I1221 23:10:10.438062   315 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:51530 with pid 32695  hello  I1221 23:10:10.440052 32599 slave.cpp:2578] Got registration for executor '2' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33894  I1221 23:10:10.440507 32599 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:33894' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/runs/63bf8626-189d-44f9-a961-b51b046d5057/pids/libprocess.pid'  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 23:10:10.440497 32733 process.cpp:998] libprocess is initialized on 172.17.0.2:33285 for 16 cpus  I1221 23:10:10.441426 32733 logging.cpp:172] Logging to STDERR  I1221 23:10:10.441916 32599 slave.cpp:2578] Got registration for executor '1' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:51530  I1221 23:10:10.442277 32599 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:51530' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/runs/468fb0f8-4dd0-448c-8e64-207cc4c81c63/pids/libprocess.pid'  I1221 23:10:10.442314   303 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.442684   339 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.443305 32599 slave.cpp:1796] Sending queued task '2' to executor '2' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:33894  I1221 23:10:10.443668 32733 exec.cpp:134] Version: 0.27.0  I1221 23:10:10.443768 32599 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:60995  I1221 23:10:10.444797   320 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.445159   339 exec.cpp:220] Executor::registered took 241955ns  Registered executor on 6ccf2ee56b13  I1221 23:10:10.445430   339 exec.cpp:295] Executor asked to run task '2'  Starting task 2  I1221 23:10:10.445519   339 exec.cpp:304] Executor::launchTask took 65600ns  I1221 23:10:10.445794 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.445888 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.446015 32599 slave.cpp:1796] Sending queued task '1' to executor '1' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:51530  Forked command at 362  I1221 23:10:10.446622 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.446990   320 exec.cpp:220] Executor::registered took 324561ns  sh -c 'echo hello'  Registered executor on 6ccf2ee56b13  I1221 23:10:10.447427   320 exec.cpp:295] Executor asked to run task '1'  Starting task 1  I1221 23:10:10.447540   320 exec.cpp:304] Executor::launchTask took 84828ns  Forked command at 363  sh -c 'echo hello'  hello  I1221 23:10:10.449244   334 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  hello  I1221 23:10:10.450145 32597 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33894  I1221 23:10:10.451092 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.451117   324 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.451370 32595 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.451637 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.451710 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.451652 32595 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.451838 32593 master.cpp:4687] Status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.451974 32593 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.452203 32593 master.cpp:6347] Updating the state of task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.451875 32595 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:60995  I1221 23:10:10.452308 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.452432 32603 no_executor_framework.cpp:160] Task '0' is in state TASK_RUNNING  I1221 23:10:10.452468 32603 sched.cpp:919] Scheduler::statusUpdate took 46157ns  I1221 23:10:10.452905 32606 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:51530  I1221 23:10:10.453151 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.453403   302 exec.cpp:341] Executor received status update acknowledgement 61620dd2-95c0-4570-b174-24ed26e0a289 for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.453454 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.453496 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.453488 32606 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.453840 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.453954 32603 master.cpp:3844] Processing ACKNOWLEDGE call 61620dd2-95c0-4570-b174-24ed26e0a289 for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.454144 32606 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.454195 32606 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:33894  I1221 23:10:10.454293 32600 master.cpp:4687] Status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.454344 32600 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.454502 32600 master.cpp:6347] Updating the state of task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.454665 32600 no_executor_framework.cpp:160] Task '2' is in state TASK_RUNNING  I1221 23:10:10.454691 32600 sched.cpp:919] Scheduler::statusUpdate took 31056ns  I1221 23:10:10.454856   335 exec.cpp:341] Executor received status update acknowledgement b774dc4c-940c-41d5-be2a-ed3a9e8379a7 for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.454898 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.455098 32600 master.cpp:3844] Processing ACKNOWLEDGE call b774dc4c-940c-41d5-be2a-ed3a9e8379a7 for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.455250 32605 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.455487 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.455533 32605 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.455538 32607 master.cpp:4687] Status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.455574 32607 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.455574 32605 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:51530  I1221 23:10:10.455685 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.455713 32607 master.cpp:6347] Updating the state of task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.455813 32606 no_executor_framework.cpp:160] Task '1' is in state TASK_RUNNING  I1221 23:10:10.455916 32606 sched.cpp:919] Scheduler::statusUpdate took 105226ns  I1221 23:10:10.455812   344 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:33285 with pid 32733  I1221 23:10:10.456140 32607 master.cpp:3844] Processing ACKNOWLEDGE call 7a5d5a7f-975d-4970-949e-cfa4d08a4a30 for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.456163   312 exec.cpp:341] Executor received status update acknowledgement 7a5d5a7f-975d-4970-949e-cfa4d08a4a30 for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.456812 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.456807 32606 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.457012 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.457720 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.457751 32607 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.457883 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.458319 32604 slave.cpp:2578] Got registration for executor '3' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33285  I1221 23:10:10.458766 32604 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:33285' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/runs/7f37bf6f-50f1-45a9-ae7a-20b1b829cc38/pids/libprocess.pid'  I1221 23:10:10.460311 32604 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.461024   349 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.461304 32603 slave.cpp:1796] Sending queued task '3' to executor '3' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:33285  I1221 23:10:10.462968   349 exec.cpp:220] Executor::registered took 273965ns  Registered executor on 6ccf2ee56b13  I1221 23:10:10.463317   349 exec.cpp:295] Executor asked to run task '3'  Starting task 3  I1221 23:10:10.463429   349 exec.cpp:304] Executor::launchTask took 87510ns  Forked command at 381  sh -c 'echo hello'  hello  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 23:10:10.466104 32747 process.cpp:998] libprocess is initialized on 172.17.0.2:39520 for 16 cpus  I1221 23:10:10.466836   351 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.467958 32604 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33285  I1221 23:10:10.468389 32595 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.468441 32595 status_update_manager.cpp:497] Creating StatusUpdate stream for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.469084 32595 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.470094 32595 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.470365 32597 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.470433 32747 logging.cpp:172] Logging to STDERR  I1221 23:10:10.470557 32597 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.470603 32597 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:33285  I1221 23:10:10.470706 32604 master.cpp:4687] Status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.470757 32604 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.470984 32604 master.cpp:6347] Updating the state of task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.471132 32595 no_executor_framework.cpp:160] Task '3' is in state TASK_RUNNING  I1221 23:10:10.471163 32595 sched.cpp:919] Scheduler::statusUpdate took 46297ns  I1221 23:10:10.471405 32595 master.cpp:3844] Processing ACKNOWLEDGE call f4c28f0e-5d99-46e2-8e97-be35f47a8447 for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.471519   346 exec.cpp:341] Executor received status update acknowledgement f4c28f0e-5d99-46e2-8e97-be35f47a8447 for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.471691 32595 status_update_manager.cpp:392] Received status update acknowledgement (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.471897 32595 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.472843 32608 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.476374 32747 exec.cpp:134] Version: 0.27.0  I1221 23:10:10.478024   373 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:39520 with pid 32747  I1221 23:10:10.480216 32607 slave.cpp:2578] Got registration for executor '4' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:39520  I1221 23:10:10.480746 32607 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:39520' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/runs/bc8306b3-d4ba-4bd6-b5f5-03dfb4759738/pids/libprocess.pid'  I1221 23:10:10.482753 32597 slave.cpp:1796] Sending queued task '4' to executor '4' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:39520  I1221 23:10:10.483325   379 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  Registered executor on 6ccf2ee56b13  I1221 23:10:10.485102   379 exec.cpp:220] Executor::registered took 269656ns  I1221 23:10:10.485415   379 exec.cpp:295] Executor asked to run task '4'  Starting task 4  I1221 23:10:10.485508   379 exec.cpp:304] Executor::launchTask took 63624ns  sh -c 'echo hello'  Forked command at 382  hello  I1221 23:10:10.488653   377 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.489480 32593 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:39520  I1221 23:10:10.489858 32595 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.489914 32595 status_update_manager.cpp:497] Creating StatusUpdate stream for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.490532 32595 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.491937 32595 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.492247 32597 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.492422 32597 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.492470 32597 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:39520  I1221 23:10:10.492741 32606 master.cpp:4687] Status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.492871 32606 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.493093   372 exec.cpp:341] Executor received status update acknowledgement c8c2c7b7-8040-48e9-84d6-75b9fbbb3419 for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.493377 32606 master.cpp:6347] Updating the state of task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.493441 32595 no_executor_framework.cpp:160] Task '4' is in state TASK_RUNNING  I1221 23:10:10.493469 32595 sched.cpp:919] Scheduler::statusUpdate took 37984ns  I1221 23:10:10.493696 32595 master.cpp:3844] Processing ACKNOWLEDGE call c8c2c7b7-8040-48e9-84d6-75b9fbbb3419 for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.493988 32599 status_update_manager.cpp:392] Received status update acknowledgement (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.494120 32599 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.495306 32598 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  Command exited with status 0 (pid: 361)  I1221 23:10:10.543287   304 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.544509 32593 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:60995  I1221 23:10:10.544711 32593 slave.cpp:5520] Terminating task 0  I1221 23:10:10.546165 32608 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.546311 32608 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.547626 32608 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.547925 32594 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.548090 32594 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.548177 32594 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:60995  I1221 23:10:10.548307 32605 master.cpp:4687] Status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.548375 32605 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.548635 32605 master.cpp:6347] Updating the state of task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)  I1221 23:10:10.548717 32608 no_executor_framework.cpp:160] Task '0' is in state TASK_FINISHED  I1221 23:10:10.548748 32608 sched.cpp:919] Scheduler::statusUpdate took 39141ns  I1221 23:10:10.549018   301 exec.cpp:341] Executor received status update acknowledgement 133ecf74-5795-4890-a47d-02b397e9084c for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.549098 32605 master.cpp:3844] Processing ACKNOWLEDGE call 133ecf74-5795-4890-a47d-02b397e9084c for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.549120 32607 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.4; mem(*):128; disk(*):128) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.549159 32605 master.cpp:6413] Removing task 0 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.549618 32593 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  Command exited with status 0 (pid: 362)  Command exited with status 0 (pid: 363)  I1221 23:10:10.550003 32593 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.550715 32593 status_update_manager.cpp:528] Cleaning up status update stream for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.551157 32599 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.551211 32599 slave.cpp:5561] Completing task 0    ...  ...  ...  I1221 23:10:10.598156   374 exec.cpp:396] Executor::shutdown took 14264ns  I1221 23:10:10.598224 32605 slave.cpp:3417] master@172.17.0.2:40874 exited  W1221 23:10:10.598253 32605 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected  I1221 23:10:10.598274   374 exec.cpp:80] Scheduling shutdown of the executor  I1221 23:10:10.600291 32593 slave.cpp:601] Slave terminating  I1221 23:10:10.603379 32569 slave.cpp:601] Slave terminating  I1221 23:10:10.605034 32604 slave.cpp:601] Slave terminating  [       OK ] ExamplesTest.NoExecutorFramework (7895 ms)  ",Bug,Major,Resolved,"2015-12-29 19:49:32","2015-12-29 19:49:32",3
"Apache Mesos","Add mechanism for testing recovery of HTTP based executors","Currently, the slave process generates a process ID every time it is initialized via {{process::ID::generate}} function call. This is a problem for testing HTTP executors as it can't retry if there is a disconnection after an agent restart since the prefix is incremented.     There are a couple of ways to fix this: - Add a constructor to {{Slave}} exclusively for testing that passes on a fixed {{ID}} instead of relying on {{ID::generate}}. - Currently we delegate to slave(1)@ i.e. (1) when nothing is specified as the URL in libprocess i.e. {{127.0.0.1:43915/api/v1/executor}} would delegate to {{slave(1)@127.0.0.1:43915/api/v1/executor}}. Instead of defaulting to (1), we can default to the last known active ID.",Bug,Major,Resolved,"2015-12-29 03:51:14","2015-12-29 03:51:14",5
"Apache Mesos","Add `dist` target to CMake solution",,Bug,Major,Resolved,"2015-12-23 21:33:35","2015-12-23 21:33:35",3
"Apache Mesos","Consolidate docker store slave flags","Currently there are too many slave flags for configuring the docker store/puller. We can remove the following flags:  docker_auth_server_port docker_local_archives_dir docker_registry_port docker_puller  And consolidate them into the existing flags.",Improvement,Major,Resolved,"2015-12-22 21:20:49","2015-12-22 21:20:49",3
"Apache Mesos","Pull provisioner from linux filesystem isolator to Mesos containerizer.","The rationale behind this change is that many of the image specifications (e.g., Docker/Appc) are not just for filesystems. They also specify runtime configurations (e.g., environment variables, volumes, etc) for the container.  Provisioner should return those runtime configurations to the Mesos containerizer and Mesos containerizer will delegate the isolation of those runtime configurations to the relevant isolator.  Here is what it will be look like eventually. We could do those changes in phases: 1) Provisioner will return a ProvisionInfo which includes a 'rootfs' and image specific runtime configurations (could be the Docker/Appc manifest). 2) Then, the Mesos containerizer will generate a ContainerConfig (a protobuf which includes rootfs, sandbox, docker/appc manifest, similar to OCI's host independent config.json) and pass that to each isolator in 'prepare'. Imaging in the future, a DockerRuntimeIsolator takes the docker manifest from ContainerConfig and prepare the container. 3) The isolator's prepare function will return a ContainerLaunchInfo (contains environment variables, namespaces, etc.) which will be used by Mesos containerize to launch containers. Imaging that information will be passed to the launcher in the future.  We can do the renaming (ContainerPrepareInfo -> ContainerLaunchInfo) later.  ",Task,Major,Resolved,"2015-12-22 21:01:39","2015-12-22 21:01:39",5
"Apache Mesos","Docker containers left running on disk after reviewbot builds","The Mesos Reviewbot builds recently failed due to Docker containers being left running on the disk, eventually leading to a full disk: https://issues.apache.org/jira/browse/INFRA-10984  These containers should be automatically cleaned up to avoid this problem in the future.",Bug,Major,Resolved,"2015-12-21 19:53:26","2015-12-21 19:53:26",3
"Apache Mesos","Enable passing docker image cmd runtime config to provisioner","Cmd is the command to run when starting a container. We should be able to collect Cmd config information from a docker image, and pass it back to provisioner.",Improvement,Major,Resolved,"2015-12-21 19:41:18","2015-12-21 19:41:18",1
"Apache Mesos","Enable passing docker image environment variables runtime config to provisioner","Collect environment variables runtime config information from a docker image, and save as a map. Pass it back to provisioner, and handling environment variables merge issue.",Improvement,Major,Resolved,"2015-12-21 19:38:35","2015-12-21 19:38:35",1
"Apache Mesos","Exposed docker/appc image manifest to mesos containerizer.","Collect docker image manifest from disk(which contains all runtime configurations), and pass it back to provisioner, so that mesos containerizer can grab all necessary info from provisioner.",Improvement,Major,Resolved,"2015-12-21 19:34:59","2015-12-21 19:34:59",2
"Apache Mesos","Document isolator internals.","Document isolators from developer perspective, possibly covering:  * linux isolators * posix isolators * filesystem, network isolators",Documentation,Major,Open,"2015-12-21 19:30:37","2015-12-21 19:30:37",4
"Apache Mesos","Document isolators from user perspective.","The documentation should cover:  * Purpose of isolators (business/user perspective). * What is the criteria for choosing/picking between available isolators. * Behavior of each individual isolator, constraints on which platforms/versions are supported, etc.",Documentation,Major,Accepted,"2015-12-21 19:27:38","2015-12-21 19:27:38",4
"Apache Mesos","Document containerizer from user perspective.","Add documentation that covers:  * Purpose of containerizers from a use case perspective. * What purpose does each containerizer (mesos. docker, compose) serve. * What criteria could be used to choose a containerizer.",Documentation,Major,Resolved,"2015-12-21 19:22:41","2015-12-21 19:22:41",3
"Apache Mesos","Test for Quota Status Endpoint",,Bug,Major,Resolved,"2015-12-21 17:10:58","2015-12-21 17:10:58",3
"Apache Mesos","Introduce HTTP endpoint /weights for updating weight",,Task,Major,Resolved,"2015-12-21 08:15:09","2015-12-21 08:15:09",5
"Apache Mesos","Document how to program with dynamic reservations and persistent volumes","Specifically, some of the gotchas around:  * Retrying reservation attempts after a timeout * Fuzzy-matching resources to determine whether a reservation/PV is successful * Represent client state as a state machine and repeatedly move toward successful terminate stats  Should also point to persistent volume example framework. We should also ask Gabriel and others (Arango?) who have built frameworks with PVs/DRs for feedback.",Documentation,Major,Resolved,"2015-12-19 18:48:32","2015-12-19 18:48:32",3
"Apache Mesos","PersistentVolumeTest.BadACLDropCreateAndDestroy is flaky",,Bug,Major,Resolved,"2015-12-19 18:18:39","2015-12-19 18:18:39",1
"Apache Mesos","Add an example bug due to a lack of defer() to the defer() documentation","In the past, some bugs have been introduced into the codebase due to a lack of {{defer()}} where it should have been used. It would be useful to add an example of this to the {{defer()}} documentation.",Documentation,Minor,Resolved,"2015-12-19 00:22:14","2015-12-19 00:22:14",2
"Apache Mesos","Write new logging-related documentation","This should include: * Default logging behavior for master, agent, framework, executor, task. * Master/agent: ** A summary of log-related flags. ** {{glog}} specific options. * Separation of master/agent logs from container logs. * The {{ContainerLogger}} module.",Documentation,Major,Resolved,"2015-12-18 23:59:22","2015-12-18 23:59:22",3
"Apache Mesos","Document that frameworks that participate in a role should cooperate",,Documentation,Minor,Resolved,"2015-12-18 23:28:53","2015-12-18 23:28:53",2
"Apache Mesos","Race in SSL socket shutdown ","libprocess Socket shares the ownership of the file descriptor with libevent. In the destructor of the libprocess libevent_ssl socket, we call ssl shutdown which is executed asynchronously. This causes the libprocess socket file descriptor tobe closed (and possibly reused) when the same file descriptor could be used bylibevent/ssl. Since we set the shutdown options as SSL_RECEIVED_SHUTDOWN, we leave the any write operations to continue with possibly closed file descriptor.  This issue manifests as junk characters written to the file that has been handled the closed socket file descriptor (by OS) that has the above issue.",Bug,Major,Resolved,"2015-12-18 23:13:38","2015-12-18 23:13:38",5
"Apache Mesos","Test case(s) for weights + allocation behavior","As far as I can see, we currently have NO test cases for behavior when weights are defined.",Task,Major,Resolved,"2015-12-18 19:30:34","2015-12-18 19:30:34",2
"Apache Mesos","Disk Resource Reservation is NOT Enforced for Persistent Volumes","If I create a persistent volume on a reserved disk resource, I am able to write data in excess of my reserved size.  Disk resource reservation should be enforced just as cpus and mem reservations are enforced.",Bug,Major,Resolved,"2015-12-18 19:12:38","2015-12-18 19:12:38",3
"Apache Mesos","Add dynamic reservation tests with no principal","Currently, there exist no dynamic reservation tests that include authorization of a framework that is registered with no principal. This should be added in order to more comprehensively test the dynamic reservation code.",Improvement,Major,Resolved,"2015-12-18 07:23:26","2015-12-18 07:23:26",1
"Apache Mesos","MesosContainerizer* tests leak FDs (pipes)","If you run: {{bin/mesos-tests.sh --gtest_filter=*MesosContainerizer* --gtest_repeat=-1 --gtest_break_on_failure}}  And then check: {{lsof | grep mesos}}  The number of open pipes will grow linearly with the number of test repetitions.",Bug,Major,Accepted,"2015-12-18 01:32:52","2015-12-18 01:32:52",2
"Apache Mesos","Port `process/file.hpp`",,Bug,Major,Resolved,"2015-12-17 22:10:39","2015-12-17 22:10:39",3
"Apache Mesos","Add documentation for API Versioning","Currently, we don't have any documentation for:  - How Mesos implements API versioning ? - How are protobufs versioned and how does mesos handle them internally ? - What do contributors need to do when they make a change to a external user facing protobuf ?  The relevant design doc: https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b ",Bug,Major,Resolved,"2015-12-17 19:59:07","2015-12-17 19:59:07",3
"Apache Mesos","Investigate switching to fixed point scalar resources",,Task,Major,Resolved,"2015-12-17 19:28:26","2015-12-17 19:28:26",5
"Apache Mesos","Create a Design Doc for dynamic weights.","A short design doc for dynamic weights, it will focus on /weights API and the changes to the allocator API.",Documentation,Major,Resolved,"2015-12-17 11:19:24","2015-12-17 11:19:24",3
"Apache Mesos","Avoid using absolute URLs in documentation pages","Links from one documentation page to another should not use absolute URLs (e.g., {{http://mesos.apache.org/documentation/latest/...}}) for several good reasons. For instance, absolute URLs break when the docs are generated/previewed locally.",Documentation,Minor,Resolved,"2015-12-17 03:22:04","2015-12-17 03:22:04",1
"Apache Mesos","Serialize docker v1 image spec as protobuf","Currently we only support v2 docker manifest serialization method. When we read docker image spec locally from disk, we should be able to parse v1 docker manifest as protobuf, which will make it easier to gather runtime config and other necessary info.",Improvement,Major,Resolved,"2015-12-17 02:31:02","2015-12-17 02:31:02",2
"Apache Mesos","Jenkins builds for Centos fail with missing 'which' utility and incorrect 'java.home'","Jenkins builds are now consistently failing for centos 7, withe the failure:  checking value of Java system property 'java.home'... /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64/jre configure: error: could not guess JAVA_HOME  They also fail early on during 'bootstrap' with a missing 'which' command.  The solution is to update support/docker_build.sh to install 'which' as well as make sure the proper versions of java are installed during the installation process.  The problem here is that we install maven BEFORE installing java-1.7.0-openjdk-devel, causing maven to pull in a dependency on java-1.8.0-openjdk. This causes problems with finding the proper java.home in our mesos/configure script because of the mismatch between the most up to date jre (1.8.0) and the most up to date development tools (1.7.0).  We can either update the script to pull in the 1.8 devel tools or move our dependence on maven until AFTER our installation of java-1.7.0-openjdk-devel.  Unclear what the best solution is.",Bug,Major,Resolved,"2015-12-16 21:58:54","2015-12-16 21:58:54",3
"Apache Mesos","Move operator<< definitions to .cpp files and include <iosfwd> in .hpp where possible.","We often include complex headers like {{<ostream>}} in .hpp files to define {{operator<<()}} inline (e.g. mesos/authorizer/authorizer.hpp). Instead, we can move definitions to corresponding .cpp files and replace stream headers with {{iosfwd}}, for example, this is partially done for {{URI}} in mesos/uri/uri.hpp.",Improvement,Minor,Resolved,"2015-12-16 15:37:09","2015-12-16 15:37:09",3
"Apache Mesos","Extend `Master` to authorize persistent volumes","This ticket is the second in a series that adds authorization support for persistent volumes.  Methods {{Master::authorizeCreateVolume()}} and {{Master::authorizeDestroyVolume}} must be added to allow the Master to authorize these operations.",Bug,Major,Resolved,"2015-12-15 21:08:26","2015-12-15 21:08:26",1
"Apache Mesos","Add persistent volume support to the Authorizer","This ticket is the first in a series that adds authorization support for persistent volume creation and destruction.  Persistent volumes should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Create}} and {{Destroy}} into the ACL.    ACLs for volume creation and destruction must be added to {{authorizer.proto}}, and the appropriate function overloads must be added to the Authorizer.",Bug,Major,Resolved,"2015-12-15 21:04:40","2015-12-15 21:04:40",1
"Apache Mesos","Create a user doc for Executor HTTP API","We need a user doc similar to the corresponding one for the Scheduler HTTP API.",Bug,Major,Resolved,"2015-12-15 19:18:18","2015-12-15 19:18:18",3
"Apache Mesos","ContentType/SchedulerTest.Decline is slow.","The {{ContentType/SchedulerTest.Decline}} test takes more than {{1s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:52:32","2015-12-15 17:52:32",1
"Apache Mesos","HookTest.VerifySlaveLaunchExecutorHook is slow.","The {{HookTest.VerifySlaveLaunchExecutorHook}} test takes more than {{5s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:50:53","2015-12-15 17:50:53",1
"Apache Mesos","GarbageCollectorIntegrationTest.Restart is slow","The {{GarbageCollectorIntegrationTest.Restart}} test takes more than {{5s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:48:17","2015-12-15 17:48:17",3
"Apache Mesos","OversubscriptionTest.RemoveCapabilitiesOnSchedulerFailover is slow.","The {{OversubscriptionTest.RemoveCapabilitiesOnSchedulerFailover}} test takes more than {{1s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:46:34","2015-12-15 17:46:34",1
"Apache Mesos","OversubscriptionTest.UpdateAllocatorOnSchedulerFailover is slow.","The {{OversubscriptionTest.UpdateAllocatorOnSchedulerFailover}} test takes more than {{1s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:45:32","2015-12-15 17:45:32",1
"Apache Mesos","MasterTest.OfferTimeout is slow.","The {{MasterTest.OfferTimeout}} test takes more than {{1s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:39:49","2015-12-15 17:39:49",1
"Apache Mesos","MasterTest.LaunchCombinedOfferTest is slow.","The {{MasterTest.LaunchCombinedOfferTest}} test takes more than {{2s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:38:26","2015-12-15 17:38:26",1
"Apache Mesos","MasterTest.MasterInfoOnReElection is slow.","The {{MasterTest.MasterInfoOnReElection}} test takes more than {{1s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:37:13","2015-12-15 17:37:13",1
"Apache Mesos","MasterTest.RecoverResources is slow.","The {{MasterTest.RecoverResources}} test takes more than {{1s}} to finish on my Mac OS 10.10.4: ",Improvement,Minor,Resolved,"2015-12-15 17:35:59","2015-12-15 17:35:59",1
"Apache Mesos","Log recover tests are slow.","On Mac OS 10.10.4, some tests take longer than {{1s}} to finish: ",Improvement,Minor,Resolved,"2015-12-15 17:23:42","2015-12-15 17:23:42",1
"Apache Mesos","Rename shutdown_frameworks to teardown_frameworks","The mesos is now using teardown framework to shutdown a framework but the acls are still using shutdown_framework, it is better to rename shutdown_framework to teardown_framework for acl to keep consistent.  This is a post review request for https://reviews.apache.org/r/40829/",Bug,Minor,Resolved,"2015-12-15 05:37:26","2015-12-15 05:37:26",2
"Apache Mesos","GMock warning in SlaveTest.ContainerizerUsageFailure","  Occurs deterministically for me on OSX 10.10",Bug,Major,Resolved,"2015-12-14 23:19:10","2015-12-14 23:19:10",1
"Apache Mesos","Implement container logger module metadata recovery","The {{ContainerLoggers}} are intended to be isolated from agent failover, in the same way that executors do not crash when the agent process crashes.  For default {{ContainerLogger}} s, like the {{SandboxContainerLogger}} and the (tentatively named) {{TruncatingSandboxContainerLogger}}, the log files are exposed during agent recovery regardless.  For non-default {{ContainerLogger}} s, the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs.  This can be implemented as part of {{Containerizer::recover}}.",Task,Major,Resolved,"2015-12-14 20:32:54","2015-12-14 20:32:54",3
"Apache Mesos","Clean up authentication implementation for quota","To authenticate quota requests we allowed {{QuotaHandler}} to call private {{Http::authenticate()}} function. Once MESOS-3231 lands we do not need neither this injection, nor {{authenticate()}} calls in the {{QuotaHandler}}.",Improvement,Major,Resolved,"2015-12-14 13:36:15","2015-12-14 13:36:15",1
"Apache Mesos","Reserve/UnReserve Dynamic Reservation Endpoints allow reservations on non-existing roles","When working with Dynamic reservations via the /reserve and /unreserve endpoints, it is possible to reserve resources for roles that have not been specified via the --roles flag on the master.  However, these roles are not usable because the roles have not been defined, nor are they added to the list of roles available.   Per the mailing list, changing roles after the fact is not possible at this time. (That may be another JIRA), more importantly, the /reserve and /unreserve end points should not allow reservation of roles not specified by --roles.  ",Bug,Minor,Resolved,"2015-12-13 18:27:31","2015-12-13 18:27:31",2
"Apache Mesos","Modularize plain-file logging for executor/task logs launched with the Docker Containerizer","Adding a hook inside the Docker containerizer is slightly more involved than the Mesos containerizer.  Docker executors/tasks perform plain-file logging in different places depending on whether the agent is in a Docker container itself || Agent || Code || | Not in container | {{DockerContainerizerProcess::launchExecutorProcess}} | | In container | {{Docker::run}} in a {{mesos-docker-executor}} process |  This means a {{ContainerLogger}} will need to be loaded or hooked into the {{mesos-docker-executor}}.  Or we will need to change how piping in done in {{mesos-docker-executor}}.",Task,Major,Resolved,"2015-12-11 23:51:29","2015-12-11 23:51:29",3
"Apache Mesos","Add a ContainerLogger module that restrains log sizes","One of the major problems this logger module aims to solve is overflowing executor/task log files.  Log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).  We should add a {{ContainerLogger}} module that truncates logs as it reaches a configurable maximum size.  Additionally, we should determine if the web UI's {{pailer}} needs to be changed to deal with logs that are not append-only.  This will be a non-default module which will also serve as an example for how to implement the module.",Improvement,Major,Resolved,"2015-12-11 23:40:23","2015-12-11 23:40:23",3
"Apache Mesos","Document how the fetcher can reach across a proxy connection.","The fetcher uses libcurl for downloading content from HTTP, HTTPS, etc. There is no source code in the pertinent parts of net.hpp that deals with proxy settings. However, libcurl automatically picks up certain environment variables and adjusts its settings accordingly. See man libcurl-tutorial for details. See section Proxies, subsection Environment Variables. If you follow this recipe in your Mesos agent startup script, you can use a proxy.   We should document this in the fetcher (cache) doc (http://mesos.apache.org/documentation/latest/fetcher/). ",Documentation,Major,Resolved,"2015-12-11 09:44:39","2015-12-11 09:44:39",1
"Apache Mesos","Refactor sorter factories in allocator and improve comments around them.","For clarity we want to refactor the factory section in the allocator and explain the purpose (and necessity) of all sorters.",Improvement,Major,Resolved,"2015-12-11 08:40:08","2015-12-11 08:40:08",3
"Apache Mesos","Ensure `Content-Type` field is set for some responses.","As pointed out by [~<USER> in https://reviews.apache.org/r/40905/, we should make sure we set the {{Content-Type}} files for some responses.",Improvement,Major,Resolved,"2015-12-11 08:33:19","2015-12-11 08:33:19",3
"Apache Mesos","Construct the error string in `MethodNotAllowed`.","Consider constructing the error string in {{MethodNotAllowed}} rather than at the invocation site. Currently we want all error messages follow the same pattern, so instead of writing  we can write something like  ",Improvement,Minor,Resolved,"2015-12-11 08:30:05","2015-12-11 08:30:05",3
"Apache Mesos","Add tests for quotas + empty roles (no registered frameworks)",,Task,Major,Resolved,"2015-12-10 18:36:03","2015-12-10 18:36:03",2
"Apache Mesos","Fix possible race conditions in registry client tests.","RegistryClient tests show flakiness which manifests as socket timeouts or unexpected buffer showing up in the blobs. Investigate them for possible race conditions.",Improvement,Major,Resolved,"2015-12-10 17:50:12","2015-12-10 17:50:12",5
"Apache Mesos","Add field VIP to message Port","We would like to extend the Mesos protocol buffer 'Port' to include an optional repeated string named VIP - to map it to a well known virtual IP, or virtual hostname for discovery purposes.  We also want this field exposed in DiscoveryInfo in state.json.",Wish,Trivial,Resolved,"2015-12-10 15:58:28","2015-12-10 15:58:28",2
"Apache Mesos","Clean up libprocess gtest macros","This ticket is regarding the libprocess gtest helpers in {{3rdparty/libprocess/include/process/gtest.hpp}}.  The pattern in this file seems to be a set of macros:  * {{AWAIT_ASSERT_<STATE>_FOR}} * {{AWAIT_ASSERT_<STATE>}} -- default of 15 seconds * {{AWAIT_<STATE>\_FOR}} -- alias for {{AWAIT_ASSERT_<STATE>_FOR}} * {{AWAIT_<STATE>}} -- alias for {{AWAIT_ASSERT_<STATE>}} * {{AWAIT_EXPECT_<STATE>_FOR}} * {{AWAIT_EXPECT_<STATE>}} -- default of 15 seconds  (1) {{AWAIT_EQ_FOR}} should be added for completeness.  (2) In {{gtest}}, we've got {{EXPECT_EQ}} as well as the {{bool}}-specific versions: {{EXPECT_TRUE}} and {{EXPECT_FALSE}}.  We should adopt this pattern in these helpers as well. Keeping the pattern above in mind, the following are missing:  * {{AWAIT_ASSERT_TRUE_FOR}} * {{AWAIT_ASSERT_TRUE}} * {{AWAIT_ASSERT_FALSE_FOR}} * {{AWAIT_ASSERT_FALSE}} * {{AWAIT_EXPECT_TRUE_FOR}} * {{AWAIT_EXPECT_FALSE_FOR}}  (3) There are HTTP response related macros at the bottom of the file, e.g. {{AWAIT_EXPECT_RESPONSE_STATUS_EQ}}, however these are missing their {{ASSERT}} counterparts.  -(4) The reason for (3) presumably is because we reach for {{EXPECT}} over {{ASSERT}} in general due to the test suite crashing behavior of {{ASSERT}}. If this is the case, it would be worthwhile considering whether macros such as {{AWAIT_READY}} should alias {{AWAIT_EXPECT_READY}} rather than {{AWAIT_ASSERT_READY}}.-  (5) There are a few more missing macros, given {{AWAIT_EQ_FOR}} and {{AWAIT_EQ}} which aliases to {{AWAIT_ASSERT_EQ_FOR}} and {{AWAIT_ASSERT_EQ}} respectively, we should also add {{AWAIT_TRUE_FOR}}, {{AWAIT_TRUE}}, {{AWAIT_FALSE_FOR}}, and {{AWAIT_FALSE}} as well.",Task,Major,Resolved,"2015-12-10 12:05:52","2015-12-10 12:05:52",2
"Apache Mesos","Implement `WindowsError` to correspond with `ErrnoError`.","In the C standard library, `errno` records the last error on a thread. You can pretty-print it with `strerror`.  In Stout, we report these errors with `ErrnoError`.  The Windows API has something similar, called `GetLastError()`. The way to pretty-print this is hilariously unintuitive and terrible, so in this case it is actually very beneficial to wrap it with something similar to `ErrnoError`, maybe called `WindowsError`.",Bug,Major,Resolved,"2015-12-10 02:14:08","2015-12-10 02:14:08",5
"Apache Mesos","HTTPConnectionTest.ClosingResponse is flaky","Output of the test: ",Bug,Minor,Resolved,"2015-12-10 01:55:08","2015-12-10 01:55:08",1
"Apache Mesos","Implement `os::mkdtemp` for Windows","Used basically exclusively for testing, this insecure and otherwise-not-quite-suitable-for-prod function needs to work to run what will eventually become the FS tests.",Bug,Major,Resolved,"2015-12-10 01:14:33","2015-12-10 01:14:33",5
"Apache Mesos","`os::strerror_r` breaks the Windows build","`os::strerror_r` does not exist on Windows.",Bug,Major,Resolved,"2015-12-10 00:58:36","2015-12-10 00:58:36",1
"Apache Mesos","Design document for interactive terminal for mesos containerizer","As a first step to address the use cases, propose a design document covering the requirement, design and implementation details.",Task,Major,Accepted,"2015-12-09 17:05:05","2015-12-09 17:05:05",4
"Apache Mesos","Quota doesn't allocate resources on slave joining.","See attached patch. {{framework1}} is not allocated any resources, despite the fact that the resources on {{agent2}} can safely be allocated to it without risk of violating {{quota1}}. If I understand the intended quota behavior correctly, this doesn't seem intended.  Note that if the framework is added _after_ the slaves are added, the resources on {{agent2}} are allocated to {{framework1}}.",Bug,Blocker,Resolved,"2015-12-09 01:30:44","2015-12-09 01:30:44",5
"Apache Mesos","parallel make tests does not build all test targets","When inside 3rdparty/libprocess: Running {{make -j8 tests}} from a clean build does not yield the {{libprocess-tests}} binary. Running it a subsequent time triggers more compilation and ends up yielding the {{libprocess-tests}} binary. This suggests the {{test}} target is not being built correctly.",Bug,Major,Resolved,"2015-12-08 20:40:56","2015-12-08 20:40:56",1
"Apache Mesos","Allow interactive terminal for mesos containerizer","Today mesos containerizer does not have a way to run tasks that require interactive sessions. An example use case is running a task that requires a manual password entry from an operator. Another use case could be debugging (gdb). ",Story,Major,Accepted,"2015-12-08 20:11:00","2015-12-08 20:11:00",10
"Apache Mesos","Create light-weight executor only and scheduler only mesos eggs","Currently, when running tasks in docker containers, if the executor uses the mesos.native python library, the execution environment inside the container (OS, native libs, etc) must match the execution environment outside the container fairly closely in order to load the mesos.so library.  The solution here can be to introduce a much lighter weight python egg, mesos.executor, which only includes code (and dependencies) needed to create and run an MesosExecutorDriver.  Executors can then use this native library instead of mesos.native.",Improvement,Major,Resolved,"2015-12-07 22:13:46","2015-12-07 22:13:46",5
"Apache Mesos","Modularize existing plain-file logging for executor/task logs launched with the Mesos Containerizer","Once a module for executor/task output logging has been introduced, the default module will mirror the existing behavior.  Executor/task stdout/stderr is piped into files within the executor's sandbox directory.  The files are exposed in the web UI, via the {{/files}} endpoint.",Task,Major,Resolved,"2015-12-07 21:01:42","2015-12-07 21:01:42",2
"Apache Mesos","Introduce a module for logging executor/task output","Existing executor/task logs are logged to files in their sandbox directory, with some nuances based on which containerizer is used (see background section in linked document).  A logger for executor/task logs has the following requirements: * The logger is given a command to run and must handle the stdout/stderr of the command. * The handling of stdout/stderr must be resilient across agent failover.  Logging should not stop if the agent fails. * Logs should be readable, presumably via the web UI, or via some other module-specific UI.",Task,Major,Resolved,"2015-12-07 20:53:37","2015-12-07 20:53:37",5
"Apache Mesos","Implement implicit roles","See also design doc: MESOS-4000.",Improvement,Major,Resolved,"2015-12-07 19:21:14","2015-12-07 19:21:14",5
"Apache Mesos","Add tests for quota authentication and authorization.",,Task,Major,Resolved,"2015-12-07 17:14:13","2015-12-07 17:14:13",3
"Apache Mesos","Continue test suite execution across crashing tests.","Currently, mesos-tests.sh exits when a test crashes. This is inconvenient when trying to find out all tests that fail.   mesos-tests.sh should rate a test that crashes as failed and continue the same way as if the test merely returned with a failure result and exited properly.",Improvement,Major,Accepted,"2015-12-07 14:50:32","2015-12-07 14:50:32",8
"Apache Mesos","Tests for master failover in presence of quota.",,Task,Major,Accepted,"2015-12-07 09:09:32","2015-12-07 09:09:32",5
"Apache Mesos","Expose recovery parameters from Hierarchical allocator","While implementing recovery in the hierarchical allocator, we introduced some internal constants that influence the recovery process: {{ALLOCATION_HOLD_OFF_RECOVERY_TIMEOUT}} and {{AGENT_RECOVERY_FACTOR}}. We should expose these parameters for operators to configure.  However, I am a bit reluctant to expose them as master flags, because they are implementation specific. It would be nice to combine all hierarchical allocator-related flags into one (maybe JSON) file, similar to how we do it for modules.",Improvement,Major,Accepted,"2015-12-07 09:06:16","2015-12-07 09:06:16",3
"Apache Mesos","libevent_ssl_socket assertion fails ","Have been seeing the following socket  receive error frequently:    In this case this was a HTTP get over SSL. The url being:  https://dseasb33srnrn.cloudfront.net:443/registry-v2/docker/registry/v2/blobs/sha256/44/44be94a95984bb47dc3a193f59bf8c04d5e877160b745b119278f38753a6f58f/data?Expires=1449259252&Signature=Q4CQdr1LbxsiYyVebmetrx~lqDgQfHVkGxpbMM3PoISn6r07DXIzBX6~tl1iZx9uXdfr~5awH8Kxwh-y8b0dTV3mLTZAVlneZlHbhBAX9qbYMd180-QvUvrFezwOlSmX4B3idvo-zK0CarUu3Ev1hbJz5y3olwe2ZC~RXHEwzkQ_&Key-Pair-Id=APKAJECH5M7VWIS5YZ6Q   *Steps to reproduce:*  1. Run master 2. Run slave from your build directory as  as:     3. Run mesos-execute from your build directory as :  ",Bug,Major,Resolved,"2015-12-04 19:32:43","2015-12-04 19:32:43",8
"Apache Mesos","ReservationTest.ACLMultipleOperations is flaky","Observed from the CI: https://builds.apache.org/job/Mesos/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu%3A14.04,label_exp=docker%7C%7CHadoop/1319/changes",Bug,Major,Resolved,"2015-12-04 08:53:06","2015-12-04 08:53:06",2
"Apache Mesos","Agent should not return partial state when a request is made to /state endpoint during recovery.","Currently when a user is hitting /state.json on the agent, it may return partial state if the agent has failed over and is recovering. There is currently no clear way to tell if this is the case when looking at a response, so the user may incorrectly interpret the agent as being empty of tasks.  We could consider exposing the 'state' enum of the agent in the endpoint:    This may be a bit tricky to maintain as far as backwards-compatibility of the endpoint, if we were to alter this enum.  Exposing this would allow users to be more informed about the state of the agent.",Task,Major,Resolved,"2015-12-03 22:43:32","2015-12-03 22:43:32",3
"Apache Mesos","Add ContainerInfo to internal Task protobuf.","In what seems like an oversight, when ContainerInfo was added to TaskInfo, it was not added to our internal Task protobuf.  Also, unlike the agent, it appears that the master does not use protobuf::createTask. We should try remove the manual construction in the master in favor of construction through protobuf::createTask.  Partial contents of ContainerInfo should be exposed through state endpoints on the master and the agent. ",Task,Major,Resolved,"2015-12-03 21:34:54","2015-12-03 21:34:54",3
"Apache Mesos","Investigate remaining flakiness in MasterMaintenanceTest.InverseOffersFilters","Per comments in MESOS-3916, the fix for that issue decreased the degree of flakiness, but it seems that some intermittent test failures do occur -- should be investigated.  *Flakiness in task acknowledgment*   This is a race between [launching and acknowledging two tasks|https://github.com/apache/mesos/blob/75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5/src/tests/master_maintenance_tests.cpp#L1486-L1517].  The status updates for each task are not necessarily received in the same order as launching the tasks.  *Flakiness in first inverse offer filter* See [this comment in MESOS-3916|https://issues.apache.org/jira/browse/MESOS-3916?focusedCommentId=15027478&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15027478] for the explanation.  The related logs are above the comment.",Bug,Minor,Resolved,"2015-12-03 16:53:08","2015-12-03 16:53:08",1
"Apache Mesos","Do not use `Resource.role` for resources in quota request.","To be consistent with other operator endpoints and to adhere to the principal of least surprise, move role from each {{Resource}} in quota set request to the request itself.   {{Resource.role}} is used for reserved resources. Since quota is not a direct reservation request, to avoid confusion we shall not reuse this field for communicating the role for which quota should be reserved.  Food for thought: Shall we try to keep internal storage protobufs as close as possible to operator's JSON to provide some sort of a schema or decouple those two for the sake of flexibility?",Improvement,Major,Resolved,"2015-12-03 16:39:17","2015-12-03 16:39:17",1
"Apache Mesos","Respond with `MethodNotAllowed` if a request uses an unsupported method.","We are inconsistent right now in how we respond to endpoint requests with unsupported methods: both {{MethodNotAllowed}} and {{BadRequest}} are used. We are also not consistent in the error message we include in the body.  This ticket proposes use {{MethodNotAllowed}} with standardized message text.",Improvement,Minor,Resolved,"2015-12-03 10:48:08","2015-12-03 10:48:08",1
"Apache Mesos","MemoryPressureMesosTest tests fail on CentOS 6.6","{{MemoryPressureMesosTest.CGROUPS_ROOT_Statistics}} and {{MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery}} fail on CentOS 6.6. It seems that mounted cgroups are not properly cleaned up after previous tests, so multiple hierarchies are detected and thus an error is produced:  ",Bug,Major,Accepted,"2015-12-02 22:32:06","2015-12-02 22:32:06",3
"Apache Mesos","MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky","{code:title=Output from passed test} [----------] 1 test from MemoryPressureMesosTest 1+0 records in 1+0 records out 1048576 bytes (1.0 MB) copied, 0.000430889 s, 2.4 GB/s [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery I1202 11:09:14.319327  5062 exec.cpp:134] Version: 0.27.0 I1202 11:09:14.333317  5079 exec.cpp:208] Executor registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 Registered executor on ubuntu Starting task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162 sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done' Forked command at 5085 I1202 11:09:14.391739  5077 exec.cpp:254] Received reconnect request from slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 I1202 11:09:14.398598  5082 exec.cpp:231] Executor re-registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 Re-registered executor on ubuntu Shutting down Sending SIGTERM to process tree at pid 5085 Killing the following process trees: [  -+- 5085 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done   \--- 5086 dd count=512 bs=1M if=/dev/zero of=./temp  ] [       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (1096 ms)   Notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent.",Bug,Major,Resolved,"2015-12-02 19:19:09","2015-12-02 19:19:09",1
"Apache Mesos","Enable `Env` specified in docker image can be returned from docker pull","Currently docker pull only return an image structure, which only contains entrypoint info. We have docker inspect as a subprocess inside docker pull, which contains many other useful information of a docker image. We should be able to support returning environment variables information from the image.",Improvement,Major,Resolved,"2015-12-02 17:54:33","2015-12-02 17:54:33",3
"Apache Mesos","Install instructions for CentOS 6.6 lead to errors running `perf`.","After using the current installation instructions in the getting started documentation, {{perf}} will not run on CentOS 6.6 because the version of elfutils included in devtoolset-2 is not compatible with the version of {{perf}} installed by {{yum}}. Installing and using devtoolset-3, however (http://linux.web.cern.ch/linux/scientific6/docs/softwarecollections.shtml) fixes this issue. This could be resolved by updating the getting started documentation to recommend installing devtoolset-3.",Improvement,Minor,Resolved,"2015-12-02 01:59:53","2015-12-02 01:59:53",1
"Apache Mesos","ContentType/SchedulerTest is flaky.","SSL build, [Ubuntu 14.04|https://github.com/<USER>mesos-vagrant-ci/blob/master/ubuntu14/setup.sh], non-root test run.  ",Bug,Major,Resolved,"2015-12-01 00:06:03","2015-12-01 00:06:03",3
"Apache Mesos","RegistryClientTest.SimpleRegistryPuller is flaky","From ASF CI: https://builds.apache.org/job/Mesos/1289/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/console    Logs from a previous run that passed: ",Bug,Major,Resolved,"2015-11-30 18:50:33","2015-11-30 18:50:33",4
"Apache Mesos","Remove quota from Registry for quota remove request","When a remove quota requests hits the endpoint and passes validation, quota should be removed from the registry before the allocator is notified about the change.",Improvement,Major,Resolved,"2015-11-26 12:09:34","2015-11-26 12:09:34",1
"Apache Mesos","Introduce filter for non-revocable resources in `Resources`","{{Resources}} class defines some handy filters, like {{revocable()}}, {{unreserved()}}, and so on. This ticket proposes to add one more: {{nonRevocable()}}.",Improvement,Minor,Resolved,"2015-11-26 09:47:19","2015-11-26 09:47:19",1
"Apache Mesos","Introduce remove endpoint for quota","This endpoint is for removing quotas via the DELETE method.",Task,Major,Resolved,"2015-11-25 14:38:33","2015-11-25 14:38:33",3
"Apache Mesos","Introduce status endpoint for quota","This endpoint is for querying quota status via the GET method.",Task,Major,Resolved,"2015-11-25 14:35:56","2015-11-25 14:35:56",5
"Apache Mesos","RegistryClientTest.SimpleRegistryPuller doesn't compile with GCC 5.1.1","GCC 5.1.1 has {{-Werror=sign-compare}} in {{-Wall}} and stumbles over a comparison between signed and unsigned int in {{provisioner_docker_tests.cpp}}.",Bug,Trivial,Resolved,"2015-11-25 10:48:13","2015-11-25 10:48:13",1
"Apache Mesos","Support workdir runtime configuration from image ","We need to support workdir runtime configuration returned from image such as Dockerfile.",Improvement,Major,Resolved,"2015-11-24 23:25:54","2015-11-24 23:25:54",2
"Apache Mesos","Support default entrypoint and command runtime config in Mesos containerizer","We need to use the entrypoint and command runtime configuration returned from image to be used in Mesos containerizer.",Improvement,Major,Resolved,"2015-11-24 23:24:52","2015-11-24 23:24:52",3
"Apache Mesos","Pass agent work_dir to isolator modules","Some isolator modules can benefit from access to the agent's {{work_dir}}. For example, the DVD isolator (https://github.com/emccode/mesos-module-dvdi) is currently forced to mount external volumes in a hard-coded directory. Making the {{work_dir}} accessible to the isolator via {{Isolator::recover()}} would allow the isolator to mount volumes within the agent's {{work_dir}}. This can be accomplished by simply adding an overloaded signature for {{Isolator::recover()}} which includes the {{work_dir}} as a parameter.",Bug,Major,Resolved,"2015-11-24 21:12:38","2015-11-24 21:12:38",1
"Apache Mesos","ReservationEndpointsTest.UnreserveAvailableAndOfferedResources is flaky","Showed up on ASF CI: ( test kept looping on and on and ultimately failing the build after 300 minutes ) https://builds.apache.org/job/Mesos/COMPILER=gcc,CONFIGURATION=--verbose,OS=ubuntu%3A14.04,label_exp=docker%7C%7CHadoop/1269/changes    Logs from a good run:   ",Bug,Major,Resolved,"2015-11-24 19:57:50","2015-11-24 19:57:50",1
"Apache Mesos","Implicit roles: Design Doc",,Task,Major,Resolved,"2015-11-23 22:10:46","2015-11-23 22:10:46",2
"Apache Mesos","libprocess: document when, why defer() is necessary","Current rules around this are pretty confusing and undocumented, as evidenced by some recent bugs in this area.  Some example snippets in the mesos source code that were a result of this confusion and are indeed bugs:  1. https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/provisioner/docker/registry_client.cpp#L754  ",Documentation,Minor,Resolved,"2015-11-23 19:50:53","2015-11-23 19:50:53",1
"Apache Mesos","Refactor registry client/puller to avoid JSON and struct.","We should get rid of all JSON and struct for message passing as function returned type. By using the methods provided by spec.hpp to refactor all unnecessary JSON message and struct in registry client and registry puller. Also, remove all redundant check in registry client that are already checked by spec validation. ",Improvement,Major,Resolved,"2015-11-23 19:09:53","2015-11-23 19:09:53",3
"Apache Mesos","Tests for allocator recovery.","The allocator recover() call was introduced for correct recovery in presence of quota. We should add test verifying the correct behavior.",Task,Major,Open,"2015-11-23 10:26:49","2015-11-23 10:26:49",5
"Apache Mesos","Tests for rescinding offers for quota",,Task,Major,Resolved,"2015-11-23 10:15:07","2015-11-23 10:15:07",1
"Apache Mesos","Tests for quota support in `allocate()` function.",,Task,Major,Resolved,"2015-11-23 10:06:17","2015-11-23 10:06:17",3
"Apache Mesos","Tests for quota request validation","Tests should include: * JSON validation; * Absence of irrelevant fields; * Semantic validation.",Task,Major,Resolved,"2015-11-23 09:59:42","2015-11-23 09:59:42",3
"Apache Mesos","Implement recovery in the Hierarchical allocator","The built-in Hierarchical allocator should implement the recovery (in the presence of quota).",Task,Major,Resolved,"2015-11-23 09:48:47","2015-11-23 09:48:47",3
"Apache Mesos","Replace `QuotaInfo` with `Quota` in allocator interface","After introduction of C++ wrapper `Quota` for `QuotaInfo`, all allocator methods using `QuotaInfo` should be updated.",Improvement,Major,Resolved,"2015-11-23 00:24:28","2015-11-23 00:24:28",3
"Apache Mesos","C++ HTTP Scheduler Library does not work with SSL enabled","The C++ HTTP scheduler library does not work against Mesos when SSL is enabled (without downgrade).  The fix should be simple: * The library should detect if SSL is enabled. * If SSL is enabled, connections should be made with HTTPS instead of HTTP.",Bug,Major,Resolved,"2015-11-20 20:01:16","2015-11-20 20:01:16",3
"Apache Mesos","SSL build of mesos causes flaky testsuite.","When running the tests of an SSL build of Mesos on CentOS 7.1, I see spurious test failures that are, so far, not reproducible.  The following tests did fail for me in complete runs but did seem fine when running them individually, in repetition.           Vagrantfile generator: ",Bug,Major,Resolved,"2015-11-20 19:23:33","2015-11-20 19:23:33",5
"Apache Mesos","CgroupsAnyHierarchyMemoryPressureTest tests fail on CentOS 6.7.","    The Vagrant generator script: ",Bug,Major,Resolved,"2015-11-20 17:48:57","2015-11-20 17:48:57",2
"Apache Mesos","Failing 'make distcheck' on Mac OS X 10.10.5, also 10.11.","Non-root 'make distcheck.  ",Bug,Major,Accepted,"2015-11-20 16:56:13","2015-11-20 16:56:13",2
"Apache Mesos","Failing 'make distcheck' on Debian 8, somehow SSL-related.","As non-root: make distcheck.   ",Bug,Major,Resolved,"2015-11-20 15:17:35","2015-11-20 15:17:35",3
"Apache Mesos","Add integration tests for quota","These tests should verify whether quota implements declared functionality. This will require the whole pipeline: master harness code and an allocator implementation (in contrast to to isolated master and allocator tests).",Task,Major,Accepted,"2015-11-20 11:50:48","2015-11-20 11:50:48",8
"Apache Mesos","Ensure resources in `QuotaInfo` protobuf do not contain `role`","{{QuotaInfo}} protobuf currently stores per-role quotas, including {{Resource}} objects. These resources are neither statically nor dynamically reserved, hence they may not contain {{role}} field. We should ensure this field is unset, as well as update validation routine for {{QuotaInfo}}",Bug,Major,Resolved,"2015-11-20 11:32:36","2015-11-20 11:32:36",3
"Apache Mesos","LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs and LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs_Big_Quota fail on Debian 8.","sudo ./bin/mesos-test.sh --gtest_filter=LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs   ",Bug,Blocker,Resolved,"2015-11-20 11:25:53","2015-11-20 11:25:53",2
"Apache Mesos","Standardize quota endpoints","To be consistent with other operator endpoints, require a single JSON object in the request as opposed to key-value pairs encoded in a string.",Improvement,Major,Resolved,"2015-11-19 18:23:27","2015-11-19 18:23:27",3
"Apache Mesos","Make HDFS tool wrappers asynchronous.","The existing HDFS tool wrappers (src/hdfs/hdfs.hpp) are synchronous. They use os::shell to shell out the 'hadoop' commands. This makes it very hard to be reused at other locations in the code base.  The URI fetcher HDFS plugin will try to re-use the existing HDFS tool wrappers. In order to do that, we need to make it asynchronous first.",Task,Major,Resolved,"2015-11-18 22:17:45","2015-11-18 22:17:45",5
"Apache Mesos","User CGroup Isolation tests fail on Centos 6.","UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup and UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup fail on CentOS 6.6 with similar output when libevent and SSL are enabled.      ",Bug,Major,Resolved,"2015-11-18 17:03:19","2015-11-18 17:03:19",3
"Apache Mesos","Add operator documentation for /weight endpoint","This JIRA ticket will update the related doc to apply to dynamic weights, and add an new operator guide for dynamic weights which describes basic usage of the /weights endpoint.",Task,Major,Resolved,"2015-11-18 08:39:56","2015-11-18 08:39:56",2
"Apache Mesos","Support dynamic weight in allocator","This JIRA will focus on update the allocator API to support weight update of a role.",Task,Major,Resolved,"2015-11-18 05:24:23","2015-11-18 05:24:23",5
"Apache Mesos","/reserve and /unreserve should be permissive under a master without authentication.","Currently, the {{/reserve}} and {{/unreserve}} endpoints do not work without authentication enabled on the master. When authentication is disabled on the master, these endpoints should just be permissive.",Bug,Major,Resolved,"2015-11-18 00:46:38","2015-11-18 00:46:38",1
"Apache Mesos","ubsan error in net::IP::create(sockaddr const&): misaligned address","Running ubsan from GCC 5.2 on the current Mesos unit tests yields this, among other problems:  ",Bug,Minor,Resolved,"2015-11-17 17:15:30","2015-11-17 17:15:30",2
"Apache Mesos","Consider allowing setting quotas for the default '*' role.","Investigate use cases and implications of the possibility to set quota for the '*' role. For example, having quota for '*' set can effectively reduce the scope of the quota capacity heuristic.",Task,Major,Resolved,"2015-11-17 15:18:51","2015-11-17 15:18:51",2
"Apache Mesos","Test DockerContainerizerTest.ROOT_DOCKER_Launch_Executor fails.","   ",Bug,Major,Open,"2015-11-17 15:11:56","2015-11-17 15:11:56",2
"Apache Mesos","Document possible task state transitions for framework authors","We should document the possible ways in which the state of a task can evolve over time; what happens when an agent is partitioned from the master; and more generally, how we recommend that framework authors develop fault-tolerant schedulers and do task state reconciliation.",Documentation,Major,Resolved,"2015-11-17 06:42:33","2015-11-17 06:42:33",5
"Apache Mesos","Libprocess: Unify the initialization of the MetricsProcess and ReaperProcess","Related to this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L949-L950].  The {{MetricsProcess}} and {{ReaperProcess}} are global processes (singletons) which are initialized upon first use.  The two processes could be initialized alongside the {{gc}}, {{help}}, {{logging}}, {{profiler}}, and {{system}} (statistics) processes inside {{process::initialize}}.  This is also necessary for libprocess re-initialization.",Task,Major,Resolved,"2015-11-16 21:12:07","2015-11-16 21:12:07",3
"Apache Mesos","Automate the process of landing commits for committers","This script should do the following things  1) Apply a chain of reviews to a local branch 2) Push the commits upstream 3) Mark the reviews as submitted 4) Optionally close any attached JIRA tickets  ",Task,Major,Resolved,"2015-11-15 21:55:23","2015-11-15 21:55:23",3
"Apache Mesos","ROOT tests fail on Mesos 0.26 on Ubuntu/CentOS","Running {{0.26.0-rc1}} on both CentOS 7.1 and Ubuntu 14.04 with {{sudo}} privileges, causes segfaults when running Docker tests.  Logs attached.",Bug,Major,Resolved,"2015-11-15 04:21:38","2015-11-15 04:21:38",2
"Apache Mesos","Modularize URI fetcher plugin interface.  ","So that we can add custom URI fetcher plugins using modules.",Task,Major,Reviewable,"2015-11-13 20:37:57","2015-11-13 20:37:57",3
"Apache Mesos","Add HDFS based URI fetcher plugin.","This plugin uses HDFS client to fetch artifacts. It can support schemes like hdfs/hftp/s3/s3n  It'll shell out the hadoop command to do the actual fetching.",Task,Major,Resolved,"2015-11-13 20:35:24","2015-11-13 20:35:24",3
"Apache Mesos","Implement AuthN handling in Master for the Scheduler endpoint","If authentication(AuthN) is enabled on a master, frameworks attempting to use the HTTP Scheduler API can't register.    Authorization(AuthZ) is already supported for HTTP based frameworks.",Bug,Major,Resolved,"2015-11-13 20:05:27","2015-11-13 20:05:27",5
"Apache Mesos","MasterMaintenanceTest.InverseOffersFilters is flaky","Verbose Logs: ",Bug,Major,Resolved,"2015-11-13 16:49:01","2015-11-13 16:49:01",3
"Apache Mesos","Upgrade vendored Boost","We should upgrade the vendored version of Boost to a newer version. Benefits:  * -Should properly fix MESOS-688- * -Should fix MESOS-3799- * Generally speaking, using a more modern version of Boost means we can take advantage of bug fixes, optimizations, and new features.",Bug,Minor,Resolved,"2015-11-13 16:14:10","2015-11-13 16:14:10",5
"Apache Mesos","Disallow empty string roles","Having an empty role (empty string) looks like a terrible idea, but we do not prohibit it. I think we should add corresponding checks and update the docs to officially disallow empty roles.",Improvement,Minor,Accepted,"2015-11-13 12:24:54","2015-11-13 12:24:54",3
"Apache Mesos","Rescind offers in order to satisfy quota","When a quota request comes in, we may need to rescind a certain amount of outstanding offers in order to satisfy it. Because resources are allocated in the allocator, there can be a race between rescinding and allocating. This race makes it hard to determine the exact amount of offers that should be rescinded in the master.",Task,Major,Resolved,"2015-11-13 09:56:40","2015-11-13 09:56:40",3
"Apache Mesos","Add a `--force` flag to disable sanity check in quota","There are use cases when an operator may want to disable the sanity check for quota endpoints (MESOS-3074), even if this renders the cluster under quota. For example, an operator sets quota before adding more agents in order to make sure that no non-quota allocations from new agents are made. ",Task,Major,Resolved,"2015-11-13 09:04:11","2015-11-13 09:04:11",1
"Apache Mesos","Libprocess: Implement cleanup of the SocketManager in process::finalize","The {{socket_manager}} and {{process_manager}} are intricately tied together.  Currently, only the {{process_manager}} is cleaned up by {{process::finalize}}.  To clean up the {{socket_manager}}, we must close all sockets and deallocate any existing {{HttpProxy}} or {{Encoder}} objects.  And we should prevent further objects from being created/tracked by the {{socket_manager}}.  *Proposal* # Clean up all processes other than {{gc}}.  This will clear all links and delete all {{HttpProxy}} s while {{socket_manager}} still exists. # Close all sockets via {{SocketManager::close}}.  All of {{socket_manager}} 's state is cleaned up via {{SocketManager::close}}, including termination of {{HttpProxy}} (termination is idempotent, meaning that killing {{HttpProxy}} s via {{process_manager}} is safe). # At this point, {{socket_manager}} should be empty and only the {{gc}} process should be running.  (Since we're finalizing, assume there are no threads trying to spawn processes.)  {{socket_manager}} can be deleted. # {{gc}} can be deleted.  This is currently a leaked pointer, so we'll also need to track and delete that. # {{process_manager}} should be devoid of processes, so we can proceed with cleanup (join threads, stop the {{EventLoop}}, etc).",Task,Major,Resolved,"2015-11-13 00:33:16","2015-11-13 00:33:16",5
"Apache Mesos","isolator module headers depend on picojson headers","When trying to build an isolator module, stout headers end up depending on {{picojson.hpp}} which is not installed.  ",Bug,Major,Resolved,"2015-11-13 00:11:32","2015-11-13 00:11:32",3
"Apache Mesos","Five new docker-related slave flags are not covered by the configuration documentation.","These flags were added to slave/flags.cpp, but are not mentioned in docs/configuration.md:    add(&Flags::docker_auth_server,       docker_auth_server,       Docker authentication server,       auth.docker.io);    add(&Flags::docker_auth_server_port,       docker_auth_server_port,       Docker authentication server port,       443);   add(&Flags::docker_puller_timeout_secs,       docker_puller_timeout,       Timeout value in seconds for pulling images from Docker registry,       60);    add(&Flags::docker_registry,       docker_registry,       Default Docker image registry server host,       registry-1.docker.io);   add(&Flags::docker_registry_port,       docker_registry_port,       Default Docker registry server port,       443); ",Documentation,Minor,Resolved,"2015-11-12 15:43:28","2015-11-12 15:43:28",1
"Apache Mesos","Add authorization for '/create-volume' and '/destroy-volume' HTTP endpoints","This is the fourth in a series of tickets that adds authorization support for persistent volumes.  We need to add ACL authorization for the '/create-volume' and '/destroy-volume' HTTP endpoints. In other complementary work, authorization for frameworks performing {{CREATE}} and {{DESTROY}} operations is being added by MESOS-3065.  This will consist of adding authorization calls into the HTTP endpoint code in {{src/master/http.cpp}}, as well as tests for both failed & successful calls to '/create-volumes' and '/destroy-volumes' with authorization. We also must ensure that the {{principal}} field of {{Resource.DiskInfo.Persistence}} is being populated correctly.",Improvement,Major,Resolved,"2015-11-12 04:45:59","2015-11-12 04:45:59",2
"Apache Mesos","Enable mesos-reviewbot project on jenkins to use docker","As a first step to adding capability for building multiple configurations on reviewbot, we need to change the build scripts to use docker. ",Task,Major,Resolved,"2015-11-11 18:54:55","2015-11-11 18:54:55",3
"Apache Mesos","Wrong syntax and inconsistent formatting of JSON examples in flag documentation","The JSON examples in the documentation of the commandline flags ({{mesos-master.sh --help}} and {{mesos-slave.sh --help}}) don't have a consistent formatting. Furthermore, some examples aren't even compliant JSON because they have trailing commas were they shouldn't.",Bug,Minor,Resolved,"2015-11-11 14:11:39","2015-11-11 14:11:39",1
"Apache Mesos","Identify and implement test cases for handling a race between optimistic lender and tenant offers.","An example is the when lender launches the task on an agent followed by a  borrower launching a task on the same agent before the optimistic offer is rescinded. ",Bug,Major,Accepted,"2015-11-11 13:59:37","2015-11-11 13:59:37",13
"Apache Mesos","Identify and implement test cases for verifying eviction logic in the agent",,Bug,Major,Open,"2015-11-11 13:56:25","2015-11-11 13:56:25",13
"Apache Mesos","Add accounting for reservation slack in the allocator.","MESOS-XXX: Optimsistic accounter    MESOS-4146: flatten & allocationSlack for Optimistic Offer    MESOS-XXX: Allocate the allocation_slack resources to framework      Here's some consideration about `ALLOCATION_SLACK`:  1. 'Old' resources (available/total) did not include ALLOCATION_SLACK 2. After `Quota`, `remainingClusterResources.contains` should not check ALLOCATION_SLACK; if there no enough resources,  master can still offer ALLOCATION_SALCK resources. 3. In sorter, it'll not include ALLOCATION_SLACK; as those resources are borrowed from other role/framework 4. If either normal resources or ALLOCATION_SLACK resources are allocable/!filtered, it can be offered to framework 5. Currently, allocator will assign all ALLOCATION_SALCK resources in slave to one framework  MESOS-XXX: Update ALLOCATION_SLACK for dynamic reservation (updateAllocation)        MESOS-XXX: Add ALLOCATION_SLACK when slaver register/re-register (addSlave)      No need to handle `removeSlave`, it'll all related info from `slaves` including `optimistic`.  MESOS-XXX: return resources to allocator (recoverResources)  ",Bug,Major,Accepted,"2015-11-11 13:54:41","2015-11-11 13:54:41",13
"Apache Mesos","Update reservation slack allocator state during agent failover.",,Bug,Major,Accepted,"2015-11-11 13:51:55","2015-11-11 13:51:55",13
"Apache Mesos","Rebuild reservation slack allocator state during master failover.",,Bug,Major,Accepted,"2015-11-11 13:50:22","2015-11-11 13:50:22",13
"Apache Mesos","Implement tests for verifying allocator resource math.","Write a test to ensure that the allocator performs the reservation slack calculations correctly.",Bug,Major,Accepted,"2015-11-11 13:38:18","2015-11-11 13:38:18",8
"Apache Mesos","Add a helper function to the Agent to retrieve the list of executors that are using optimistically offered, revocable resources.","In the agent, add a helper function to get the list of the exeuctor using ALLOCATION_SLACK.  It's short term solution which is different the design document, because master did not have executor for command line executor. Send evicatble executors from master to slave will addess in post-MVP after MESOS-1718.   ",Bug,Major,Open,"2015-11-11 13:27:09","2015-11-11 13:27:09",5
"Apache Mesos","Add a helper function to the Agent to check available resources before launching a task. ","Launching a task using revocable resources should be funnelled through an accounting system:  * If a task is launched using revocable resources, the resources must not be in use when launching the task.  If they are in use, then the task should fail to start. * If a task is launched using reserved resources, the resources must be made available.  This means potentially evicting tasks which are using revocable resources.  Both cases could be implemented by adding a check in Slave::runTask, like a new helper method:  ",Bug,Major,Accepted,"2015-11-11 13:23:54","2015-11-11 13:23:54",5
"Apache Mesos","Add notion of evictable task to RunTaskMessage",,Bug,Major,Reviewable,"2015-11-11 13:17:40","2015-11-11 13:17:40",2
"Apache Mesos","Modify Oversubscription documentation to explicitly forbid the QoS Controller from killing executors running on optimistically offered resources.","The oversubcription documentation currently assumes that oversubscribed resources ({{USAGE_SLACK}}) are the only type of revocable resources.  Optimistic offers will add a second type of revocable resource ({{ALLOCATION_SLACK}}) that should not be acted upon by oversubscription components.  For example, the [oversubscription doc|http://mesos.apache.org/documentation/latest/oversubscription/] says the following: {quote} NOTE: If any resource used by a task or executor is revocable, the whole container is treated as a revocable container and can therefore be killed or throttled by the QoS Controller. {quote} which we may amend to something like: {quote} NOTE: If any resource used by a task or executor is revocable usage slack, the whole container is treated as an oversubscribed container and can therefore be killed or throttled by the QoS Controller. {quote}",Bug,Major,Accepted,"2015-11-11 13:16:40","2015-11-11 13:16:40",2
"Apache Mesos","Support distinguishing revocable resources in the Resource protobuf.","Add enum type into RevocableInfo:   * Framework need to assign RevocableInfo when launching task; if it’s not assign, use reserved resources. Framework need to identify which resources it’s using * Oversubscription resources need to assign the type by Agent (MESOS-3930) * Update Oversubscription document that OO has over-subscribe the Allocation Slack and recommend QoS to handle the usage slack only. (MESOS-3889)   ",Bug,Major,Open,"2015-11-11 13:15:07","2015-11-11 13:15:07",2
"Apache Mesos","Add a flag to master to enable optimistic offers. ",,Bug,Major,Reviewable,"2015-11-11 13:12:42","2015-11-11 13:12:42",3
"Apache Mesos","Corrected style in hierarchical allocator","The built-in allocator code has some style issues (namespaces in the .cpp file, unfortunate formatting) which should be corrected for readability.",Improvement,Minor,Resolved,"2015-11-11 09:57:47","2015-11-11 09:57:47",1
"Apache Mesos","Add support to apply-reviews.py to update SVN when necessary. ","{quote} That said, this can be automated as a step in apply-reviews script. For example, the script can check if something in site/ (or docs/ ?) is being committed and if yes, also do an svn update. @artem do you want to take this on as you revamp the apply-reviews script?  On Tue, Nov 10, 2015 at 1:23 AM, Adam Bordelon <<EMAIL>> wrote:  > Since it's still a manual process, the website is usually only updated a) > when we have a new release to announce, or b) when some other blog-worthy > content arises (e.g. MesosCon). {quote}  https://mail-archives.apache.org/mod_mbox/mesos-dev/201511.mbox/%3CCAAkWvAzqJQ9kmdpcAQ_F%2Bh1bNnzBrRkNQZXkwjWzTRiHUf66fg%40mail.gmail.com%3E",Bug,Major,Open,"2015-11-11 06:52:34","2015-11-11 06:52:34",3
"Apache Mesos","Libprocess: Implement process::Clock::finalize","Tracks this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L974-L975].  The {{Clock}} is initialized with a callback that, among other things, will dereference the global {{process_manager}} object.  When libprocess is shutting down, the {{process_manager}} is cleaned up.  Between cleanup and termination of libprocess, there is some chance that a {{Timer}} will time out and result in dereferencing {{process_manager}}.  *Proposal*  * Implement {{Clock::finalize}}.  This would clear: ** existing timers ** process-specific clocks ** ticks * Change {{process::finalize}}. *# Resume the clock.  (The clock is only paused during some tests.)  When the clock is not paused, the callback does not dereference {{process_manager}}. *# Clean up {{process_manager}}.  This terminates all the processes that would potentially interact with {{Clock}}. *# Call {{Clock::finalize}}.",Task,Major,Resolved,"2015-11-10 21:54:57","2015-11-10 21:54:57",3
"Apache Mesos","Implement `stout/os/pstree.hpp` on Windows",,Bug,Major,Resolved,"2015-11-10 19:10:41","2015-11-10 19:10:41",2
"Apache Mesos","Propose a guideline for log messages","We are rather inconsistent in the way we write log messages. It would be helpful to come up with a style and document various aspects of logs, including but not limited to: * Usage of backticks and/or single quotes to quote interpolated variables; * Usage of backticks and/or single quotes to quote types and other names; * Usage of tenses and other grammatical forms; * Proper way of nesting [error] messages;",Documentation,Major,Accepted,"2015-11-10 18:38:20","2015-11-10 18:38:20",5
"Apache Mesos","Incorrect and inconsistent include order for <gmock/gmock.h> and <gtest/gtest.h>.","We currently have an inconsistent (and mostly incorrect) include order for <gmock/gmock.h> and <gtest/gtest.h> (see below). Some files include them (incorrectly)  between the c and cpp standard header, while other correclt include them afterwards. According to the [Google Styleguide| https://google.github.io/styleguide/cppguide.html#Names_and_Order_of_Includes] the second include order is correct.   {code:title=external_containerizer_test.cpp} #include <unistd.h>  #include <gmock/gmock.h>  #include <string> ",Bug,Minor,Resolved,"2015-11-10 18:28:43","2015-11-10 18:28:43",1
"Apache Mesos","Draft operator documentation for quota","Draft an operator guide for quota which describes basic usage of the endpoints and few basic and advanced usage cases.",Task,Major,Resolved,"2015-11-10 17:25:07","2015-11-10 17:25:07",5
"Apache Mesos","Account dynamic reservations towards quota.","Dynamic reservations—whether allocated or not—should be accounted towards role's quota. This requires update in at least two places: * The built-in allocator, which actually satisfies quota; * The sanity check in the master.",Task,Critical,Resolved,"2015-11-10 16:45:42","2015-11-10 16:45:42",3
"Apache Mesos","Investigate recovery for the Hierarchical allocator","The built-in Hierarchical allocator should implement the recovery (in the presence of quota).",Task,Major,Resolved,"2015-11-10 16:36:43","2015-11-10 16:36:43",3
"Apache Mesos","Enhance allocator interface with the recovery() method","There are some scenarios (e.g. quota is set for some roles) when it makes sense to notify an allocator about the recovery. Introduce a method into the allocator interface that allows for this.",Task,Major,Resolved,"2015-11-10 16:33:27","2015-11-10 16:33:27",3
"Apache Mesos","Make apply-review.sh use apply-reviews.py",,Bug,Major,Resolved,"2015-11-10 08:00:43","2015-11-10 08:00:43",1
"Apache Mesos","Make `Resource.DiskInfo.Persistence.principal` a required field","A `principal` field is being added to the `Resource.DiskInfo.Persistence` message to facilitate authorization of persistent volume creation/deletion. In the long-run it should be a required field, but it's being initially introduced as optional to avoid breaking existing frameworks. The field should be changed to required at the end of a deprecation cycle.",Improvement,Major,Resolved,"2015-11-10 00:50:57","2015-11-10 00:50:57",1
"Apache Mesos","Simplify and/or document the libprocess initialization synchronization logic","Tracks this [TODO|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L749].  The [synchronization logic of libprocess|https://github.com/apache/mesos/commit/cd757cf75637c92c438bf4cd22f21ba1b5be702f#diff-128d3b56fc8c9ec0176fdbadcfd11fc2] [predates abstractions|https://github.com/apache/mesos/commit/6c3b107e4e02d5ba0673eb3145d71ec9d256a639#diff-0eebc8689450916990abe080d86c2acb] like {{process::Once}}, which is used in almost all other one-time initialization blocks.    The logic should be documented.  It can also be simplified (see the [review description|https://reviews.apache.org/r/39949/]).  Or it can be replaced with {{process::Once}}.",Task,Minor,Resolved,"2015-11-09 22:21:14","2015-11-09 22:21:14",1
"Apache Mesos","Investigate the requirements of programmatically re-initializing libprocess","This issue is for investigating what needs to be added/changed in {{process::finalize}} such that {{process::initialize}} will start on a clean slate.  Additional issues will be created once done.  Also see [the parent issue|MESOS-3820].  {{process::finalize}} should cover the following components: * {{__s__}} (the server socket) ** {{delete}} should be sufficient.  This closes the socket and thereby prevents any further interaction from it. * {{process_manager}} ** Related prior work: [MESOS-3158] ** Cleans up the garbage collector, help, logging, profiler, statistics, route processes (including [this one|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L963], which currently leaks a pointer). ** Cleans up any other {{spawn}} 'd process. ** Manages the {{EventLoop}}. * {{Clock}} ** The goal here is to clear any timers so that nothing can deference {{process_manager}} while we're finalizing/finalized.  It's probably not important to execute any remaining timers, since we're shutting down libprocess.  This means: *** The clock should be {{paused}} and {{settled}} before the clean up of {{process_manager}}. *** Processes, which might interact with the {{Clock}}, should be cleaned up next. *** A new {{Clock::finalize}} method would then clear timers, process-specific clocks, and {{tick}} s; and then {{resume}} the clock. * {{__address__}} (the advertised IP and port) ** Needs to be cleared after {{process_manager}} has been cleaned up.  Processes use this to communicate events.  If cleared prematurely, {{TerminateEvents}} will not be sent correctly, leading to infinite waits. * {{socket_manager}} ** The idea here is to close all sockets and deallocate any existing {{HttpProxy}} or {{Encoder}} objects. ** All sockets are created via {{__s__}}, so cleaning up the server socket prior will prevent any new activity. * {{mime}} ** This is effectively a static map. ** It should be possible to statically initialize it. * Synchronization atomics {{initialized}} & {{initializing}}. ** Once cleanup is done, these should be reset.  *Summary*: * Implement {{Clock::finalize}}.  [MESOS-3882] * Implement {{~SocketManager}}.  [MESOS-3910] * Make sure the {{MetricsProcess}} and {{ReaperProcess}} are reinitialized.  [MESOS-3934] * (Optional) Clean up {{mime}}. * Wrap everything up in {{process::finalize}}.",Task,Major,Resolved,"2015-11-09 22:13:04","2015-11-09 22:13:04",2
"Apache Mesos","Authorize set quota requests.","When quotas are requested they should authorize their roles. This ticket will authorize quota requests with ACLs. The existing authorization support that has been implemented in MESOS-1342 will be extended to add a `request_quotas` ACL.",Task,Major,Resolved,"2015-11-09 18:56:25","2015-11-09 18:56:25",5
"Apache Mesos","Authenticate quota requests","Quota requests need to be authenticated. This ticket will authenticate quota requests using credentials provided by the {{Authorization}} field of the HTTP request. This is similar to how authentication is implemented in {{Master::Http}}.",Task,Major,Resolved,"2015-11-09 18:52:56","2015-11-09 18:52:56",3
"Apache Mesos","Add github support to apply-reviews.py.",,Bug,Major,Resolved,"2015-11-09 14:57:47","2015-11-09 14:57:47",3
"Apache Mesos","Draft quota limits design document","In the design documents for Quota (https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I/edit#) the proposed MVP does not include quota limits. Quota limits represent an upper bound of resources that a role is allowed to use. The task of this ticket is to outline a design document on how to implement quota limits when the quota MVP is implemented.",Task,Major,Resolved,"2015-11-09 14:21:30","2015-11-09 14:21:30",5
"Apache Mesos","Draft Design Doc for first Step External Volume MVP","As part of the overall design doc for global resources we would like to introduce improvements for Docker Volume Driver isolator module (https://github.com/emccode/mesos-module-dvdi). Currently the isolator module is controlled by setting environment variables as follows:  We should develop a more structured way for passing these settings to the isolator module which is in line with the overall goal of global resources.",Task,Major,Resolved,"2015-11-09 12:55:30","2015-11-09 12:55:30",3
"Apache Mesos","Add mtime-related fetcher tests",,Bug,Major,Resolved,"2015-11-09 12:14:18","2015-11-09 12:14:18",2
"Apache Mesos","Finalize design for generalized Authorizer interface","Finalize the structure the interface and achieve consensus on the design doc proposed in MESOS-2949.  https://docs.google.com/document/d/1-XARWJFUq0r_TgRHz_472NvLZNjbqE4G8c2JL44OSMQ/edit",Task,Major,Resolved,"2015-11-09 09:39:17","2015-11-09 09:39:17",5
"Apache Mesos","Investigate recent crashes in Command Executor","Post https://reviews.apache.org/r/38900 i.e. updating CommandExecutor to support rootfs. There seem to be some tests showing frequent crashes due to assert violations.  {{FetcherCacheTest.SimpleEviction}} failed due to the following log:    The reason seems to be a race between the executor receiving a {{RunTaskMessage}} before {{ExecutorRegisteredMessage}} leading to the {{CHECK_SOME(executorInfo)}} failure.  Link to complete log: https://issues.apache.org/jira/browse/MESOS-2831?focusedCommentId=14995535&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14995535  Another related failure from {{ExamplesTest.PersistentVolumeFramework}}    Full logs at: https://builds.apache.org/job/Mesos/1191/COMPILER=gcc,CONFIGURATION=--verbose,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull",Bug,Major,Resolved,"2015-11-08 07:30:54","2015-11-08 07:30:54",2
"Apache Mesos","Corrected style in Makefiles","Order of files in Makefiles is not strictly alphabetic",Bug,Major,Resolved,"2015-11-07 12:41:48","2015-11-07 12:41:48",1
"Apache Mesos","Refactor Environment::mkdtemp into TemporaryDirectoryTest.","As part of [MESOS-3762], many tests were changed from one {{TemporaryDirectoryTest}} to another {{TemporaryDirectoryTest}}.  One subtle difference is that the name of the temporary directory no longer contains the name of the test.  In [MESOS-3847], the duplicate {{TemporaryDirectoryTest}} was removed.  The original {{TemporaryDirectoryTest}} called [{{environment->mkdtemp}}|https://github.com/apache/mesos/blob/master/src/tests/environment.cpp#L494].  We would like the naming, which is valuable for debugging, to be available for a majority of tests.  (A majority of tests inherit from {{TemporaryDirectoryTest}} in some way.)  Note: * Any additional directories created via {{environment->mkdtemp}} are cleaned up after the test. * We don't want mesos-specific logic in Stout, like the {{umount}} shell command in {{Environment::TearDown}}. * Temp directories created via {{environment->mkdtemp}} can be placed in another folder via the [{{TMPDIR}} environment variable|https://github.com/apache/mesos/blob/4a703b91151568e21b8772af51c22e23b105313c/src/tests/environment.cpp#L892-L905].  We want to look at this variable in {{TemporaryDirectoryTest}} too.  *Proposed change:* Move the temporary directory logic from {{Environment::mkdtemp}} to {{TemporaryDirectoryTest}}.  *Tests that need to change* | {{log_tests.cpp}} | {{LogZooKeeperTest}} | We can change {{ZooKeeperTest}} to inherit from {{TemporaryDirectoryTest}} to get rid of code duplication | | {{tests/mesos.cpp}} | {{MesosTest::CreateSlaveFlags}} | {{MesosTest}} already inherits from {{TemporaryDirectoryTest}}. | | {{tests/script.hpp}} | {{TEST_SCRIPT}} | This is used for the {{ExampleTests}}.  We can define a test class that inherits appropriately. | | {{docker_tests.cpp}} | {{*}} | Already inherits from {{MesosTest}}. |",Task,Minor,Accepted,"2015-11-07 00:00:42","2015-11-07 00:00:42",3
"Apache Mesos","Root tests for LinuxFilesystemIsolatorTest are broken","The refactor in [MESOS-3762] ended up exposing some differences in the {{TemporaryDirectoryTest}} classes (one in Stout, one in Mesos-proper).  The tests that broke (during tear down):   As per an offline discussion between [~<USER> and [~<USER>, the solution is to merge the two {{TemporaryDirectoryTest}} classes and to fix the tear down of {{LinuxFilesystemIsolatorTest}}.",Bug,Minor,Resolved,"2015-11-06 22:08:00","2015-11-06 22:08:00",2
"Apache Mesos","Update documentation for FetcherCache mtime-related changes",,Documentation,Major,Resolved,"2015-11-06 13:30:51","2015-11-06 13:30:51",1
"Apache Mesos","Rootfs in provisioner test doesn't handle symlink directories properly","Currently Rootfs doesn't fully copy the directory structure over, and also doesn't create the symlinks in the new rootfs and will cause shell and other binaries that rely on the symlinks to no longer function.",Bug,Major,Resolved,"2015-11-06 02:29:06","2015-11-06 02:29:06",4
"Apache Mesos","/help endpoints do not work for nested paths","Mesos displays the list of all supported endpoints starting at a given path prefix using the {{/help}} suffix, e.g. {{master:5050/help}}.  It seems that the {{help}} functionality is broken for URL's having nested paths e.g. {{master:5050/help/master/machine/down}}. The response returned is: {quote} Malformed URL, expecting '/help/id/name/' {quote}",Bug,Minor,Resolved,"2015-11-05 00:29:42","2015-11-05 00:29:42",2
"Apache Mesos","Document operator HTTP endpoints","These are not exhaustively documented; they probably should be.  Some endpoints have docs: e.g., {{/reserve}} and {{/unreserve}} are described in the reservation doc page. But it would be good to have a single page that lists all the endpoints and their semantics.",Documentation,Minor,Resolved,"2015-11-04 22:54:02","2015-11-04 22:54:02",3
"Apache Mesos","Enable mesos-reviewbot project on jenkins to use SSL","Currently mesos-reviewbot project does  not support parameterized configuration. This limits the project from building using --enable-ssl (and others) configuration arguments for building mesos.  ",Improvement,Major,Accepted,"2015-11-03 22:37:03","2015-11-03 22:37:03",3
"Apache Mesos","Test-only libprocess reinitialization","*Background* Libprocess initialization includes the spawning of a variety of global processes and the creation of the server socket which listens for incoming requests.  Some properties of the server socket are configured via environment variables, such as the IP and port or the SSL configuration.  In the case of tests, libprocess is initialized once per test binary.  This means that testing different configurations (SSL in particular) is cumbersome as a separate process would be needed for every test case.  *Proposal* # Add some optional code between some tests like:  See [MESOS-3863] for more on {{process::finalize}}.",Epic,Major,Resolved,"2015-11-02 23:40:16","2015-11-02 23:40:16",3
"Apache Mesos","Add documentation explaining roles","Docs currently talk about resources, static/dynamic reservations, but don't explain what a role concept is to begin with.",Improvement,Minor,Resolved,"2015-11-02 23:03:02","2015-11-02 23:03:02",2
"Apache Mesos","Add checks to make sure isolators and the launcher are compatible.","There's a recent change regarding the picking of which launcher (Linux or Posix) to use https://reviews.apache.org/r/39604  In our environment, cgroups are not auto-mounted after reboot. We rely on Mesos itself to mount all relevant cgroups hierachies.  After the reboot, the above patch detects that 'freezer' hierarchy is not mounted, therefore, decided to use the Posix launcher (if --launcher is not specified explictly).  Port mapping isolator requires network namespace to be created for each container (thus requires Linux launcher). But we don't have a check to verify that launcher and isolators are compatible.  Slave thus starts fine and task failed with weird error like:   It does take us quite a few time to figure out the root cause.",Bug,Major,Accepted,"2015-10-30 23:47:37","2015-10-30 23:47:37",2
"Apache Mesos","Containerizer attempts to create Linux launcher by default ","Mesos containerizer attempts to create a Linux launcher by default without verifying whether the necessary prerequisites (such as availability of cgroups) are met.",Bug,Major,Resolved,"2015-10-23 23:20:59","2015-10-23 22:20:59",3
"Apache Mesos","process::io::write takes parameter as void* which could be const","In libprocess we have    which expects a non-{{const}} {{void*}} for its {{data}} parameter. Under the covers {{data}} appears to be handled as a {{const}} (like one would expect from the signature its inspiration {{::write}}).  This function is not used too often, but since it expects a non-{{const}} value for {{data}} automatic conversions to {{void*}} from other pointer types are disabled; instead callers seem cast manually to {{void*}} -- often with C-style casts.  We should sync this method's signature with that of {{::write}}.  In addition to following the expected semantics of {{::write}}, having this work without casts with any pointer value {{data}} would make it easier to interface this with character literals, or raw data ptrs from STL containers (e.g. {{Container::data}}). It would probably also indirectly eliminate temptation to use C-casts.",Improvement,Major,Resolved,"2015-10-23 09:49:53","2015-10-23 08:49:53",2
"Apache Mesos","Master should not store arbitrarily sized data in ExecutorInfo.","From a comment in [MESOS-3771]:  Master should not be storing the {{data}} fields from {{ExecutorInfo}}.  We currently [store the entire object|https://github.com/apache/mesos/blob/master/src/master/master.hpp#L262-L271], which means master would be at high risk of OOM-ing if a bunch of executors were started with big {{data}} blobs. * Master should scrub out unneeded bloat from {{ExecutorInfo}} before storing it. * We can use an alternate internal object, like we do for {{TaskInfo}} vs {{Task}}; see [this|https://github.com/apache/mesos/blob/master/src/messages/messages.proto#L39-L41].",Improvement,Major,Accepted,"2015-10-23 00:31:21","2015-10-22 23:31:21",3
"Apache Mesos","Cannot start mesos local on a Debian GNU/Linux 8 docker machine","We updated the mesos version to 0.25.0 in our Marathon docker image, that runs our integration tests. We use mesos local for those tests. This fails with this message:    The setup worked with mesos 0.24.0. The Dockerfile is here: https://github.com/<USER>marathon/blob/mv/mesos_0.25/Dockerfile        [~<USER> Can you please assign to the correct person?",Bug,Major,Resolved,"2015-10-22 19:44:58","2015-10-22 18:44:58",3
"Apache Mesos","Backticks are not mentioned in Mesos C++ Style Guide","As far as I can tell, current practice is to quote code excerpts and object names with backticks when writing comments. For example:    However, I don't see this documented in our C++ style guide at all. It should be added.",Documentation,Minor,Resolved,"2015-10-21 19:40:41","2015-10-21 18:40:41",1
"Apache Mesos","Use URI content modification time to trigger fetcher cache updates.","Instead of using checksums to trigger fetcher cache updates, we can for starters use the content modification time (mtime), which is available for a number of download protocols, e.g. HTTP and HDFS.  Proposal: Instead of just fetching the content size, we fetch both size  and mtime together. As before, if there is no size, then caching fails and we fall back on direct downloading to the sandbox.   Assuming a size is given, we compare the mtime from the fetch URI with the mtime known to the cache. If it differs, we update the cache. (As a defensive measure, a difference in size should also trigger an update.)   Not having an mtime available at the fetch URI is simply treated as a unique valid mtime value that differs from all others. This means that when initially there is no mtime, cache content remains valid until there is one. Thereafter,  anew lack of an mtime invalidates the cache once. In other words: any change from no mtime to having one or back is the same as encountering a new mtime.  Note that this scheme does not require any new protobuf fields. ",Improvement,Major,Resolved,"2015-10-21 15:52:42","2015-10-21 14:52:42",5
"Apache Mesos","Replace Master/Slave Terminology Phase I - Rename flag names and deprecate old ones",,Task,Major,Resolved,"2015-10-21 15:04:33","2015-10-21 14:04:33",3
"Apache Mesos","MasterAllocatorTest.SlaveLost is slow.","The {{MasterAllocatorTest.SlaveLost}} takes more that {{5s}} to complete. A brief look into the code hints that the stopped agent does not quit immediately (and hence its resources are not released by the allocator) because [it waits for the executor to terminate|https://github.com/apache/mesos/blob/master/src/tests/master_allocator_tests.cpp#L717]. {{5s}} timeout comes from {{EXECUTOR_SHUTDOWN_GRACE_PERIOD}} agent constant.  Possible solutions: * Do not wait until the stopped agent quits (can be flaky, needs deeper analysis). * Decrease the agent's {{executor_shutdown_grace_period}} flag. * Terminate the executor faster (this may require some refactoring since the executor driver is created in the {{TestContainerizer}} and we do not have direct access to it. ",Improvement,Minor,Resolved,"2015-10-21 11:18:35","2015-10-21 10:18:35",1
"Apache Mesos","RegistryClientTest.SimpleGetBlob is flaky","{{RegistryClientTest.SimpleGetBlob}} fails about 1/5 times.  This was encountered on OSX.  {code:title=Repro} bin/mesos-tests.sh --gtest_filter=*RegistryClientTest.SimpleGetBlob* --gtest_repeat=10 --gtest_break_on_failure   {code:title=Less common failure} [ RUN      ] RegistryClientTest.SimpleGetBlob ../../src/tests/containerizer/provisioner_docker_tests.cpp:926: Failure (socket).failure(): Failed accept: connection error: error:00000000:lib(0):func(0):reason(0) {code}",Bug,Major,Resolved,"2015-10-20 22:03:03","2015-10-20 21:03:03",4
"Apache Mesos","Consistency of quoted strings in error messages","Example log output:  {quote} I1020 18:56:02.933956  1790 slave.cpp:1270] Got assigned task 13 for framework 496620b9-4368-4a71-b741-68216f3d909f-0000 I1020 18:56:02.934185  1790 slave.cpp:1386] Launching task 13 for framework 496620b9-4368-4a71-b741-68216f3d909f-0000 I1020 18:56:02.934408  1790 slave.cpp:1618] Queuing task '13' for executor default of framework '496620b9-4368-4a71-b741-68216f3d909f-0000 I1020 18:56:02.935417  1790 slave.cpp:1760] Sending queued task '13' to executor 'default' of framework 496620b9-4368-4a71-b741-68216f3d909f-0000 {quote}  Aside from the typo (unmatched quote) in the third line, these log messages using quoting inconsistently: sometimes task, executor, and framework IDs are quoted, other times they are not.  We should probably adopt a general rule, a la http://www.postgresql.org/docs/9.4/static/error-style-guide.html . My proposal: when interpolating a variable, only use quotes if it is possible that the value might contain whitespace or punctuation (in the latter case, the punctuation should probably be escaped).",Bug,Major,Accepted,"2015-10-20 20:50:29","2015-10-20 19:50:29",3
"Apache Mesos","Mesos JSON API creates invalid JSON due to lack of binary data / non-ASCII handling","Spark encodes some binary data into the ExecutorInfo.data field.  This field is sent as a bytes Protobuf value, which can have arbitrary non-UTF8 data.  If you have such a field, it seems that it is splatted out into JSON without any regards to proper character encoding:    I suspect this is because the HTTP api emits the executorInfo.data directly:    I think this may be because the custom JSON processing library in stout seems to not have any idea of what a byte array is.  I'm guessing that some implicit conversion makes it get written as a String instead, but:    Thank you for any assistance here.  Our cluster is currently entirely down -- the frameworks cannot handle parsing the invalid JSON produced (it is not even valid utf-8) ",Bug,Critical,Resolved,"2015-10-20 19:05:36","2015-10-20 18:05:36",2
"Apache Mesos","Need for http::put request method","As we decided to create a more restful api for managing Quota request. Therefore we also want to use the HTTP put request and hence need to enable the libprocess/http to send put request besides get and post requests.",Task,Minor,Resolved,"2015-10-20 05:35:36","2015-10-20 04:35:36",1
"Apache Mesos","Refactor SSLTest fixture such that MesosTest can use the same helpers.","In order to write tests that exercise SSL with other components of Mesos, such as the HTTP scheduler library, we need to use the setup/teardown logic found in the {{SSLTest}} fixture.  Currently, the test fixtures have separate inheritance structures like this:  where {{::testing::Test}} is a gtest class.  The plan is the following: # Change {{SSLTest}} to inherit from {{TemporaryDirectoryTest}}.  This will require moving the setup (generation of keys and certs) from {{SetUpTestCase}} to {{SetUp}}.  At the same time, *some* of the cleanup logic in the SSLTest will not be needed. # Move the logic of generating keys/certs into helpers, so that individual tests can call them when needed, much like {{MesosTest}}. # Write a child class of {{SSLTest}} which has the same functionality as the existing {{SSLTest}}, for use by the existing tests that rely on {{SSLTest}} or the {{RegistryClientTest}}. # Have {{MesosTest}} inherit from {{SSLTest}} (which might be renamed during the refactor).  If Mesos is not compiled with {{--enable-ssl}}, then {{SSLTest}} could be {{#ifdef}}'d into any empty class.  The resulting structure should be like: ",Task,Major,Resolved,"2015-10-19 23:43:33","2015-10-19 22:43:33",3
"Apache Mesos","Document messages.proto","The messages we pass between Mesos components are largely undocumented.  See this [TODO|https://github.com/apache/mesos/blob/19f14d06bac269b635657960d8ea8b2928b7830c/src/messages/messages.proto#L23].",Improvement,Major,Resolved,"2015-10-19 19:08:18","2015-10-19 18:08:18",3
"Apache Mesos","0.26.0 Release","Manage the release of Apache Mesos version 0.26.0.   The Mesos 0.26.0 release will aim at being timely and at improving robustness. It will not be gated by new features. However, there may be blockers when it comes to bugs or incompleteness of existing features.  Once these blockers are resolved, we will start deferring unresolved issues by Priority and Status until we are ready to make the first cut.  Here is how you can stay informed and help out.  h3. Users - Note the is blocked by links in this ticket for major targeted features. - Check out the 0.26.0 [dashboard|https://issues.apache.org/jira/secure/Dashboard.jspa?selectPageId=12327111] for status indicators. - See the in-progress [Release Notes|https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12333528&styleName=Html&projectId=12311242&Create=Create&atl_token=A5KQ-2QAV-T4JA-FDED%7C675829c365428965ebec16702c62d3637db57d84%7Clin] to see what's committed so far. - Add comments to issues describing your problems or use cases.  h3. Issue Reporters - Set Target Version to 0.26.0, if appropriate. - Set Priority for fixing in 0.26.0. - Ask around on IRC or dev@ for a Shepherd!  h3. Developers - Newbies: Check out [Accepted, Unassigned, 'newbie'] issues. - Looking for something meatier to work on? [Accepted, Unassigned for 0.26] - For Shepherdless issues, find a Shepherd before diving too deep!! - Update Target Version and Priority, as needed. - Discuss your intended design on the JIRA, perhaps sharing a design doc. - Update the Status to In Progress and Reviewable as you go. - Assign yourself if you are working on it. Un-assign yourself in case you stop before finishing.  h3. Committers - Accept and Shepherd all relevant [Shepherdless issues]. - Update Target Version and Priority, as needed. - Add 'newbie' label to any easy ones.  h3. Important JIRA fields - Target Version: Set to 0.26.0 if you want the issue to be addressed in 0.26. - Priority: Indicates how important it is for the issue to be fixed in the next release (0.26.0 in this case). If you want to update a Priority, please add a comment explaining your reason, and only change the Priority up/down one level. - Blocked-by Links: major features and critical tickets can be linked as blockers to this ticket to give a high-level overview of what we plan to land in 0.26. Non-critical issues should just set the Target Version. ",Task,Major,Resolved,"2015-10-19 14:24:40","2015-10-19 13:24:40",5
"Apache Mesos","Generalized HTTP Authentication Modules","Libprocess is going to factor out an authentication interface: MESOS-3231  Here we propose that Mesos can provide implementations for this interface as Mesos modules.",Task,Major,Resolved,"2015-10-19 11:23:21","2015-10-19 10:23:21",13
"Apache Mesos","Test the HTTP Scheduler library with SSL enabled","Currently, the HTTP Scheduler library does not support SSL-enabled Mesos.   (You can manually test this by spinning up an SSL-enabled master and attempt to run the event-call framework example against it.)  We need to add tests that check the HTTP Scheduler library against SSL-enabled Mesos: * with downgrade support, * with required framework/client-side certifications, * with/without verification of certificates (master-side), * with/without verification of certificates (framework-side), * with a custom certificate authority (CA)  These options should be controlled by the same environment variables found on the [SSL user doc|http://mesos.apache.org/documentation/latest/ssl/].  Note: This issue will be broken down into smaller sub-issues as bugs/problems are discovered.",Story,Major,Resolved,"2015-10-16 23:27:30","2015-10-16 22:27:30",13
"Apache Mesos","CentOS 6 dependency install fails at Maven","It seems the Apache Maven dependencies have changed such that following the Getting Started docs for CentOS 6.6 will fail at Maven installation:  ",Documentation,Major,Resolved,"2015-10-16 19:30:12","2015-10-16 18:30:12",1
"Apache Mesos","MESOS_NATIVE_JAVA_LIBRARY not set on MesosContainerize tasks with --executor_environmnent_variables","When using --executor_environment_variables, and having MESOS_NATIVE_JAVA_LIBRARY in the environment of mesos-slave, the mesos containerizer does not set MESOS_NATIVE_JAVA_LIBRARY itself.  Relevant code: https://github.com/apache/mesos/blob/14f7967ef307f3d98e3a4b93d92d6b3a56399b20/src/slave/containerizer/containerizer.cpp#L281  It sees that the variable is in the mesos-slave's environment (os::getenv), rather than checking if it is set in the environment variable set.",Bug,Major,Resolved,"2015-10-16 18:45:36","2015-10-16 17:45:36",2
"Apache Mesos","Configuration docs are missing --enable-libevent and --enable-ssl","The {{\-\-enable-libevent}} and {{\-\-enable-ssl}} config flags are currently not documented in the Configuration docs with the rest of the flags. They should be added.",Documentation,Major,Resolved,"2015-10-16 17:46:42","2015-10-16 16:46:42",1
"Apache Mesos","HTTP scheduler library does not gracefully parse invalid resource identifiers","If you pass a nonsense string for master into a framework using the C++ HTTP scheduler library, the framework segfaults.  For example, using the example frameworks:  {code:title=Scheduler Driver} build/src/test-framework --master=asdf://127.0.0.1:5050  Failed to create a master detector for 'asdf://127.0.0.1:5050': Failed to parse 'asdf://127.0.0.1:5050'  Results in   {code:title=Stack Trace} * thread #2: tid = 0x28b6bb, 0x0000000100ad03ca libmesos-0.26.0.dylib`mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 42 at scheduler.cpp:213, stop reason = EXC_BAD_ACCESS (code=1, address=0x0)   * frame #0: 0x0000000100ad03ca libmesos-0.26.0.dylib`mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 42 at scheduler.cpp:213     frame #1: 0x0000000100ad05f2 libmesos-0.26.0.dylib`virtual thunk to mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 34 at scheduler.cpp:210     frame #2: 0x00000001022b60f3 libmesos-0.26.0.dylib`::resume() + 931 at process.cpp:2449     frame #3: 0x00000001022c131c libmesos-0.26.0.dylib`::operator()() + 268 at process.cpp:2174     frame #4: 0x00000001022c0fa2 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __invoke<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35) &, const std::__1::atomic<bool> &> + 27 at __functional_base:415     frame #5: 0x00000001022c0f87 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __apply_functor<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::tuple<std::__1::reference_wrapper<const std::__1::atomic<bool> > >, 0, std::__1::tuple<> > + 55 at functional:2060     frame #6: 0x00000001022c0f50 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] operator()<> + 41 at functional:2123     frame #7: 0x00000001022c0f27 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __invoke<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > >> + 14 at __functional_base:415     frame #8: 0x00000001022c0f19 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __thread_execute<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > >> + 25 at thread:337     frame #9: 0x00000001022c0f00 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() + 368 at thread:347     frame #10: 0x00007fff964c705a libsystem_pthread.dylib`_pthread_body + 131     frame #11: 0x00007fff964c6fd7 libsystem_pthread.dylib`_pthread_start + 176     frame #12: 0x00007fff964c43ed libsystem_pthread.dylib`thread_start + 13 {code}",Bug,Major,Resolved,"2015-10-16 00:24:28","2015-10-15 23:24:28",1
"Apache Mesos","Provide diagnostic output in agent log when fetching fails","When fetching fails, the fetcher has written log output to stderr in the task sandbox, but it is not easy to get to. It may even be impossible to get to if one only has the agent log available and no more access to the sandbox. This is for instance the case when looking at output from a CI run.  The fetcher actor in the agent detects if the external fetcher program claims to have succeeded or not. When it exits with an error code, we could grab the fetcher log from the stderr file in the sandbox and append it to the agent log.  This is similar to this patch: https://reviews.apache.org/r/37813/  The difference is that the output of the latter is triggered by test failures outside the fetcher, whereas what is proposed here is triggering upon failures inside the fetcher.",Bug,Minor,Resolved,"2015-10-15 13:35:56","2015-10-15 12:35:56",2
"Apache Mesos","Mesos does not set Content-Type for 400 Bad Request","While integrating with the HTTP Scheduler API I encountered the following scenario.  The message below was serialized to protobuf and sent as the POST body {code:title=message} call {   type: ACKNOWLEDGE,   acknowledge: {     uuid: <bytes>,     agentID: { value: 20151012-182734-16777343-5050-8978-S2 },     taskID: { value: task-1 }   } }   I received the following response {code:title=Response Headers} HTTP/1.1 400 Bad Request Date: Wed, 14 Oct 2015 23:21:36 GMT Content-Length: 74  Failed to validate Scheduler::Call: Expecting 'framework_id' to be present {code}  Even though my accept header made no mention of {{text/plain}} the message body returned to me is {{text/plain}}. Additionally, there is no {{Content-Type}} header set on the response so I can't even do anything intelligently in my response handler.",Bug,Major,Resolved,"2015-10-15 00:41:12","2015-10-14 23:41:12",2
"Apache Mesos","Support docker local store pull same image simultaneously ","The current local store implements get() using the local puller. For all requests of pulling same docker image at the same time, the local puller just untar the image tarball as many times as those requests are, and cp all of them to the same directory, which wastes time and bear high demand of computation. We should be able to support the local store/puller only do these for the first time, and the simultaneous pulling request should wait for the promised future and get it once the first pulling finishes. ",Improvement,Major,Resolved,"2015-10-14 22:22:53","2015-10-14 21:22:53",3
"Apache Mesos","Incorrect sed syntax for Mac OSX","The build currently fails on OSX:    This is because the sed command uses the wrong syntax for OSX: you need  to instruct sed to not use a backup file.",Bug,Blocker,Resolved,"2015-10-14 18:58:49","2015-10-14 17:58:49",2
"Apache Mesos","Speed up FaultToleranceTest.FrameworkReregister test","FaultToleranceTest.FrameworkReregister test takes more than one second to complete:   There must be a {{1s}} timeout somewhere which we should mitigate via {{Clock::advance()}}.",Improvement,Major,Resolved,"2015-10-14 16:48:21","2015-10-14 15:48:21",1
"Apache Mesos","Prototype quota request authorization","When quotas are requested they should authorize their roles.  This ticket will authorize quota requests with ACLs. The existing authorization support that has been implemented in MESOS-1342 will be extended to add a `request_quotas` ACL.",Task,Major,Resolved,"2015-10-13 19:23:47","2015-10-13 18:23:47",5
"Apache Mesos","Prototype quota request authentication","Quota requests need to be authenticated.  This ticket will authenticate quota requests using credentials provided by the `Authorization` field of the HTTP request. This is similar to how authentication is implemented in `Master::Http`.",Task,Major,Resolved,"2015-10-13 19:16:47","2015-10-13 18:16:47",5
"Apache Mesos","Tests for Quota support in master","Allocator-agnostic tests for quota support in the master. They can be divided into several groups: * Heuristic check; * Master failover; * Functionality and quota guarantees.",Improvement,Major,Resolved,"2015-10-13 18:49:29","2015-10-13 17:49:29",5
"Apache Mesos","Implement Quota support in allocator","The built-in Hierarchical DRF allocator should support Quota. This includes (but not limited to): adding, updating, removing and satisfying quota; avoiding both overcomitting resources and handing them to non-quota'ed roles in presence of master failover.  A [design doc for Quota support in Allocator|https://issues.apache.org/jira/browse/MESOS-2937] provides an overview of a feature set required to be implemented.",Bug,Major,Resolved,"2015-10-13 13:13:06","2015-10-13 12:13:06",5
"Apache Mesos","Master recovery in presence of quota","Quota complicates master failover in several ways. The new master should determine if it is possible to satisfy the total quota and notify an operator in case it's not (imagine simultaneous failovers of multiple agents). The new master should hint the allocator how many agents might reconnect in the future to help it decide how to satisfy quota before the majority of agents reconnect.",Task,Major,Resolved,"2015-10-13 13:03:59","2015-10-13 12:03:59",5
"Apache Mesos","Update Allocator interface to support quota","An allocator should be notified when a quota is being set/updated or removed. Also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota.",Bug,Major,Resolved,"2015-10-13 12:46:29","2015-10-13 11:46:29",3
"Apache Mesos","HTTP Pipelining doesn't keep order of requests","[HTTP 1.1 Pipelining|https://en.wikipedia.org/wiki/HTTP_pipelining] describes a mechanism by which multiple HTTP request can be performed over a single socket. The requirement here is that responses should be send in the same order as requests are being made.  Libprocess has some mechanisms built in to deal with pipelining when multiple HTTP requests are made, it is still, however, possible to create a situation in which responses are scrambled respected to the requests arrival.  Consider the situation in which there are two libprocess processes, {{processA}} and {{processB}}, each running in a different thread, {{thread2}} and {{thread3}} respectively. The [{{ProcessManager}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L374] runs in {{thread1}}.  {{processA}} is of type {{ProcessA}} which looks roughly as follows:    {{processB}} is from type {{ProcessB}} which is just like {{ProcessA}} but routes {{bar}} instead of {{foo}}.  The situation in which the bug arises is the following:  # Two requests, one for {{http://server_uri/(1)/foo}} and one for {{http://server_uri/(2)//bar}} are made over the same socket. # The first request arrives to [{{ProcessManager::handle}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2202] which is still running in {{thread1}}. This one creates an {{HttpEvent}} and delivers to the handler, in this case {{processA}}. # [{{ProcessManager::deliver}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2361] enqueues the HTTP event in to the {{processA}} queue. This happens in {{thread1}}. # The second request arrives to [{{ProcessManager::handle}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2202] which is still running in {{thread1}}. Another {{HttpEvent}} is created and delivered to the handler, in this case {{processB}}. # [{{ProcessManager::deliver}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2361] enqueues the HTTP event in to the {{processB}} queue. This happens in {{thread1}}. # {{Thread2}} is blocked, so {{processA}} cannot handle the first request, it is stuck in the queue. # {{Thread3}} is idle, so it picks up the request to {{processB}} immediately. # [{{ProcessBase::visit(HttpEvent)}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L3073] is called in {{thread3}}, this one in turn [dispatches|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L3106] the response's future to the {{HttpProxy}} associated with the socket where the request came.  At the last point, the bug is evident, the request to {{processB}} will be send before the request to {{processA}} even if the handler takes a long time and the {{processA::bar()}} actually finishes before. The responses are not send in the order the requests are done.  h1. Reproducer  The following is a test which successfully reproduces the issue:  {code:title=3rdparty/libprocess/src/tests/http_tests.cpp} #include <process/latch.hpp  using process::Latch; using testing::InvokeWithoutArgs;  // This tests tries to force a situation in which HTTP Pipelining is scrambled. // It does so by having two actors to which three requests are made, the first // two requests to the first actor and a third request to the second actor. // The first request will block the first actor long enough to allow the second // actor to process the third request. Since the first actor will not be able to // handle any event until it is done processing the first request, the third // request is finished before the second even starts. // The ultimate goal of the test is to alter the order in which // `ProcessBase::visit(HttpEvent)` is executed for the different events // respect to the order in which the requests arrived. TEST(HTTPConnectionTest, ComplexPipelining) {   Http server1, server2;    Future<http::Request> get1, get2, get3;   Latch latch;    EXPECT_CALL(*server1.process, get(_))     .WillOnce(DoAll(FutureArg<0>(&get1),                     InvokeWithoutArgs([&latch]() { latch.await(); }),                     Return(http::OK(1))))     .WillOnce(DoAll(FutureArg<0>(&get2),                     Return(http::OK(2))));    EXPECT_CALL(*server2.process, get(_))     .WillOnce(DoAll(FutureArg<0>(&get3),                     Return(http::OK(3))));      auto url1 = http::URL(       http,       server1.process->self().address.ip,       server1.process->self().address.port,       server1.process->self().id + /get);     auto url2 = http::URL(       http,       server1.process->self().address.ip,       server1.process->self().address.port,       server2.process->self().id + /get);    // Create a connection to the server for HTTP pipelining.   Future<http::Connection> connect = http::connect(url1);    AWAIT_READY(connect);    http::Connection connection = connect.get();    http::Request request1;   request1.method = GET;   request1.url = url1;   request1.keepAlive = true;   request1.body = 1;   Future<http::Response> response1 = connection.send(request1);    http::Request request2 = request1;   request2.body = 2;   Future<http::Response> response2 = connection.send(request2);    http::Request request3;   request3.method = GET;   request3.url = url2;   request3.keepAlive = true;   request3.body = 3;   Future<http::Response> response3 = connection.send(request3);    // Verify that request1 arrived at server1 and it is the right request.   // Now server1 is blocked processing request1 and cannot pick up more events   // in the queue.   AWAIT_READY(get1);   EXPECT_EQ(request1.body, get1->body);    // Verify that request3 arrived at server2 and it is the right request.   AWAIT_READY(get3);   EXPECT_EQ(request3.body, get3->body);    // Request2 hasn't been picked up since server1 is still blocked serving   // request1.   EXPECT_TRUE(get2.isPending());    // Free server1 so it can serve request2.   latch.trigger();    // Verify that request2 arrived at server1 and it is the right request.   AWAIT_READY(get2);   EXPECT_EQ(request2.body, get2->body);    // Wait for all responses.   AWAIT_READY(response1);   AWAIT_READY(response2);   AWAIT_READY(response3);    // If pipelining works as expected, even though server2 finished processing   // its request before server1 even began with request2, the responses should   // arrive in the order they were made.   EXPECT_EQ(request1.body, response1->body);   EXPECT_EQ(request2.body, response2->body);   EXPECT_EQ(request3.body, response3->body);    AWAIT_READY(connection.disconnect());   AWAIT_READY(connection.disconnected()); } {code}",Bug,Major,Resolved,"2015-10-12 17:02:16","2015-10-12 16:02:16",3
"Apache Mesos","Deprecate resource_monitoring_interval flag","This parameter should be deprecated after 0.23.0 release as it has no use now. ",Bug,Major,Resolved,"2015-10-12 04:25:10","2015-10-12 03:25:10",1
"Apache Mesos","JSON parsing allows non-whitespace trailing characters","Picojson supports a streaming mode in which a stream containing a series of JSON values can be repeatedly parsed. For this reason, it does not return an error when passed a string containing a valid JSON value followed by non-whitespace trailing characters.  However, in addition to the four-argument {{picojson::parse()}} that we're using, picojson contains a two-argument {{parse()}} function (https://github.com/kazuho/picojson/blob/master/picojson.h#L938-L942) which accepts a {{std::string}} and should probably validate its input to ensure it doesn't contain trailing characters. A pull request has been filed for this change at https://github.com/kazuho/picojson/pull/70 and if it's merged, we can switch to the two-argument function call. In the meantime, we should provide such input validation ourselves in {{JSON::parse()}}.",Bug,Major,Resolved,"2015-10-11 02:55:02","2015-10-11 01:55:02",1
"Apache Mesos","Enable building mesos.apache.org locally in a Docker container.","We should make it easy for everyone to modify the website and be able to generate it locally before pushing to upstream. ",Bug,Major,Resolved,"2015-10-09 23:42:13","2015-10-09 22:42:13",3
"Apache Mesos","Clarify error message 'could not chown work directory'","When deploying a framework I encountered the error message 'could not chown work directory'.  It took me a while to figure out that this happened because my framework was registered as a user on my host machine which did not exist on the Docker container and the agent was running as root.  I suggest to clarify this message by pointing out to either set {{--switch-user}}  to {{false}} or to run the framework as the same user as the agent.",Documentation,Minor,Resolved,"2015-10-09 16:35:50","2015-10-09 15:35:50",1
"Apache Mesos","Get Container Name information when launching a container task","We want to get the Docker Name (or Docker ID, or both) when launching a container task with mesos. The container name is generated by mesos itself (i.e. mesos-77e5fde6-83e7-4618-a2dd-d5b10f2b4d25, obtained with docker ps) and it would be nice to expose this information to frameworks so that this information can be used, for example by Marathon to give this information to users via a REST API.  To go a bit in depth with our use case, we have files created by fluentd logdriver that are named with Docker Name or Docker ID (full or short) and we need a mapping for the users of the REST API and thus the first step is to make this information available from mesos. ",Improvement,Major,"In Progress","2015-10-09 11:48:37","2015-10-09 10:48:37",3
"Apache Mesos","Port slave/containerizer/isolator.hpp to Windows",,Task,Major,Resolved,"2015-10-09 09:24:47","2015-10-09 08:24:47",3
"Apache Mesos","Port process/collect.hpp to Windows",,Task,Major,Resolved,"2015-10-09 08:59:22","2015-10-09 07:59:22",1
"Apache Mesos","Implement stout/os/windows/stat.hpp",,Task,Major,Resolved,"2015-10-08 23:53:07","2015-10-08 22:53:07",8
"Apache Mesos","Implement stout/os/windows/read.hpp and write.hpp",,Task,Major,Resolved,"2015-10-08 23:47:56","2015-10-08 22:47:56",2
"Apache Mesos","Implement stout/os/windows/ls.hpp",,Task,Major,Resolved,"2015-10-08 23:45:54","2015-10-08 22:45:54",3
"Apache Mesos","Implement stout/os/windows/killtree.hpp","killtree() is implemented using Windows Job Objects. The processes created by the  executor are associated with a job object using `create_job'. killtree() is simply terminating the job object.   Helper functions: `create_job` function creates a job object whose name is derived from the `pid` and associates the `pid` process with the job object. Every process started by the process which is part of the job object becomes part of the job object. The job name should match the name used in `kill_job`. The jobs should be create with JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE and allow the caller to decide how to handle the returned handle.   `kill_job` function assumes the process identified by `pid` is associated with a job object whose name is derive from it. Every process started by the process which is part of the job object becomes part of the job object. Destroying the task will close all such processes.",Task,Major,Resolved,"2015-10-08 23:44:24","2015-10-08 22:44:24",5
"Apache Mesos","Add support for github and variable base URLs to apply-reviews.py","From Adam's email on dev@ list:  I have used the '-g' feature for github PRs in the past, and we should continue to support that model, so that new Mesos contributors don't have to create new RB accounts and learn a new process just for quick documentation changes, etc.  As a side note, now that the Myriad incubator project has migrated to Apache git and we can no longer merge PRs directly, we were hoping to take advantage of a tool like apply-reviews to apply our PR patches. It looks like apply-reviews.sh only specifies 'mesos' in the GITHUB_URL/API_URL. Would apply-reviews.py be just as easy to reuse for another project (i.e. Myriad)?",Task,Major,Resolved,"2015-10-08 21:50:28","2015-10-08 20:50:28",3
"Apache Mesos","Port slave/containerizer/mesos/launch.cpp to Windows","Important subset of the dependency tree follows:  slave/containerizer/mesos/launch.cpp: os, protobuf, launch launch: subcommand subcommand: flags flags.hpp: os.hpp, path.hpp, fetch.hpp",Task,Major,Resolved,"2015-10-08 21:37:53","2015-10-08 20:37:53",3
"Apache Mesos","Port slave/containerizer/mesos/containerizer.cpp to Windows","Important subset of the dependency tree follows:  slave/containerizer/mesos/containerizer.cpp: isolator, collect, defer, io, metrics, reap, subprocess, fs, os, path, protobuf_utils, paths, slave, containerizer, fetcher, launcher, posix, disk, containerizer, launch, provisioner",Task,Major,Resolved,"2015-10-08 21:35:40","2015-10-08 20:35:40",3
"Apache Mesos","Create slave/containerizer/isolators/filesystem/windows.cpp","Should look a lot like the posix.cpp flavor. Important subset of the dependency tree follows for the posix flavor:  slave/containerizer/isolators/filesystem/posix.cpp: filesystem/posix, fs, os, path filesystem/posix: flags, isolator",Task,Major,Resolved,"2015-10-08 21:29:25","2015-10-08 20:29:25",3
"Apache Mesos","Port slave/containerizer/isolator.cpp to Windows","Important subset of the dependency tree follows:  isolator.hpp: dispatch.hpp, path.hpp isolator: process dispatch.hpp: process.hpp ",Task,Major,Resolved,"2015-10-08 21:28:30","2015-10-08 20:28:30",3
"Apache Mesos","Port slave/containerizer/fetcher.cpp","Important subset of the dependency tree follows:  slave/containerizer/fetcher.cpp: slave, fetcher, collect, dispatch, net collect: future, defer, process fetcher: type_utils, future, process, subprocess dispatch.hpp: process.hpp net.hpp: ip, networking stuff future.hpp: pid.hpp defer.hpp: deferred.hpp, dispatch.hpp deferred.hpp: dispatch.hpp, pid.hpp type_utils.hpp: uuid.hpp subprocess: os, future",Task,Major,Resolved,"2015-10-08 21:27:07","2015-10-08 20:27:07",3
"Apache Mesos","Port slave/state.cpp","Important subset of changes this depends on:  slave/state.cpp: pid, os, path, protobuf, paths, state pid.hpp: address.hpp, ip.hpp address.hpp: ip.hpp, net.hpp net.hpp: ip, networking stuff state: type_utils, pid, os, path, protobuf, uuid type_utils.hpp: uuid.hpp",Task,Major,Resolved,"2015-10-08 21:17:23","2015-10-08 20:17:23",3
"Apache Mesos","Port slave/paths.cpp to Windows","Important subset of dependency tree of changes necessary:  slave/paths.cpp: os, path",Task,Major,Resolved,"2015-10-08 21:12:56","2015-10-08 20:12:56",1
"Apache Mesos","ExamplesTest.PersistentVolumeFramework does not work in OS X El Capitan","The example persistent volume framework test does not pass in OS X El Capitan. It seems to be executing the {{<build_dir>/src/.libs/mesos-executor}} directly while it should be executing the wrapper script at {{<build_dir>/src/mesos-executor}} instead. The no-executor framework passes however, which seem to have a very similar configuration with the persistent volume framework. The following is the output that shows the {{dyld}} load error:  ",Bug,Major,Resolved,"2015-10-08 01:27:00","2015-10-08 00:27:00",3
"Apache Mesos","Test build failure due to comparison between signed and unsigned integers","Compilation fails on OpenSUSE Tumbleweed (Linux 4.1.6, gcc 5.1.1, glibc 2.22) with the following errors:  ",Bug,Blocker,Resolved,"2015-10-08 00:23:27","2015-10-07 23:23:27",1
"Apache Mesos","Formalize all headers and metadata for HTTP API Event Stream","From an HTTP standpoint the current set of headers returned when connecting to the HTTP scheduler API are insufficient.  {code:title=current headers} HTTP/1.1 200 OK Transfer-Encoding: chunked Date: Wed, 30 Sep 2015 21:07:16 GMT Content-Type: application/json  {code:title=Response} HTTP/1.1 200 OK Connection: keep-alive Transfer-Encoding: chunked Content-Type: application/x-protobuf Content-Encoding: recordio Cache-Control: no-transform {code}  When Content-Encoding is used it is recommended to set {{Cache-Control: no-transform}} to signal to any proxies that no transformation should be applied to the the content encoding [Section 14.11 RFC 2616|http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.11].  ",Improvement,Blocker,Resolved,"2015-10-07 19:37:45","2015-10-07 18:37:45",5
"Apache Mesos","Framework process hangs after master failover when number frameworks > libprocess thread pool size","When running multi framework instances per process, if the number of framework created exceeds the libprocess threads then during master failover the zookeeper updates can cause deadlock. E.g. On a machine with 24 cpus, if the framework instance count exceeds 24 ( per process)  then when the master fails over all the libprocess threads block updating the cache ( GroupProcess) leading to deadlock. Below is the stack trace of one the libprocess thread :    Solution:   Create master detector per url instead of per framework. Will send the review request.   ",Bug,Major,Resolved,"2015-10-06 18:34:27","2015-10-06 17:34:27",3
"Apache Mesos","Propagate Isolator::prepare() failures to the framework","Currently, if {{Isolator::prepare}} fails for some isolator(s), we simply return a generic message about container being destroyed during launch.  It would be especially helpful if a third-party isolator modules could report the error back to the framework.",Task,Major,Resolved,"2015-10-06 02:11:41","2015-10-06 01:11:41",2
"Apache Mesos","Framework failover when framework is 'active' does not trigger allocation.","FWICT, this is just a consequence of some technical debt in the master code. When an active framework fails over, we do not go through the deactivation->activation code paths, and so:  (1) The framework's filters in the allocator remain after the failover. (2) The failed over framework does not receive an immediate allocation (it has to wait for the next allocation interval).  If the framework had disconnected first, then the failover goes through the deactivation->activation code paths.  This also means that some tests take longer to run than necessary.",Bug,Minor,Accepted,"2015-10-05 23:42:06","2015-10-05 22:42:06",5
"Apache Mesos","MemoryPressureMesosTest.CGROUPS_ROOT_Statistics and CGROUPS_ROOT_SlaveRecovery are flaky","I am install Mesos 0.24.0 on 4 servers which have very similar hardware and software configurations.   After performing {{../configure}}, {{make}}, and {{make check}} some servers have completed successfully and other failed on test {{[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics}}.  Is there something I should check in this test?   ",Bug,Major,Resolved,"2015-10-05 23:31:42","2015-10-05 22:31:42",1
"Apache Mesos","Add a test module for ip-per-container support","With the addition of {{NetworkInfo}} to allow frameworks to request IP-per-container for their tasks, we should add a simple module that mimics the behavior of a real network-isolation module for testing purposes. We can then add this module in {{src/examples}} and write some tests against it.  This module can also serve as a template module for third-party network isolation provides for building their own network isolator modules.",Task,Major,Resolved,"2015-10-05 21:55:24","2015-10-05 20:55:24",3
"Apache Mesos","rename libprocess tests to libprocess-tests","Stout tests are in a binary named {{stout-tests}}, Mesos tests are in {{mesos-tests}}, but libprocess tests are just {{tests}}. It would be helpful to name them {{libprocess-tests}} ",Bug,Trivial,Resolved,"2015-10-05 20:59:50","2015-10-05 19:59:50",1
"Apache Mesos","Introduce stream IDs in HTTP Scheduler API","Currently, the HTTP Scheduler API has no concept of Sessions aka {{SessionID}} or a {{TokenID}}. This is useful in some failure scenarios. As of now, if a framework fails over and then subscribes again with the same {{FrameworkID}} with the {{force}} option set, the Mesos master would subscribe it.  If the previous instance of the framework/scheduler tries to send a Call , e.g. {{Call::KILL}} with the same previous {{FrameworkID}} set, it would be still accepted by the master leading to erroneously killing a task.  This is possible because we do not have a way currently of distinguishing connections. It used to work in the previous driver implementation due to the master also performing a {{UPID}} check to verify if they matched and only then allowing the call. Following the design process, we will implemented stream IDs for Mesos HTTP schedulers; each ID will be associated with a single subscription connection, and the scheduler must include it as a header in all non-subscribe calls sent to the master.",Task,Major,Resolved,"2015-10-05 18:47:32","2015-10-05 17:47:32",5
"Apache Mesos","License headers show up all over doxygen documentation.","Currently license headers are commented in something resembling Javadoc style,    Since we use Javadoc-style comment blocks for doxygen documentation all license headers appear in the generated documentation, potentially and likely hiding the actual documentation.  Using {{/*}} to start the comment blocks would be enough to hide them from doxygen, but would likely also result in a largish (though mostly uninteresting) patch.",Documentation,Minor,Resolved,"2015-10-05 09:38:28","2015-10-05 08:38:28",2
"Apache Mesos","FetcherCacheTest.LocalUncachedExtract is flaky","From ASF CI: https://builds.apache.org/job/Mesos/866/COMPILER=clang,CONFIGURATION=--verbose,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/console   ",Bug,Major,Resolved,"2015-10-04 02:02:01","2015-10-04 01:02:01",2
"Apache Mesos","V1 API java/python protos are not generated","The java/python protos for the V1 api should be generated according to the Makefile; however, they do not show up in the generated build directory.",Bug,Blocker,Resolved,"2015-10-02 20:07:55","2015-10-02 19:07:55",2
"Apache Mesos","Mesos does not kill orphaned docker containers","After upgrade to 0.24.0 we noticed hanging containers appearing. Looks like there were changes between 0.23.0 and 0.24.0 that broke cleanup.  Here's how to trigger this bug:  1. Deploy app in docker container. 2. Kill corresponding mesos-docker-executor process 3. Observe hanging container  Here are the logs after kill:    Another issue: if you restart mesos-slave on the host with orphaned docker containers, they are not getting killed. This was the case before and I hoped for this trick to kill hanging containers, but it doesn't work now.  Marking this as critical because it hoards cluster resources and blocks scheduling.",Bug,Major,Resolved,"2015-10-02 13:26:18","2015-10-02 12:26:18",5
"Apache Mesos","Refactor registry_client","Refactor registry client component to:  - Make methods shorter for readability - Pull out structs not related to registry client into common namespace.",Task,Major,Resolved,"2015-10-01 18:52:55","2015-10-01 17:52:55",5
"Apache Mesos","Make Scheduler Library use HTTP Pipelining Abstraction in Libprocess","Currently, the scheduler library sends calls in order by chaining them and sending them only when it has received a response for the earlier call. This was done because there was no HTTP Pipelining abstraction in Libprocess {{process::post}}.  However once {{MESOS-3332}} is resolved, we should be now able to use the new abstraction.",Bug,Major,Resolved,"2015-10-01 17:54:26","2015-10-01 16:54:26",8
"Apache Mesos","Support TCP checks in Mesos.","In Marathon we have the ability to specify Health Checks for: - Command (Mesos supports this) - HTTP (see progress in MESOS-2533) - TCP missing  See here for reference: https://<USER>github.io/marathon/docs/health-checks.html  Since we made good experiences with those 3 options in Marathon, I see a lot of value, if Mesos would also support them. ",Improvement,Major,Resolved,"2015-10-01 08:21:48","2015-10-01 07:21:48",8
"Apache Mesos","Revocable task CPU shows as zero in /state.json","The slave's state.json reports revocable task resources as zero:    Also, there is no indication that a task uses revocable CPU. It would be great to have this type of info.",Bug,Major,Resolved,"2015-09-30 23:09:19","2015-09-30 22:09:19",2
"Apache Mesos","JSON-based credential files do not work correctly","Specifying the following credentials file:   Then hitting a master endpoint with:   Does not work. This is contrary to the text-based credentials file which works:   Currently, the password in a JSON-based credentials file needs to be base64-encoded in order for it to work: ",Bug,Major,Resolved,"2015-09-30 21:31:17","2015-09-30 20:31:17",1
"Apache Mesos","Make the Command Scheduler use the HTTP Scheduler Library","We should make the Command Scheduler in {{src/cli/executor.cpp}} use the Scheduler Library {{src/scheduler/scheduler.cpp}} instead of the Scheduler Driver.",Task,Major,Resolved,"2015-09-30 21:18:53","2015-09-30 20:18:53",3
"Apache Mesos","Implement  HTTPCommandExecutor that uses the Executor Library ","Instead of using the {{MesosExecutorDriver}} , we should make the {{CommandExecutor}} in {{src/launcher/executor.cpp}} use the new Executor HTTP Library that we create in {{MESOS-3550}}.   This would act as a good validation of the {{HTTP API}} implementation.",Task,Major,Resolved,"2015-09-30 21:14:18","2015-09-30 20:14:18",13
"Apache Mesos","mesos.cli broken in 0.24.x","The issue was initially reported on the mailing list: http://www.mail-archive.com/<EMAIL>/msg04670.html  The format of the master data stored in zookeeper has changed but the mesos.cli does not reflect these changes causing tools like {{mesos-tail}} and {{mesos-ps}} to fail.  Example error from {{mesos-tail}}:    The problem exists in https://github.com/<USER>mesos-cli/blob/master/mesos/cli/master.py#L107. The code should be along the lines of:    This causes the master address to come back correctly.",Bug,Major,Resolved,"2015-09-30 09:03:01","2015-09-30 08:03:01",1
"Apache Mesos","Allocator changes trigger large re-compiles","Due to the templatized nature of the allocator, even small changes trigger large recompiles of the code-base. This make iterating on changes expensive for developers.",Improvement,Major,Resolved,"2015-09-30 01:08:46","2015-09-30 00:08:46",3
"Apache Mesos","LIBPROCESS_IP not passed when executor's environment is specified","When the executor's environment is specified explicitly via {{\-\-executor_environment_variables}}, {{LIBPROCESS_IP}} will not be passed, leading to errors in some cases - for example, when no DNS is available.",Bug,Major,Resolved,"2015-09-30 01:01:10","2015-09-30 00:01:10",2
"Apache Mesos","CHECK failure due to floating point precision on reservation request","result.cpus() == cpus() check is failing due to ( double == double ) comparison problem.    Root Cause :   Framework requested 0.1 cpu reservation for the first task. So far so good. Next Reserve operation — lead to double operations resulting in following double values :   results.cpus() : 23.9999999999999964472863211995 cpus() : 24  And the check ( result.cpus() == cpus() ) failed.    The double arithmetic operations caused results.cpus() value to be :  23.9999999999999964472863211995 and hence ( 23.9999999999999964472863211995 == 24 ) failed.   ",Task,Major,Resolved,"2015-09-29 22:17:14","2015-09-29 21:17:14",3
"Apache Mesos","Replace use of strerror with thread-safe alternatives strerror_r / strerror_l.","{{strerror()}} is not required to be thread safe by POSIX and is listed as unsafe on Linux:  http://pubs.opengroup.org/onlinepubs/9699919799/ http://man7.org/linux/man-pages/man3/strerror.3.html  I don't believe we've seen any issues reported due to this. We should replace occurrences of strerror accordingly, possibly offering a wrapper in stout to simplify callsites.",Bug,Major,Resolved,"2015-09-29 20:39:46","2015-09-29 19:39:46",3
"Apache Mesos","Create a Executor Library based on the new Executor HTTP API","Similar to the Scheduler Library {{src/scheduler/scheduler.cpp}} , we would need a Executor Library that speaks the new Executor HTTP API. ",Task,Major,Resolved,"2015-09-29 20:01:26","2015-09-29 19:01:26",5
"Apache Mesos","Libevent termination triggers Broken Pipe","When the libevent loop terminates and we unblock the {{SIGPIPE}} signal, the pending {{SIGPIPE}} instantly triggers and causes a broken pipe when the test binary stops running. ",Bug,Major,Resolved,"2015-09-28 23:44:14","2015-09-28 22:44:14",2
"Apache Mesos","Validate that slave's work_dir is a shared mount in its own peer group when LinuxFilesystemIsolator is used.","To address this TODO in the code:  ",Bug,Major,Resolved,"2015-09-28 23:31:03","2015-09-28 22:31:03",3
"Apache Mesos","Figure out how to enforce 64-bit builds on Windows.","We need to make sure people don't try to compile Mesos on 32-bit architectures. We don't want a Windows repeat of something like this:  https://issues.apache.org/jira/browse/MESOS-267",Task,Major,Resolved,"2015-09-26 02:05:58","2015-09-26 01:05:58",3
"Apache Mesos","Add an abstraction to manage the life cycle of file descriptors.","In order to avoid missing {{close()}} calls on file descriptors, or double-closing file descriptors, it would be nice to add a reference counted {{FileDescriptor}} in a similar way to what we've done for Socket. This will be closed automatically when the last reference goes away, and double closes can be prevented via internal state.",Improvement,Major,Reviewable,"2015-09-25 21:31:10","2015-09-25 20:31:10",5
"Apache Mesos","Fix file descriptor leakage / double close in the code base",,Bug,Major,Resolved,"2015-09-25 21:29:03","2015-09-25 20:29:03",3
"Apache Mesos","Building mesos from source fails when OS language is not English","Line 963 of mesos/3rdparty/libprocess/3rdparty/stout/tests/os_tests.cpp contains the following:    EXPECT_TRUE(strings::contains(result.get(), No such file or directory));  But this does not match when your locale is not English. When changing it to what my terminal gives me: Bestand of map bestaat niet then it works just fine. ",Bug,Minor,Resolved,"2015-09-25 14:29:11","2015-09-25 13:29:11",2
"Apache Mesos","Add user doc for networking support in Mesos 0.25.0",,Documentation,Major,Resolved,"2015-09-25 05:58:35","2015-09-25 04:58:35",2
"Apache Mesos","Support Subscribe Call for HTTP based Executors","We need to add a {{subscribe(...)}} method in {{src/slave/slave.cpp}} to introduce the ability for HTTP based executors to subscribe and then receive events on the persistent HTTP connection. Most of the functionality needed would be similar to {{Master::subscribe}} in {{src/master/master.cpp}}.",Task,Major,Resolved,"2015-09-25 03:29:00","2015-09-25 02:29:00",5
"Apache Mesos","Cgroups Test Filters aborts tests on Centos 6.6 ","Running make check on centos 6.6 causes all tests to abort due to CHECK_SOME test in CgroupsFIlter:  ",Bug,Major,Resolved,"2015-09-24 20:30:09","2015-09-24 19:30:09",1
"Apache Mesos","Don't retry close() on EINTR.","On Linux, retrying close on EINTR is dangerous because the fd is already released and we may accidentally close a newly opened fd (from another thread), see:  http://ewontfix.com/4/ http://lwn.net/Articles/576478/ http://lwn.net/Articles/576591/  It appears that other OSes, like HPUX, require a retry of close on EINTR. The Austin Group recently proposed changes to POSIX to require that the EINTR case need a retry, but EINPROGRESS be used for when a retry should not occur:  http://austingroupbugs.net/view.php?id=529  However, Linux does not follow this and so we need to remove our EINTR retries.  Some more links for posterity:  https://github.com/wahern/cqueues/issues/56#issuecomment-108656004 https://code.google.com/p/chromium/issues/detail?id=269623 https://codereview.chromium.org/23455051/ ",Bug,Major,Resolved,"2015-09-24 20:29:27","2015-09-24 19:29:27",1
"Apache Mesos","Synchronize V1 helper functions with pre-v1",,Task,Blocker,Resolved,"2015-09-24 19:30:28","2015-09-24 18:30:28",5
"Apache Mesos","Build instructions for CentOS 6.6 should include `sudo yum update`","Neglecting to run {{sudo yum update}} on CentOS 6.6 currently causes the build to break when building {{mesos-0.25.0.jar}}. The build instructions for this platform on the Getting Started page should be changed accordingly.",Bug,Major,Resolved,"2015-09-23 23:41:33","2015-09-23 22:41:33",1
"Apache Mesos","Introduce MESOS_SANDBOX environment variable in Mesos containerizer.","Similar to Docker containerizer, if a container changes rootfs, we'll have two environment variables:  MESOS_DIRECTORY: the path in the host filesystem MESOS_SANDBOX: the path in the container filesystem",Task,Major,Resolved,"2015-09-23 22:50:41","2015-09-23 21:50:41",3
"Apache Mesos","configure cannot find libevent headers in CentOS 6","If libevent is installed via {{sudo yum install libevent-headers}}, running {{../configure --enable-libevent}} will fail to discover the libevent headers:  ",Bug,Major,Resolved,"2015-09-23 18:57:07","2015-09-23 17:57:07",2
"Apache Mesos","Add a test for os::realpath()",,Task,Major,Resolved,"2015-09-23 07:33:08","2015-09-23 06:33:08",1
"Apache Mesos","Add implementation for sha256 based file content verification.",https://reviews.apache.org/r/38747/,Task,Major,Resolved,"2015-09-23 00:59:16","2015-09-22 23:59:16",3
"Apache Mesos","Create interface for digest verifier","Add interface for digest verifier so that we can add implementations for digest types like sha256, sha512 etc",Task,Major,Resolved,"2015-09-23 00:54:48","2015-09-22 23:54:48",2
"Apache Mesos","Expose maintenance user doc via the documentation home page","The committed docs can be found here: http://mesos.apache.org/documentation/latest/maintenance/  We need to add a link to {{docs/home.md}} Also, the doc needs some minor formatting tweaks.",Task,Trivial,Resolved,"2015-09-22 19:49:58","2015-09-22 18:49:58",1
"Apache Mesos","Enable ubuntu builds in ASF CI","I've disabled ubuntu:14.04 builds on ASF CI because the job randomly fails on fetching packages.    We need to figure out what the problem is and fix it before enabling testing on ubuntu.",Task,Major,Resolved,"2015-09-22 18:26:50","2015-09-22 17:26:50",1
"Apache Mesos","Mesos UI fails to represent JSON entities","The Mesos UI is broken, it seems to fail to represent JSON from /state. This may have been introduced with https://reviews.apache.org/r/38028 ",Bug,Major,Resolved,"2015-09-22 17:53:23","2015-09-22 16:53:23",1
"Apache Mesos","Add support for exposing Accept/Decline responses for inverse offers","Current implementation of maintenance primitives does not support exposing Accept/Decline responses of frameworks to the cluster operators.   This functionality is necessary to provide visibility to operators into whether a given framework is ready to comply with the posted maintenance schedule.",Bug,Major,Resolved,"2015-09-22 17:16:44","2015-09-22 16:16:44",2
"Apache Mesos","Make hook execution order deterministic","Currently, when using multiple hooks of the same type, the execution order is implementation-defined.   This is because in src/hook/manager.cpp, the list of available hooks is stored in a {{hashmap<string, Hook*>}}. A hashmap is probably unnecessary for this task since the number of hooks should remain reasonable. A data structure preserving ordering should be used instead to allow the user to predict the execution order of the hooks. I suggest that the execution order should be the order in which hooks are specified with {{--hooks}} when starting an agent/master.  This will be useful when combining multiple hooks after MESOS-3366 is done.",Improvement,Major,Resolved,"2015-09-22 00:55:06","2015-09-21 23:55:06",3
"Apache Mesos","LinuxFilesystemIsolator should make the slave's work_dir a shared mount.","So that when a user task is forked, it does not hold extra references to the sandbox mount and provisioner bind backend mounts. If we don't do that, we could get the following error message when cleaning up bind backend mount points and sandbox mount points.  ",Bug,Major,Resolved,"2015-09-21 18:55:22","2015-09-21 17:55:22",3
"Apache Mesos","Add const accessor to Master flags","It would make sense to have an accessor to the master's flags, especially for tests.  For example, see [this test|https://github.com/apache/mesos/blob/2876b8c918814347dd56f6f87d461e414a90650a/src/tests/master_maintenance_tests.cpp#L1231-L1235].",Task,Trivial,Reviewable,"2015-09-21 17:26:34","2015-09-21 16:26:34",2
"Apache Mesos","Refactor Executor struct in Slave to handle HTTP based executors","Currently, the {{struct Executor}} in slave only supports executors connected via message passing (driver). We should refactor it to add support for HTTP based Executors similar to what was done for the Scheduler API {{struct Framework}} in {{src/master/master.hpp}}",Task,Major,Resolved,"2015-09-21 16:51:52","2015-09-21 15:51:52",3
"Apache Mesos","Refactor Status Update method on Agent to handle HTTP based Executors","Currently, receiving a status update sent from slave to itself , {{runTask}} , {{killTask}} and status updates from executors are handled by the {{Slave::statusUpdate}} method on Slave. The signature of the method is {{void Slave::statusUpdate(StatusUpdate update, const UPID& pid)}}.   We need to create another overload of it that can also handle HTTP based executors which the previous PID based function can also call into. The signature of the new function could be:  {{void Slave::statusUpdate(StatusUpdate update, Executor* executor)}}  The HTTP Executor would also call into this new function via {{src/slave/http.cpp}}",Task,Major,Resolved,"2015-09-20 20:19:19","2015-09-20 19:19:19",8
"Apache Mesos","TestContainerizer should not modify global environment variables.","Currently the {{TestContainerizer}} modifies the environment variables. Since these are global variables, this can cause other threads reading these variables to get inconsistent results, or even segfault if they happen to read while the environment is being changed.  Synchronizing within the TestContainerizer is not sufficient. We should pass the environment variables into a fork, or set them on the command line of an execute.",Bug,Major,Resolved,"2015-09-20 19:03:08","2015-09-20 18:03:08",8
"Apache Mesos","RegistryTokenTest.ExpiredToken test is flaky","RegistryTokenTest.ExpiredToken test is flaky. Here is the error I got on OSX after running it for several times:  ",Bug,Major,Resolved,"2015-09-20 01:32:19","2015-09-20 00:32:19",3
"Apache Mesos","UserCgroupIsolatorTest failed on CentOS 6.6","UserCgroupIsolatorTest use /sys/fs/cgroup as cgroups_hierarchy. But CentOS 6.6 cgroups_hierarchy is /cgroup. Need change to follow the way in ContainerizerTest.",Bug,Major,Resolved,"2015-09-19 20:19:02","2015-09-19 19:19:02",1
"Apache Mesos","Improve apply_reviews.sh script to apply chain of reviews","Currently the support/apply-review.sh script allows an user (typically committer) to apply a single review on top the HEAD. Since Mesos contributors typically submit a chain of reviews for a given issue it makes sense for the script to apply the whole chain recursively.",Improvement,Major,Resolved,"2015-09-19 00:13:03","2015-09-18 23:13:03",8
"Apache Mesos","Provide the users with a fully writable filesystem","In the first phase of filesystem provisioning and isolation we are disallowing (or at least should, especially in the case of CopyBackend) users to write outside the sandbox without explicitly mounting specific volumes into the container. We do this even when OverlayBackend can potentially support a empty writable top layer.  However in the real world use of containers (and for people coming from the VM world), users and applications often are used to being able to write to the full filesystem (restricted by plain file system permissions) with reasons ranging from applications being non-portable (filesystem-wise) to the need to do custom installs at run time to system directories (inside its container).  In general, it's a good practice to restrict the application to write to confined locations and software dependencies can be managed through pre-packaged layers but these often introduce a high entry barrier for users.  We should discuss a solution that gives the users the option to write to a full filesystem with a filesystem layer on top of provisioned images and optionally enable persistence of that layer through persistent volumes. This has implication in the management of user namespaces and resource reservations and requires a thorough design.",Story,Major,Accepted,"2015-09-18 22:31:10","2015-09-18 21:31:10",13
"Apache Mesos","Add metrics for filesystem isolation and image provisioning.","We need to know about:  1) Errors encountered while provisioning root filesystems 2) Errors encountered while cleaning up root filesystems 3) Number of containers changing root filesystem ...",Task,Major,Resolved,"2015-09-18 19:51:46","2015-09-18 18:51:46",2
"Apache Mesos","Change /machine/up and /machine/down endpoints to take an array","With [MESOS-3312] committed, the {{/machine/up}} and {{/machine/down}} endpoints should also take an input as an array.  It is important to change this before maintenance primitives are released: https://reviews.apache.org/r/38011/  Also, a minor change to the error message from these endpoints: https://reviews.apache.org/r/37969/",Task,Major,Resolved,"2015-09-18 00:38:33","2015-09-17 23:38:33",1
"Apache Mesos","Segfault when accepting or declining inverse offers","Discovered while writing a test for filters (in regards to inverse offers).  Fix here: https://reviews.apache.org/r/38470/",Bug,Blocker,Resolved,"2015-09-18 00:31:51","2015-09-17 23:31:51",1
"Apache Mesos","Add flag to disable hostname lookup","In testing / buildinging DCOS we've found that we need to set --hostname explicitly on the masters. For our uses IP and `hostname` must always be the same thing.   More in general, under certain circumstances, dynamic lookup of {{hostname}}, while successful, provides undesirable results; we would also like, in those circumstances, be able to just set the hostname to the chosen IP address (possibly set via the {{\-\- ip_discovery_command}} method).  We suggest adding a {{\-\-no-hostname-lookup}}.  Note that we can introduce this flag as {{--hostname-lookup}} with a default to 'true' (which is the current semantics) and that way someone can do {{\-\-no-hostname-lookup}} or {{\-\-hostname-lookup=false}}. ",Improvement,Major,Resolved,"2015-09-17 23:35:12","2015-09-17 22:35:12",3
"Apache Mesos","Higher level construct for expressing process dispatch","Since mesos code is based on the actor model and dispatching an interface  asynchronously is a large part of the code base, generalizing the concept of  asynchronously dispatching an interface would eliminate the need to manual  programming of the dispatch boilerplate.  An example usage:  For a simple interface like:    Today the developer has to do the following:  a. Write a wrapper class that implements the same interface to add the  dispatching boilerplate.  b. Spend precious time in reviews.  c. Risk introducing bugs.  None of the above steps add any value to the executable binary.  The wrapper class would look like:   At the caller/client site, the code would look like:                                                                         Proposal  We should use C++'s rich language semnatics to express the intent and avoid  the boilerplate we write manually.  The basic intent of the code that leads to all the boilerplate above is:  a. An interface that provides a set of functionality.  b. An implementation of the interface.  c. Ability to dispatch that interface asynchronously using actor.  C++ has a rich set of generics that can be used to express above.  Components  ProcessDispatcher  This component will dispatch an interface implementation asychronously using  the process framework.  This component can be expressed as:    DispatchInterface  Any interface that provides an implementation that can be dispatched can be  expressed using this component.  This component can be expressed as:     Usage:  Simple usage                                                                     Collecting the interface in a container    The advantages of using the generic dispatcher:  Saves time by avoiding to write all the boilerplate and going through review cycles. Less bugs. Focus on real problem and not boilerplate. ",Bug,Major,Accepted,"2015-09-17 15:28:30","2015-09-17 14:28:30",6
"Apache Mesos","Expand the range of integer precision in json <-> protobuf conversions to include unsigned integers","The previous changes (MESOS-3345) to support integer precision when converting JSON <-> Protobuf did not support precision for unsigned integers between {{INT64_MAX}} and {{UINT64_MAX}}.  (There's some loss, but the conversion is still as good/bad as it was with doubles.)    This problem is due to a limitation in the JSON parsing library we use (PicoJSON), which parses integers as {{int64_t}}.    Some possible solutions or things to investigate:  * We can patch PicoJSON to parse some large values as {{uint64_t}}.  * We can investigate using another parsing library.  * If we want extra precision beyond 64 or 80 bits per double, one possibility is the [GMP library|https://gmplib.org/].  We'd still need to change the parsing library though.    *NOTE: The [asV1Protobuf|https://github.com/apache/mesos/blob/d10a33acc426dda9e34db995f16450faf898bb3b/src/common/http.cpp#L172-L423] copy needs to also be updated.*",Bug,Minor,Open,"2015-09-16 23:05:43","2015-09-16 22:05:43",5
"Apache Mesos","Windows: Port protobuf_tests.hpp","We have ported `stout/protobuf.hpp`, but to make the `protobuf_tests.cpp` file to work, we need to port `stout/uuid.hpp`.",Task,Major,Resolved,"2015-09-16 08:15:57","2015-09-16 07:15:57",2
"Apache Mesos","Unmount irrelevant host mounts in the new container's mount namespace.","As described in this [TODO|https://github.com/apache/mesos/blob/e601e469c64594dd8339352af405cbf26a574ea8/src/slave/containerizer/isolators/filesystem/linux.cpp#L418]: {noformat:title=}   // TODO(<USER>: Try to unmount work directory mounts and persistent   // volume mounts for other containers to release the extra   // references to those mounts. {noformat}  This will a best effort attempt to alleviate the race condition between provisioner's container cleanup and new containers copying host mount table.",Task,Major,Resolved,"2015-09-15 21:59:32","2015-09-15 20:59:32",3
"Apache Mesos","Unify the implementations of the image provisioners.","The current design uses separate provisioner implementation for each type of image (e.g., APPC, DOCKER).  This creates a lot of code duplications. Since we already have a unified provisioner backend (e.g., copy, bind, overlayfs), we should be able to unify the implementations of image provisioners and hide the image specific logics in the corresponding 'Store' implementation.",Task,Major,Resolved,"2015-09-15 19:05:39","2015-09-15 18:05:39",5
"Apache Mesos","LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithoutRootFilesystem fails on CentOS 7.1","Just ran ROOT tests on CentOS 7.1 and had the following failure (clean build, just pulled from {{master}}): ",Bug,Major,Resolved,"2015-09-15 04:13:44","2015-09-15 03:13:44",2
"Apache Mesos","Support running filesystem isolation with Command Executor in MesosContainerizer",,Bug,Major,Resolved,"2015-09-14 21:53:03","2015-09-14 20:53:03",4
"Apache Mesos","process::collect and process::await do not perform discard propagation.","When aggregating futures with collect, one may discard the outer future:    Discard requests should propagate down into the inner futures being collected.",Bug,Major,Resolved,"2015-09-14 21:11:06","2015-09-14 20:11:06",3
"Apache Mesos","Modify LinuxLauncher to support Systemd","Implement the solution described in MESOS-3352 in the LinuxLauncher  In order to avoid the migration of cgroup pids by Systemd we can use the {{delegate=true}} flag. This guards Systemd from migrating the pids that are descendants of the process launched by a Systemd unit.  In order for this strategy to work, the {{delegate}} flag must be supported by the Systemd version. Support for this was introduced in Systemd v218; however, it has also been backported to v208 for RHEL7 and CentOS7 [here|http://centoserrata.nagater.net/item/CEBA-2015-0037-CentOS-7.i386.x86_64.html] with the package [systemd-208-20|https://rhn.redhat.com/errata/RHBA-2015-1155.html]. It is highly recommended to upgrade to this package if running those operating systems.  Once the {{delegate=true}} flag has been set, the cgroups that are manually manipulated by the agent will no longer be migrated *during the lifetime of the agent*.  This still leaves the problem of tasks being migrated _after the agent has stopped running_ (voluntarily or not). In order to deal with the problem we propose the following solution:  If an agent is running on a Systemd initialized machine, then the agent will create a Systemd slice with a life-time that is independent of the agent and {{delegate=true}}. The linux launcher (used when cgroups isolators are enabled) will then assign the cgroup name for any executor that is launched to this separate slice. The consequence of this is that when the agent unit is terminated, the separate slice will continue to delegate the cgroups preventing Systemd from migrating the pids. A side benefit of this is that we can maintain the {{KillMode=control-group}} flag on the agent and terminate all agent specific services such as the {{fetcher}} without terminating the tasks. This provides for a nice clean-up.  This solution will still require that the agent unit be launched with the {{delegate=true}} flag such that there is no race during the transition of the pids from the agent to the separate slice.  The agent will be responsible for verifying the slice is still available upon recovery, and warning the operator if it notices that the tasks it is recovering are no longer associated with this separate slice, as this can cause *silent* loss of isolation of existing tasks.",Task,Major,Resolved,"2015-09-14 21:03:40","2015-09-14 20:03:40",8
"Apache Mesos","Support fetching AppC images into the store","So far AppC store is read only and depends on out of band mechanisms to get the images. We need to design a way to support fetching in a native way.  As commented on MESOS-2824: It's unacceptable to have either have: * the slave to be blocked for extended period of time (minutes) which delays the communication between the executor and scheduler, or * the first task that uses this image to be blocked for a long time to wait for the container image to be ready.  The solution needs to enable the operator to prefetch a list of preferred images without introducing the above problems.",Task,Major,Resolved,"2015-09-14 19:39:01","2015-09-14 18:39:01",5
"Apache Mesos","Perf event isolator stops performing sampling if a single timeout occurs.","Currently the perf event isolator times out a sample after a fixed extra time of 2 seconds on top of the sample time elapses:    This should be based on the reap interval maximum.  Also, the code stops sampling altogether when a single timeout occurs. We've observed time outs during normal operation, so it would be better for the isolator to continue performing perf sampling in the case of timeouts. It may also make sense to continue sampling in the case of errors, since these may be transient.",Bug,Major,Resolved,"2015-09-14 19:29:27","2015-09-14 18:29:27",3
"Apache Mesos","Factor out V1 API test helper functions","We currently have some helper functionality for V1 API tests. This is copied in a few test files. Factor this out into a common place once the API is stabilized.    We can also update the helpers in {{/tests/mesos.hpp}} to support the V1 API.  This would let us get ride of lines like:  In favor of: ",Improvement,Major,Resolved,"2015-09-13 21:48:19","2015-09-13 20:48:19",2
"Apache Mesos","Log source address replicated log recieved broadcasts","Currently Mesos doesn't log what machine a replicated log status broadcast was recieved from:   It would be really useful for debugging replicated log startup issues to have info about where the message came from (libprocess address, ip, or hostname) the message came from",Improvement,Minor,Resolved,"2015-09-12 06:55:39","2015-09-12 05:55:39",2
"Apache Mesos","Publish egg for 0.24.0 to PyPI","0.24.0 was released, but the python egg has not been published.",Task,Major,Resolved,"2015-09-12 00:47:15","2015-09-11 23:47:15",1
"Apache Mesos","Docker containerizer does not symlink persistent volumes into sandbox","For the ArangoDB framework I am trying to use the persistent primitives. nearly all is working, but I am missing a crucial piece at the end: I have successfully created a persistent disk resource and have set the persistence and volume information in the DiskInfo message. However, I do not see any way to find out what directory on the host the mesos slave has reserved for us. I know it is ${MESOS_SLAVE_WORKDIR}/volumes/roles/<myRole>/<NAME>_<UUID> but we have no way to query this information anywhere. The docker containerizer does not automatically mount this directory into our docker container, or symlinks it into our sandbox. Therefore, I have essentially no access to it. Note that the mesos containerizer (which I cannot use for other reasons) seems to create a symlink in the sandbox to the actual path for the persistent volume. With that, I could mount the volume into our docker container and all would be well.",Bug,Major,Resolved,"2015-09-11 15:06:11","2015-09-11 14:06:11",5
"Apache Mesos","Refactor the plain JSON parsing in the docker containerizer","Two functions in the Docker-related code take a string and parse it to JSON: * {{Docker::Container::create}} in {{src/docker/docker.cpp}} * {{Token::create}} in {{src/slave/containerizer/provisioners/docker/token_manager.cpp}}  This JSON is then validated (lots of if-elses) and used via the {{JSON::Value}} accessors.  We could instead use a protobuf and the related Stout JSON->Protobuf conversion function.",Improvement,Minor,Accepted,"2015-09-10 19:22:05","2015-09-10 18:22:05",3
"Apache Mesos","mesos-execute does not support credentials","mesos-execute does not appear to support passing credentials. This makes it impossible to use on a cluster where framework authentication is required.",Bug,Major,Resolved,"2015-09-10 01:39:04","2015-09-10 00:39:04",2
"Apache Mesos","Rewrite perf events code","Our current code base invokes and parses `perf stat`, which sucks, because cmdline output is not a stable ABI at all, it can break our code at any time, for example MESOS-2834.  We should use the stable API perf_event_open(2). With this patch https://reviews.apache.org/r/37540/, we already have the infrastructure for the implementation, so it should not be hard to rewrite all the perf events code.",Task,Minor,"In Progress","2015-09-09 01:51:17","2015-09-09 00:51:17",5
"Apache Mesos","Remove unused executor protobuf","The executor protobuf definition living outside the v1/ directory is unused, it should  be removed to avoid confusion.",Task,Major,Resolved,"2015-09-08 23:26:45","2015-09-08 22:26:45",1
"Apache Mesos","Document a test pattern for expediting event firing","We use {{Clock::advance()}} extensively in tests to expedite event firing and minimize overall {{make check}} time. Document this pattern for posterity.",Documentation,Minor,Resolved,"2015-09-07 17:08:49","2015-09-07 16:08:49",3
"Apache Mesos","Add executor protobuf to v1","A new protobuf for Executor was introduced in Mesos for the HTTP API, it needs to be added to /v1 so it reflects changes made on v1/mesos.proto. This protobuf is ought to be changed as the executor HTTP API design evolves.",Task,Major,Resolved,"2015-09-05 03:34:25","2015-09-05 02:34:25",1
"Apache Mesos","Add device support in cgroups abstraction","Add support for [device cgroups|https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt] to aid isolators controlling access to devices.  In the future, we could think about how to numerate and control access to devices as resource or task/container policy",Task,Major,Resolved,"2015-09-04 19:16:55","2015-09-04 18:16:55",3
"Apache Mesos","Allow resources/attributes discovery","In heterogeneous clusters, tasks sometimes have strong constraints on the type of hardware they need to execute on. The current solution is to use custom resources and attributes on the agents. Detecting non-standard resources/attributes requires wrapping the mesos-slave binary behind a script and use custom code to probe the agent. Unfortunately, this approach doesn't allow composition. The solution would be to provide a hook/module mechanism to allow users to use custom code performing resources/attributes discovery.  Please review the detailed document below: https://docs.google.com/document/d/15OkebDezFxzeyLsyQoU0upB0eoVECAlzEkeg0HQAX9w  Feel free to express comments/concerns by annotating the document or by replying to this issue. ",Improvement,Major,Resolved,"2015-09-04 02:36:07","2015-09-04 01:36:07",3
"Apache Mesos","Export per container SNMP statistics","We need to export the per container SNMP statistics too, from its /proc/net/snmp.",Task,Major,Resolved,"2015-09-04 02:16:34","2015-09-04 01:16:34",5
"Apache Mesos","Update quota design doc based on user comments and offline syncs","We got plenty of feedback from different parties, which we would like to persist in the design doc for posterity.",Documentation,Major,Resolved,"2015-09-02 13:48:39","2015-09-02 12:48:39",3
"Apache Mesos","Scope out approaches to deal with logging to finite disks (i.e. log rotation|capped-size logging).","For the background, see the parent story [MESOS-3348].  For the work/design/discussion, see the linked design document (below).  ",Task,Major,Resolved,"2015-09-01 21:48:01","2015-09-01 20:48:01",5
"Apache Mesos","Problem Statement Summary for Systemd Cgroup Launcher","There have been many reports of cgroups related issues when running Mesos on Systemd. Many of these issues are rooted in the manual manipulation of the cgroups filesystem by Mesos. This task is to describe the problem in a 1-page summary, and elaborate on the suggested 2 part solution: 1. Using the {{delegate=true}} flag for the slave 2. Implementing a Systemd launcher to run executors with tighter Systemd integration.",Task,Major,Resolved,"2015-09-01 16:07:00","2015-09-01 15:07:00",5
"Apache Mesos","Removing mount point fails with EBUSY in LinuxFilesystemIsolator.","When running the tests as root, we found PersistentVolumeTest.AccessPersistentVolume fails consistently on some platforms.    Turns out that the 'rmdir' after the 'umount' fails with EBUSY because there's still some references to the mount.  FYI [~<USER> [~<USER>",Bug,Major,Resolved,"2015-09-01 03:26:07","2015-09-01 02:26:07",5
"Apache Mesos","Add either log rotation or capped-size logging (for tasks)","Tasks currently log their output (i.e. stdout/stderr) to files (the sandbox) on an agent's disk.  In some cases, the accumulation of these logs can completely fill up the agent's disk and thereby kill the task or machine.  To prevent this, we should either implement a log rotation mechanism or capped-size logging.  This would be used by executors to control the amount of logs they keep.    Master/agent logs will not be affected.  We will first scope out several possible approaches for log rotation/capping in a design document (see [MESOS-3356]).  Once an approach is chosen, this story will be broken down into some corresponding issues.",Story,Major,Resolved,"2015-09-01 01:44:50","2015-09-01 00:44:50",13
"Apache Mesos","Add filter support for inverse offers","A filter attached to the inverse offer can be used by the framework to control when it wants to be contacted again with the inverse offer, since future circumstances may change the viability of the maintenance schedule.  The “filter” for InverseOffers is identical to the existing mechanism for re-offering Offers to frameworks.",Task,Major,Resolved,"2015-09-01 00:18:54","2015-08-31 23:18:54",5
"Apache Mesos","Expand the range of integer precision when converting into/out of json.","For [MESOS-3299], we added some protobufs to represent time with integer precision.  However, this precision is not maintained through protobuf <-> JSON conversion, because of how our JSON encoders/decoders convert numbers to floating point.  To maintain precision, we can try one of the following: * Try using a {{long double}} to represent a number. * Add logic to stringify/parse numbers without loss when possible. * Try representing {{int64_t}} as a string and parse it as such? * Update PicoJson and add a compiler flag, i.e. {{-DPICOJSON_USE_INT64}}   In all cases, we'll need to make sure that: * Integers are properly stringified without loss. * The JSON decoder parses the integer without loss. * We have some unit tests for big (close to {{INT32_MAX}}/{{INT64_MAX}}) and small integers.",Task,Minor,Resolved,"2015-08-31 22:56:23","2015-08-31 21:56:23",5
"Apache Mesos","Rate Limiting functionality for HTTP Frameworks","We need to build rate limiting functionality for frameworks connecting via the Scheduler HTTP API similar to the PID based frameworks.  Link to the rate-limiting section from design doc: https://docs.google.com/document/d/1pnIY_HckimKNvpqhKRhbc9eSItWNFT-priXh_urR-T0/edit#heading=h.kzgdk4d5fmba  - This ticket deals with refactoring the existing PID based framework functionality and extend it for HTTP frameworks. - The second part of notifying the framework when rate-limiting is active i.e. returning a status of 429 can be undertook as part of MESOS-1664",Task,Major,Open,"2015-08-31 21:57:51","2015-08-31 20:57:51",5
"Apache Mesos","Command-line flags should take precedence over OS Env variables","Currently, it appears that re-defining a flag on the command-line that was already defined via a OS Env var ({{MESOS_*}}) causes the Master to fail with a not very helpful message.  For example, if one has {{MESOS_QUORUM}} defined, this happens:   which is not very helpful.  Ideally, we would parse the flags with a well-known priority (command-line first, environment last) - but at the very least, the error message should be more helpful in explaining what the issue is.",Improvement,Major,Resolved,"2015-08-31 19:06:29","2015-08-31 18:06:29",2
"Apache Mesos","Implement filtering mechanism for (Scheduler API Events) Testing","Currently, our testing infrastructure does not have a mechanism of filtering/dropping HTTP events of a particular type from the Scheduler API response stream.  We need a {{DROP_HTTP_CALLS}} abstraction that can help us to filter a particular event type.    This helper code is duplicated in at least two places currently, Scheduler Library/Maintenance Primitives tests.  - The solution can be as trivial as moving this helper function to a common test-header. - Implement a {{DROP_HTTP_CALLS}} similar to what we do for other protobufs via {{DROP_CALLS}}.",Task,Major,Resolved,"2015-08-31 17:26:12","2015-08-31 16:26:12",3
"Apache Mesos","Dynamic reservations are not counted as used resources in the master","Dynamically reserved resources should be considered used or allocated and hence reflected in Mesos bookkeeping structures and {{state.json}}.  I expanded the {{ReservationTest.ReserveThenUnreserve}} test with the following section: {code}   // Check that the Master counts the reservation as a used resource.   {     Future<process::http::Response> response =       process::http::get(master.get(), state.json);     AWAIT_READY(response);      Try<JSON::Object> parse = JSON::parse<JSON::Object>(response.get().body);     ASSERT_SOME(parse);      Result<JSON::Number> cpus =       parse.get().find<JSON::Number>(slaves[0].used_resources.cpus);      ASSERT_SOME_EQ(JSON::Number(1), cpus);   } {code} and got   Idea for new resources states: https://docs.google.com/drawings/d/1aquVIqPY8D_MR-cQjZu-wz5nNn3cYP3jXqegUHl-Kzc/edit",Bug,Minor,Resolved,"2015-08-31 16:53:39","2015-08-31 15:53:39",3
"Apache Mesos","Refactored libprocess SSL tests."," Refactor SSL test fixture to be available for reuse by other projects. Currently the fixture class and its the symbols it depends on are not present in libproces's include files.",Task,Major,Resolved,"2015-08-31 15:31:14","2015-08-31 14:31:14",3
"Apache Mesos","FlagsBase copy-ctor leads to dangling pointer.","Per [#3328], ubsan detects the following problem:  [ RUN ] FaultToleranceTest.ReregisterCompletedFrameworks /mesos/3rdparty/libprocess/3rdparty/stout/include/stout/flags/flags.hpp:303:25: runtime error: load of value 33, which is not a valid value for type 'bool'  I believe what is going on here is the following: * The test calls StartMaster(), which does MesosTest::CreateMasterFlags() * MesosTest::CreateMasterFlags() allocates a new master::Flags on the stack, which is subsequently copy-constructed back to StartMaster() * The FlagsBase constructor is: bq. {{FlagsBase() { add(&help, help, ..., false); }}} where help is a member variable -- i.e., it is allocated on the stack in this case. * {{FlagsBase()::add}} captures {{&help}}, e.g.:  * The implicit copy constructor for FlagsBase is just going to copy the lambda above, i.e., the result of the copy constructor will have a lambda that points into MesosTest::CreateMasterFlags()'s stack frame, which is bad news.  Not sure the right fix -- comments welcome. You could define a copy-ctor for FlagsBase that does something gross (basically remove the old help flag and define a new one that points into the target of the copy), but that seems, well, gross.  Probably not a pressing-problem to fix -- AFAICS worst symptom is that we end up reading one byte from some random stack location when serving {{state.json}}, for example.",Bug,Major,Resolved,"2015-08-30 02:45:52","2015-08-30 01:45:52",8
"Apache Mesos","Support HTTP Pipelining in libprocess (http::post)","Currently , {{http::post}} in libprocess, does not support HTTP pipelining. Each call as of know sends in the {{Connection: close}} header, thereby, signaling to the server to close the TCP socket after the response.  We either need to create a new interface for supporting HTTP pipelining , or modify the existing {{http::post}} to do so.  This is needed for the Scheduler/Executor library implementations to make sure Calls are sent in order to the master. Currently, in order to do so, we send in the next request only after we have received a response for an earlier call that results in degraded performance.  ",Task,Major,Resolved,"2015-08-28 21:20:38","2015-08-28 20:20:38",8
"Apache Mesos","Make use of C++11 atomics","Now that we require C++11, we can make use of std::atomic. For example:  * libprocess/process.cpp uses a bare int + __sync_synchronize() for running * __sync_synchronize() is used in logging.hpp in libprocess and fork.hpp in stout * sched/sched.cpp uses a volatile int for running -- this is wrong, volatile is not sufficient to ensure safe concurrent access * volatile is used in a few other places -- most are probably dubious but I haven't looked closely",Bug,Major,Resolved,"2015-08-28 01:11:23","2015-08-28 00:11:23",2
"Apache Mesos","Auto-generate protos for stout tests","Stout protobufs (AFAIK right now it's just a single file {{protobuf_tests.proto}}) are not generated automatically. Including proto generation step would be cleaner and more convenient.",Improvement,Minor,Accepted,"2015-08-27 02:44:48","2015-08-27 01:44:48",2
"Apache Mesos","Spurious fetcher message about extracting an archive","The fetcher emits a spurious log message about not extracting an archive with .tgz extension, even though the tarball is extracted correctly.  ",Bug,Trivial,Resolved,"2015-08-26 20:53:47","2015-08-26 19:53:47",1
"Apache Mesos","Mesos will not build when configured with gperftools enabled","Mesos configured with {{--enable-perftools}} currently will not build on OSX 10.10.4 or Ubuntu 14.04, possibly because the bundled gperftools-2.0 is not current. The stable release is now 2.4, which builds successfully on both of these platforms.  This issue is resolved when Mesos will build successfully out of the box with gperftools enabled. After this ticket is resolved, the libprocess profiler should be tested to confirm that it still works and if not, it should be fixed.",Bug,Major,Resolved,"2015-08-26 19:56:29","2015-08-26 18:56:29",2
"Apache Mesos","Rework Jenkins build script","Mesos Jenkins build script needs to be reworked to support the following:  - Wider test coverage (libevent, libssl, root tests, Docker tests). - More OS/compiler Docker images for testing Mesos. - Excluding tests on per-image basis. - Reproducing the test image locally. ",Task,Major,Reviewable,"2015-08-26 05:26:41","2015-08-26 04:26:41",3
"Apache Mesos","Factor out JSON to repeated protobuf conversion","In general, we have the collection of protobuf messages as another protobuf message, which makes JSON -> protobuf conversion straightforward. This is not always the case, for example, {{Resources}} class is not a protobuf, though protobuf-convertible.  To facilitate conversions like JSON -> {{Resources}} and avoid writing code for each particular case, we propose to introduce {{JSON::Array}} -> {{repeated protobuf}} conversion. With this in place, {{JSON::Array}} -> {{Resources}} boils down to {{JSON::Array}} -> {{repeated Resource}} -> (extra c-tor call) -> {{Resources}}.",Improvement,Major,Resolved,"2015-08-26 03:37:17","2015-08-26 02:37:17",2
"Apache Mesos",SlaveTest.HTTPSchedulerSlaveRestart,"Observed on ASF CI  ",Bug,Major,Resolved,"2015-08-25 23:18:41","2015-08-25 22:18:41",2
"Apache Mesos","Support provisioning images specified in volumes.","This is related to MESOS-3095 and MESOS-3227.  The idea is that we should allow command executor to run under host filesystem and provision the filesystem for the user. The command line executor will then chroot into user's root filesystem.  This solves the issue that the command executor is not launchable in the user specified root filesystem.   The design doc is here: https://docs.google.com/document/d/16hyLVRL0nz-KBts1J5stGyxZPniFPbPbs7R-ZRQVCH4/edit?usp=sharing",Task,Major,Resolved,"2015-08-24 20:48:06","2015-08-24 19:48:06",3
"Apache Mesos","Define the container rootfs directories within the slave work_dir.","A few motivations:  1) Given the design in MESOS-3004 it became apparent that we need to support multiple images in a container and these images can be of different image types. (There are no sufficient reasons or major obstacles that force us not to allow it and it obviously gives the users more flexibility).  2) Also, even though we currently allow only one backend for each provisioner, when we update a running slave there can be multiple backends left in each container that we need to launch tasks with, or at least recover. We should evaluate in the future whether to support multiple backends and choose among them dynamically based on image characteristics.  3) Since the rootfs' lifecycle tie with the running containers and should be cleaned up after containers die, it fits into the pattern of {{word_dir}} and we can manage them inside the work dir without needing to ask the operator to specify more flags. ",Task,Major,Resolved,"2015-08-24 19:08:59","2015-08-24 18:08:59",2
"Apache Mesos","Configurable size of completed task / framework history","We try to make Mesos work with multiple frameworks and mesos-dns at the same time. The goal is to have set of frameworks per team / project on a single Mesos cluster.  At this point our mesos state.json is at 4mb and it takes a while to assembly. 5 mesos-dns instances hit state.json every 5 seconds, effectively pushing mesos-master CPU usage through the roof. It's at 100%+ all the time.  Here's the problem:    Active tasks are just 6% of state.json response:    I see four options that can improve the situation:  1. Add query string param to exclude completed tasks from state.json and use it in mesos-dns and similar tools. There is no need for mesos-dns to know about completed tasks, it's just extra load on master and mesos-dns.  2. Make history size configurable.  3. Make JSON serialization faster. With 10000s of tasks even without history it would take a lot of time to serialize tasks for mesos-dns. Doing it every 60 seconds instead of every 5 seconds isn't really an option.  4. Create event bus for mesos master. Marathon has it and it'd be nice to have it in Mesos. This way mesos-dns could avoid polling master state and switch to listening for events.  All can be done independently.  Note to <USER>folks: please start distributing debug symbols with your distribution. I was asking for it for a while and it is really helpful: https://github.com/<USER>marathon/issues/1497#issuecomment-104182501  Perf report for leading master:   !http://i.imgur.com/iz7C3o0.png!  I'm on 0.23.0.",Bug,Major,Resolved,"2015-08-24 17:50:13","2015-08-24 16:50:13",3
"Apache Mesos","Remove remnants of LIBPROCESS_STATISTICS_WINDOW","As seen in MESOS-1283, LIBPROCESS_STATISTICS_WINDOW is no longer needed since metrics now require specification of a window size, and default to no history if not provided.  Some commented-out code remnants associated with this environment variable still remain and should be removed.",Improvement,Trivial,Resolved,"2015-08-22 00:47:53","2015-08-21 23:47:53",1
"Apache Mesos","Add a protobuf to represent time with integer precision.","Existing timestamps in the protobufs use {{double}} to encode time.  Generally, the field represents seconds (with the decimal component to represent smaller denominations of time).  This is less than ideal.  Instead, we should use integers, so as to not lose data (and to be able to compare value reliably).  Something like: ",Task,Major,Resolved,"2015-08-19 22:46:56","2015-08-19 21:46:56",1
"Apache Mesos","Failing ROOT_ tests on CentOS 7.1 - MesosContainerizerLaunchTest","h2. MesosContainerizerLaunchTest  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  ",Bug,Blocker,Resolved,"2015-08-19 00:50:40","2015-08-18 23:50:40",5
"Apache Mesos","Failing ROOT_ tests on CentOS 7.1 - LinuxFilesystemIsolatorTest","h2. LinuxFilesystemIsolatorTest  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  ",Bug,Blocker,Resolved,"2015-08-19 00:49:18","2015-08-18 23:49:18",5
"Apache Mesos","Failing ROOT_ tests on CentOS 7.1 - ContainerizerTest","h2. ContainerizerTest.ROOT_CGROUPS_BalloonFramework  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  ",Bug,Blocker,Resolved,"2015-08-19 00:48:02","2015-08-18 23:48:02",5
"Apache Mesos","Failing ROOT_ tests on CentOS 7.1 - UserCgroupIsolatorTest","h2. UserCgroupIsolatorTest  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  ",Bug,Blocker,Resolved,"2015-08-19 00:47:00","2015-08-18 23:47:00",5
"Apache Mesos","Failing ROOT_ tests on CentOS 7.1 - LimitedCpuIsolatorTest","h2. LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  ",Bug,Blocker,Resolved,"2015-08-19 00:40:20","2015-08-18 23:40:20",5
"Apache Mesos","Master should drop HTTP calls when it's recovering","Much like what we do with PID based frameworks, master should drop HTTP calls if it's not the leader and/or still recovering.",Bug,Major,Resolved,"2015-08-18 18:47:42","2015-08-18 17:47:42",3
"Apache Mesos","Add DockerRegistry unit tests","Add unit tests suite for docker registry implementation.  This could include:  - Creating mock docker registry server - Using openssl library for digest functions.",Task,Major,Resolved,"2015-08-18 18:10:49","2015-08-18 17:10:49",5
"Apache Mesos","Implement docker registry client","Implement the docker registry client as per design document:  https://docs.google.com/document/d/1kE-HXPQl4lQgamPIiaD4Ytdr-N4HeQc4fnE93WHR4X4/edit",Task,Major,Resolved,"2015-08-18 18:08:23","2015-08-18 17:08:23",5
"Apache Mesos","downloadWithHadoop tries to access Error() for a valid Try<bool>","This was reported while trying to install Hadoop / Mesos integration:   This is, however, a genuine bug in {{src/launcher/fetcher.cpp#L99}}:   The root cause is that (probably) the HDFS client is not available on the slave; however, we do not {{error()}} but rather return a {{false}} result.  The bug is exposed in the {{return}} line, where we try to retrieve {{available.error()}} (which is not there - it's just `false`).  This was a 'latent' bug that has been exposed by (my) recent refactoring of {{os::shell}} which is used by {{hdfs.available()}} under the covers.",Bug,Major,Resolved,"2015-08-18 08:47:13","2015-08-18 07:47:13",1
"Apache Mesos","JSON representation of Protobuf should use base64 encoding for 'bytes' fields.","Currently we encode 'bytes' fields as UTF-8 strings, which is lossy for binary data due to invalid byte sequences! In order to encode binary data in a lossless fashion, we can encode 'bytes' fields in base64.  Note that this is also how proto3 does its encoding (see [here|https://developers.google.com/protocol-buffers/docs/proto3?hl=en#json]), so this would make migration easier as well.",Bug,Major,Resolved,"2015-08-17 23:36:02","2015-08-17 22:36:02",3
"Apache Mesos","Create a user doc for Scheduler HTTP API","We need to convert the design doc into user doc that we can add to our docs folder.",Documentation,Major,Resolved,"2015-08-17 19:25:38","2015-08-17 18:25:38",3
"Apache Mesos","Master fails to access replicated log after network partition","In a 5 node cluster with 3 masters and 2 slaves, and ZK on each node, when a network partition is forced, all the masters apparently lose access to their replicated log. The leading master halts. Unknown reasons, but presumably related to replicated log access. The others fail to recover from the replicated log. Unknown reasons. This could have to do with ZK setup, but it might also be a Mesos bug.   This was observed in a Chronos test drive scenario described in detail here: https://github.com/mesos/chronos/issues/511  With setup instructions here: https://github.com/mesos/chronos/issues/508  ",Bug,Major,Resolved,"2015-08-17 17:11:15","2015-08-17 16:11:15",8
"Apache Mesos","EventCall Test Framework is flaky","Observed this on ASF CI. h/t [~<EMAIL>]  Looks like the HTTP scheduler never sent a SUBSCRIBE request to the master.  ",Bug,Major,Resolved,"2015-08-16 20:23:36","2015-08-16 19:23:36",5
"Apache Mesos","JSON serialization/deserialization of bytes is incorrect","Currently, we use our own serialization of bytes in json.hpp but we use picojson for deserialization.  We've observed that for some bytes the serialization results in a string that is incorrectly decoded by picojson.  Example:  String = \\\/\b\f\n\r\t\x00\x19 !#[]\x7F\xFF  Result of our own encoding:  \\\\\\\\\\/\\b\\f\\n\\r\\t\\u0000\\u0019 !#[]\\u007f\xFF\  picojson's encoding: \\\\\\\\\\/\\b\\f\\n\\r\\t\\u0000\\u0019 !#[]\\u007F\\u00FF\  Fix: We just use picojson to serialize bytes for consistency.",Bug,Major,Resolved,"2015-08-15 00:20:04","2015-08-14 23:20:04",2
"Apache Mesos","Stopping/Completing maintenance needs to reactivate agents.","After using the {{/maintenance/stop}} endpoint to end maintenance on a machine, any deactivated agents must be reactivated and allowed to register with the master.",Task,Major,Resolved,"2015-08-15 00:00:25","2015-08-14 23:00:25",5
"Apache Mesos","Starting maintenance needs to deactivate agents and kill tasks.","After using the {{/maintenance/start}} endpoint to begin maintenance on a machine, agents running on said machine should: * Be deactivated such that no offers are sent from that agent.  (Investigate if {{Master::deactivate(Slave*)}} can be used or modified for this purpose.) * Kill all tasks still running on the agent (See MESOS-1475). * Prevent other agents on that machine from registering or sending out offers.  This will likely involve some modifications to {{Master::register}} and {{Master::reregister}}.  ",Task,Major,Resolved,"2015-08-14 23:55:30","2015-08-14 22:55:30",8
"Apache Mesos","HTTPTest.NestedGet is flaky","[ RUN      ] HTTPTest.NestedGet ../../../3rdparty/libprocess/src/tests/http_tests.cpp:459: Failure Value of: response.get().status   Actual: 202 Accepted Expected: http::statuses[200] Which is: 200 OK *** Aborted at 1439569965 (unix time) try date -d @1439569965 if you are using GNU date *** PC: @           0x63abe8 testing::UnitTest::AddTestPartResult() *** SIGSEGV (@0x0) received by PID 25766 (TID 0x7f499415c780) from PID 0; stack trace: ***     @     0x7f499224dca0 (unknown)     @           0x63abe8 testing::UnitTest::AddTestPartResult()     @           0x62f6af testing::internal::AssertHelper::operator=()     @           0x43cd78 HTTPTest_NestedGet_Test::TestBody()     @           0x65935e testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @           0x653c5e testing::internal::HandleExceptionsInMethodIfSupported<>()     @           0x6349a3 testing::Test::Run()     @           0x635128 testing::TestInfo::Run()     @           0x635778 testing::TestCase::Run()     @           0x63c0e2 testing::internal::UnitTestImpl::RunAllTests()     @           0x65a11d testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @           0x654958 testing::internal::HandleExceptionsInMethodIfSupported<>()     @           0x63ae08 testing::UnitTest::Run()     @           0x4877f9 RUN_ALL_TESTS()     @           0x487613 main     @     0x7f49915739f4 __libc_start_main ",Bug,Major,Resolved,"2015-08-14 17:34:18","2015-08-14 16:34:18",2
"Apache Mesos","Cgroup CHECK fails test harness","CHECK in clean up of ContainerizerTest causes test harness to abort rather than fail or skip only perf related tests.  [ RUN      ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch [       OK ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch (628 ms) [----------] 24 tests from SlaveRecoveryTest/0 (38986 ms total)  [----------] 4 tests from MesosContainerizerSlaveRecoveryTest [ RUN      ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics ../../src/tests/mesos.cpp:720: Failure cgroups::mount(hierarchy, subsystem): 'perf_event' is already attached to another hierarchy ------------------------------------------------------------- We cannot run any cgroups tests that require a hierarchy with subsystem 'perf_event' because we failed to find an existing hierarchy or create a new one (tried '/tmp/mesos_test_cgroup/perf_event'). You can either remove all existing hierarchies, or disable this test case (i.e., --gtest_filter=-MesosContainerizerSlaveRecoveryTest.*). ------------------------------------------------------------- F0811 17:23:43.874696 12955 mesos.cpp:774] CHECK_SOME(cgroups): '/tmp/mesos_test_cgroup/perf_event' is not a valid hierarchy  *** Check failure stack trace: ***     @     0x7fb2fb4835fd  google::LogMessage::Fail()     @     0x7fb2fb48543d  google::LogMessage::SendToLog()     @     0x7fb2fb4831ec  google::LogMessage::Flush()     @     0x7fb2fb485d39  google::LogMessageFatal::~LogMessageFatal()     @           0x4e3f98  _CheckFatal::~_CheckFatal()     @           0x82f25a  mesos::internal::tests::ContainerizerTest<>::TearDown()     @           0xc030e3  testing::internal::HandleExceptionsInMethodIfSupported<>()     @           0xbf9050  testing::Test::Run()     @           0xbf912e  testing::TestInfo::Run()     @           0xbf9235  testing::TestCase::Run()     @           0xbf94e8  testing::internal::UnitTestImpl::RunAllTests()     @           0xbf97a4  testing::UnitTest::Run()     @           0x4a9df3  main     @     0x7fb2f9371ec5  (unknown)     @           0x4b63ee  (unknown) Build step 'Execute shell' marked build as failure",Bug,Major,Resolved,"2015-08-12 01:44:05","2015-08-12 00:44:05",2
"Apache Mesos","Ignore no statistics condition for containers with no qdisc","In PortMappingStatistics::execute, we log the following errors to stderr if the egress rate limiting qdiscs are not configured inside the container.    This can occur because of an error reading the qdisc (statistics function return an error) or because the qdisc does not exist (function returns none).    We should not log an error when the qdisc does not exist since this is normal behaviour if the container is created without rate limiting.  We do not want to gate this function on the slave rate limiting flag since we would have to compare the behaviour against the flag value at the time the container was created.",Bug,Major,Resolved,"2015-08-11 20:59:09","2015-08-11 19:59:09",2
"Apache Mesos","http::get API evaluates host wrongly","Currently libprocess http API sets the Host header field from the peer socket address (IP:port). The problem is that socket address might not be right HTTP server and might be just a proxy. ",Bug,Major,Resolved,"2015-08-11 19:42:38","2015-08-11 18:42:38",1
"Apache Mesos","Update FrameworkInfo.user on framework reregistration","Part 1 - Add user to master's state and update user in all slaves that have the registered framework.  Part 2 - Add test and also user to slave's internal state.",Improvement,Major,Accepted,"2015-08-08 07:33:08","2015-08-08 06:33:08",5
"Apache Mesos","HTTP requests with nested path are not properly handled by libprocess","For example, if master adds a route /api/v1/scheduler,  a handler named api/v1/scheduler is added to 'master' libprocess.  But when a request is posted to the above path, process::visit() looks for a http handler named api instead of api/v1/scheduler.  Ideally libprocess should look for handlers in the following preference order:  api/v1/scheduler  --> api/v1 --> api  ",Bug,Major,Resolved,"2015-08-07 22:31:45","2015-08-07 21:31:45",2
"Apache Mesos","Updated slave task label decorator hook to pass in ExecutorInfo.","If that task being launched has a command executor, there is no way for the hook to determine the executor-id for that task. The executor-id is sometimes required by the label decorators for accounting purposes and for preparing ground for executor-environment-decorator (which is not passed the TaskInfo).",Task,Major,Resolved,"2015-08-07 19:43:32","2015-08-07 18:43:32",1
"Apache Mesos","FetcherCacheHttpTest.HttpCachedSerialized and FetcherCacheHttpTest.HttpCachedConcurrent are flaky","On OSX, {{make clean && make -j8 V=0 check}}:   This was encountered just once out of 3+ {{make check}}s.",Bug,Major,Open,"2015-08-07 18:43:35","2015-08-07 17:43:35",2
"Apache Mesos","Implement image chroot support into command executor",,Improvement,Major,Resolved,"2015-08-07 02:37:41","2015-08-07 01:37:41",3
"Apache Mesos","Introduce an Either type.","We currently don't have an abstraction in stout to capture the notion of having a container with many types and a single value. For example, in our abstractions like Try, rather than being able to say {{Either<Error, Value> t}} we must encode two Options ({{Option<Error>}}, {{Option<T>}}) with the implicit invariant that exactly one will be set.  This also comes in handy in many other places in the code. Note that we have the ability to (1) use C++11 unions now, as well as (2) use boost's variant directly instead of introducing Either. However, creating a named union every time this is needed is verbose, and unions require that we externally track which member is set. For variant, we already use this (e.g. json.hpp), but we can benefit from the better naming as Either.  Many languages expose Either as having only two values, left and right. I'd propose making this two or more, as is the case with variant.",Improvement,Major,Accepted,"2015-08-07 02:21:40","2015-08-07 01:21:40",5
"Apache Mesos","some variables in version.hpp use `Type &var` instead of `Type& var`","Some variables in  3rdparty/libprocess/3rdparty/stout/include/stout/version.hpp violate Mesos code style of biding '&' and '*' to the type name  (as opposed to binding to the variable name).",Bug,Minor,Resolved,"2015-08-07 01:52:05","2015-08-07 00:52:05",1
"Apache Mesos","Implement token manager for docker registry","Implement the following: - A component that fetches JSON web authorization token from a given registry. - Caches the token keyed on registry, service and scope - Validates the cache for expiry date  Nice to have: - Cache gets pruned as tokens are aged beyond expiration time. ",Task,Major,Resolved,"2015-08-06 21:59:36","2015-08-06 20:59:36",4
"Apache Mesos","Implement docker registry client","Implement the following functionality:  - fetch manifest from remote registry based on authorization method dictated by the registry. - fetch image layers from remote registry  based on authorization method dictated by the registry.. ",Task,Major,Resolved,"2015-08-06 21:49:39","2015-08-06 20:49:39",5
"Apache Mesos","CgroupsAnyHierarchyWithPerfEventTest failing on Ubuntu 14.04","[ RUN      ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_Perf ../../src/tests/containerizer/cgroups_tests.cpp:172: Failure (cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/sys/fs/cgroup/perf_event/mesos_test': Device or resource busy ../../src/tests/containerizer/cgroups_tests.cpp:190: Failure (cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/sys/fs/cgroup/perf_event/mesos_test': Device or resource busy [  FAILED  ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_Perf (9 ms) [----------] 1 test from CgroupsAnyHierarchyWithPerfEventTest (9 ms total) ",Bug,Major,Accepted,"2015-08-06 06:47:42","2015-08-06 05:47:42",3
"Apache Mesos","Design doc for docker registry token manager","Create design document for describing the component and interaction between Docker Registry Client and remote Docker Registry for token based authorization.",Task,Major,Resolved,"2015-08-05 21:29:14","2015-08-05 20:29:14",2
"Apache Mesos","As a Java developer I want a simple way to obtain information about Master from ZooKeeper","With the new JSON {{MasterInfo}} published to ZK, we want to provide a simple library class for Java Framework developers to retrieve info about the masters and the leader.",Story,Major,Resolved,"2015-08-05 20:08:53","2015-08-05 19:08:53",2
"Apache Mesos","As a Python developer I want a simple way to obtain information about Master from ZooKeeper","With the new JSON {{MasterInfo}} published to ZK, we want to provide a simple library class for Python developers to retrieve info about the masters and the leader.",Story,Major,Resolved,"2015-08-05 20:08:32","2015-08-05 19:08:32",2
"Apache Mesos","Fetch checksum files to inform fetcher cache use","This is the first part of phase 1 as described in the comments for MESOS-2073. We add a field to CommandInfo::URI that contains the URI of a checksum file. When this file has new content, then the contents of the associated value URI needs to be refreshed in the fetcher cache.   In this implementation step, we just add the above basic functionality (download, checksum comparison). In later steps, we will add more control flow to cover corner cases and thus make this feature more useful. ",Improvement,Minor,Accepted,"2015-08-05 12:34:59","2015-08-05 11:34:59",3
"Apache Mesos","C++ style guide is not rendered correctly (code section syntax disregarded)","Some paragraphs at the bottom of docs/mesos-c++-style-guide.md containing code sections are not rendered correctly by the web site generator. It looks fine in a github gist and apparently the syntax used is correct.   ",Bug,Minor,Resolved,"2015-08-05 10:11:19","2015-08-05 09:11:19",1
"Apache Mesos","No need to checkpoint container root filesystem path.","Given the design discussed in [MESOS-3004|https://issues.apache.org/jira/browse/MESOS-3004], one container might have multiple provisioned root filesystems. Only checkpointing the root filesystem for ContainerInfo::image does not make sense.  Also, we realized that checkpointing container root filesystem path is not necessary because each provisioner should be able to destroy root filesystems for a given container based on a canonical directory layout (e.g., <appc_rootfs_dir>/<container_id>/xxx).",Task,Major,Resolved,"2015-08-05 00:45:16","2015-08-04 23:45:16",3
"Apache Mesos","MasterAuthorizationTest.DuplicateRegistration test is flaky","[ RUN      ] MasterAuthorizationTest.DuplicateRegistration Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7' I0804 22:16:01.578500 26185 leveldb.cpp:176] Opened db in 2.188338ms I0804 22:16:01.579172 26185 leveldb.cpp:183] Compacted db in 645075ns I0804 22:16:01.579211 26185 leveldb.cpp:198] Created db iterator in 15766ns I0804 22:16:01.579227 26185 leveldb.cpp:204] Seeked to beginning of db in 1658ns I0804 22:16:01.579238 26185 leveldb.cpp:273] Iterated through 0 keys in the db in 313ns I0804 22:16:01.579282 26185 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0804 22:16:01.579787 26212 recover.cpp:449] Starting replica recovery I0804 22:16:01.580075 26212 recover.cpp:475] Replica is in EMPTY status I0804 22:16:01.581014 26205 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0804 22:16:01.581357 26211 recover.cpp:195] Received a recover response from a replica in EMPTY status I0804 22:16:01.581761 26207 recover.cpp:566] Updating replica status to STARTING I0804 22:16:01.582334 26218 master.cpp:377] Master 20150804-221601-2550141356-59302-26185 (d6d349cd895b) started on 172.17.0.152:59302 I0804 22:16:01.582355 26218 master.cpp:379] Flags at startup: --acls= --allocation_interval=1secs --allocator=HierarchicalDRF --authenticate=true --authenticate_slaves=true --authenticators=crammd5 --credentials=/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials --framework_sorter=drf --help=false --initialize_driver_logging=true --log_auto_initialize=true --logbufsecs=0 --logging_level=INFO --max_slave_ping_timeouts=5 --quiet=false --recovery_slave_removal_limit=100% --registry=replicated_log --registry_fetch_timeout=1mins --registry_store_timeout=25secs --registry_strict=true --root_submissions=true --slave_ping_timeout=15secs --slave_reregister_timeout=10mins --user_sorter=drf --version=false --webui_dir=/mesos/mesos-0.24.0/_inst/share/mesos/webui --work_dir=/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/master --zk_session_timeout=10secs I0804 22:16:01.582711 26218 master.cpp:424] Master only allowing authenticated frameworks to register I0804 22:16:01.582722 26218 master.cpp:429] Master only allowing authenticated slaves to register I0804 22:16:01.582728 26218 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials' I0804 22:16:01.582929 26204 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421543ns I0804 22:16:01.582950 26204 replica.cpp:323] Persisted replica status to STARTING I0804 22:16:01.583032 26218 master.cpp:468] Using default 'crammd5' authenticator I0804 22:16:01.583132 26211 recover.cpp:475] Replica is in STARTING status I0804 22:16:01.583154 26218 master.cpp:505] Authorization enabled I0804 22:16:01.583356 26214 whitelist_watcher.cpp:79] No whitelist given I0804 22:16:01.583411 26217 hierarchical.hpp:346] Initialized hierarchical allocator process I0804 22:16:01.583976 26213 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0804 22:16:01.584187 26209 recover.cpp:195] Received a recover response from a replica in STARTING status I0804 22:16:01.584581 26213 master.cpp:1495] The newly elected leader is master@172.17.0.152:59302 with id 20150804-221601-2550141356-59302-26185 I0804 22:16:01.584609 26213 master.cpp:1508] Elected as the leading master! I0804 22:16:01.584627 26213 master.cpp:1278] Recovering from registrar I0804 22:16:01.584656 26204 recover.cpp:566] Updating replica status to VOTING I0804 22:16:01.584770 26212 registrar.cpp:313] Recovering registrar I0804 22:16:01.585261 26218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 370526ns I0804 22:16:01.585285 26218 replica.cpp:323] Persisted replica status to VOTING I0804 22:16:01.585412 26216 recover.cpp:580] Successfully joined the Paxos group I0804 22:16:01.585667 26216 recover.cpp:464] Recover process terminated I0804 22:16:01.586047 26213 log.cpp:661] Attempting to start the writer I0804 22:16:01.587164 26211 replica.cpp:477] Replica received implicit promise request with proposal 1 I0804 22:16:01.587549 26211 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 358261ns I0804 22:16:01.587568 26211 replica.cpp:345] Persisted promised to 1 I0804 22:16:01.588173 26209 coordinator.cpp:230] Coordinator attemping to fill missing position I0804 22:16:01.589316 26208 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0804 22:16:01.589700 26208 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 351778ns I0804 22:16:01.589721 26208 replica.cpp:679] Persisted action at 0 I0804 22:16:01.590698 26213 replica.cpp:511] Replica received write request for position 0 I0804 22:16:01.590754 26213 leveldb.cpp:438] Reading position from leveldb took 31557ns I0804 22:16:01.591147 26213 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 321842ns I0804 22:16:01.591167 26213 replica.cpp:679] Persisted action at 0 I0804 22:16:01.591790 26217 replica.cpp:658] Replica received learned notice for position 0 I0804 22:16:01.592133 26217 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 315281ns I0804 22:16:01.592155 26217 replica.cpp:679] Persisted action at 0 I0804 22:16:01.592180 26217 replica.cpp:664] Replica learned NOP action at position 0 I0804 22:16:01.592686 26211 log.cpp:677] Writer started with ending position 0 I0804 22:16:01.593729 26205 leveldb.cpp:438] Reading position from leveldb took 26394ns I0804 22:16:01.596165 26209 registrar.cpp:346] Successfully fetched the registry (0B) in 11.343104ms I0804 22:16:01.596281 26209 registrar.cpp:445] Applied 1 operations in 26242ns; attempting to update the 'registry' I0804 22:16:01.598415 26212 log.cpp:685] Attempting to append 178 bytes to the log I0804 22:16:01.598563 26215 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0804 22:16:01.599324 26215 replica.cpp:511] Replica received write request for position 1 I0804 22:16:01.599778 26215 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 420523ns I0804 22:16:01.599800 26215 replica.cpp:679] Persisted action at 1 I0804 22:16:01.600349 26204 replica.cpp:658] Replica received learned notice for position 1 I0804 22:16:01.600684 26204 leveldb.cpp:343] Persisting action (199 bytes) to leveldb took 310315ns I0804 22:16:01.600706 26204 replica.cpp:679] Persisted action at 1 I0804 22:16:01.600723 26204 replica.cpp:664] Replica learned APPEND action at position 1 I0804 22:16:01.601632 26213 registrar.cpp:490] Successfully updated the 'registry' in 5.287936ms I0804 22:16:01.601747 26213 registrar.cpp:376] Successfully recovered registrar I0804 22:16:01.601826 26215 log.cpp:704] Attempting to truncate the log to 1 I0804 22:16:01.601948 26210 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0804 22:16:01.602145 26208 master.cpp:1305] Recovered 0 slaves from the Registry (139B) ; allowing 10mins for slaves to re-register I0804 22:16:01.602859 26219 replica.cpp:511] Replica received write request for position 2 I0804 22:16:01.603181 26219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284713ns I0804 22:16:01.603209 26219 replica.cpp:679] Persisted action at 2 I0804 22:16:01.603984 26211 replica.cpp:658] Replica received learned notice for position 2 I0804 22:16:01.604313 26211 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 302445ns I0804 22:16:01.604365 26211 leveldb.cpp:401] Deleting ~1 keys from leveldb took 29354ns I0804 22:16:01.604387 26211 replica.cpp:679] Persisted action at 2 I0804 22:16:01.604408 26211 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0804 22:16:01.616402 26185 sched.cpp:164] Version: 0.24.0 I0804 22:16:01.616902 26209 sched.cpp:262] New master detected at master@172.17.0.152:59302 I0804 22:16:01.617000 26209 sched.cpp:318] Authenticating with master master@172.17.0.152:59302 I0804 22:16:01.617019 26209 sched.cpp:325] Using default CRAM-MD5 authenticatee I0804 22:16:01.617324 26212 authenticatee.cpp:115] Creating new client SASL connection I0804 22:16:01.617550 26209 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.617641 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(259)@172.17.0.152:59302 I0804 22:16:01.617858 26208 authenticator.cpp:92] Creating new server SASL connection I0804 22:16:01.618140 26216 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0804 22:16:01.618191 26216 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0804 22:16:01.618324 26213 authenticator.cpp:197] Received SASL authentication start I0804 22:16:01.618413 26213 authenticator.cpp:319] Authentication requires more steps I0804 22:16:01.618557 26216 authenticatee.cpp:252] Received SASL authentication step I0804 22:16:01.618664 26216 authenticator.cpp:225] Received SASL authentication step I0804 22:16:01.618703 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0804 22:16:01.618719 26216 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0804 22:16:01.618778 26216 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0804 22:16:01.618820 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0804 22:16:01.618834 26216 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.618839 26216 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.618857 26216 authenticator.cpp:311] Authentication success I0804 22:16:01.618954 26219 authenticatee.cpp:292] Authentication success I0804 22:16:01.619035 26204 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.619083 26219 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(259)@172.17.0.152:59302 I0804 22:16:01.619309 26208 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302 I0804 22:16:01.619335 26208 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302 I0804 22:16:01.619494 26208 sched.cpp:746] Will retry registration in 439203ns if necessary I0804 22:16:01.619627 26217 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.619695 26217 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*' I0804 22:16:01.620848 26217 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302 I0804 22:16:01.620929 26217 sched.cpp:746] Will retry registration in 2.099193326secs if necessary I0804 22:16:01.621036 26210 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.621083 26210 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*' I0804 22:16:01.621727 26217 master.cpp:1876] Subscribing framework default with checkpointing disabled and capabilities [  ] I0804 22:16:01.621981 26208 sched.cpp:262] New master detected at master@172.17.0.152:59302 I0804 22:16:01.622131 26208 sched.cpp:318] Authenticating with master master@172.17.0.152:59302 I0804 22:16:01.622153 26208 sched.cpp:325] Using default CRAM-MD5 authenticatee I0804 22:16:01.622323 26212 authenticatee.cpp:115] Creating new client SASL connection I0804 22:16:01.622324 26210 hierarchical.hpp:391] Added framework 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:01.622369 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:01.622386 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 28592ns I0804 22:16:01.622511 26210 sched.cpp:640] Framework registered with 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:01.622586 26210 sched.cpp:654] Scheduler::registered took 48005ns I0804 22:16:01.622592 26208 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.622673 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(260)@172.17.0.152:59302 I0804 22:16:01.622923 26205 authenticator.cpp:92] Creating new server SASL connection I0804 22:16:01.623112 26204 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0804 22:16:01.623133 26216 master.cpp:1870] Dropping SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302: Re-authentication in progress I0804 22:16:01.623144 26204 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0804 22:16:01.623258 26215 authenticator.cpp:197] Received SASL authentication start I0804 22:16:01.623313 26215 authenticator.cpp:319] Authentication requires more steps I0804 22:16:01.623394 26215 authenticatee.cpp:252] Received SASL authentication step I0804 22:16:01.623512 26212 authenticator.cpp:225] Received SASL authentication step I0804 22:16:01.623546 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0804 22:16:01.623564 26212 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0804 22:16:01.623603 26212 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0804 22:16:01.623622 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0804 22:16:01.623631 26212 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.623636 26212 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.623649 26212 authenticator.cpp:311] Authentication success I0804 22:16:01.623777 26212 authenticatee.cpp:292] Authentication success I0804 22:16:01.623846 26212 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.623913 26212 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(260)@172.17.0.152:59302 I0804 22:16:01.624130 26212 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302 I0804 22:16:02.583772 26218 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:02.583818 26218 hierarchical.hpp:908] Performed allocation for 0 slaves in 80538ns I0804 22:16:03.585110 26211 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:03.585156 26211 hierarchical.hpp:908] Performed allocation for 0 slaves in 69272ns I0804 22:16:04.586539 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:04.586586 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 79232ns I0804 22:16:05.587239 26209 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:05.587293 26209 hierarchical.hpp:908] Performed allocation for 0 slaves in 85128ns I0804 22:16:06.587935 26212 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:06.587985 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 78141ns I0804 22:16:07.588817 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:07.588865 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 81433ns I0804 22:16:08.589857 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:08.589906 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 71929ns I0804 22:16:09.591085 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:09.591133 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 78223ns I0804 22:16:10.591737 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:10.591785 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71894ns I0804 22:16:11.593166 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:11.593221 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 89782ns I0804 22:16:12.593647 26212 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:12.593689 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 69426ns I0804 22:16:13.594154 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:13.594202 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 70581ns I0804 22:16:14.594712 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:14.594758 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71201ns I0804 22:16:15.595412 26219 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:15.595464 26219 hierarchical.hpp:908] Performed allocation for 0 slaves in 85183ns I0804 22:16:16.596201 26217 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:16.596247 26217 hierarchical.hpp:908] Performed allocation for 0 slaves in 95132ns ../../src/tests/master_authorization_tests.cpp:794: Failure Failed to wait 15secs for frameworkRegisteredMessage I0804 22:16:16.624354 26212 master.cpp:966] Framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 disconnected I0804 22:16:16.624398 26212 master.cpp:2092] Disconnecting framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.624445 26212 master.cpp:2116] Deactivating framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.624686 26212 master.cpp:988] Giving framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 0ns to failover I0804 22:16:16.625641 26219 hierarchical.hpp:474] Deactivated framework 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:16.626688 26218 master.cpp:4180] Framework failover timeout, removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.626734 26218 master.cpp:4759] Removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.627074 26218 master.cpp:858] Master terminating I0804 22:16:16.627218 26215 hierarchical.hpp:428] Removed framework 20150804-221601-2550141356-59302-26185-0000 ../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <98-98 02-AC 54-2B 00-00>, 1-byte object <97>, 1-byte object <D2>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] MasterAuthorizationTest.DuplicateRegistration (15056 ms) ",Bug,Major,Resolved,"2015-08-04 23:24:48","2015-08-04 22:24:48",1
"Apache Mesos","Libev handle_async can deadlock with run_in_event_loop","Due to the arbitrary nature of the functions that are executed in handle_async, invoking them under the (A) {{watchers_mutex}} can lead to deadlocks if (B) is acquired before calling {{run_in_event_loop}} and (B) is also acquired within the arbitrary function.   This was introduced in https://github.com/apache/mesos/commit/849fc4d361e40062073324153ba97e98e294fdf2",Bug,Blocker,Resolved,"2015-08-04 18:58:28","2015-08-04 17:58:28",3
"Apache Mesos","Remove unused 'fatal' and 'fatalerror' macros","There exist {{fatal}} and {{fatalerror}} macros in both {{libprocess}} and {{stout}}. None of them are currently used as we favor {{glog}}'s {{LOG(FATAL)}}, and therefore should be removed.",Task,Major,Resolved,"2015-08-04 18:41:45","2015-08-04 17:41:45",1
"Apache Mesos","Validate Quota Requests.","We need to validate quota requests in terms of syntactical and semantical correctness.",Task,Major,Resolved,"2015-08-04 14:30:23","2015-08-04 13:30:23",3
"Apache Mesos","MemIsolatorTest/{0,1}.MemUsage fails on OS X","Looks like this is due to {{mlockall}} being unimplemented on OS X.  ",Bug,Major,Resolved,"2015-08-04 00:43:35","2015-08-03 23:43:35",2
"Apache Mesos","Always set TaskStatus.executor_id when sending a status update message from Executor","Currently, the Executor doesn't always set TaskStatus.executor_id. This prevents the Slave TaskStatus label decorator hook from knowing the executor id.  An appropriate place to automatically fill in the executor_id is ExecutorProcesS::sendStatusUpdate() since we are already filling in some other information here.",Task,Major,Resolved,"2015-08-03 23:58:44","2015-08-03 22:58:44",1
"Apache Mesos","Fix master metrics for scheduler calls","Currently the master increments metrics for old style messages from the driver but not when it receives Calls. Since the driver is now sending Calls, master should update metrics correctly.",Bug,Major,Resolved,"2015-08-03 22:10:03","2015-08-03 21:10:03",3
"Apache Mesos","Implement a 'read-only' AppC Image Store","It's going to be derived from this: https://reviews.apache.org/r/34140/ (and other related patches) but in the initial 'read-only' version the store's content is prepared by out-of-band mechanisms so the store component in Mesos only needs to provide access to images already in it and recover images upon slave restart.  This greatly simplifies the initial version's responsibility and test cases. Features that fetch the images into the store will be added later and they will take into consideration its impact on task start latency and slave restart responsiveness, etc.",Task,Major,Resolved,"2015-08-03 20:49:53","2015-08-03 19:49:53",5
"Apache Mesos","Implement AppC image discovery.","Appc spec specifies two image discovery mechanisms: simple and meta discovery. We need to have an abstraction for image discovery in AppcStore. For MVP, we can implement the simple discovery first.  Update: simple discovery is removed from the spec. Meta discovery is the only discovery mechanism right now in the spec. Simple discovery is already shipped (we support an arbitrary operator specified [URI prefix|https://github.com/apache/mesos/blob/master/docs/container-image.md#appc-support-and-current-limitations]). So this ticket should focus on implementing Meta discovery.",Task,Major,Accepted,"2015-08-03 20:48:38","2015-08-03 19:48:38",5
"Apache Mesos","ContainerInfo::Image::AppC::id should be optional","As I commented here: https://reviews.apache.org/r/34136/  Currently ContainerInfo::Image::Appc is defined as the following  {noformat:title=}     message AppC {       required string name = 1;       required string id = 2;       optional Labels labels = 3;     } {noformat}  In which the {{id}} is a required field. When users specify the image in tasks they likely will not use an image id (much like when you use docker or rkt to launch containers, you often use {{ubuntu}} or {{ubuntu:latest}} and seldom a SHA512 ID) and we should change it to be optional.  The motivating scenario is that: if the frameworks in the Mesos use something like {{image=ubuntu:14.04}} to run a task and {{image=ubuntu}} defaults to {{image=ubuntu:latest}}, the operator can swap the latest version for all new tasks requesting {{image=ubuntu}}. If they allow users to specify {{image=ubuntu:live}}, they can swap the live version under the covers as well. This allows the operator to release important image updates (e.g., security patches) and have it picked up by new tasks in the cluster without asking the users to update their job/task configs.",Bug,Major,Resolved,"2015-08-03 20:47:01","2015-08-03 19:47:01",1
"Apache Mesos","Implement a utility for computing hash","It is useful for both appc and docker to compute and verify image hash.",Task,Major,Resolved,"2015-08-03 20:40:35","2015-08-03 19:40:35",2
"Apache Mesos","TimeTest.Now fails with --enable-libevent","[ RUN      ] TimeTest.Now ../../../3rdparty/libprocess/src/tests/time_tests.cpp:50: Failure Expected: (Microseconds(10)) < (Clock::now() - t1), actual: 8-byte object <10-27 00-00 00-00 00-00> vs 0ns [  FAILED  ] TimeTest.Now (0 ms)",Bug,Major,Resolved,"2015-08-03 20:18:38","2015-08-03 19:18:38",2
"Apache Mesos","Refactor Subprocess logic in linux/perf.cpp to use common subroutine","MESOS-2834 will enhance the perf isolator to support the different output formats provided by difference kernel versions.  In order to achieve this, it requires to execute the perf --version command.   We should decompose the existing Subcommand processing in perf so that we can share the implementation between the multiple uses of perf.",Bug,Major,Resolved,"2015-07-31 22:06:36","2015-07-31 21:06:36",3
"Apache Mesos","Documentation images do not load","Any images which are referenced from the generated docs ({{docs/*.md}}) do not show up on the website.  For example: * [Architecture|http://mesos.apache.org/documentation/latest/architecture/] * [External Containerizer|http://mesos.apache.org/documentation/latest/external-containerizer/] * [Fetcher Cache Internals|http://mesos.apache.org/documentation/latest/fetcher-cache-internals/] * [Maintenance|http://mesos.apache.org/documentation/latest/maintenance/]   * [Oversubscription|http://mesos.apache.org/documentation/latest/oversubscription/] ",Documentation,Minor,Resolved,"2015-07-31 00:43:07","2015-07-30 23:43:07",3
"Apache Mesos","Make Master::registerFramework() and Master::reregisterFramework() call into Master::subscribe()","Currently Master::subscribe() calls into Master::registerFramework() and Master::reregisterFramework(). We should do it the other way around to be consistent with how we did all the other calls.",Improvement,Major,Resolved,"2015-07-31 00:37:35","2015-07-30 23:37:35",3
"Apache Mesos","Create a test abstraction for preparing test rootfs.","Several tests need this abstraction, so it's better to unify them. For example, src/tests/containerizer/launch_tests.cpp needs to create a test rootfs. We also need that to test filesystem isolators.  The test rootfs can be created by copying files/directories from host file system.",Task,Major,Resolved,"2015-07-30 23:06:05","2015-07-30 22:06:05",3
"Apache Mesos","Perform a self bind mount of rootfs itself in fs::chroot::enter.","Syscall 'pivot_root' requires that the old and the new root are not in the same filesystem. Otherwise, the user will receive a Device or resource busy error.  Currently, we rely on the provisioner to prepare the rootfs and do proper bind mount if needed so that pivot_root can succeed. The drawback of this approach is that it potentially pollutes the host mount table which requires cleanup logics.  For instance, in the test, we create a test rootfs by copying the host files. We need to do a self bind mount so that we can pivot_root on it. That pollute the host mount table and it might leak mounts if test crashes before we do the lazy umount: https://github.com/apache/mesos/blob/master/src/tests/containerizer/launch_tests.cpp#L96-L102  What I propose is that we always perform a recursive self bind mount of rootfs itself in fs::chroot::enter (after enter the new mount namespace). Seems that this is also done in libcontainer: https://github.com/opencontainers/runc/blob/master/libcontainer/rootfs_linux.go#L402",Bug,Major,Resolved,"2015-07-30 22:59:20","2015-07-30 21:59:20",2
"Apache Mesos","Fetcher logs erroneous message when successfully extracting an archive","When fetching an asset while not using the cache, the fetcher may erroneously report this: Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: .  This message appears in the stderr log in the sandbox no matter whether extraction succeeded or not. It should be absent after successful extraction. ",Bug,Minor,Resolved,"2015-07-30 12:09:59","2015-07-30 11:09:59",1
"Apache Mesos","Mark Path::basename, Path::dirname as const functions.","The functions Path::basename and Path::dirname in stout/path.hpp are not marked const, although they could. Marking them const would remove some ambiguities in the usage of these functions.",Improvement,Trivial,Resolved,"2015-07-29 15:18:35","2015-07-29 14:18:35",1
"Apache Mesos","Fetcher Tests use EXPECT while subsequent logic relies on the outcome.","The fetcher tests use EXPECT validation for critical measures (e.g. non-empty results) and the subsequent logic releis on this (i.e. by accessing the first element). In such cases we should use ASSERT/CHECK.",Bug,Minor,Resolved,"2015-07-29 14:09:06","2015-07-29 13:09:06",1
"Apache Mesos","FrameworkInfo should only be updated if the re-registration is valid","See Ben Mahler's comment in https://reviews.apache.org/r/32961/ FrameworkInfo should not be updated if the re-registration is invalid. This can happen in a few cases under the branching logic, so this requires some refactoring. Notice that a  can be generated  both inside  as well as from inside ",Bug,Major,Resolved,"2015-07-29 09:00:05","2015-07-29 08:00:05",2
"Apache Mesos","MesosZooKeeperTest fixture can have side effects across tests","MesosZooKeeperTest fixture doesn't restart the ZooKeeper server for each test. This means if a test shuts down the ZooKeeper server, the next test (using the same fixture) might fail.   For an example see https://reviews.apache.org/r/36807/",Bug,Major,Resolved,"2015-07-28 22:01:03","2015-07-28 21:01:03",2
"Apache Mesos","Design doc for versioning the HTTP API","In concert with the release of the HTTP API, we would also like to come up with a versioning strategy. This enables to do a meaningful 1.0 release.",Documentation,Major,Resolved,"2015-07-28 21:52:04","2015-07-28 20:52:04",3
"Apache Mesos","Design doc for docker image registry client","Create design document for the docker registry Authenticator component so that we have a baseline for the implementation. ",Bug,Major,Resolved,"2015-07-28 18:46:50","2015-07-28 17:46:50",3
"Apache Mesos","Persist and recover quota to/from Registry","To persist quotas across failovers, the Master should save them in the registry. To support this, we shall: * Introduce a Quota state variable in registry.proto; * Extend the Operation interface so that it supports a ‘Quota’ accumulator (see src/master/registrar.hpp); * Introduce AddQuota / RemoveQuota operations; * Recover quotas from the registry on failover to the Master’s internal::master::Role struct; * Extend RegistrarTest with quota-specific tests.  NOTE: Registry variable can be rather big for production clusters (see MESOS-2075). While it should be fine for MVP to add quota information to registry, we should consider storing Quota separately, as this does not need to be in sync with slaves update. However, currently adding more variable is not supported by the registrar.  While the Agents are reregistering (note they may fail to do so), the information about what part of the quota is allocated is only partially available to the Master. In other words, the state of the quota allocation is reconstructed as Agents reregister. During this period, some roles may be under quota from the perspective of the newly elected Master.  The same problem exists on the allocator side: it may think the cluster is under quota and may eagerly try to satisfy quotas before enough Agents reregister, which may result in resources being allocated to frameworks beyond their quota. To address this issue and also to avoid panicking and generating under quota alerts, the Master should give a certain amount of time for the majority (e.g. 80%) of the Agents to reregister before reporting any quota status and notifying the allocator about granted quotas.",Task,Major,Resolved,"2015-07-28 18:02:06","2015-07-28 17:02:06",5
"Apache Mesos","Introduce QuotaInfo message","A {{QuotaInfo}} protobuf message is internal representation for quota related information (e.g. for persisting quota). The protobuf message should be extendable for future needs and allows for easy aggregation across roles and operator principals. It may also be used to pass quota information to allocators.",Task,Major,Resolved,"2015-07-28 17:53:29","2015-07-28 16:53:29",3
"Apache Mesos","Proper handling of 'query' and/or 'fragment' out of 'path' in http handler.","The libprocess http.cpp post/get handlers currently do not consider query and fragments parts of the path correctly.  E.g. ",Task,Major,Accepted,"2015-07-28 10:46:36","2015-07-28 09:46:36",1
"Apache Mesos","Provide a means to check http connection equality for streaming connections.","If one uses an http::Pipe::Writer to stream a response, one cannot compare the writer with another to see if the connection has changed.  This is useful for example, in the master's http api when there is asynchronous disconnection logic. When we handle the disconnection, it's possible for the scheduler to have re-subscribed, and so the master needs to tell if the disconnection event is relevant for the current connection before taking action.",Task,Major,Resolved,"2015-07-28 02:55:20","2015-07-28 01:55:20",3
"Apache Mesos","Document using the gold linker for faster development on linux.","The [gold linker|https://en.wikipedia.org/wiki/Gold_(linker)] seems to provide a decent speedup (about ~20%) on a parallel build. From a quick test:  {noformat: title=timings for make check -j24 GTEST_FILTER= w/ 24 hyperthreaded cores} gold:  real 7m18.526s user 81m21.213s sys 5m17.224s  default ld:  real 9m7.908s user 85m13.466s sys 5m52.199s  sudo /usr/sbin/alternatives --altdir /opt/rh/devtoolset-2/root/etc/alternatives --admindir /opt/rh/devtoolset-2/root/var/lib/alternatives --set ld /opt/rh/devtoolset-2/root/usr/bin/ld.gold  sudo update-alternatives --install /usr/bin/ld ld /usr/bin/gold 1 {noformat}  Ideally we could this out on the website, with instructions for each OS.",Improvement,Major,Accepted,"2015-07-28 02:10:20","2015-07-28 01:10:20",3
"Apache Mesos","CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseRSS Flaky","Test will occasionally with:  [ RUN      ] CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseUnlockedRSS ../../src/tests/containerizer/cgroups_tests.cpp:1103: Failure helper.increaseRSS(getpagesize()): Failed to sync with the subprocess ../../src/tests/containerizer/cgroups_tests.cpp:1103: Failure helper.increaseRSS(getpagesize()): The subprocess has not been spawned yet [  FAILED  ] CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseUnlockedRSS (223 ms)",Bug,Major,Open,"2015-07-28 00:21:29","2015-07-27 23:21:29",3
"Apache Mesos","Libprocess Process: Join runqueue workers during finalization","The lack of synchronization between ProcessManager destruction and the thread pool threads running the queued processes means that the shared state that is part of the ProcessManager gets destroyed prematurely. Synchronizing the ProcessManager destructor with draining the work queues and stopping the workers will allow us to not require leaking the shared state to avoid use beyond destruction.",Improvement,Major,Resolved,"2015-07-27 22:35:13","2015-07-27 21:35:13",3
"Apache Mesos","Enable Mesos Agent Node to use arbitrary script / module to figure out IP, HOSTNAME","Following from MESOS-2902 we want to enable the same functionality in the Mesos Agents too.  This is probably best done once we implement the new {{os::shell}} semantics, as described in MESOS-3142.",Story,Major,Resolved,"2015-07-27 19:24:19","2015-07-27 18:24:19",1
"Apache Mesos","Add tests for HTTPS SSL socket communication","Unit tests are lacking for the following cases:  1. HTTPS Post with None payload.  2. Verification of HTTPS payload on the SSL socket(maybe decode to a Request object) 3. http -> ssl socket 4. https -> raw socket.",Bug,Minor,Accepted,"2015-07-27 17:21:43","2015-07-27 16:21:43",3
"Apache Mesos","Need for HTTP delete requests","As we decided to create a more restful api for managing Quota request. Therefore we also want to use the HTTP Delete request and hence need to enable the libprocess/http to send delete request besides get and post requests.",Improvement,Major,Resolved,"2015-07-27 14:39:10","2015-07-27 13:39:10",1
"Apache Mesos","Resolve issue with hanging tests with Zookeeper","See MESOS-2736 for the original issue; the submitted [Review|https://reviews.apache.org/r/36663] currently has no tests, the one posted in the subsequent [r/3687|https://reviews.apache.org/r/36807] currently hangs when ran after the other {{TEST_F(MasterZooKeeperTest, LostZooKeeperCluster)}}.  The issue is around the {{await()}} in {{StartMaster()}} ({{cluster.hpp #430}}) that waits indefinitely for the master recovery. ",Bug,Major,Resolved,"2015-07-25 01:56:16","2015-07-25 00:56:16",1
"Apache Mesos","Add a new API call to the allocator to update available resources","This ticket is to track the {{updateAvailable}} API call being added to the allocator which updates the available resources in the allocator. It's used for master endpoints for dynamic reservation and persistent volumes. {{updateAvailable}} is similar to {{updateSlave}} except that {{updateAvailable}} never leaves the allocator in an over-allocated state.",Task,Major,Resolved,"2015-07-24 22:25:47","2015-07-24 21:25:47",8
"Apache Mesos","Using a unresolvable hostname crashes the framework on registration","The following commands trigger the crash:    The crash output:  ",Task,Major,Resolved,"2015-07-24 22:21:38","2015-07-24 21:21:38",1
"Apache Mesos","Update Homebrew formula for Mesos (Mac OSX)","We have pushed a [pull request|https://github.com/Homebrew/homebrew/pull/42099] to Homebrew for the new 0.23 formula.  Once accepted, we must verify that this works on a Mac OSX device. This would also be a great time to ensure our documentation is up-to-date.  Currently, the Homebrew check fails, as they have deprecated SHA-1 checksums:   Don't know enough about Homebrew to really figure out what is going on here; nor how to fix this. The Mesos SHA-256 has been correctly entered and computed via the [Online SHA/MD5 calculator|https://md5file.com/calculator].  I guess, we should go download the packages and compute their SHA-256 and/or research from the respective download sites whether they publish the SHA.",Task,Trivial,Resolved,"2015-07-24 20:22:32","2015-07-24 19:22:32",1
"Apache Mesos","Disable endpoints rule fails to recognize HTTP path delegates","In mesos, one can use the flag {{--firewall_rules}} to disable endpoints. Disabled endpoints will return a _403 Forbidden_ response whenever someone tries to access endpoints.  Libprocess support adding one default delegate for endpoints, which is the default process id which handles endpoints if no process id was given. For example, the default id of the master libprocess process is {{master}} which is also set as the delegate for the master system process, so a request to the endpoint {{http://master-address:5050/state.json}} will effectively be resolved by {{http://master-address:5050/master/state.json}}. But if one disables  {{/state.json}} because of how delegates work, it can still access {{/master/state.json}}.  The only workaround is to disabled both enpoints.",Bug,Major,Resolved,"2015-07-24 11:07:30","2015-07-24 10:07:30",2
"Apache Mesos","As a Developer I want a better way to run shell commands","When reviewing the code in [r/36425|https://reviews.apache.org/r/36425/] [~<USER> noticed that there is a better abstraction that is possible to introduce for {{os::shell()}} that will simplify the caller's life.  Instead of having to handle all possible outcomes, we propose to refactor {{os::shell()}} as follows:    where the returned string is {{stdout}} and, should the program be signaled, or exit with a non-zero exit code, we will simply return a {{Failure}} with an error message that will encapsulate both the returned/signaled state, and, possibly {{stderr}}.  And some test driven development:   Alternatively, the caller can ask to have {{stderr}} conflated with {{stdout}}:   However, {{stderr}} will be ignored by default:   An analysis of existing usage shows that in almost all cases, the caller only cares {{if not error}}; in fact, the actual exit code is read only once, and even then, in a test case.  We believe this will simplify the API to the caller, and will significantly reduce the length and complexity at the calling sites (<6 LOC against the current 20+).",Story,Major,Resolved,"2015-07-24 07:50:28","2015-07-24 06:50:28",2
"Apache Mesos","Compiler warning when mocking function type has an enum return type.","The purpose of this ticket is to document a very cryptic error message (actually a warning that gets propagated by {{-Werror}}) that gets generated by {{clang-3.5}} from {{gmock}} source code when trying to mock a perfectly innocent-looking function.  h3. Problem  The following code is attempting to mock a {{MesosExecutorDriver}}:  {code} class MockMesosExecutorDriver : public MesosExecutorDriver { public:   MockMesosExecutorDriver(mesos::Executor* executor)     : MesosExecutorDriver(executor) {}    MOCK_METHOD1(sendStatusUpdate, Status(const TaskStatus&)); }; {code}  The above code generates the following error message:    The source of the issue here is that {{Status}} is an {{enum}}. In {{gmock-1.6.0/include/gmock/internal/gmock-internal-utils.h}} you can find the following function:    This function gets called with the return type of a mocked function. In our case,  the return type of the mocked function is {{Status}}.  Attempting to compile the following minimal example with {{clang-3.5}} reproduces the error message:    * See it online on [GCC Explorer|https://goo.gl/t1FepZ]  Note that if the type is not an {{enum}}, the warning is not generated. This is why existing mocked functions that return non-{{enum}} types such as {{Future<void>}} does not encounter this issue.  h3. Solutions  The simplest solution is to add {{-Wno-null-deference}} to {{mesos_tests_CPPFLAGS}} in {{src/Makefile.am}}.    Another solution is to upgrade {{gmock}} from *1.6* to *1.7* because this problem is solved in the newer versions.  In gmock 1.7   Add volatile could avoid this warning. https://goo.gl/opCiLC  ",Bug,Major,Resolved,"2015-07-24 02:25:00","2015-07-24 01:25:00",3
"Apache Mesos","Implement Docker remote puller","Given a Docker image name and registry host URL, fetches the image. If necessary, it will download the manifest and layers from the registry host. It will place the layers and image manifest into persistent store. Done when a Docker image can be successfully stored and retrieved using 'put' and 'get' methods.",Improvement,Major,Resolved,"2015-07-23 21:35:48","2015-07-23 20:35:48",5
"Apache Mesos","Incorporate CMake into standard documentation","Right now it's anyone's guess how to build with CMake. If we want people to use it, we should put up documentation. The central challenge is that the CMake instructions will be slightly different for different platforms.  For example, on Linux, the gist of the build is basically the same as autotools; you pull down the system dependencies (like APR, _etc_.), and then:  ``` ./bootstrap mkdir build-cmake && cd build-cmake cmake .. make ```  But, on Windows, it will be somewhat more complicated. There is no bootstrap step, for example, because Windows doesn't have bash natively. And even when we put that in, you'll still have to build the glog stuff out-of-band because CMake has no way of booting up Visual Studio and calling build.  So practically, we need to figure out:  * What our build story is for different platforms * Write specific instructions for our core target platforms.",Task,Major,Resolved,"2015-07-23 20:14:47","2015-07-23 19:14:47",13
"Apache Mesos","PersistentVolumeTest.SlaveRecovery test fails on OSX","With a clean build ({{make clean}}) running this tests fails:  {code} GTEST_FILTER=PersistentVolumeTest.* make check {code}  This is the log: ",Bug,Major,Resolved,"2015-07-23 19:28:36","2015-07-23 18:28:36",2
"Apache Mesos","Publish MasterInfo to ZK using JSON","Following from MESOS-2340, which now allows Master to correctly decode JSON information ({{MasterInfo}}) published to Zookeeper, we can now enable the Master Leader Contender to serialize it too in JSON.",Story,Major,Resolved,"2015-07-23 07:55:09","2015-07-23 06:55:09",2
"Apache Mesos","Port bootstrap to CMake","Bootstrap does a lot of significant things, like setting up the git commit hooks. We will want something like bootstrap to run also on systems that don't have bash -- ideally this should just run in CMake itself.",Bug,Major,Resolved,"2015-07-23 06:33:02","2015-07-23 05:33:02",5
"Apache Mesos","Isolator::prepare() should return Executor environment vars as well","Sometimes the Isolators need to pass on some environment variables for the Executor that is being launched. For example, to successfully launch an executor inside a network namespace, one needs to set LIBPROCESS_IP to point to the container IP, otherwise the executor tries to bind to the Slave IP which may be invalid inside the namespace. Another example is where the file system isolator should be able to specify the WORK_DIR depending on if a new rootfs is used.",Task,Major,Resolved,"2015-07-23 04:54:20","2015-07-23 03:54:20",2
"Apache Mesos","Allow slave to forward messages through the master for HTTP schedulers.","The master currently has no install handler for {{ExecutorToFramework}} messages and the slave directly sends these messages to the scheduler driver, bypassing the master entirely.  We need to preserve this behavior for the driver, but HTTP schedulers will not have a libprocess 'pid'. We'll have to ensure that the {{RunTaskMessage}} and {{UpdateFrameworkMessage}} have an optional pid. For now the master will continue to set the pid, but 0.24.0 slaves will know to send messages through the master when the 'pid' is not available.",Task,Major,Resolved,"2015-07-23 00:44:26","2015-07-22 23:44:26",5
"Apache Mesos","Master should send heartbeats on the subscription connection","In order to deal with network partitions and ensuring network intermediately do not close the persistent subscription connection, master must periodically send heartbeats.   The expectation with schedulers is that they resubscribe when they do not receive heartbeats for some time.",Task,Major,Resolved,"2015-07-22 23:11:25","2015-07-22 22:11:25",3
"Apache Mesos","Custom isolators should implement Isolator instead of IsolatorProcess.","Similar to MESOS-2213, we should not restrict custom isolators to use libprocess Process. We should do a similar refactor as we did for MESOS-2213.",Task,Major,Resolved,"2015-07-22 19:16:02","2015-07-22 18:16:02",3
"Apache Mesos","Move all MesosContainerizer related files under src/slave/containerizer/mesos","Currently, some MesosContainerizer specific files are not in the correct location. For example:   They should be put under src/slave/containerizer/mesos/",Task,Major,Resolved,"2015-07-22 18:59:34","2015-07-22 17:59:34",2
"Apache Mesos","Improve task reconciliation documentation.","Include additional information about task reconciliation that explain why the master may not return the states of all tasks immediately and why an explicit task reconciliation algorithm is necessary.",Improvement,Minor,Resolved,"2015-07-22 10:37:01","2015-07-22 09:37:01",1
"Apache Mesos","DOCKER_HOST env variable stopped working for executors","With https://reviews.apache.org/r/36282/ no environment variables are available anymore in the docker executors. Hence, setting DOCKER_HOST outside of Mesos stopped working.  Setups which use a remote Docker daemon or tools like Powerstrip stopped working.",Bug,Major,Accepted,"2015-07-22 07:21:37","2015-07-22 06:21:37",2
"Apache Mesos","Updating persistent volumes after slave restart is problematic.","Just realize that while reviewing https://reviews.apache.org/r/34135  Since we don't checkpoint 'resources' in Mesos containerizer, when slave restarts and recovers, the 'resources' in Container struct will be empty, but there are symlinks exists in the sandbox.  We'll end up with trying to create already exist symlinks (and fail). I think we should ignore the creation if it already exists.",Bug,Major,Resolved,"2015-07-22 01:07:14","2015-07-22 00:07:14",3
"Apache Mesos","Add configurable UNIMPLEMENTED macro to stout","During the transition to support for windows, it would be great if we had the ability to use a macro that marks functions as un-implemented. To support being able to find all the unimplemented functions easily at compile time, while also being able to run the tests at the same time, we can add a configuration flag that controls whether this macro aborts or expands to a static assertion.",Improvement,Major,Resolved,"2015-07-21 21:04:23","2015-07-21 20:04:23",2
"Apache Mesos","Always disable SSLV2","The SSL protocol mismatch tests are failing on Centos7 when matching SSLV2 with SSLV2. Since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise.",Bug,Major,Resolved,"2015-07-21 20:55:57","2015-07-21 19:55:57",2
"Apache Mesos","Remove pthread specific code from Mesos",,Improvement,Major,Resolved,"2015-07-21 20:52:09","2015-07-21 19:52:09",3
"Apache Mesos","Remove pthread specific code from Libprocess",,Improvement,Major,Resolved,"2015-07-21 20:51:31","2015-07-21 19:51:31",3
"Apache Mesos","Remove pthread specific code from Stout",,Improvement,Major,Resolved,"2015-07-21 20:50:42","2015-07-21 19:50:42",3
"Apache Mesos","Pass ContainerId into `slaveExecutorEnvironmentDecorator` hook",,Task,Major,Resolved,"2015-07-21 18:48:34","2015-07-21 17:48:34",1
"Apache Mesos","Pass ExecutorInfo argument into Isolator::isolate().","Some isolators need to lookup the executor environment variables to customize their isolation needs. Currently, one has to use the prepare() call to cache the executor-info to use it later during isolate() call.",Task,Major,Resolved,"2015-07-21 18:45:41","2015-07-21 17:45:41",2
"Apache Mesos","Convert mesos::slave::{Limitation,ExecutorRunState} into protobufs.","Published RR: https://reviews.apache.org/r/36718/",Task,Major,Resolved,"2015-07-21 18:36:37","2015-07-21 17:36:37",1
"Apache Mesos","Simplify JSON::* by providing jsonify along the lines of stringify","We want to be able to do things like:  ",Task,Major,Accepted,"2015-07-21 18:33:13","2015-07-21 17:33:13",3
"Apache Mesos","Add resource usage section to containerizer documentation","Currently, the containerizer documentation doesn't touch upon the usage() API and how to interpret the collected statistics.",Documentation,Major,Accepted,"2015-07-21 18:18:38","2015-07-21 17:18:38",3
"Apache Mesos","Fetcher should perform cache eviction based on cache file usage patterns.","Currently, the fetcher uses a trivial strategy to select eviction victims: it picks the first cache file it finds in linear iteration. This means that potentially a file that has just been used gets evicted the next moment. This performance loss can be avoided by even the simplest enhancement of the selection procedure.  Proposed approach: determine an effective yet relatively uncomplex and quick algorithm and implement it in `FetcherProcess::Cache::selectVictims(const Bytes& requiredSpace)`. Suggestion: approximate MRU-retention somehow.  Unit-test what actually happens!",Improvement,Major,Resolved,"2015-07-21 12:19:29","2015-07-21 11:19:29",8
"Apache Mesos","Harden the CMake system-dependency-locating routines","Currently the Mesos project has two flavors of dependency: (1) the dependencies we expect are already on the system (_e.g._, apr, libsvn), and (2) the dependencies that are historically bundled with Mesos (_e.g._, glog).  Dependency type (1) requires solid modules that will locate them on any system: Linux, BSD, or Windows. This would come for free if we were using CMake 3.0, but we're using CMake 2.8 so that Ubuntu users can install it out of the box, instead of upgrading CMake first.  This is additionally useful for dependency type (2), where we will expect to have to use these routines when we support both the rebundled dependencies in the `3rdparty/` folder, and system installations of those dependencies.",Task,Major,Resolved,"2015-07-21 02:33:46","2015-07-21 01:33:46",3
"Apache Mesos","Expand CMake build system to support building the containerizer and associated components","In other tasks in epic MESOS-898, we implement a CMake-based build system that allows us to build process library, the process tests, and the stout tests.  For the CMake build system MVP, it's important that we expand this to build the containerizer, associated modules, and all related tests.",Task,Major,Resolved,"2015-07-21 02:28:51","2015-07-21 01:28:51",3
"Apache Mesos","Add autotools-style Mesos distributions to the CMake build system","In the autoconf-based build system, we there is a notion of building a distribution of Mesos. Essentially, it is a version of Mesos that is configured for a specific platform (Ubuntu, say); so, if a consumer knows their platform, and there is a Mesos distribution, they need only run `make all` and Mesos builds. This allows the consumer to skip the configure step.  In CMake, it should be possible to do this (should be!), and we should explore making it work after we complete the MVP.",Task,Major,Accepted,"2015-07-21 02:25:33","2015-07-21 01:25:33",3
"Apache Mesos","Define CMake style guide","The short story is that it is important to be principled about how the CMake build system is maintained, because there CMake language makes it difficult to statically verify that a configuration is correct. It is not unique in this regard, but (make is arguably even worse) but it is something that's important to make sure we get right.  The longer story is, CMake's language is dynamically scoped and often has somewhat odd defaults for variable values (_e.g._, IIRC, target names passed to ExternalProject_Add default to PREFIX instead of erroring out). This means that it is rare to get a configuration-time error (_i.e._, CMake usually doesn't say something like hey this variable isn't defined), and in large projects, this can make it very difficult to know where definitions come from, or whether it's important that one config routine runs before another. Dynamic scoping also makes it particularly easy to write spaghetti code, which is clearly undesirable for something as important as a build system.  Thus, it is particularly important that we lay down our expectations for how the CMake system is to be structured. This might include:  * Function naming (_e.g._, making it easy to tell whether a function was defined by us, and where it was defined; so we might say that we want our functions to have an underscore to start, and start with the package the come from, like libprocess, so that we know where to look for the definition.) * What assertions we want to check variable values against, so that we can replace subtle errors (_e.g._, a library is accidentally named something silly like PREFIX.0.0.1) with an obvious ones (_e.g._, You have failed to define your target name, so CMake has defaulted to 'PREFIX'; please check your configuration routines) * Decisions of what goes where. (_e.g._, the most complex parts of the CMake MVPs is in the configuration routines, like `MesosConfigure.cmake`; to curb this, we should have strict rules about what goes in that file vs other files, and how we know what is to be run before what. Part of this should probably be prominent comments explaining the structure of the project, so that people aren't confused!) * And so on.",Task,Major,Resolved,"2015-07-21 02:21:41","2015-07-21 01:21:41",3
"Apache Mesos","Extend CMake build system to support building against third-party libraries from either the system or the local Mesos rebundling","Currently Mesos has third-party dependencies of two types: (1) those that are expected to be on the system (such as APR, libsvn, _etc_.), and (2) those that have been historically bundled as tarballs inside the Mesos repository, and are not expected to be on the system when Mesos is installed (these are located in the `3rdparty/` directory, and includes things like boost and glog).  For type (2), the MVP of the CMake-based build system will always pull down a fresh tarball from an external source, instead of using the bundled tarballs in the `3rdparty/` folder.  However, many CI systems do not have Internet access, so in the long term, we need to provide many options for getting these dependencies.",Task,Major,Resolved,"2015-07-21 01:53:36","2015-07-21 00:53:36",5
"Apache Mesos","Port flag generation logic from the autotools solution to CMake","One major barrier to widespread adoption of the CMake-based build system (other than the fact that we haven't implemented it yet!) is that most of our institutional knowledge of the quirks of how to build Mesos across many platforms is tied up in files like `configure.ac`.  Therefore, a good CMake-based build system will require us to go through these files systematically and manually port this logic to CMake (as well as testing it).",Task,Major,Accepted,"2015-07-21 01:39:14","2015-07-21 00:39:14",3
"Apache Mesos","Separate OS-specific code in the libprocess library","This issue tracks changes for all files under {{3rdparty/libprocess/include/}} and {{3rdparty/libprocess/src}}.  The changes will be based on this commit: https://github.com/<USER>mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c",Task,Major,Resolved,"2015-07-21 00:22:08","2015-07-20 23:22:08",5
"Apache Mesos","Separate OS-specific code in the stout library","This issue tracks changes for all files under {{3rdparty/libprocess/3rdparty/stout/}}  The changes will be based on this commit: https://github.com/<USER>mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c",Task,Major,Resolved,"2015-07-21 00:17:17","2015-07-20 23:17:17",5
"Apache Mesos","Standardize separation of Windows/Linux-specific OS code","There are 50+ files that must be touched to separate OS-specific code.  First, we will standardize the changes by using stout/abort.hpp as an example. The review/discussion can be found here: https://reviews.apache.org/r/36625/",Task,Major,Resolved,"2015-07-21 00:09:33","2015-07-20 23:09:33",3
"Apache Mesos","Validation of Docker Layers Pulled From Docker Registry","Docker layers should be verified against their checksum digests before they are stored to ensure the integrity of the docker layer content. This includes supporting sha256, sha384, sha512 hash algorithms.",Improvement,Major,Accepted,"2015-07-20 23:51:46","2015-07-20 22:51:46",3
"Apache Mesos","Validation of Docker Image Manifests from Docker Registry","Docker image manifests pulled from remote Docker registries should be verified against their signature digest before they are used. ",Improvement,Major,Resolved,"2015-07-20 23:45:45","2015-07-20 22:45:45",3
"Apache Mesos","Implement WindowsContainerizer and WindowsDockerContainerizer","The MVP for Windows support is a containerizer that (1) runs on Windows, and (2) runs and passes all the tests that are relevant to the Windows platform (_e.g._, not the tests that involve cgroups). To do this we require at least a `WindowsContainerizer` (to be implemented alongside the `MesosContainerizer`), which provides no meaningful (_e.g._) process namespacing (much like the default unix containerizer). In the long term (hopefully before MesosCon) we want to support also the Windows container API. This will require implementing a separate containerizer, maybe called `WindowsDockerContainerizer`.  Since the Windows container API is actually officially supported through the Docker interface (_i.e._, MSFT actually ported the Docker engine to Windows, and that is the official API), the interfaces (like the fetcher) shouldn't change much. The tests probably will have to change, as we don't have access to any isolation primitives like cgroups for those tests.  Outstanding TODO([~<USER>): Flesh out this description when more details are available, regarding: * The container API for Windows (when we know them) * The nuances of Windows vs Linux (when we know them) * etc.",Task,Major,Resolved,"2015-07-20 23:44:31","2015-07-20 22:44:31",13
"Apache Mesos","OS-specific code touched by the containerizer tests is not Windows compatible","In the process of adding the Cmake build system, [~<USER> noted and stubbed out all OS-specific code. That sweep (mostly of libprocess and stout) is here: https://github.com/<USER>mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52  Instead of having inline {{#if defined(...)}}, the OS-specific code will be separated into directories. The Windows code will be stubbed out.",Story,Minor,Resolved,"2015-07-20 23:40:54","2015-07-20 22:40:54",13
"Apache Mesos","Authentication for Communicating with Docker Registry","In order to pull Docker images from Docker Hub and private Docker registries, the provisioner must support two primary authentication frameworks to authenticate with the registries, basic authentication and the OAuth2.0 authorization framework, as per the docker registry spec. A Docker registry can also operate in standalone mode and may not require authentication.",Improvement,Major,Resolved,"2015-07-20 23:39:53","2015-07-20 22:39:53",5
"Apache Mesos","PoC running command executor with image provisioner","This is to implement a PoC of the alternative design choices with MESOS-3004",Improvement,Major,Resolved,"2015-07-20 23:39:35","2015-07-20 22:39:35",3
"Apache Mesos","Support HTTPS requests in libprocess","In order to pull images from Docker registries, https calls are needed to securely communicate with the registry hosts. Currently, only http requests are supported through libprocess. Now that SSL sockets are available through libprocess, support for https can be added.",Improvement,Major,Resolved,"2015-07-20 23:25:16","2015-07-20 22:25:16",3
"Apache Mesos","Configure Jenkins to run Docker tests","Add a jenkin job to run the Docker tests",Improvement,Major,"In Progress","2015-07-20 22:19:40","2015-07-20 21:19:40",2
"Apache Mesos","Update scheduler library to send REQUEST call","See MESOS-2913 for context.  From the dev list it looks like users depend on this call for their custom allocator, so we need to support it going forward.",Task,Major,Resolved,"2015-07-20 20:29:20","2015-07-20 19:29:20",2
"Apache Mesos","Update scheduler driver to send SUBSCRIBE call","See MESOS-2913 for context.",Task,Major,Resolved,"2015-07-20 20:26:40","2015-07-20 19:26:40",2
"Apache Mesos","Typos in oversubscription doc","* In docs/oversubscription.md: there are three cases where revocable is written as recovable, including the name of a JSON field.    * Also in `docs/oversubscription.md`: the last sentence doesn't make sense    Maybe should say To select a custom... or To install a custom...",Documentation,Trivial,Resolved,"2015-07-20 11:42:06","2015-07-20 10:42:06",1
"Apache Mesos","Doing 'clone' on Linux with the CLONE_NEWUSER namespace type can drop root privileges.","The namespace tests attempt to clone a process with all namespaces that are available from the kernel which includes the 'user' namespace in Ubuntu 14.04 which causes the child process to be user 'nobody' instead of user 'root' after invoking 'clone' which is bad because the test requires that the child process is 'root' and so things fail (because of insufficient permissions). For now, we explicitly ignore the 'user' namespace in the tests, but this issue is to track exactly how we might want to manage this going forward.",Bug,Major,Accepted,"2015-07-19 19:11:53","2015-07-19 18:11:53",5
"Apache Mesos","Perf related tests rely on 'cycles' which might not always be present.","When running the tests on Ubuntu 14.04 the 'cycles' value collected by perf is always 0, meaning certain tests always fail. These lines in the test have been commented out for now and a TODO has been attached which links to this JIRA issue, since the solution is unclear. In particular, 'cycles' might not properly be counted because it is a hardware counter and this particular machine was a virtual machine. Either way, we should determine the best events to collect from perf in either VM or physical settings.",Bug,Major,Resolved,"2015-07-19 19:01:05","2015-07-19 18:01:05",5
"Apache Mesos","`sudo make distcheck` fails on Ubuntu 14.04 (and possibly other OSes too)","Running tests as root causes a large number of failures.   Full log attached.",Bug,Blocker,Resolved,"2015-07-18 23:56:52","2015-07-18 22:56:52",2
"Apache Mesos","Recovered resources are not re-allocated until the next allocation delay.","Currently, when resources are recovered, we do not perform an allocation for that slave. Rather, we wait until the next allocation interval.  For small task, high throughput frameworks, this can have a significant impact on overall throughput, see the following thread: http://markmail.org/thread/y6mzfwzlurv6nik3  We should consider immediately performing a re-allocation for the slave upon resource recovery.",Improvement,Major,Reviewable,"2015-07-17 23:36:03","2015-07-17 22:36:03",5
"Apache Mesos","Registry recovery does not recover the maintenance object.","Persisted info is fetched from the registry when a master is elected or after failover.  Currently, this process involves 3 steps: * Fetch the registry. * Start an operation to add the new master to the fetched registry. * Check the success of the operation and finish recovering. These methods can be found in src/master/registrar.cpp   Since the maintenance schedule is stored in a separate key, the recover process must also fetch a new maintenance object.  This object needs to be passed along to the master along with the existing registry object.  Possible test(s): * src/tests/registrar_tests.cpp ** Change the Recovery test to include checks for the new object.",Task,Major,Resolved,"2015-07-17 22:21:22","2015-07-17 21:21:22",5
"Apache Mesos","Add Labels to TaskStatus and expose them via state.json","This would allow the executors and Slave modules to expose some meta-data to frameworks and Mesos-DNS via state.json.  A typical use case is to allow the containers to expose their IP to framework/Mesos-DNS.",Task,Critical,Resolved,"2015-07-17 20:54:51","2015-07-17 19:54:51",2
"Apache Mesos","Add capacity heuristic for quota requests in Master","We need to to validate quota requests in the Mesos Master as outlined in the Design Doc: https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I  This ticket aims to validate satisfiability (in terms of available resources) of a quota request using a heuristic algorithm in the Mesos Master, rather than validating the syntax of the request.",Improvement,Major,Resolved,"2015-07-17 15:49:09","2015-07-17 14:49:09",3
"Apache Mesos","Introduce HTTP endpoints for Quota","We need to implement the HTTP endpoints for Quota as outlined in the Design Doc: (https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I). ",Epic,Major,Resolved,"2015-07-17 15:45:14","2015-07-17 14:45:14",3
"Apache Mesos","Unify initialization of modularized components","h1.Introduction  As it stands right now, default implementations of modularized components are required to have a non parametrized {{create()}} static method. This allows to write tests which can cover default implementations and modules based on these default implementations on a uniform way.  For example, with the interface {{Foo}}:    With a default implementation:    This allows to create typed tests which look as following:    The test will be applied to each of types in the template parameters of {{FooTestTypes}}. This allows to test different implementation of an interface. In our code, it tests default implementations and a module which uses the same default implementation.  The class {{tests::Module<typename T, ModuleID N>}} needs a little explanation, it is a wrapper around {{ModuleManager}} which allows the tests to encode information about the requested module in the type itself instead of passing a string to the factory method. The wrapper around create, the real important method looks as follows:    h1.The Problem  Consider the following implementation of {{Foo}}:    As it can be seen, this implementation cannot be used as a default implementation since its create API does not match the one of {{test::Module<>}}: {{create()}} has a different signature for both types. It is still a common situation to require initialization parameters for objects, however this constraint (keeping both interfaces alike) forces default implementations of modularized components to have default constructors, therefore the tests are forcing the design of the interfaces.  Implementations which are supposed to be used as modules only, i.e. non default implementations are allowed to have constructor parameters, since the actual signature of their factory method is, this factory method's function is to decode the parameters and call the appropriate constructor:    where parameters is just an array of key-value string pairs whose interpretation is left to the specific module. Sadly, this call is wrapped by  {{ModuleManager}} which only allows module parameters to be passed from the command line and does not offer a programmatic way to feed construction parameters to modules.  h1.The Ugly Workaround  With the requirement of a default constructor and parameters devoid {{create()}} factory function, a common pattern (see [Authenticator|https://github.com/apache/mesos/blob/9d4ac11ed757aa5869da440dfe5343a61b07199a/include/mesos/authentication/authenticator.hpp]) has been introduced to feed construction parameters into default implementation, this leads to adding an {{initialize()}} call to the public interface, which will have {{Foo}} become:    {{ParameterFoo}} will thus look as follows:    Look that this {{initialize()}} method now has to be implemented by all descendants of {{Foo}}, even if there's a {{DatabaseFoo}} which takes is return value for {{hello()}} from a DB, it will need to support {{int}} as an initialization parameter.  The problem is more severe the more specific the parameter to {{initialize()}} is. For example, if there is a very complex structure implementing ACLs, all implementations of an authorizer will need to import this structure even if they can completely ignore it.  In the {{Foo}} example if {{ParameterFoo}} were to become the default implementation of {{Foo}}, the tests would look as follows:  ",Improvement,Major,Resolved,"2015-07-17 10:29:00","2015-07-17 09:29:00",3
"Apache Mesos","Registry operations do not exist for manipulating maintanence schedules","In order to modify the maintenance schedule in the replicated registry, we will need Operations (src/master/registrar.hpp).  The operations will likely correspond to the HTTP API: * UpdateMaintenanceSchedule: Given a blob representing a maintenance schedule, perform some verification on the blob.  Write the blob to the registry.   * StartMaintenance:  Given a set of machines, verify then transition machines from Draining to Deactivated. * StopMaintenance:  Given a set of machines, verify then transition machines from Deactivated to Normal.  Remove affected machines from the schedule(s).",Task,Major,Resolved,"2015-07-16 19:38:53","2015-07-16 18:38:53",8
"Apache Mesos","Registry operations are hardcoded for a single key (Registry object)","This is primarily a refactoring.  The prototype for modifying the registry is currently:   In order to support Maintenance schedules (possibly Quotas as well), there should be an alternate prototype for Maintenance.  Something like:   The existing RegistrarProcess::update (src/master/registrar.cpp) should be refactored to allow for more than one key.  If necessary, refactor existing operations defined in src/master/master.hpp (AdminSlave, ReadminSlave,  RemoveSlave).",Task,Major,Resolved,"2015-07-16 19:22:57","2015-07-16 18:22:57",5
"Apache Mesos","Implement a streaming response decoder for events stream","We need a streaming response decoder to de-serialize chunks sent from the master on the events stream.  From the HTTP API design doc: Master encodes each Event in RecordIO format, i.e. a string representation of length of the event in bytes followed by JSON or binary Protobuf  (possibly compressed) encoded event.  As of now for getting the basic features right , this is being done in the test-cases:    Two things need to happen: - We need master to emit events in RecordIO format i.e. event size followed by the serialized event instead of just the serialized events as is the case now. - The decoder class should then abstract away the logic of reading the response and de-serializing events from the stream.  Ideally, the decoder should work with both json and protobuf responses. ",Task,Major,Resolved,"2015-07-16 19:08:07","2015-07-16 18:08:07",3
"Apache Mesos","Replicated registry needs a representation of maintenance schedules","In order to persist maintenance schedules across failovers of the master, the schedule information must be kept in the replicated registry.  This means adding an additional message in the Registry protobuf in src/master/registry.proto.  The status of each individual slave's maintenance will also be persisted in this way.   Note: There can be multiple SlaveID's attached to a single hostname.",Task,Major,Resolved,"2015-07-16 19:05:31","2015-07-16 18:05:31",3
"Apache Mesos","Add framework authorization for persistent volume","This is the third in a series of tickets that adds authorization support to persistent volumes.  When a framework creates a persistent volume, create ACLs are checked to see if the framework (FrameworkInfo.principal) or the operator (Credential.user) is authorized to create persistent volumes. If not authorized, the create operation is rejected.  When a framework destroys a persistent volume, destroy ACLs are checked to see if the framework (FrameworkInfo.principal) or the operator (Credential.user) is authorized to destroy the persistent volume created by a framework or operator (Resource.DiskInfo.principal). If not authorized, the destroy operation is rejected.  A separate ticket will use the structures created here to enable authorization of the /create and /destroy HTTP endpoints: https://issues.apache.org/jira/browse/MESOS-3903",Task,Major,Resolved,"2015-07-16 17:52:45","2015-07-16 16:52:45",5
"Apache Mesos","Add 'principal' field to 'Resource.DiskInfo.Persistence'","In order to support authorization for persistent volumes, we should add the {{principal}} to {{Resource.DiskInfo}}, analogous to {{Resource.ReservationInfo.principal}}.",Task,Major,Resolved,"2015-07-16 17:32:19","2015-07-16 16:32:19",1
"Apache Mesos","Add authorization for dynamic reservation","Dynamic reservations should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Reserve}} and {{Unreserve}} into the ACL.    When a framework/operator reserves resources, reserve ACLs are checked to see if the framework ({{FrameworkInfo.principal}}) or the operator ({{Credential.user}}) is authorized to reserve the specified resources. If not authorized, the reserve operation is rejected.  When a framework/operator unreserves resources, unreserve ACLs are checked to see if the framework ({{FrameworkInfo.principal}}) or the operator ({{Credential.user}}) is authorized to unreserve the resources reserved by a framework or operator ({{Resource.ReservationInfo.principal}}). If not authorized, the unreserve operation is rejected.",Task,Major,Resolved,"2015-07-16 17:10:09","2015-07-16 16:10:09",2
"Apache Mesos","Expose docker container IP in Master's state.json","We want to expose docker container IP to Mesos-DNS. One potential solution is to make it available via Master's state.json. We can set a label Docker.NetworkSettings.IPAddress in TaskStatus message (when it is sent the first time with TASK_RUNNING status).",Task,Critical,Resolved,"2015-07-16 15:49:43","2015-07-16 14:49:43",2
"Apache Mesos","FTP response code for success not recognized by fetcher.","The response code for successful HTTP requests is 200, the response code for successful FTP file transfers is 226. The fetcher currently only checks for a response code of 200 even for FTP URIs. This results in failed fetching even though the resource gets downloaded successfully. This has been found by a dedicated external test using an FTP server. ",Bug,Major,Resolved,"2015-07-16 14:18:24","2015-07-16 13:18:24",1
"Apache Mesos","Master doesn't properly handle SUBSCRIBE call","Master::subscribe() incorrectly handles re-registration. It handles it as a registration request (not re-registration) because of a bug in the if loop (should have been !frameworkInfo.has_id()).  ",Bug,Major,Resolved,"2015-07-15 18:20:39","2015-07-15 17:20:39",2
"Apache Mesos","performance issues with port ranges comparison","Testing in an environment with lots of frameworks (>200), where the frameworks permanently decline resources they don't need. The allocator ends up spending a lot of time figuring out whether offers are refused (the code path through {{HierarchicalAllocatorProcess::isFiltered()}}.  In profiling a synthetic benchmark, it turns out that comparing port ranges is very expensive, involving many temporary allocations. 61% of Resources::contains() run time is in operator -= (Resource). 35% of Resources::contains() run time is in Resources::_contains().  The heaviest call chain through {{Resources::_contains}} is:    mesos::coalesce(Value_Ranges) gets done a lot and ends up being really expensive. The heaviest parts of the inverted call chain are:   ",Bug,Major,Resolved,"2015-07-15 06:11:06","2015-07-15 05:11:06",8
"Apache Mesos","Failing ROOT_ tests on CentOS 7.1","Running `sudo make check` on CentOS 7.1 for Mesos 0.23.0-rc3 causes several several failures/errors:   ...  ... ",Bug,Blocker,Resolved,"2015-07-15 01:50:21","2015-07-15 00:50:21",5
"Apache Mesos","Stout's UUID re-seeds a new random generator during each call to UUID::random.","Per [~<USER> and [~<USER>'s observations on MESOS-2940, stout's UUID abstraction is re-seeding the random generator during each call to {{UUID::random()}}, which is really expensive.  This is confirmed in the perf graph from MESOS-2940.",Bug,Major,Resolved,"2015-07-14 22:59:16","2015-07-14 21:59:16",3
"Apache Mesos","Maintenance information is not populated in case of failover","When a master starts up, or after a master has failed, it must re-populate maintenance information (i.e. from the registry to the local state).  Particularly, {{Master::recover}} in {{src/master/master.cpp}} should be changed to process maintenance information.",Task,Major,Resolved,"2015-07-14 21:53:13","2015-07-14 20:53:13",3
"Apache Mesos","Slaves are not deactivated upon reaching a maintenance window","After a maintenance window is reached, the slave should be deactivated to prevent further tasks from utilizing it.   * For slaves that have completely drained, simply deactivate the slave.  See Master::deactivate(Slave*). * For tasks which have not explicitly declined the InverseOffers (i.e. they've accepted them or do not understand InverseOffers), send kill signals.  See Master::killTask * If a slave has tasks that have declined the InverseOffers, do not deactivate the slave.  Possible test(s): * SlaveDrainedTest ** Start master, slave. ** Set maintenance to now. ** Check that slave gets deactivated * InverseOfferAgnosticTest ** Start master, slave, framework. ** Have a task run on the slave (ignores InverseOffers). ** Set maintenance to now. ** Check that task gets killed. ** Check that slave gets deactivated. * InverseOfferAcceptanceTest ** Start master, slave, framework. ** Run a task on the slave. ** Set maintenance to future. ** Have task accept InverseOffer. ** Check task gets killed, slave gets deactivated. * InverseOfferDeclinedTest ** Start master, slave, framework. ** Run task on slave. ** Set maintenance to future. ** Have task decline maintenance with reason. ** Check task lives, slave still active.",Task,Major,Resolved,"2015-07-14 20:02:40","2015-07-14 19:02:40",8
"Apache Mesos","Master does not handle InverseOffers in the Accept call (Event/Call API)","InverseOffers are similar to Offers in that they are Accepted or Declined based on their OfferID.    Some additional logic may be neccesary in Master::accept (src/master/master.cpp) to gracefully handle the acceptance of InverseOffers. * The InverseOffer needs to be removed from the set of pending InverseOffers. * The InverseOffer should not result any errors/warnings.    Note: accepted InverseOffers do not preclude further InverseOffers from being sent to the framework.  Instead, an accepted InverseOffer merely signifies that the framework is _currently_ fine with the expected downtime.",Task,Major,Resolved,"2015-07-14 19:42:43","2015-07-14 18:42:43",3
"Apache Mesos","Master/Allocator does not send InverseOffers to resources to be maintained","Offers are currently sent from master/allocator to framework via ResourceOffersMessage's.  InverseOffers, which are roughly equivalent to negative Offers, can be sent in the same package. In src/messages/messages.proto   Sent InverseOffers can be tracked in the master's local state: i.e. In src/master/master.hpp:   One actor (master or allocator) should populate the new InverseOffers field. * In master (src/master/master.cpp) ** Master::offer is where the ResourceOffersMessage and Offer object is constructed. ** The same method could also check for maintenance and send InverseOffers. * In the allocator (src/master/allocator/mesos/hierarchical.hpp) ** HierarchicalAllocatorProcess::allocate is where slave resources are aggregated an sent off to the frameworks. ** InverseOffers (i.e. negative resources) allocation could be calculated in this method. ** A change to Master::offer (i.e. the offerCallback) may be necessary to account for the negative resources.  Possible test(s): * InverseOfferTest ** Start master, slave, framework. ** Accept resource offer, start task. ** Set maintenance schedule to the future. ** Check that InverseOffer(s) are sent to the framework. ** Decline InverseOffer. ** Check that more InverseOffer(s) are sent. ** Accept InverseOffer. ** Check that more InverseOffer(s) are sent.",Task,Major,Resolved,"2015-07-14 19:24:39","2015-07-14 18:24:39",8
"Apache Mesos","Decline call does not include an optional reason, in the Event/Call API","In the Event/Call API, the Decline call is currently used by frameworks to reject resource offers.  In the case of InverseOffers, the framework could give additional information to the operators and/or allocator, as to why the InverseOffer is declined. i.e. Suppose a cluster running some consensus algorithm is given an InverseOffer on one of its nodes.  It may decline saying Too few nodes (or, more verbosely, Specified InverseOffer would lower the number of active nodes below quorum).  This change requires the following changes: * include/mesos/scheduler/scheduler.proto:  * src/master/master.cpp Change Master::decline to either store the reason, or log it. * Add a declineOffer overload in the (Mesos)SchedulerDriver with an optional reason. ** Extend the interface in include/mesos/scheduler.hpp ** Add/change the declineOffer method in src/sched/sched.cpp",Task,Major,Reviewable,"2015-07-14 18:34:37","2015-07-14 17:34:37",3
"Apache Mesos","Allow executors binding IP to be different than Slave binding IP.","Currently, the Slave will bind either to the loopback IP (127.0.0.1) or to the IP passed via the '--ip' flag. When it launches a containerized executor (e.g, via Mesos Containerizer), the executor inherits the binding IP of the Slave. This is due to the fact that the '--ip' flags sets the environment variable `LIBPROCESS_IP` to the passed IP. The executor then inherits this environment variable and is forced to bind to the Slave IP.  If an executor is running in its own containerized environment, with a separate IP than that of the Slave, currently there is no way of forcing it to bind to its own IP. A potential solution is to use the executor environment decorator hooks to update LIBPROCESS_IP environment variable for the executor.",Improvement,Critical,Resolved,"2015-07-14 02:26:24","2015-07-14 01:26:24",2
"Apache Mesos","Resource offers do not contain Unavailability, given a maintenance schedule","Given a schedule, defined elsewhere, any resource offers to affected slaves must include an Unavailability field.  The maintenance schedule for a single slave should be held in [persistent storage|MESOS-2075] and locally by the master.  i.e. In src/master/master.hpp:  The new field should be populated via an API call (see [MESOS-2067]).  The Unavailability field can be added to Master::offer (src/master/master.cpp).   Possible test(s): * PendingUnavailibilityTest ** Start master, slave. ** Check unavailability of offer == none. ** Set unavailability to the future. ** Check offer has unavailability. ",Task,Major,Resolved,"2015-07-14 01:53:33","2015-07-14 00:53:33",8
"Apache Mesos","Add a SUPPRESS call to the scheduler","SUPPRESS call is the complement to the current REVIVE call i.e., it will inform Mesos to stop sending offers to the framework.   For the scheduler driver to send only Call messages (MESOS-2913), DeactivateFrameworkMessage needs to be converted to Call(s). We can implement this by having the driver send a SUPPRESS call followed by a DECLINE call for outstanding offers.",Improvement,Major,Resolved,"2015-07-14 00:40:27","2015-07-13 23:40:27",3
"Apache Mesos","As a Developer I would like a standard way to run a Subprocess in libprocess","As part of MESOS-2830 and MESOS-2902 I have been researching the ability to run a {{Subprocess}} and capture the {{stdout / stderr}} along with the exit status code.  {{process::subprocess()}} offers much of the functionality, but in a way that still requires a lot of handiwork on the developer's part; we would like to further abstract away the ability to just pass a string, an optional set of command-line arguments and then collect the output of the command (bonus: without blocking).",Story,Major,Open,"2015-07-13 23:16:39","2015-07-13 22:16:39",3
"Apache Mesos","Document containerizer launch ","We currently dont have enough documentation for the containerizer component. This task adds documentation for containerizer launch sequence. The mail goals are: - Have diagrams (state, sequence, class etc) depicting the containerizer launch process. - Make the documentation newbie friendly. - Usable for future design discussions.",Documentation,Minor,Resolved,"2015-07-13 19:32:48","2015-07-13 18:32:48",3
"Apache Mesos","ProcessTest.Cache fails and hangs"," The tests then finish running, but the gtest framework fails to terminate and uses 100% CPU.",Bug,Minor,Accepted,"2015-07-09 19:54:46","2015-07-09 18:54:46",5
"Apache Mesos","0.22.x scheduler driver drops 0.23.x reconciliation status updates due to missing StatusUpdate.uuid.","In the process of fixing MESOS-2940, we accidentally introduced a non-backwards compatible change:  --> StatusUpdate.uuid was required in 0.22.x and was always set. --> StatusUpdate.uuid is optional in 0.23.x and the master is not setting it for master-generated updates.  In 0.22.x, the scheduler driver ignores the 'uuid' for master/driver generated updates already. I'd suggest the following fix:  # In 0.23.x, rather than not setting StatusUpdate.uuid, set it to an empty string. # In 0.23.x, ensure the scheduler driver also ignores empty StatusUpdate.uuids. # In 0.24.x, stop setting StatusUpdate.uuid.",Bug,Blocker,Resolved,"2015-07-09 18:58:56","2015-07-09 17:58:56",3
"Apache Mesos","HTTP endpoint authN is enabled merely by specifying --credentials","If I set `--credentials` on the master, framework and slave authentication are allowed, but not required. On the other hand, http authentication is now required for authenticated endpoints (currently only `/shutdown`). That means that I cannot enable framework or slave authentication without also enabling http endpoint authentication. This is undesirable.  Framework and slave authentication have separate flags (`\--authenticate` and `\--authenticate_slaves`) to require authentication for each. It would be great if there was also such a flag for http authentication. Or maybe we get rid of these flags altogether and rely on ACLs to determine which unauthenticated principals are even allowed to authenticate for each endpoint/action.",Bug,Major,Resolved,"2015-07-09 07:45:49","2015-07-09 06:45:49",8
"Apache Mesos","Factoring out the pattern for URL generation ","fetcher_test.cpp uses the following code for generating URLs:  string url = http:// + net::getHostname(process.self().address.ip).get() + : + stringify(process.self().address.port) + / + process.self().id  it would be good to isolate that code in a function, and replace the code above with something like:  string url = http:// + endpoint_url(process, uri_test); ",Task,Minor,Resolved,"2015-07-09 06:57:19","2015-07-09 05:57:19",1
"Apache Mesos","Implement Docker Image Provisioner Reference Store","Create a comprehensive store to look up an image and tag's associated image layer ID. Implement add, remove, save, and update images and their associated tags.",Improvement,Major,Resolved,"2015-07-08 23:33:25","2015-07-08 22:33:25",3
"Apache Mesos","Expose major, minor and patch components from stout Version  ","Stout version class does not expose version components, preventing computations manipulation of version information.  Solution is to make major, minor and patch public.",Improvement,Major,Resolved,"2015-07-08 23:03:03","2015-07-08 22:03:03",1
"Apache Mesos","A mechanism for messages between Master modules and Slave modules","A slave module should be able to send a message to a master module and vice-versa to allow out-of-band communication between master/slave modules.",Task,Major,Resolved,"2015-07-08 03:06:34","2015-07-08 02:06:34",8
"Apache Mesos","Make container-IP available via Master endpoint",,Task,Major,Resolved,"2015-07-08 03:04:23","2015-07-08 02:04:23",5
"Apache Mesos","Add task status update hooks for Master/Slave","The task termination hooks are needed for doing task-specific cleanup in Master/Slave.",Task,Major,Resolved,"2015-07-08 03:03:07","2015-07-08 02:03:07",3
"Apache Mesos","Add hooks for Slave exits","The hook will be triggered on slave exits. A master hook module can use this to do Slave-specific cleanups.  In our particular use case, the hook would trigger cleanup of IPs assigned to the given Slave (see the [design doc | https://docs.google.com/document/d/17mXtAmdAXcNBwp_JfrxmZcQrs7EO6ancSbejrqjLQ0g/edit#]).",Task,Major,Resolved,"2015-07-08 02:55:20","2015-07-08 01:55:20",2
"Apache Mesos","Extend ContainerInfo to include NetworkInfo message","As per the [design doc|https://docs.google.com/document/d/17mXtAmdAXcNBwp_JfrxmZcQrs7EO6ancSbejrqjLQ0g], we need to enable frameworks to specify network requirements. The proposed message could be along the lines of:  ",Task,Major,Resolved,"2015-07-08 02:37:25","2015-07-08 01:37:25",2
"Apache Mesos","Support existing message passing optimization with Event/Call.","See the thread here: http://markmail.org/thread/wvapc7vkbv7z6gbx  The scheduler driver currently sends framework messages directly to the slave, when possible:    The slave always sends messages directly to the scheduler driver:   In order for the scheduler driver to receive Events from the master, it needs enough information to continue directly sending messages to slaves. This was previously accomplished by sending the slave's pid inside the [offer message|https://github.com/apache/mesos/blob/0.23.0-rc1/src/messages/messages.proto#L168]:    We could add an 'Address' to the Offer protobuf to provide the scheduler driver with the same information:    The path prefix is required for testing purposes, where we can have multiple slaves within a process (e.g. {{localhost:5051/slave(1)/state.json}} vs. {{localhost:5051/slave(2)/state.json}}).  This provides enough information to allow the scheduler driver to continue to directly send messages to the slaves, which unblocks MESOS-2910.",Task,Major,Resolved,"2015-07-08 02:07:07","2015-07-08 01:07:07",1
"Apache Mesos","Reproduce systemd cgroup behavior ","It has been noticed before that systemd reorganizes cgroup hierarchy created by mesos slave. Because of this mesos is no longer able to find the cgroup, and there is also a chance of undoing the isolation that mesos slave puts in place. ",Task,Major,Resolved,"2015-07-07 22:55:38","2015-07-07 21:55:38",5
"Apache Mesos","Libevent SSL doesn't use EPOLL","we currently disable to epoll in libevent to allow SSL to work. It would be more scalable if we didn't have to do that.",Improvement,Major,Resolved,"2015-07-07 21:40:54","2015-07-07 20:40:54",8
"Apache Mesos","Add cgroups memory stats API","cgroups API current does expose stats from the memory namespace. Having this API would enable isolators to use its various fields(eg. rss, rss_huge, writeback etc) in use cases like usage metrics.",Task,Major,Accepted,"2015-07-07 21:03:49","2015-07-07 20:03:49",2
"Apache Mesos","SSL tests can fail depending on hostname configuration","Depending on how /etc/hosts is configured, the SSL tests can fail with a bad hostname match for the certificate. We can avoid this by explicitly matching the hostname for the certificate to the IP that will be used during the test.",Bug,Blocker,Resolved,"2015-07-07 20:38:04","2015-07-07 19:38:04",3
"Apache Mesos","Design support running the command executor with provisioned image for running a task in a container","Mesos Containerizer uses the command executor to actually launch the user defined command, and the command executor then can communicate with the slave about the process lifecycle. When we provision a new container with the user specified image, we also need to be able to run the command executor in the container to support the same semantics. One approach is to dynamically mount in a static binary of the command executor with all its dependencies in a special directory so it doesn't interfere with the provisioned root filesystem and configure the mesos containerizer to run the command executor in that directory.",Improvement,Major,Resolved,"2015-07-07 20:24:28","2015-07-07 19:24:28",5
"Apache Mesos","Rename Option<T>::get(const T& _t) to getOrElse() broke network isolator","Change to Option from get() to getOrElse() breaks network isolator.  Building with '../configure --with-network-isolator' generates the following error:  ../../src/slave/containerizer/isolators/network/port_mapping.cpp: In static member function 'static Try<mesos::slave::Isolator*> mesos::internal::slave::PortMappingIsolatorProcess::create(const mesos::internal::slave::Flags&)': ../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: error: no matching function for call to 'Option<std::basic_string<char> >::get(const char [1]) const'        flags.resources.get(),                              ^ ../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: note: candidates are: In file included from ../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp:26:0,                  from ../../3rdparty/libprocess/include/process/check.hpp:19,                  from ../../3rdparty/libprocess/include/process/collect.hpp:7,                  from ../../src/slave/containerizer/isolators/network/port_mapping.cpp:30: ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note: const T& Option<T>::get() const [with T = std::basic_string<char>]    const T& get() const { assert(isSome()); return t; }             ^ ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note:   candidate expects 0 arguments, 1 provided ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note: T& Option<T>::get() [with T = std::basic_string<char>]    T& get() { assert(isSome()); return t; }       ^ ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note:   candidate expects 0 arguments, 1 provided make[2]: *** [slave/containerizer/isolators/network/libmesos_no_3rdparty_la-port_mapping.lo] Error 1 make[2]: Leaving directory `/home/<USER>sandbox/mesos.master/build/src' make[1]: *** [check] Error 2 make[1]: Leaving directory `/home/<USER>sandbox/mesos.master/build/src' make: *** [check-recursive] Error 1 ",Bug,Blocker,Resolved,"2015-07-07 19:44:29","2015-07-07 18:44:29",1
"Apache Mesos","Create a demo HTTP API client","We want to create a simple demo HTTP API Client (in Java, Python or Go) that can serve as an example framework for people who will want to use the new API for their Frameworks.  The scope should be fairly limited (eg, launching a simple Container task?) but sufficient to exercise most of the new API endpoint messages/capabilities.  Scope: TBD  Non-Goals:   - create a best-of-breed Framework to deliver any specific functionality; - create an Integration Test for the HTTP API.",Bug,Major,Resolved,"2015-07-07 19:20:38","2015-07-07 18:20:38",8
"Apache Mesos","SSL connection failure causes failed CHECK.",,Bug,Blocker,Resolved,"2015-07-07 03:43:00","2015-07-07 02:43:00",3
"Apache Mesos","Standardize use of Path ","As per the discussion in MESOS-2965, the use of the Path object should be standardized: * Functions which effectively use Paths (as strings) should instead take Paths. * Functions which modify and return Paths (as strings) should instead return Paths. * Extraneous uses of Path.value should be removed.",Improvement,Minor,Accepted,"2015-07-06 18:58:53","2015-07-06 17:58:53",3
"Apache Mesos","Design doc for creating user namespaces inside containers",,Improvement,Major,Open,"2015-07-06 18:54:57","2015-07-06 17:54:57",5
"Apache Mesos","Document  per container unique egress flow and network queueing statistics","Document new network isolation capabilities in 0.23",Bug,Major,Resolved,"2015-07-06 18:52:47","2015-07-06 17:52:47",3
"Apache Mesos","Compilation Error on Mac OS 10.10.4 with clang 3.5.0","Compiling 0.23.0 (rc1) produces compilation errors on Mac OS 10.10.4 with {{g++}} based on LLVM 3.5. It looks like the issue was introduced in {{a5640ad813e6256b548fca068f04fd9fa3a03eda}}, https://reviews.apache.org/r/32838. In contrast to the commit message, compiling the rc with gcc4.4 on CentOS worked fine for me.   According to 0.23 release notes and MESOS-2604, we should support clang 3.5.     Compiler version:  ",Bug,Major,Resolved,"2015-07-06 14:18:24","2015-07-06 13:18:24",1
"Apache Mesos","Docker version output is not compatible with Mesos","We currently use docker version to get Docker version, in Docker master branch and soon in Docker 1.8 [1] the output for this command changes. The solution for now will be to use the unchanged docker --version output, in the long term we should consider stop using the cli and use the API instead.   [1] https://github.com/docker/docker/pull/14047",Bug,Major,Resolved,"2015-07-02 09:56:55","2015-07-02 08:56:55",1
"Apache Mesos","Deprecating '.json' extension in files endpoints url","Remove the '.json' extension on endpoints such as `/files/browse.json` so it become `/files/browse`",Improvement,Major,Resolved,"2015-07-02 08:52:48","2015-07-02 07:52:48",1
"Apache Mesos","Deprecating '.json' extension in slave endpoints url","Remove the '.json' extension on endpoints such as `/slave/state.json` so it become `/slave/state`",Improvement,Major,Resolved,"2015-07-02 08:48:27","2015-07-02 07:48:27",1
"Apache Mesos","Allow runtime configuration to be returned from provisioner","Image specs also includes execution configuration (e.g: Env, user, ports, etc). We should support passing those information from the image provisioner back to the containerizer.",Improvement,Major,Resolved,"2015-07-01 19:59:58","2015-07-01 18:59:58",5
"Apache Mesos","SSL tests don't work with --gtest_shuffle",,Bug,Major,Resolved,"2015-07-01 03:22:04","2015-07-01 02:22:04",3
"Apache Mesos","stout flags can't have their defaults reset","Stout flags don't remember their default values, and so can't have their defaults reset. This makes it hard to reset flags to their defaults between tests.",Bug,Major,Open,"2015-07-01 03:17:35","2015-07-01 02:17:35",5
"Apache Mesos","SSL tests don't work with --gtest_repeat","commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f Author: <USER><<EMAIL>> Date:   Wed Jul 1 16:16:52 2015 -0700      MESOS-2973: Allow SSL tests to run using gtest_repeat.          The SSL ctx object carried some settings between reinitialize()     calls. Re-construct the object to avoid this state transition.          Review: https://reviews.apache.org/r/36074",Bug,Major,Resolved,"2015-07-01 02:51:24","2015-07-01 01:51:24",3
"Apache Mesos","Serialize Docker image spec as protobuf","The Docker image specification defines a schema for the metadata json that it puts into each image. Currently the docker image provisioner needs to be able to parse and understand this metadata json, and we should create a protobuf equivelent schema so we can utilize the json to protobuf conversion to read and validate the metadata.",Improvement,Major,Resolved,"2015-07-01 01:37:32","2015-07-01 00:37:32",3
"Apache Mesos","Implement OverlayFS based provisioner backend","Part of the image provisioning process is to call a backend to create a root filesystem based on the image on disk layout. The problem with the copy backend is that it's both waste of IO and space, and bind only can deal with one layer. Overlayfs backend allows us to utilize the filesystem to merge multiple filesystems into one efficiently.",Improvement,Major,Resolved,"2015-07-01 01:36:37","2015-07-01 00:36:37",5
"Apache Mesos","Implement shared copy based provisioner backend","Currently Appc and Docker both implemented its own copy backend, but most of the logic is the same where the input is just a image name with its dependencies. We can refactor both so that we just have one implementation that is shared between both provisioners, so appc and docker can reuse the shared copy backend.",Improvement,Major,Resolved,"2015-07-01 01:27:48","2015-07-01 00:27:48",3
"Apache Mesos","Missing doxygen documentation for libprocess socket interface ","Convert existing comments to doxygen format.  ",Improvement,Major,Resolved,"2015-06-30 22:18:34","2015-06-30 21:18:34",5
"Apache Mesos","socket::peer() and socket::address() might fail with SSL enabled","libevent SSL currently uses a secondary FD so we need to virtualize the get() function on socket interface. ",Improvement,Major,Resolved,"2015-06-30 22:09:57","2015-06-30 21:09:57",5
"Apache Mesos","Add implicit cast to string operator to Path.","For example:   does not have an overload for   The implementation should be something like:  ",Improvement,Minor,Resolved,"2015-06-30 22:04:09","2015-06-30 21:04:09",2
"Apache Mesos","libprocess io does not support peek()","Finally, I so wish we could just do:    from: https://reviews.apache.org/r/31207/",Improvement,Minor,Resolved,"2015-06-30 21:55:37","2015-06-30 20:55:37",3
"Apache Mesos","Configure Jenkins to build ssl",,Improvement,Major,Resolved,"2015-06-30 19:37:22","2015-06-30 18:37:22",5
"Apache Mesos","Slave fails with Abort stacktrace when DNS cannot resolve hostname","If the DNS cannot resolve the hostname-to-IP for a slave node, we correctly return an {{Error}} object, but we then fail with a segfault.  This code adds a more user-friendly message and exits normally (with an {{EXIT_FAILURE}} code).  For example, forcing {{net::getIp()}} to always return an {{Error}}, now causes the slave to exit like this:  ",Bug,Major,Resolved,"2015-06-30 19:36:49","2015-06-30 18:36:49",1
"Apache Mesos","Add cpuacct subsystem utils to cgroups","Current cgroups implementation does not have a cpuacct subsystem implementation. This subsystem reports important metrics like user and system CPU ticks spent by a process. cgroups namespace has subsystem specific utilities for cpu, memory etc. It could use other subsystems specific utils (eg. cpuacct).  In the future, we could also view cgroups as a mesos-subsystem with  features like event notifications.  Although refactoring cgroups would be a different epic, listing the possible tasks:   -  Have hierarchies, subsystems abstracted to represent the domain    - Create  cgroups service   -  cgroups service listen to update events from the OS on files like stats. This would be an interrupt based system(maybe use linux fsnotify)   - cgroups service services events to mesos (containers for example).  ",Task,Major,Resolved,"2015-06-30 18:14:10","2015-06-30 17:14:10",2
"Apache Mesos","Update Call protobuf to move top level FrameworkInfo inside Subscribe","It is better for FrameworkInfo to be only included in 'Subscribe' message (that needs to be added) instead of for every call. Instead the top level Call should contain a FrameworkID to identify the framework making the call.",Improvement,Major,Resolved,"2015-06-29 21:07:38","2015-06-29 20:07:38",3
"Apache Mesos","Add version to MasterInfo","This will help schedulers figure out the version of the master that they are interacting with. See MESOS-2736 for additional context.",Improvement,Major,Resolved,"2015-06-29 21:05:40","2015-06-29 20:05:40",1
"Apache Mesos","Stack trace in isolator tests on Linux VM","PerfEventIsolatorTest fails with stack trace when run in Linux VM  [----------] 1 test from PerfEventIsolatorTest [ RUN      ] PerfEventIsolatorTest.ROOT_CGROUPS_Sample F0629 11:38:17.088412 14114 isolator_tests.cpp:837] CHECK_SOME(isolator): Failed to create PerfEvent isolator, invalid events: { cycles, task-clock }  *** Check failure stack trace: ***     @     0x2ab5e5aeeb1a  google::LogMessage::Fail()     @     0x2ab5e5aeea66  google::LogMessage::SendToLog()     @     0x2ab5e5aee468  google::LogMessage::Flush()     @     0x2ab5e5af137c  google::LogMessageFatal::~LogMessageFatal()     @           0x864b0c  _CheckFatal::~_CheckFatal()     @           0xc458ed  mesos::internal::tests::PerfEventIsolatorTest_ROOT_CGROUPS_Sample_Test::TestBody()     @          0x119fb17  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x119ac9e  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x118305f  testing::Test::Run()     @          0x1183782  testing::TestInfo::Run()     @          0x1183d0a  testing::TestCase::Run()     @          0x11889d4  testing::internal::UnitTestImpl::RunAllTests()     @          0x11a09ae  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x119b9c3  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x11878e0  testing::UnitTest::Run()     @           0xcdc8c7  main     @     0x2ab5e7fdbec5  (unknown)     @           0x861a89  (unknown) make[3]: *** [check-local] Aborted (core dumped)  [ RUN      ] UserCgroupIsolatorTest/2.ROOT_CGROUPS_UserCgroup F0629 11:49:38.763434 18836 isolator_tests.cpp:1200] CHECK_SOME(isolator): Failed to create PerfEvent isolator, invalid events: { cpu-cycles }  *** Check failure stack trace: ***     @     0x2ba40eb2db1a  google::LogMessage::Fail()     @     0x2ba40eb2da66  google::LogMessage::SendToLog()     @     0x2ba40eb2d468  google::LogMessage::Flush()     @     0x2ba40eb3037c  google::LogMessageFatal::~LogMessageFatal()     @           0x864b0c  _CheckFatal::~_CheckFatal()     @           0xc5ddb1  mesos::internal::tests::UserCgroupIsolatorTest_ROOT_CGROUPS_UserCgroup_Test<>::TestBody()     @          0x119fc43  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x119adca  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x118318b  testing::Test::Run()     @          0x11838ae  testing::TestInfo::Run()     @          0x1183e36  testing::TestCase::Run()     @          0x1188b00  testing::internal::UnitTestImpl::RunAllTests()     @          0x11a0ada  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x119baef  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x1187a0c  testing::UnitTest::Run()     @           0xcdc9f3  main     @     0x2ba41101aec5  (unknown)     @           0x861a89  (unknown) make[3]: *** [check-local] Aborted (core dumped)  ",Bug,Major,Resolved,"2015-06-29 19:47:48","2015-06-29 18:47:48",1
"Apache Mesos","Inefficient container usage collection","docker containerizer currently collects usage statistics by calling os's process statistics (eg ps ). There is scope for making this efficient, say by querying cgroups file system.  ",Improvement,Major,Resolved,"2015-06-26 19:22:15","2015-06-26 18:22:15",3
"Apache Mesos","Implement current mesos Authorizer in terms of generalized Authorizer interface","In order to maintain compatibility with existent versions of Mesos, as well as to prove the flexibility of the generalized {{mesos::Authorizer}} design, the current authorization mechanism through ACL definitions needs to run under the updated interface without any changes being noticeable by the current authorization users.",Task,Major,Resolved,"2015-06-26 15:27:09","2015-06-26 14:27:09",8
"Apache Mesos","Draft design for generalized Authorizer interface","As mentioned in MESOS-2948 the current {{mesos::Authorizer}} interface is rather inflexible if new _Actions_ or _Objects_ need to be added.  A new API needs to be designed in a way that allows for arbitrary _Actions_ and _Objects_ to be added to the authorization mechanism without having to recompile mesos.",Task,Major,Resolved,"2015-06-26 15:24:05","2015-06-26 14:24:05",3
"Apache Mesos","Authorizer Module: Implementation, Integration & Tests","h4.Motivation Provide an example authorizer module based on the {{LocalAuthorizer}} implementation. Make sure that such authorizer module can be fully unit- and integration- tested within the mesos test suite. ",Improvement,Major,Resolved,"2015-06-26 14:00:40","2015-06-26 13:00:40",8
"Apache Mesos","Authorizer Module: Interface design","h4.Motivation Design an interface covering authorizer modules while staying minimally invasive in regards to changes to the existing {{LocalAuthorizer}} implementation. ",Improvement,Major,Resolved,"2015-06-26 13:55:04","2015-06-26 12:55:04",2
"Apache Mesos","Use of EXPECT in test and relying on the checked condition afterwards.","In docker_containerizer_test we have the following pattern.    As we rely on the value afterwards we should use ASSERT_NE instead. In that case the test will fail immediately. ",Bug,Minor,Resolved,"2015-06-26 13:18:19","2015-06-26 12:18:19",1
"Apache Mesos","mesos fails to compile under mac when libssl and libevent are enabled","../configure --enable-debug --enable-libevent --enable-ssl && make  produces the following error:  poll.cpp' || echo '../../../3rdparty/libprocess/'`src/libevent_poll.cpp libtool: compile:  g++ -DPACKAGE_NAME=\libprocess\ -DPACKAGE_TARNAME=\libprocess\ -DPACKAGE_VERSION=\0.0.1\ -DPACKAGE_STRING=\libprocess 0.0.1\ -DPACKAGE_BUGREPORT=\\ -DPACKAGE_URL=\\ -DPACKAGE=\libprocess\ -DVERSION=\0.0.1\ -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\.libs/\ -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBCURL=1 -DHAVE_EVENT2_EVENT_H=1 -DHAVE_LIBEVENT=1 -DHAVE_EVENT2_THREAD_H=1 -DHAVE_LIBEVENT_PTHREADS=1 -DHAVE_OPENSSL_SSL_H=1 -DHAVE_LIBSSL=1 -DHAVE_LIBCRYPTO=1 -DHAVE_EVENT2_BUFFEREVENT_SSL_H=1 -DHAVE_LIBEVENT_OPENSSL=1 -DUSE_SSL_SOCKET=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBDL=1 -I. -I../../../3rdparty/libprocess -I../../../3rdparty/libprocess/include -I../../../3rdparty/libprocess/3rdparty/stout/include -I3rdparty/boost-1.53.0 -I3rdparty/libev-4.15 -I3rdparty/picojson-4f93734 -I3rdparty/glog-0.3.3/src -I3rdparty/ry-http-parser-1c3624a -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0 -g1 -O0 -std=c++11 -stdlib=libc++ -DGTEST_USE_OWN_TR1_TUPLE=1 -MT libprocess_la-libevent_poll.lo -MD -MP -MF .deps/libprocess_la-libevent_poll.Tpo -c ../../../3rdparty/libprocess/src/libevent_poll.cpp  -fno-common -DPIC -o libprocess_la-libevent_poll.o mv -f .deps/libprocess_la-socket.Tpo .deps/libprocess_la-socket.Plo mv -f .deps/libprocess_la-subprocess.Tpo .deps/libprocess_la-subprocess.Plo mv -f .deps/libprocess_la-libevent.Tpo .deps/libprocess_la-libevent.Plo mv -f .deps/libprocess_la-metrics.Tpo .deps/libprocess_la-metrics.Plo In file included from ../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:11: In file included from ../../../3rdparty/libprocess/include/process/queue.hpp:9: ../../../3rdparty/libprocess/include/process/future.hpp:849:7: error: no viable conversion from 'const process::Future<const process::Future<process::network::Socket> >' to 'const process::network::Socket'  set(u);      ^ ../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:769:10: note: in instantiation of function template specialization 'process::Future<process::network::Socket>::Future<process::Future<const process::Future<process::network::Socket> > >' requested here  return accept_queue.get()         ^ ../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit move constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to      'process::network::Socket &&' for 1st argument class Socket      ^ ../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit copy constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to      'const process::network::Socket &' for 1st argument class Socket      ^ ../../../3rdparty/libprocess/include/process/future.hpp:411:21: note: passing argument to parameter '_t' here  bool set(const T& _t);                    ^ 1 error generated. make[4]: *** [libprocess_la-libevent_ssl_socket.lo] Error 1 make[4]: *** Waiting for unfinished jobs.... mv -f .deps/libprocess_la-libevent_poll.Tpo .deps/libprocess_la-libevent_poll.Plo mv -f .deps/libprocess_la-openssl.Tpo .deps/libprocess_la-openssl.Plo mv -f .deps/libprocess_la-process.Tpo .deps/libprocess_la-process.Plo make[3]: *** [all-recursive] Error 1 make[2]: *** [all-recursive] Error 1 make[1]: *** [all] Error 2 make: *** [all-recursive] Error 1",Bug,Blocker,Resolved,"2015-06-26 04:24:22","2015-06-26 03:24:22",2
"Apache Mesos","Create documentation for using SSL",,Documentation,Blocker,Resolved,"2015-06-25 23:41:54","2015-06-25 22:41:54",5
"Apache Mesos","Add a benchmark for task reconciliation.","Per MESOS-2940, it would be great to have a benchmark for task reconciliation, given large numbers of tasks.  This can guide attempts at improving performance.",Task,Major,Accepted,"2015-06-25 22:36:14","2015-06-25 21:36:14",1
"Apache Mesos","Reconciliation is expensive for large numbers of tasks.","We've observed that both implicit and explicit reconciliation are expensive for large numbers of tasks:  {noformat: title=Explicit O(100,000) tasks: 70secs} I0625 20:55:23.716320 21937 master.cpp:3863] Performing explicit task state reconciliation for N tasks of framework F (NAME) at S@IP:PORT I0625 20:56:34.812464 21937 master.cpp:5041] Removing task T with resources R of framework F on slave S at slave(1)@IP:PORT (HOST)   Let's add a benchmark to see if there are any bottlenecks here, and to guide improvements.",Improvement,Critical,Resolved,"2015-06-25 22:33:27","2015-06-25 21:33:27",3
"Apache Mesos","Testing the new workflow","This is a simple test story to try out the new workflow.  Unfortunately, testing and getting it to work seems to be something that actually does take up time, so I'm tracking this here.",Story,Major,Resolved,"2015-06-25 19:23:34","2015-06-25 18:23:34",3
"Apache Mesos","Linux docker inspect crashes","On linux,  when a simple task is being executed on docker container executor, the sandbox stderr shows a backtrace:  *** Aborted at 1435254156 (unix time) try date -d @1435254156 if you are using GNU date *** PC: @     0x7ffff2b1364d (unknown) *** SIGSEGV (@0xfffffffffffffff8) received by PID 88424 (TID 0x7fffe88fb700) from PID 18446744073709551608; stack trace: ***     @     0x7ffff25a4340 (unknown)     @     0x7ffff2b1364d (unknown)     @     0x7ffff2b724df (unknown)     @           0x4a6466 Docker::Container::~Container()     @     0x7ffff5bfa49a Option<>::~Option()     @     0x7ffff5c15989 Option<>::operator=()     @     0x7ffff5c09e9f Try<>::operator=()     @     0x7ffff5c09ee3 Result<>::operator=()     @     0x7ffff5c0a938 process::Future<>::set()     @     0x7ffff5bff412 process::Promise<>::set()     @     0x7ffff5be53e3 Docker::___inspect()     @     0x7ffff5be3cf8 _ZZN6Docker9__inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationENS2_6FutureISsEERKNS2_10SubprocessEENKUlRKSG_E1_clESL_     @     0x7ffff5be91e9 _ZZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferEENUlSM_E_clESM_     @     0x7ffff5be9d9d _ZNSt17_Function_handlerIFvRKN7process6FutureISsEEEZNKS2_5onAnyIZN6Docker9__inspectERKSsRKNS0_5OwnedINS0_7PromiseINS7_9ContainerEEEEERK6OptionI8DurationES2_RKNS0_10SubprocessEEUlS4_E1_vEES4_OT_NS2_6PreferEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_     @     0x7ffff5c1eadd std::function<>::operator()()     @     0x7ffff5c15e07 process::Future<>::onAny()     @     0x7ffff5be93a1 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferE     @     0x7ffff5be87f6 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_EESM_OT_     @     0x7ffff5be459c Docker::__inspect()     @     0x7ffff5be337c _ZZN6Docker8_inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationEENKUlvE_clEv     @     0x7ffff5be8c5a _ZZNK7process6FutureI6OptionIiEE5onAnyIZN6Docker8_inspectERKSsRKNS_5OwnedINS_7PromiseINS5_9ContainerEEEEERKS1_I8DurationEEUlvE_vEERKS3_OT_NS3_10LessPreferEENUlSL_E_clESL_     @     0x7ffff5be9b36 _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyIZN6Docker8_inspectERKSsRKNS0_5OwnedINS0_7PromiseINS9_9ContainerEEEEERKS2_I8DurationEEUlvE_vEES6_OT_NS4_10LessPreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_     @     0x7ffff5c1e9b3 std::function<>::operator()()     @     0x7ffff6184a1a _ZN7process8internal3runISt8functionIFvRKNS_6FutureI6OptionIiEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_     @     0x7ffff617e64d process::Future<>::set()     @     0x7ffff6752e46 process::Promise<>::set()     @     0x7ffff675faec process::internal::cleanup()     @     0x7ffff6765293 _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EE6__callIvIS6_EILm0ELm1ELm2EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE     @     0x7ffff6764bcd _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EEclIJS6_EvEET0_DpOT_     @     0x7ffff67642a5 _ZZNK7process6FutureI6OptionIiEE5onAnyISt5_BindIFPFvRKS3_PNS_7PromiseIS2_EERKNS_10SubprocessEESt12_PlaceholderILi1EESA_SB_EEvEES7_OT_NS3_6PreferEENUlS7_E_clES7_     @     0x7ffff676531d _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyISt5_BindIFPFvS6_PNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EESC_SD_EEvEES6_OT_NS4_6PreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_     @     0x7ffff5c1e9b3 std::function<>::operator()() (END)  ",Bug,Major,Resolved,"2015-06-25 18:54:54","2015-06-25 17:54:54",1
"Apache Mesos","Initial design document for Quota support in Allocator.","Create a design document for the Quota feature support in the built-in Hierarchical DRF allocator to be shared with the Mesos community.",Documentation,Major,Resolved,"2015-06-25 18:51:14","2015-06-25 17:51:14",5
"Apache Mesos","Create a design document for Quota support in Master","Create a design document for the Quota feature support in Mesos Master (excluding allocator) to be shared with the Mesos community.  Design Doc: https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I/edit?usp=sharing",Documentation,Major,Resolved,"2015-06-25 18:47:05","2015-06-25 17:47:05",8
"Apache Mesos","Update stout #include headers","Update stout to #include headers for symbols we rely on and reorder to comply with the style guide.",Improvement,Major,Resolved,"2015-06-25 00:47:54","2015-06-24 23:47:54",2
"Apache Mesos","Extend mesos-style.py/cpplint.py to check #include files","cpplint.py provides the capability to enforce the style guide requirements for #including everything you use and ordering files based on type but it does not work for mesos because we do use #include <...> for project files where it expects #include ....    We should update the style checker to support our include usage and then turn it on by default in the commit hook.",Bug,Major,Open,"2015-06-25 00:45:44","2015-06-24 23:45:44",1
"Apache Mesos","Invalid usage of ATOMIC_FLAG_INIT in member initialization","The C++ specification states:  The macro ATOMIC_FLAG_INIT shall be defined in such a way that it can be used to initialize an object of type atomic_flag to the clear state. The macro can be used in the form: atomic_flag guard = ATOMIC_FLAG_INIT; It is unspecified whether the macro can be used in other initialization contexts.   Clang catches this (although reports it erroneously as a braced scaled init issue) and refuses to compile libprocess.",Bug,Major,Resolved,"2015-06-24 20:42:22","2015-06-24 19:42:22",1
"Apache Mesos","fetcher.cpp - problem with certificates..?","Mesos 0.22.0/0.22.1 built and installed from sources accordingly to the instructions given [here|http://mesos.apache.org/gettingstarted/] has some problem with certificates. Every time I try to deploy something that requires downloading any resource via HTTPS (with URI specified via Marathon), such deployment fails and I get this message in failed app's sandbox:  Trying to download the same resource on the same slave with {{curl}} or {{wget}} works without problems. Moreover, when I install exactly the same version of Mesos from Mesosphere's debs on identical machines (i.e., set up by the same Ansible scripts), everything works fine as well. I guess it must be something related to the way how Mesos is built - maybe some missing switch for {{configure}} or {{make}}..?  Any ideas..?",Bug,Major,Accepted,"2015-06-24 15:12:25","2015-06-24 14:12:25",2
"Apache Mesos","Add move constructors / assignment to Future.","Now that we have C++11, let's add move constructors and move assignment operators for Future, similarly to what was done for Option. There is currently one move constructor for Future<T>, but not for T, U, and no assignment operator.",Improvement,Major,Resolved,"2015-06-24 02:43:24","2015-06-24 01:43:24",3
"Apache Mesos","Add move constructors / assignment to Result.","Now that we have C++11, let's add move constructors and move assignment operators for Result, similarly to what was done for Option.",Improvement,Major,Resolved,"2015-06-24 02:43:23","2015-06-24 01:43:23",3
"Apache Mesos","Add move constructors / assignment to Try.","Now that we have C++11, let's add move constructors and move assignment operators for Try, similarly to what was done for Option.",Improvement,Major,Resolved,"2015-06-24 02:43:21","2015-06-24 01:43:21",3
"Apache Mesos","Framework can overcommit oversubscribable resources during master failover.","This is due to a bug in the hierarchical allocator. Here is the sequence of events:  1) slave uses a fixed resource estimator which advertise 4 revocable cpus 2) a framework A launches a task that uses all the 4 revocable cpus 3) master fails over 4) slave re-registers with the new master, and sends UpdateSlaveMessage with 4 revocable cpus as oversubscribed resources 5) framework A hasn't registered yet, therefore, the slave's available resources will be 4 revocable cpus 6) framework A registered and will receive an additional 4 revocable cpus. So it can launch another task with 4 revocable cpus (that means 8 total!)  The problem is due to the way we calculate 'allocated' resource in allocator when 'updateSlave'. If the framework is not registered, the 'allocation' below is not accurate (check that if block in 'addSlave').  ",Bug,Critical,Resolved,"2015-06-24 01:01:57","2015-06-24 00:01:57",3
"Apache Mesos","Specify correct libnl version for configure check","Currently configure.ac lists 3.2.24 as the required libnl version. However, https://reviews.apache.org/r/31503 caused the minimum required version to be bumped to 3.2.26. The configure check thus fails to error out during execution and the dependency is captured only during the build step.",Bug,Blocker,Resolved,"2015-06-23 03:27:49","2015-06-23 02:27:49",1
"Apache Mesos","Port mapping isolator should cleanup unknown orphan containers after all known orphan containers are recovered during recovery.","Otherwise, the icmp/arp filter on host eth0 might be removed as a result of _cleanup if 'infos' is empty, causing subsequent '_cleanup' to fail on both known/unknown orphan containers.  ",Bug,Major,Resolved,"2015-06-22 22:12:51","2015-06-22 21:12:51",3
"Apache Mesos","Scheduler driver should send Call messages to the master","To vet the new Call protobufs, it is prudent to have the scheduler driver (sched.cpp) send Call messages to the master (similar to what we are doing with the scheduler library).",Task,Major,Resolved,"2015-06-22 21:22:24","2015-06-22 20:22:24",8
"Apache Mesos","Provide a Python library for master detection","When schedulers start interacting with Mesos master via HTTP endpoints, they need a way to detect masters.   Mesos should provide a master detection Python library to make this easy for frameworks.",Task,Major,Resolved,"2015-06-22 21:18:55","2015-06-22 20:18:55",5
"Apache Mesos","Add an Event message handler to scheduler library","Adding this handler lets master send Event messages to the library.  See MESOS-2909 for additional context.  This ticket only tracks the installation of the handler and maybe handling of a single event for testing. Additional events handling will be captured in a different ticket(s).",Task,Major,Resolved,"2015-06-22 21:15:14","2015-06-22 20:15:14",3
"Apache Mesos","Add an Event message handler to scheduler driver","Adding this handler lets master send Event messages to the driver.  See MESOS-2909 for additional context.",Task,Major,Resolved,"2015-06-22 21:14:41","2015-06-22 20:14:41",8
"Apache Mesos","Add version field to RegisterFrameworkMessage and ReregisterFrameworkMessage","In the same way we added 'version' field to RegisterSlaveMessage and ReregisterSlaveMessage, we should do it framework (re-)registration messages. This would help master determine which version of scheduler driver it is talking to.  We want this so that master can start sending Event messages to the scheduler driver (and scheduler library). In the long term, master will send a streaming response to the libraries, but in the meantime we can test the event protobufs by sending Event messages.",Task,Major,Resolved,"2015-06-22 21:08:32","2015-06-22 20:08:32",3
"Apache Mesos","Agent : Create Basic Functionality to handle /call endpoint","This is the first basic step in ensuring the basic /call functionality:   - Set up the route on the agent for api/v1/executor endpoint. - The endpoint should perform basic header/protobuf validation and return {{501 NotImplemented}} for now. - Introduce initial tests in executor_api_tests.cpp that just verify the status code. ",Task,Major,Resolved,"2015-06-22 19:36:25","2015-06-22 18:36:25",5
"Apache Mesos","Slave : Synchronous Validation for Calls","/call endpoint on the slave will return a 202 accepted code but has to do some basic validations before. In case of invalidation it will return a {{BadRequest}} back to the client.  - We need to create the required infrastructure to validate the request and then process it similar to {{src/master/validation.cpp}} in the {{namespace scheduler}} i.e. check if the protobuf is properly initialized, has the required attributes set pertaining to the call message etc.",Task,Major,Resolved,"2015-06-22 19:29:33","2015-06-22 18:29:33",3
"Apache Mesos","Add slave metric to count container launch failures","We have seen circumstances where a machine has been consistently unable to launch containers due to an inconsistent state (for example, unexpected network configuration).   Adding a metric to track container launch failures will allow us to detect and alert on slaves in such a state.",Bug,Major,Resolved,"2015-06-20 01:04:01","2015-06-20 00:04:01",1
"Apache Mesos","Network isolator should not fail when target state already exists","Network isolator has multiple instances of the following pattern:    These failures have occurred in operation due to the failure to recover or delete an orphan, causing the slave to remain on line but unable to create new resources.    We should convert the second failure message in this pattern to an information message since the final state of the system is the state that we requested.",Bug,Critical,Resolved,"2015-06-20 00:21:40","2015-06-19 23:21:40",3
"Apache Mesos","Enable Mesos to use arbitrary script / module to figure out IP, HOSTNAME","Currently Mesos tries to guess the IP, HOSTNAME by doing a reverse DNS lookup. This doesn't work on a lot of clouds as we want things like public IPs (which aren't the default DNS), there aren't FQDN names (Azure), or the correct way to figure it out is to call some cloud-specific endpoint.  If Mesos / Libprocess could load a mesos-module (Or run a script) which is provided per-cloud, we can figure out perfectly the IP / Hostname for the given environment. It also means we can ship one identical set of files to all hosts in a given provider which doesn't happen to have the DNS scheme + hostnames that libprocess/Mesos expects. Currently we have to generate host-specific config files which Mesos uses to guess.  The host-specific files break / fall apart if machines change IP / hostname without being reinstalled.",Improvement,Critical,Resolved,"2015-06-19 22:36:41","2015-06-19 21:36:41",5
"Apache Mesos","Write tests for new JSON (ZooKeeper) functionality","Follow up from MESOS-2340, need to ensure this does not break the ZooKeeper discovery functionality.",Task,Major,Resolved,"2015-06-19 16:58:53","2015-06-19 15:58:53",2
"Apache Mesos","Add queue size metrics for the allocator.","In light of the performance regression in MESOS-2891, we'd like to have visibility into the queue size of the allocator. This will enable alerting on performance problems.  We currently have no metrics in the allocator.  I will also look into MESOS-1286 now that we have gcc 4.8, current queue size gauges require a trip through the Process' queue.",Task,Critical,Resolved,"2015-06-18 22:51:26","2015-06-18 21:51:26",1
"Apache Mesos","Add benchmark for hierarchical allocator.","In light of the performance regression in MESOS-2891, we'd like to have a synthetic benchmark of the allocator code, in order to analyze and direct improvements.",Task,Critical,Resolved,"2015-06-18 22:42:31","2015-06-18 21:42:31",3
"Apache Mesos","Performance regression in hierarchical allocator.","For large clusters, the 0.23.0 allocator cannot keep up with the volume of slaves. After the following slave was re-registered, it took the allocator a long time to work through the backlog of slaves to add:  {noformat:title=45 minute delay} I0618 18:55:40.738399 10172 master.cpp:3419] Re-registered slave 20150422-211121-2148346890-5050-3253-S4695 I0618 19:40:14.960636 10164 hierarchical.hpp:496] Added slave 20150422-211121-2148346890-5050-3253-S4695 {noformat}  Empirically, [addSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L462] and [updateSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L533] have become expensive.  Some timings from a production cluster reveal that the allocator spending in the low tens of milliseconds for each call to {{addSlave}} and {{updateSlave}}, when there are tens of thousands of slaves this amounts to the large delay seen above.  We also saw a slow steady increase in memory consumption, hinting further at a queue backup in the allocator.  A synthetic benchmark like we did for the registrar would be prudent here, along with visibility into the allocator's queue size.",Bug,Blocker,Resolved,"2015-06-18 22:34:29","2015-06-18 21:34:29",3
"Apache Mesos","Sandbox URL doesn't work in web-ui when using SSL","The links to the sandbox in the web ui don't work when ssl is enabled.  This can happen if the certificate for the master and the slave do not match. This is a consequence of the redirection that happens when serving files. The resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves.",Bug,Critical,Resolved,"2015-06-18 21:03:42","2015-06-18 20:03:42",3
"Apache Mesos","Add SSL switch to python configuration","The python egg requires explicit dependencies for SSL. Add these to the python configuration if ssl is enabled.",Bug,Major,Resolved,"2015-06-18 21:01:25","2015-06-18 20:01:25",3
"Apache Mesos","Add SSL socket tests","commit beac384c77d4a9c235a813e9286716f4509bdd55 Author: <USER><<EMAIL>> Date:   Fri Jun 26 18:30:12 2015 -0700      Add SSL tests.          Review: https://reviews.apache.org/r/35889",Improvement,Major,Resolved,"2015-06-18 20:58:09","2015-06-18 19:58:09",5
"Apache Mesos","Capture some testing patterns we use in a doc","In Mesos tests we use some tricks and patterns to express certain expectations. These are not always obvious and not documented. The intent of the ticket is to kick-start the document with the description of those tricks for posterity.",Documentation,Minor,Resolved,"2015-06-18 16:49:40","2015-06-18 15:49:40",1
"Apache Mesos","Allow isolators to specify required namespaces","Currently, the LinuxLauncher looks into SlaveFlags to compute the namespaces that should be enabled when launching the executor. This means that a custom Isolator module doesn't have any way to specify dependency on a set of namespaces.  The proposed solution is to extend the Isolator interface to also export the namespaces dependency. This way the MesosContainerizer can directly query all loaded Isolators (inbuilt and custom modules) to compute the set of namespaces required by the executor. This set of namespaces is then passed on to the LinuxLauncher. ",Task,Major,Resolved,"2015-06-18 01:52:46","2015-06-18 00:52:46",5
"Apache Mesos","Do not call hook manager if no hooks installed","Hooks modules allow us to provide decorators during various aspects of a task lifecycle such as label decorator, environment decorator, etc.  Often the call into such a decorator hooks results in a new copy of labels, environment, etc., being returned to the call site. This is an unnecessary overhead if there are no hooks installed.  The proper way would be to call decorators via the hook manager only if there are some hooks installed. This would prevent unnecessary copying overhead if no hooks are available.",Improvement,Major,Resolved,"2015-06-17 22:05:41","2015-06-17 21:05:41",2
"Apache Mesos","Random recursive_mutex errors in when running make check","While running make check on OS X, from time to time {{recursive_mutex}} errors appear after running all the test successfully. Just one of the experience messages actually stops {{make check}} reporting an error.  The following error messages have been experienced:      ",Bug,Major,Resolved,"2015-06-17 17:09:22","2015-06-17 16:09:22",1
"Apache Mesos","Convert PortMappingStatistics to use automatic JSON encoding/decoding","Simplify PortMappingStatistics by using JSON::Protocol and protobuf::parse to convert ResourceStatistics to/from line format.  This change will simplify the implementation of MESOS-2332.",Bug,Major,Resolved,"2015-06-16 17:00:37","2015-06-16 16:00:37",2
"Apache Mesos","style hook prevent's valid markdown files from getting committed","According to the original [markdown specification|http://daringfireball.net/projects/markdown/syntax#p] and to the most [recent standarization|http://spec.commonmark.org/0.20/#hard-line-breaks] effort, two spaces at the end of a line create a hard line break (it breaks the line without starting a new paragraph), similar to the html code {{<br/>}}.   However, there's a hook in mesos which prevent files with trailing whitespace to be committed.",Bug,Trivial,Resolved,"2015-06-16 12:16:42","2015-06-16 11:16:42",1
"Apache Mesos","OversubscriptionTest.FixedResourceEstimator is flaky","Came up in https://reviews.apache.org/r/35395/  ",Bug,Major,Resolved,"2015-06-13 15:17:32","2015-06-13 14:17:32",1
"Apache Mesos","Slave should send oversubscribed resource information after master failover.","After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this.",Bug,Critical,Resolved,"2015-06-12 23:48:58","2015-06-12 22:48:58",3
"Apache Mesos","mesos-fetcher won't fetch uris which begin with a  ","Discovered while running mesos with marathon on top. If I launch a marathon task with a URI which is  http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz mesos will log to stderr:    It would be nice if mesos trimmed leading whitespace before doing protocol detection so that simple mistakes are just fixed. ",Bug,Minor,Resolved,"2015-06-11 23:50:14","2015-06-11 22:50:14",2
"Apache Mesos","Create the basic infrastructure to handle /scheduler endpoint","This is the first basic step in ensuring the basic {{/call}} functionality: processing a   and returning:  - {{202}} if all goes well; - {{401}} if not authorized; and - {{403}} if the request is malformed.  We'll get more sophisticated as the work progressed (eg, supporting {{415}} if the content-type is not of the right kind).",Story,Major,Resolved,"2015-06-11 21:50:45","2015-06-11 20:50:45",3
"Apache Mesos","FetcherCacheTest.LocalCachedExtract is flaky.","From jenkins:    [~<USER> not sure if there's a ticket capturing this already, sorry if this is a duplicate.",Bug,Major,Resolved,"2015-06-11 04:18:06","2015-06-11 03:18:06",1
"Apache Mesos","Resources::parse(...) allows different resources of the same name to have different types.","So code like this doesn't raise Error.   Doesn't look like allowing this adds value and this complicates resource maths/validation/reporting.  We should disallow this.",Bug,Major,Resolved,"2015-06-10 22:39:15","2015-06-10 21:39:15",2
"Apache Mesos","Report per-container metrics from host egress filter","Export in statistics.json the fq_codel flow statistics for each container.",Improvement,Major,Accepted,"2015-06-10 22:30:07","2015-06-10 21:30:07",1
"Apache Mesos","Add Docker Image Type to protobuf API",,Improvement,Major,Resolved,"2015-06-10 21:49:26","2015-06-10 20:49:26",1
"Apache Mesos","Implement Docker image provisioner","Provisions a Docker image (provisions all its dependent layers), fetch an image from persistent store, and also destroy an image.   Done when tested for local discovery and copy backend. ",Improvement,Major,Resolved,"2015-06-10 21:48:07","2015-06-10 20:48:07",3
"Apache Mesos","Implement Docker local image store","Given a local Docker image name and path to the image or image tarball, fetches the image's dependent layers, untarring if necessary. It will also parse the image layers' configuration json and place the layers and image into persistent store.  Done when a Docker image can be successfully stored and retrieved using 'put' and 'get' methods. ",Improvement,Major,Resolved,"2015-06-10 21:47:34","2015-06-10 20:47:34",5
"Apache Mesos","Local filesystem docker image discovery","Given a docker image name and the local directory where images can be found, creates a URI with a path to the corresponding image.  Done when system successfully checks for the image, untars the image if necessary, and returns the proper URI to the image.",Improvement,Major,Resolved,"2015-06-10 21:47:20","2015-06-10 20:47:20",2
"Apache Mesos","Add and document new labels field to framework info","Add and document new labels field to framework info:  ",Improvement,Major,Resolved,"2015-06-10 19:31:39","2015-06-10 18:31:39",1
"Apache Mesos","Master crashes when framework changes principal on re-registration","The master should be updated to avoid crashing when a framework re-registers with a different principal.",Bug,Critical,Resolved,"2015-06-10 18:48:47","2015-06-10 17:48:47",5
"Apache Mesos","FrameworkInfo should include a Labels field to support arbitrary, lightweight metadata","A framework instance may offer specific capabilities to the cluster: storage, smartly-balanced request handling across deployed tasks, access to 3rd party services outside of the cluster, etc. These capabilities may or may not be utilized by all, or even most mesos clusters. However, it should be possible for processes running in the cluster to discover capabilities or features of frameworks in order to achieve a higher level of functionality and a more seamless integration experience across the cluster.  A rich discovery API attached to the FrameworkInfo could result in some form of early lock-in: there are probably many ways to realize cross-framework integration and external services integration that we haven't considered yet. Rather than over-specify a discovery info message type at the framework level I think FrameworkInfo should expose a **very generic** way to supply metadata for interested consumers (other processes, tasks, etc).  Adding a Labels field to FrameworkInfo reuses an existing message type and seems to fit well with the overall intent: attaching generic metadata to a framework instance. These labels should be visible when querying a mesos master's state.json endpoint.",Improvement,Major,Resolved,"2015-06-10 15:58:04","2015-06-10 14:58:04",8
"Apache Mesos","In Resources JSON model() resources of the same name overwrite each other.","As shown here: https://github.com/apache/mesos/blob/8559d7b7356ec91795e564767588c6f4519653a5/src/common/http.cpp#L50  So if there are two cpus of different roles, whichever comes later will overwrite the previous.  We should instead aggregate different resources of the same name.  However, in the presence of revocable resources, in order to maintain backwards compatibility we should exclude revocable resources.",Bug,Major,Resolved,"2015-06-09 23:11:57","2015-06-09 22:11:57",2
"Apache Mesos","Decode network statistics from mesos-network-helper","Decode network statistics from mesos-network-helper and output to slave statistics.json",Improvement,Major,Resolved,"2015-06-09 22:54:26","2015-06-09 21:54:26",1
"Apache Mesos","Report per-container metrics for network bandwidth throttling to the slave","Report per-container metrics for network bandwidth throttling to the slave in the output of mesos-network-helper.",Improvement,Major,Resolved,"2015-06-09 22:52:24","2015-06-09 21:52:24",1
"Apache Mesos","Support different perf output formats","The output format of perf changes in 3.14 (inserting an additional field) and in again in 4.1 (appending additional) fields. See kernel commits: 410136f5dd96b6013fe6d1011b523b1c247e1ccb d73515c03c6a2706e088094ff6095a3abefd398b  Update the perf::parse() function to understand all these formats.",Improvement,Major,Resolved,"2015-06-09 05:32:10","2015-06-09 04:32:10",3
"Apache Mesos","Enable configuring Mesos with environment variables without having them leak to tasks launched","Currently if mesos is configured with environment variables (MESOS_MODULES), those show up in every task which is launched unless the executor explicitly cleans them up.   If the task being launched happens to be something libprocess / mesos based, this can often prevent the task from starting up (A scheduler has issues loading a module intended for the slave).  There are also cases where it would be nice to be able to change what the PATH is that tasks launch with (the host may have more in the path than tasks are supposed to / allowed to depend upon).",Wish,Critical,Resolved,"2015-06-08 21:21:19","2015-06-08 20:21:19",8
"Apache Mesos","FetcherCacheTest.SimpleEviction is flaky","Saw this when reviewbot was testing an unrelated review https://reviews.apache.org/r/35119/  ",Bug,Major,Accepted,"2015-06-08 20:37:52","2015-06-08 19:37:52",0
"Apache Mesos","Add an endpoint to slaves to allow launching system administration tasks","As a System Administrator often times I need to run a organization-mandated task on every machine in the cluster. Ideally I could do this within the framework of mesos resources if it is a cleanup or auditing task, but sometimes I just have to run something, and run it now, regardless if a machine has un-accounted resources  (Ex: Adding/removing a user).  Currently to do this I have to completely bypass Mesos and SSH to the box. Ideally I could tell a mesos slave (With proper authentication) to run a container with the limited special permissions needed to get the task done.",Wish,Minor,Resolved,"2015-06-08 20:10:11","2015-06-08 19:10:11",8
"Apache Mesos","FetcherCacheTest.CachedFallback test is flaky","Observed this on internal CI  ",Bug,Major,Resolved,"2015-06-08 18:13:32","2015-06-08 17:13:32",0
"Apache Mesos","Support pre-fetching images","Default container images can be specified with the --default_container_info flag to the slave. This may be a large image that will take a long time to initially fetch/hash/extract when the first container is provisioned. Add optional support to start fetching the image when the slave starts and consider not registering until the fetch is complete.  To extend that, we should support an operator endpoint so that operators can specify images to pre-fetch.",Improvement,Minor,Reviewable,"2015-06-07 06:33:58","2015-06-07 05:33:58",5
"Apache Mesos","Pass callback to the QoS Controller to retrieve ResourceUsage from Resource Monitor on demand.","We need to allow QoS Controller to call 'ResourceMonitor::usages()'. We will pass it in a lambda. ",Task,Major,Resolved,"2015-06-06 00:36:22","2015-06-05 23:36:22",2
"Apache Mesos","Add `EXPECT_NO_FUTURE_DISPATCHES` macro for tests.","We already have {{EXPECT_NO_FUTURE_MESSAGES}}, {{EXPECT_NO_FUTURE_DISPATCHES}} should be done the same way.  We already have a use case for it: https://github.com/apache/mesos/blob/master/src/tests/master_contender_detector_tests.cpp#L251",Improvement,Minor,Open,"2015-06-05 22:28:38","2015-06-05 21:28:38",1
"Apache Mesos","Document and consolidate qdisc handles","The structure of traffic control qdiscs and filters in non-trivial with the knowledge of which handles are the parents of which filters or qdiscs are in the create and recovery functions and will be needed to collect statistics on the links.  Lets pull out the constants and document them.",Improvement,Major,Resolved,"2015-06-05 21:29:39","2015-06-05 20:29:39",1
"Apache Mesos","Pass 'allocated' resources for each executor to the resource estimator.","Resource estimator obviously need this information to calculate, say the usage slack. Now the question is how. There are two approaches:  1) Pass in the allocated resources for each executor through the 'oversubscribable()' interface.  2) Let containerizer return total resources allocated for each container when 'usages()' are invoked.  I would suggest to take route (1) for several reasons:  1) Eventually, we'll need to pass in slave's total resources to the resource estimator (so that RE can calculate allocation slack). There is no way that we can get that from containerizer. The slave's total resources keep changing due to dynamic reservation. So we cannot pass in the slave total resources during initialization.  2) The current implementation of usages() might skip some containers if it fails to get statistics for that container (not an error). This will cause in-complete information to the RE.  3) We may want to calculate 'unallocated = total - allocated' so that we can send allocation slack as well. Getting 'total' and 'allocated' from two different components might result in inconsistent value. Remember that 'total' keeps changing due to dynamic reservation.",Task,Major,Resolved,"2015-06-05 01:47:42","2015-06-05 00:47:42",3
"Apache Mesos","Support revocable/non-revocable CPU updates in Mesos containerizer","MESOS-2652 provided preliminary support for revocable cpu resources only when specified in the initial resources for a container. Improve this to support updates to/from revocable cpu. Note, *any* revocable cpu will result in the entire container's cpu being treated as revocable at the cpu isolator level. Higher level logic is responsible for adding/removing based on some policy.",Improvement,Major,Resolved,"2015-06-04 20:16:00","2015-06-04 19:16:00",3
"Apache Mesos","Flaky test: FetcherCacheHttpTest.HttpCachedSerialized","FetcherCacheHttpTest.HttpCachedSerialized has been observed to fail (once  so far), but normally works fine. Here is the failure output:  [ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: resourceOffers(0x3cca8e0, @0x2b1053422b20 { 128-byte object <D0-E1 59-49 10-2B 00-00 00-00 00-00 00-00 00-00 10-A1 00-68 10-2B 00-00 A0-DE 00-68 10-2B 00-00 20-DF 00-68 10-2B 00-00 40-9C 00-68 10-2B 00-00 90-2D 00-68 10-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 10-2B 00-00 00-00 00-00 0F-00 00-00> }) Stack trace: F0604 13:08:16.377907  6813 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers *** Check failure stack trace: ***     @     0x2b10488ff6c0  google::LogMessage::Fail()     @     0x2b10488ff60c  google::LogMessage::SendToLog()     @     0x2b10488ff00e  google::LogMessage::Flush()     @     0x2b1048901f22  google::LogMessageFatal::~LogMessageFatal()     @           0x9721e4  _CheckFatal::~_CheckFatal()     @           0xb4da86  mesos::internal::tests::FetcherCacheTest::launchTask()     @           0xb53f8d  mesos::internal::tests::FetcherCacheHttpTest_HttpCachedSerialized_Test::TestBody()     @          0x116ac21  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x1165e1e  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x114e1df  testing::Test::Run()     @          0x114e902  testing::TestInfo::Run()     @          0x114ee8a  testing::TestCase::Run()     @          0x1153b54  testing::internal::UnitTestImpl::RunAllTests()     @          0x116ba93  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x1166b0f  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x1152a60  testing::UnitTest::Run()     @           0xcbc50f  main     @     0x2b104af78ec5  (unknown)     @           0x867559  (unknown) make[4]: *** [check-local] Aborted make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[3]: *** [check-am] Error 2 make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[2]: *** [check] Error 2 make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[1]: *** [check-recursive] Error 1 make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build' make: *** [distcheck] Error 1 ",Bug,Minor,Resolved,"2015-06-04 15:11:55","2015-06-04 14:11:55",2
"Apache Mesos","os::read should have one implementation","In master there are currently three implementations of the function:  https://github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#L42 https://github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#L82 https://github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#L42  All of them have fairly radically different implementations (One uses C read(), one uses c++ ifstream, one uses c fopen)  The read() based one does an excess / unnecessary copy / buffer allocation (it is going to read into one temporary buffer, then copy into the result string. Would be more efficient to do a .reserve() on the result string, and then fill the result buffer).  The ifstream/ifstreambuf_iterator ignores that you can have an error partially through reading a file / doesn't find the error or propagate it up.  The fopen() variant reads one newline separated line at a time. This could produce interesting / unexpected reading in the context of a binary file. It also causes glibc to insert null bytes at the end of the buffer it reads (excess computation). result isn't pre-allocated to be the right length, meaning that most of the continually read lines will result in realloc() and a lot of memory copies which will be inefficient on large files.",Improvement,Major,Accepted,"2015-06-04 03:48:43","2015-06-04 02:48:43",3
"Apache Mesos","Slave should call into resource estimator whenever it wants to forward oversubscribed resources","Currently, the polling of resource estimator is decoupled from the loop in the slave that forwards oversubscribed resources.  Now that the slave only sends updates when there is a change from the previous estimate, it can just poll the resource estimator whenever it wants to send an estimate. One advantage with this is that if the estimator is slow to respond, the slave doesn't keep forwarding estimates with the stale 'oversubscribable' value causing more revocable tasks to be unintentionally launched.",Bug,Major,Resolved,"2015-06-03 21:59:30","2015-06-03 20:59:30",3
"Apache Mesos","As a developer I need an easy way to convert MasterInfo protobuf to/from JSON","As a preliminary to MESOS-2340, this requires the implementation of a simple (de)serialization mechanism to JSON from/to {{MasterInfo}} protobuf.",Task,Major,Resolved,"2015-06-03 20:09:50","2015-06-03 19:09:50",3
"Apache Mesos","Jira workflow appears inconsistent","See attached screenshot - the story is in the {{Accepted}} state, so it should now have a {{Start Progress}} button, but it has a {{Stop Progress}} one instead.  Also, when in the {{In Progress}} it has an {{Accept}} button (I think) or something similar; also other states appear inconsistent.  This Story is about first looking at the workflow; ensuring the stories and their status(es) are consistent; that button in the UI are consistently applied and then correct any issues that may have been identified.  The assumption here is that the workflow is:  and, at each stage, it can be moved back by one ({{Unaccept}}, {{Stop Progress}}, {{Unresolve}}) and that, at any stage, it can be moved to {{Closed}} (for whatever reason).",Task,Major,Resolved,"2015-06-03 19:27:57","2015-06-03 18:27:57",2
"Apache Mesos","Make synchronized as primary form of synchronization.","Re-organize Synchronized to allow {{synchronized(m)}} to work on:   1. {{std::mutex}}   2. {{std::recursive_mutex}}   3. {{std::atomic_flag}}  Move synchronized.hpp into stout, so that developers don't think it's part of the utility suite for actors in libprocess.  Remove references to internal.hpp and replace them with {{std::atomic_flag}} synchronization.",Improvement,Major,Resolved,"2015-06-03 18:10:58","2015-06-03 17:10:58",8
"Apache Mesos","Log framework capabilities in the master.","Now that {{Capabilities}} has been added to FrameworkInfo, we should log these in the master when a framework (re-)registers (i.e. which capabilities are enabled and disabled). This would make debugging easier for framework developers.  Ideally, folding in the old {{checkpoint}} capability and logging that as well. In the past, the fact that {{checkpoint}} defaults to false has tripped up a lot of developers.",Improvement,Minor,Resolved,"2015-06-03 01:43:31","2015-06-03 00:43:31",1
"Apache Mesos","Remove dynamic allocation from Future<T>","Remove the dynamic allocation of `T*` inside `Future::Data`",Improvement,Major,Resolved,"2015-06-02 09:02:47","2015-06-02 08:02:47",3
"Apache Mesos","Rename Option<T>::get(const T& _t) to getOrElse() and refactor the original function","As suggested, if we want to change the name then we should refactor the original function as opposed to having 2 copies.  If we did have 2 versions of the same function, would it make more sense to delegate one of them to the other.  As of today, there is only one file need to be refactor: 3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp at line 151, 161",Improvement,Minor,Resolved,"2015-06-02 07:11:08","2015-06-02 06:11:08",3
"Apache Mesos","Export statistics on unevictable memory",,Improvement,Major,Resolved,"2015-06-02 00:59:24","2015-06-01 23:59:24",1
"Apache Mesos","Implement AppC image provisioner.","Implement a filesystem provisioner that can provision container images compliant with the Application Container Image (aci) [specification|https://github.com/appc/spec].",Improvement,Major,Resolved,"2015-06-01 22:28:40","2015-06-01 21:28:40",5
"Apache Mesos","Introduce filesystem provisioner abstraction","Optional filesystem provisioner component for the Mesos containerizer that can provision per-container filesystems.  This is different to a filesystem isolators because it just provisions a root filesystem for a container and doesn't actually do any isolation (e.g., through a mount namespace + pivot or chroot).",Improvement,Major,Resolved,"2015-06-01 22:25:56","2015-06-01 21:25:56",5
"Apache Mesos","Implement filesystem isolators","Move persistent volume support from Mesos containerizer to separate filesystem isolators, including support for container rootfs, where possible.  Use symlinks for posix systems without container rootfs. Use bind mounts for Linux with/without container rootfs.",Improvement,Major,Resolved,"2015-06-01 22:23:33","2015-06-01 21:23:33",13
"Apache Mesos","Add support for container rootfs to Mesos isolators","Mesos containers can have a different rootfs to the host. Update Isolator interface to pass rootfs during Isolator::prepare(). Update Isolators where  necessary.",Improvement,Major,Resolved,"2015-06-01 22:18:59","2015-06-01 21:18:59",1
"Apache Mesos","Remove duplicate literals in ingress & fq_codel queueing disciplines","fq_codel and ingress queueing disciplines include multiple uses of the string literals ingress and fq_codel.  Any mismatch in these would cause runtime errors which can be prevented at compile time.",Bug,Major,Resolved,"2015-06-01 20:05:08","2015-06-01 19:05:08",1
"Apache Mesos","Create a FixedResourceEstimator to return fixed amount of oversubscribable resources.","This will be useful for testing oversubscription in a real environment. Also, it will be useful for people who has a prior knowledge about the amount of resources that can be safely oversubscribed on each slave.",Task,Major,Resolved,"2015-06-01 19:52:19","2015-06-01 18:52:19",5
"Apache Mesos","Added constexpr to C++11 whitelist.","constexpr is currently used to eliminate initialization dependency issues for non-POD objects.  We should add it to the whitelist of acceptable c++11 features in the style guide.",Improvement,Major,Resolved,"2015-05-30 00:05:31","2015-05-29 23:05:31",1
"Apache Mesos","document the fetcher","For framework developers specifically, Mesos provides a fetcher to move binaries. This needs MVP documentation.  - What is it - How does it help - What protocols or schemas are supported - Can it be extended  This is important to get framework developers over the hump of learning to code against Mesos and grow the ecosystem.",Documentation,Major,Resolved,"2015-05-29 17:12:16","2015-05-29 16:12:16",5
"Apache Mesos","getQdisc function in routing::queueing::internal.cpp returns incorrect qdisc","The getQdisc function ignores the passed link parameter and returns the first qdisc of the required type from any available interface.",Bug,Major,Resolved,"2015-05-28 23:24:47","2015-05-28 22:24:47",1
"Apache Mesos","Non-POD static variables used in fq_codel and ingress.","We declare const non-POD static variables for the following:  fq_codel::HANDLE ingress::ROOT ingress::HANDLE  We can eliminate the risk of indeterminate initialization by converting to C++11 constexpr",Bug,Major,Resolved,"2015-05-28 20:54:51","2015-05-28 19:54:51",1
"Apache Mesos","Master should expose metrics about oversubscribed resources","metrics/snapshot should expose metrics on oversubscribed resources (allocated and available). ",Task,Major,Resolved,"2015-05-27 20:20:35","2015-05-27 19:20:35",5
"Apache Mesos","Slave should expose metrics about oversubscribed resources","metrics/snapshot should expose metrics on oversubscribed resources (allocated and available). ",Task,Major,Resolved,"2015-05-27 20:16:35","2015-05-27 19:16:35",2
"Apache Mesos","Define protobuf for ResourceMonitor::Usage.","We need to expose ResourceMonitor::Usage so that module writers can access it. We will define a protobuf message for that.",Task,Major,Resolved,"2015-05-27 02:05:15","2015-05-27 01:05:15",1
"Apache Mesos","SIGSEGV received during ResourceMonitorProcess::usage()","Observed in production.  {noformat:title=slave log} I0523 17:03:59.830229 56587 port_mapping.cpp:2616] Freed ephemeral ports [33792,34816) for container with pid 47791 I0523 17:03:59.849773 56587 port_mapping.cpp:2764] Successfully performed cleanup for pid 47791 *** Aborted at 1432400641 (unix time) try date -d @1432400641 if you are using GNU date *** PC: @     0x7f100fcbfd85 _ZNSt17_Function_handlerIFvRKSsEZNK7process6FutureIN5mesos8internal5slave15ResourceMonitor5UsageEE8onFailedIZNS7_22ResourceMonitorProcess5usageENS5_11ContainerIDEEUlS1_E_vEERKSA_OT_NSA_6PreferEEUlS1_E_E9_M_invokeERKSt9_Any_dataS1_ I0523 17:03:59.898959 56587 slave.cpp:3246] Executor 'thermos-1432400210944-mesos-test-exhaust_diskspace-5-4744d0fb-e0a1-4e40-bb22-56bd5cbd9524' of framework 201103282247-0000000019-0000 terminated with signal Killed I0523 17:04:03.419869 56587 slave.cpp:2547] Handling status update TASK_FAILED (UUID: 3be19404-f737-4a70-a330-d1d924a85dbb) for task 1432400210944-mesos-test-exhaust_diskspace-5-4744d0fb-e0a1-4e40-bb22-56bd5cbd9524 of framework 201103282247-0000000019-0000 from @0.0.0.0:0 I0523 17:04:03.773061 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources  I0523 17:04:03.773907 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources  I0523 17:04:03.774683 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources  I0523 17:04:03.776345 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources  *** SIGSEGV (@0x0) received by PID 56573 (TID 0x7f100a190940) from PID 0; stack trace: ***     @     0x7f100f181ca0 (unknown)     @     0x7f100fcbfd85 _ZNSt17_Function_handlerIFvRKSsEZNK7process6FutureIN5mesos8internal5slave15ResourceMonitor5UsageEE8onFailedIZNS7_22ResourceMonitorProcess5usageENS5_11ContainerIDEEUlS1_E_vEERKSA_OT_NSA_6PreferEEUlS1_E_E9_M_invokeERKSt9_Any_dataS1_     @     0x7f100fb01506 process::internal::run<>()     @     0x7f100fcc701b process::Future<>::fail()     @     0x7f100fccfbde process::internal::thenf<>()     @     0x7f100fd64bee _ZN7process8internal3runISt8functionIFvRKNS_6FutureIN5mesos18ResourceStatisticsEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_     @     0x7f100fd656dd process::Future<>::fail()     @     0x7f100fd6c332 process::Promise<>::associate()     @     0x7f100fe2777e _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos18ResourceStatisticsENS5_8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDESA_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSH_FSF_T1_ET2_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_     @     0x7f101015561a process::ProcessManager::resume()     @     0x7f10101558dc process::schedule()     @     0x7f100f17983d start_thread     @     0x7f100e96bfcd clone /usr/local/bin/mesos-slave.sh: line 102: 56573 Segmentation fault      (core dumped) $debug /usr/local/sbin/mesos-slave ${MESOS_FLAGS[@]} Slave Exit Status: 139 ",Bug,Major,Resolved,"2015-05-27 00:03:07","2015-05-26 23:03:07",1
"Apache Mesos","Slave should forward total amount of oversubscribed resources to the master","In addition to the unallocated oversubscribed resources, the slave should also send the oversubscribed resources that are already allocated.  This is needed by the master/allocator to accurately calculate the available oversubscribed resources to offer.",Task,Major,Resolved,"2015-05-26 23:33:48","2015-05-26 22:33:48",3
"Apache Mesos","Metric for cpu scheduling latency from all components","The metric will provide statistics on the scheduling latency for processes/threads in a container, i.e., statistics on the delay before application code can run. This will be the aggregate effect of the normal scheduling period, contention from other threads/processes, both in the container and on the system, and any effects from the CFS bandwidth control (if enabled) or other CPU isolation strategies.",Improvement,Major,Reviewable,"2015-05-26 23:01:32","2015-05-26 22:01:32",8
"Apache Mesos","Add validation behavior to FlagsBase","In every launcher file (ie, those containing some variation on {{main()}}) there is a minor variation on:   As this is default behavior, and we've added support for the {{--help}} flag in the {{FlagsBase}} class, we should add this too there and remove it from everywhere else.  Additionally, a recurring behavior is checking for the presence of a {{required}} flag:   or some variation thereof: we should add automatic validation for required flags during parsing.  This follows the DRY principle.",Bug,Minor,Accepted,"2015-05-26 17:16:58","2015-05-26 16:16:58",1
"Apache Mesos","Allow Resource Estimator to get Resource Usage information.","This includes two things: 1) We need to expose ResourceMonitor::Usage so that module writers can access it. We could define a protobuf message for that. 2) We need to allow ResourceEstimator to call 'ResourceMonitor::usages()'. We could either expose the ResourceMonitor, or pass in a lambda to the resources estimator.",Bug,Major,Resolved,"2015-05-22 22:37:03","2015-05-22 21:37:03",5
"Apache Mesos","Consolidate functionality in stout/net and process/http","stout/net.hpp and process/http.hpp offer overlapping functionality that could be consolidated in one place, presumably the latter, since it is more elaborate to begin with. This would also remove the dependency of the former on libcurl.  While we are at it, we could then turn net::contentLength() into a generalized, asynchronous process::http::head() call.  (Prerequisite: MESOS-2247, with the suggestion to enhance process::http, not stout, see a comment in that JIRA.) ",Improvement,Minor,Accepted,"2015-05-22 11:20:12","2015-05-22 10:20:12",8
"Apache Mesos","Explicitly-defaulted functions are not allowed by styleguide","As of right now the styleguide does not allow explicitly defaulted functions (being a c++ 11 feature). They enhance readability, are supported by all relevant compiler (GCC 4.4+ and Clang 3.0+), and are introduced by some patches (e.g. https://reviews.apache.org/r/34277/).  Therefore we should officially whitelist them in the styleguide.",Task,Major,Resolved,"2015-05-21 21:31:45","2015-05-21 20:31:45",1
"Apache Mesos","Delegating constructors are not allowed by styleguide","As of right now the styleguide does not allow delegating constructors (being a c++ 11 feature). They are already used in the code base (e.g. stout/option.hpp), are supported by all relevant compiler (GCC 4.7+ and Clang 3.0+), and enhance readability.  Therefore we should officially whitelist them in the styleguide.",Task,Major,Resolved,"2015-05-21 21:22:54","2015-05-21 20:22:54",1
"Apache Mesos","Add correction message to inform slave about QoS Controller actions","The QoS controller informs the slave about correcting actions (kill, resize, throttle best-effort containers, tasks, and so forth) through a protobuf message, called a QoSCorrection. This ticket tracks designing and creating this message.  For example: ",Task,Major,Resolved,"2015-05-21 21:15:54","2015-05-21 20:15:54",1
"Apache Mesos","Reflect in documentation that isolator flags are only relevant for Mesos Containerizer","The isolator flags are only relevant when using the Mesos Containerizer. We should reflect this in the flag description to avoid confusion.",Documentation,Major,Resolved,"2015-05-21 08:41:51","2015-05-21 07:41:51",1
"Apache Mesos","Add -> operator for Option<T>, Try<T>, Result<T>, Future<T>.","Let's add operator overloads to Option<T> to allow access to the underlying T using the `->` operator. ",Improvement,Major,Resolved,"2015-05-21 01:31:52","2015-05-21 00:31:52",3
"Apache Mesos","Update style guide: Avoid object slicing","In order to improve the safety of our code base, let's augment the style guide to: Disallow public construction of base classes so that we can avoid the object slicing problem. This is a good pattern to follow in general as it prevents subtle semantic bugs like the following: {code:title=ObjectSlicing.cpp|borderStyle=solid} #include <stdio.h> #include <vector>  class Base {   public:   Base(int _v) : v(_v) {}   virtual int get() const { return v; }   protected:   int v; };  class Derived : public Base {   public:   Derived(int _v) : Base(_v) {}   virtual int get() const { return v + 1; } };  int main() {   Base b(5);   Derived d(5);   std::vector<Base> vec;   vec.push_back(b);   vec.push_back(d);   for (const auto& v : vec) {     printf([%d]\n, v.get());   } } {code}",Improvement,Major,Accepted,"2015-05-21 00:43:30","2015-05-20 23:43:30",1
"Apache Mesos","Reduce multiple use of string literals","We have several instances of string literals (e.g. mesos-containerizer, net_tcprtt_microseconds_p50) being used in multiple locations where mismatches would result in correctness issues.  We should replace these with a single definition to reduce the risk.",Bug,Major,Resolved,"2015-05-20 22:51:38","2015-05-20 21:51:38",1
"Apache Mesos","Master should validate tasks using oversubscribed resources","Current implementation out for [review|https://reviews.apache.org/r/34310] only supports setting the priority of containers with revocable CPU if it's specified in the initial executor info resources. This should be enforced at the master.  Also master should make sure that oversubscribed resources used by the task are valid.",Task,Major,Resolved,"2015-05-19 23:48:14","2015-05-19 22:48:14",3
"Apache Mesos","Add HTB queueing discipline wrapper class","Network isolator uses a Hierarchical Token Bucket (HTB) traffic control discipline on the egress filter inside each container as the root for adding traffic filters.  A HTB wrapper is needed to access the network statistics for this interface.",Improvement,Major,Resolved,"2015-05-19 23:03:19","2015-05-19 22:03:19",3
"Apache Mesos","Extend queueing discipline wrappers to expose network isolator statistics","Export Traffic Control statistics in queueing library to enable reporting out impact of network bandwidth statistics.",Bug,Major,Resolved,"2015-05-19 19:47:03","2015-05-19 18:47:03",3
"Apache Mesos","/help generated links point to wrong URLs","As reported by <USER><<EMAIL>> (see also MESOS-329 and MESOS-913 for background):  {quote} In {{mesos/3rdparty/libprocess/src/help.cpp}} a markdown file is created, which is then converted to html through a javascript library   All endpoints point to {{/help/...}}, they need to work dynamically for reverse proxy to do its thing. {{/mesos/help}} works, and displays the endpoints, but they each need to go to their respective {{/help/...}} endpoint.   Note that this needs to work both for master, and for slaves. I think the route to slaves help is something like this: {{/mesos/slaves/20150518-210216-1695027628-5050-1366-S0/help}}, but please double check this. {quote}  The fix appears to be not too complex (as it would require to simply manipulate the generated URL) but a quick skim of the code would suggest that something more substantial may be desirable too.",Bug,Minor,Resolved,"2015-05-19 00:36:30","2015-05-18 23:36:30",2
"Apache Mesos","As a Framework User I want to be able to discover my Task's IP","The information exposed by the Framework via the {{WebUIUrl}} does not always resolves to a routable endpoint (eg, when the {{hostname}} is not publicly resolvable, or resolvable at all).  In order to facilitate service discovery (via, eg, Marathon UI) we want to add the information in {{FrameworksPid}} via the {{/state-summary}} endpoint.",Story,Major,Resolved,"2015-05-18 19:50:36","2015-05-18 18:50:36",3
"Apache Mesos","Include ExecutorInfos for custom executors in master/state.json","The slave/state.json already reports executorInfos: https://github.com/apache/mesos/blob/0.22.1/src/slave/http.cpp#L215-219  Would be great to see this in the master/state.json as well, so external tools don't have to query each slave to find out executor resources, sandbox directories, etc.",Improvement,Major,Resolved,"2015-05-15 23:13:34","2015-05-15 22:13:34",3
"Apache Mesos","Draft design doc on global resources.",,Task,Major,Resolved,"2015-05-15 21:52:50","2015-05-15 20:52:50",5
"Apache Mesos","Exposing Resources along with ResourceStatistics from resource monitor","Right now, the resource monitor returns a Usage which contains ContainerId, ExecutorInfo and ResourceStatistics. In order for resource estimator/qos controller to calculate usage slack, or tell if a container is using revokable resources or not, we need to expose the Resources that are currently assigned to the container.  This requires us the change the containerizer interface to get the Resources as well while calling 'usage()'.",Bug,Major,Resolved,"2015-05-15 20:38:08","2015-05-15 19:38:08",5
"Apache Mesos","Reported used resources for tasks in frameworks do not match slave tally","[~<USER> recently observed that according to the master's and the slave's state.json summing up the resources allocated to tasks from different frameworks on a slave does not always match the total that is reported for the slave. The latter number is sometimes higher.  It would be desirable for tools that display allocation statistics to find balanced tallies. ",Improvement,Major,Accepted,"2015-05-15 01:24:21","2015-05-15 00:24:21",3
"Apache Mesos","Add documentation for maintainers.","In order to scale the number of committers in the project, we proposed the concept of maintainers here:  http://markmail.org/thread/cjmdn3d7qfzbxhpm  To follow up on that proposal, we'll need some documentation to capture the concept of maintainers. Both how contributors can benefit from maintainer feedback and the expectations of maintainer-ship.  In order to not enforce an excessive amount of process, maintainers will initially only serve as an encouraged means to help contributors find reviewers and get meaningful feedback.",Documentation,Major,Resolved,"2015-05-14 22:44:29","2015-05-14 21:44:29",3
"Apache Mesos","Upgrade the design of MasterInfo","Currently, the {{MasterInfo}} PB only supports an {{ip}} field as an {{int32}}.  Beyond making it harder (and opaque; open to subtle bugs) for languages other than C/C++ to decode into an IPv4 octets, this does not allow Mesos to support IPv6 Master nodes.  We should consider ways to upgrade it in ways that permit us to support both IPv4 / IPv6 nodes, and, possibly, in a way that makes it easy for languages such as Java/Python that already have PB support, so could easily deserialize this information.  See also MESOS-2709 for more info.",Improvement,Major,Resolved,"2015-05-14 22:34:17","2015-05-14 21:34:17",3
"Apache Mesos","Change the interaction between the slave and the resource estimator from polling to pushing ","This will make the semantics more clear. The resource estimator can control the speed of sending resources estimation to the slave.  To avoid cyclic dependency, slave will register a callback with the resource estimator and the resource estimator will simply invoke that callback when there's a new estimation ready. The callback will be a defer to the slave's main event queue.",Bug,Major,Resolved,"2015-05-14 22:01:33","2015-05-14 21:01:33",3
"Apache Mesos","Update allocator to allocate revocable resources","The simplest way to add support for oversubscribed resources to the allocator is to simply add them to the already existing 'Slave.total' and 'Slave.available' variables. It is easy to distinguish the revocable resources by doing a .revocable() filter.    ",Task,Major,Resolved,"2015-05-14 20:11:52","2015-05-14 19:11:52",5
"Apache Mesos","Update master to handle oversubscribed resource estimate from the slave","Whenever the master gets a new oversubscribed resources estimate from the slave, it should rescind any outstanding revocable offers (with oversubscribed resources) from that slave. It should then call the allocator to update the oversubscribed resources.",Task,Major,Resolved,"2015-05-14 20:01:32","2015-05-14 19:01:32",3
"Apache Mesos","Add a new API call to the allocator to update oversubscribed resources","This tracks just the work of adding the API call to the allocator interface.  Master makes this call on the allocator whenever it gets a new oversubscribed resources estimate from the slave.",Task,Major,Resolved,"2015-05-14 19:57:37","2015-05-14 18:57:37",2
"Apache Mesos","Update DRF sorter to update total resources","DRF sorter currently keeps track of allocated resources and total resources, but there is no way to update the total resources. For oversubscription, we need the ability to update total resources because total oversubscribed resources change overtime.",Improvement,Major,Resolved,"2015-05-14 19:54:16","2015-05-14 18:54:16",2
"Apache Mesos","Add support for enabling network namespace without enabling the network isolator","Following the discussion Kapil started, it is currently not possible to enable the linux network namespace for a container without enabling the network isolator (which requires certain kernel capabilities and dependencies). Following the pattern of enabling pid namespaces (--isolation=namespaces/pid). One possible solution could be to add another one for network i.e. namespaces/network.  ",Task,Major,Resolved,"2015-05-13 23:21:54","2015-05-13 22:21:54",13
"Apache Mesos","The mesos-execute tool does not support zk:// master URLs","It appears that the {{mesos-execute}} command line tool does it's own PID validation of the {{--master}} param which prevents it from supporting clusters managed with ZooKeeper.",Bug,Major,Resolved,"2015-05-13 02:18:07","2015-05-13 01:18:07",1
"Apache Mesos","Create access to the Mesos state abstraction that does not require linking with libmesos","See src/state/state.hpp and src/java/src/org/apache/mesos/state/*.java for what the state abstraction is.  With the new HTTP API (see MESOS-2288, MESOS-2289), there will be no need to link to libmesos to a framework for it to communicate with a Mesos master. However, if a framework uses the Mesos state abstraction, either directly in C++ or through other language bindings (e.g., Java), it still needs to link with libmesos. So, in order to achieve libmesos-free frameworks that can leverage all APIs Mesos has to offer, we need a different way to access the state abstraction.   ---  One approach is to provide an HTTP API for state queries that get routed through the Mesos master, which relays them by making calls into libmesos. Details TBD, including how separate this will be from the general HTTP API. ",Improvement,Major,Resolved,"2015-05-13 00:19:15","2015-05-12 23:19:15",13
"Apache Mesos","Architecture document for per-container IP assignment, enforcement and isolation","There are many ways in which we can go around wiring up per-container IPs in Mesos.  As there are multiple underlying mechanisms and systems for keeping track of IP pools, we probably need to aim for a very flexible architecture, similar to the oversubscription project.  There are a couple of folks, companies and vendors interested in getting this capability into Mesos asap to provide a stronger networking story (https://www.mail-archive.com/<EMAIL>/msg32353.html). So let's start discussing and architecting this.",Task,Major,Resolved,"2015-05-12 18:14:57","2015-05-12 17:14:57",13
"Apache Mesos","Publish the schema for operator endpoints","We should define the schema of both requests and responses to the operator endpoints. ",Improvement,Major,Resolved,"2015-05-12 17:55:15","2015-05-12 16:55:15",2
"Apache Mesos","Deprecating '.json' extension in master endpoints urls","Add an endpoint for each master endpoint with a '.json' extension such as `/master/stats.json` so it becomes `/master/stats` after a deprecation cycle.",Improvement,Major,Resolved,"2015-05-12 17:46:24","2015-05-12 16:46:24",1
"Apache Mesos","Design Master discovery functionality for HTTP-only clients","When building clients that do not bind to {{libmesos}} and only use the HTTP API (via pure language bindings - eg, Java-only) there is no simple way to discover the Master's IP address to connect to.  Rather than relying on 'out-of-band' configuration mechanisms, we would like to enable the ability of interrogating the ZooKeeper ensemble to discover the Master's IP address (and, possibly, other information) to which the HTTP API requests can be addressed to.",Improvement,Major,Resolved,"2015-05-08 19:51:50","2015-05-08 18:51:50",3
"Apache Mesos","Design doc for the Executor HTTP API","This tracks the design of the Executor HTTP API. ",Bug,Major,Resolved,"2015-05-08 18:28:52","2015-05-08 17:28:52",2
"Apache Mesos","Incorrect zh:// URI scheme causes Slave to SegFault","I have 4 slave nodes with the same hardware, operating system and mesos configuration.   Few minutes ago, all 4 nodes were functioning well. I tried to change the config of *master* from _10.172.230.69:5050_ to _zh://10.172.230.69:2181/mesos_ and restarted them in turn. The other three had started normally but the last one got a segmentation fault as you can see below.  ",Bug,Major,Resolved,"2015-05-08 13:06:41","2015-05-08 12:06:41",2
"Apache Mesos","Add correct format template declarations to the styleguide","The general rule to format templates is to declare them as:    However, the style is not documented anywhere nor it is inherited from the Google style guide.",Documentation,Major,Resolved,"2015-05-08 04:14:35","2015-05-08 03:14:35",1
"Apache Mesos","Add tests for QoS controller corrections",,Task,Major,Resolved,"2015-05-07 22:28:40","2015-05-07 21:28:40",5
"Apache Mesos","Modularize the QoS Controller","Modularize the QoS controller to enable custom correction policies",Task,Major,Resolved,"2015-05-07 22:19:06","2015-05-07 21:19:06",3
"Apache Mesos","Compare split/flattened cgroup hierarchy for CPU oversubscription","Investigate if a flat hierarchy is sufficient for oversubscription of CPU or if a two-way split is necessary/preferred.",Task,Major,Resolved,"2015-05-06 18:33:54","2015-05-06 17:33:54",3
"Apache Mesos","Implement bi-level cpu.shares subtrees in cgroups/cpu isolator.","See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.  # Configurable bias # Change cgroup layout ** Implement roll-forward migration path in isolator recover ** Document roll-back migration path",Task,Major,Resolved,"2015-05-06 18:24:38","2015-05-06 17:24:38",8
"Apache Mesos","Determine CFS behavior with biased cpu.shares subtrees","See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.  * Understand the relationship between cpu.shares and CFS quota. * Determine range of possible bias splits * Determine how to achieve bias, e.g., should 20:1 be 20480:1024 or ~1024:50 * Rigorous testing of behavior with varying loads, particularly the combination of latency sensitive loads for high biased tasks (non-revokable), and cpu intensive loads for the low biased tasks (revokable). * Discover any performance edge cases?",Task,Major,Resolved,"2015-05-06 18:20:34","2015-05-06 17:20:34",13
"Apache Mesos","Add a /teardown endpoint on master to teardown a framework","We plan to rename /shutdown endpoint to /teardown to be compatible with the new API. /shutdown will be deprecated in 0.23.0 or later.",Task,Major,Resolved,"2015-05-05 23:57:19","2015-05-05 22:57:19",2
"Apache Mesos","Explore exposing stats from kernel","Exploratory work.  Additional tickets to follow.",Task,Major,Resolved,"2015-05-05 23:51:01","2015-05-05 22:51:01",5
"Apache Mesos","Add master flag to enable/disable oversubscription","This flag lets an operator control cluster level oversubscription.   The master should send revocable offers to framework if this flag is enabled and the framework opts in to receive them.  Master should ignore revocable resources from slaves if the flag is disabled.  Need tests for all these scenarios.",Task,Major,Resolved,"2015-05-05 23:01:15","2015-05-05 22:01:15",5
"Apache Mesos","Printing a resource should show information about reservation, disk etc","While new fields like DiskInfo and ReservationInfo have been added to Resource protobuf, the output stream operator hasn't been updated to show these. This is valuable information to have in the logs during debugging.",Improvement,Major,Resolved,"2015-05-05 20:34:01","2015-05-05 19:34:01",1
"Apache Mesos","Update Resource message to include revocable resources","Need to update Resource message with a new subtype that indicates that the resource is revocable. It might also need to specify why it is revocable (e.g., oversubscribed).  Also need to make sure all the operations on Resource(s) takes this new message into account.",Task,Major,Resolved,"2015-05-05 00:36:46","2015-05-04 23:36:46",3
"Apache Mesos","Slave should forward oversubscribable resources to the master","Slave simply forwards resource estimates from ResourceEstimator to the master.  Use a new message and handler on the master.   A slave flag for the interval between the messages. ",Task,Major,Resolved,"2015-05-04 20:06:22","2015-05-04 19:06:22",5
"Apache Mesos","Slave should kill usage slack revocable tasks if oversubscription is disabled","If oversubscription for allocation is disabled on a restarted slave (that had it previously enabled), it should kill usage slack revocable tasks.  Slave knows this information from the Resources of a container that it checkpoints and recovers.  Add a new reason OVERSUBSCRIPTION_DISABLED.",Task,Major,Accepted,"2015-05-04 19:56:33","2015-05-04 18:56:33",3
"Apache Mesos","Add a slave flag to enable oversubscription","Slave sends oversubscribable resources to master only when the flag is enabled.",Task,Major,Resolved,"2015-05-04 19:49:25","2015-05-04 18:49:25",2
"Apache Mesos","Update modules doc with hook usage example","Modules doc states the possibility of using hooks, but doesn't refer to necessary flags and usage example.",Improvement,Major,Resolved,"2015-04-30 09:43:24","2015-04-30 08:43:24",1
"Apache Mesos","Follow Google Style Guide for header file include order completely.","The header include order for Mesos actually follows the Google Styleguide but omits step 1 without mentioning this exception in the Mesos styleguide. This proposal suggests to adapt to the include order explained in the Google Styleguide i.e. include the direct headers first in the .cpp files implementing them.  A gist of the proposal can be found here:  https://gist.github.com/joerg84/65cb9611d24b2e35b69b  The corresponding Review Board review can be found here: https://reviews.apache.org/r/33646/ ",Improvement,Minor,Accepted,"2015-04-29 02:46:05","2015-04-29 01:46:05",5
"Apache Mesos","ContainerizerTest.ROOT_CGROUPS_BalloonFramework flaky","  which is from    This is the same as MESOS-2660: I've confirmed that swapping them fixed it.  ",Bug,Major,Resolved,"2015-04-29 02:21:05","2015-04-29 01:21:05",1
"Apache Mesos","Port mapping isolator causes SIGABRT during slave recovery.","There is a bug in the code. If there are namespaces created by other party (say ip netns), the slave recovery will abort.",Bug,Major,Resolved,"2015-04-28 18:52:18","2015-04-28 17:52:18",1
"Apache Mesos","Fix queuing discipline wrapper in linux/routing/queueing ","qdisc search function is dependent on matching a single hard coded handle and does not correctly test for interface, making the implementation fragile.  Additionally, the current setup scripts (using dynamically created shell commands) do not match the hard coded handles.  ",Bug,Critical,Resolved,"2015-04-27 17:32:08","2015-04-27 16:32:08",5
"Apache Mesos","ROOT_CGROUPS_Listen and ROOT_IncreaseRSS tests are flaky","[==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from CgroupsAnyHierarchyWithCpuMemoryTest [ RUN      ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen Failed to allocate RSS memory: Failed to lock memory, mlock: Resource temporarily unavailable../../../mesos/src/tests/cgroups_tests.cpp:571: Failure Failed to wait 15secs for future [  FAILED  ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen (15121 ms) [----------] 1 test from CgroupsAnyHierarchyWithCpuMemoryTest (15121 ms total)  [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (15174 ms total) [  PASSED  ] 0 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen",Bug,Major,Resolved,"2015-04-24 23:05:31","2015-04-24 22:05:31",3
"Apache Mesos","Implement a stand alone test framework that uses revocable cpu resources","Ideally this would be an example framework (or stand alone binary like load generator framework) that helps us evaluate oversubscription in a real cluster.  We need to come up with metrics that need to be exposed by this framework for evaluation (e.g., how many revocable offers, rescinds, preemptions etc).",Task,Major,Resolved,"2015-04-22 23:29:39","2015-04-22 22:29:39",5
"Apache Mesos","Update FrameworkInfo to opt in to revocable resources","Add a new field to FrameworkInfo that lets the frameworks explicitly choose revocable offers  (for backwards compatibility).",Improvement,Major,Resolved,"2015-04-22 23:27:50","2015-04-22 22:27:50",1
"Apache Mesos","Slave should act on correction events from QoS controller","Slave might want to kill revocable tasks based on correction events from the QoS controller.  The QoS controller communicates corrections through a stream (or process::Queue) to the slave which corrections it needs to carry out, in order to mitigate interference with production tasks.  The correction is communicated through a message: [code] message QoSCorrection {      enum CorrectionType {          KillExecutor = 1          // KillTask = 2          // Resize, throttle task      } optional string reason = X; optional ExecutorID executor_id = X; // optional TaskID task_id = X; } [/code]  And the slave will setup a handler to process these events. Initially, only executor termination is supported and cause the slave to issue 'containerizer->destroy()'.",Improvement,Major,Resolved,"2015-04-22 23:26:37","2015-04-22 22:26:37",8
"Apache Mesos","Update Mesos containerizer to understand revocable cpu resources","The CPU isolator needs to properly set limits for revocable and non-revocable containers.  The proposed strategy is to use a two-way split of the cpu cgroup hierarchy -- normal (non-revocable) and low priority (revocable) subtrees -- and to use a biased split of CFS cpu.shares across the subtrees, e.g., a 20:1 split (TBD). Containers would be present in only one of the subtrees. CFS quotas will *not* be set on subtree roots, only cpu.shares. Each container would set CFS quota and shares as done currently. ",Task,Major,Resolved,"2015-04-22 23:23:00","2015-04-22 22:23:00",5
"Apache Mesos","Implement QoS controller","This is a component of the slave that informs the slave about the possible corrections that need to be performed (e.g., shutdown container using recoverable resources).  This needs to be integrated with the resource monitor.  Need to figure out the metrics used for sending corrections (e.g., scheduling latency, usage, informed by executor/scheduler)  We also need to figure out the feedback loop between the QoS controller and the Resource Estimator.    ",Story,Major,Resolved,"2015-04-22 23:21:19","2015-04-22 22:21:19",3
"Apache Mesos","Modularize the Resource Estimator","Modularizing the resource estimator opens up the door for org specific implementations.  Test the estimator module.",Task,Major,Resolved,"2015-04-22 23:17:43","2015-04-22 22:17:43",3
"Apache Mesos","Implement Resource Estimator","Resource estimator is the component in the slave that estimates the amount of oversubscribable resources.  This needs to be integrated with the slave and resource monitor.",Task,Major,Resolved,"2015-04-22 23:16:49","2015-04-22 22:16:49",5
"Apache Mesos","Update Resource Monitor to return resource usage","Add usage() API call to return usage of all containers",Improvement,Major,Resolved,"2015-04-22 23:07:52","2015-04-22 22:07:52",3
"Apache Mesos","Slave should validate tasks using oversubscribed resources","The latest oversubscribed resource estimate might render a revocable task launch invalid. Slave should check this and send TASK_LOST with appropriate REASON.  We need to add a new REASON for this (REASON_RESOURCE_OVERSUBSCRIBED?).",Task,Major,Reviewable,"2015-04-22 23:06:13","2015-04-22 22:06:13",5
"Apache Mesos","Update Master to send revocable resources in separate offers","Master will send separate offers for revocable and non-revocable/regular resources. This allows master to rescind revocable offers (e.g, when a new oversubscribed resources estimate comes from the slave) without impacting regular offers.",Improvement,Major,Reviewable,"2015-04-22 23:05:12","2015-04-22 22:05:12",3
"Apache Mesos","Design doc for resource oversubscription",,Task,Major,Resolved,"2015-04-22 23:02:43","2015-04-22 22:02:43",13
"Apache Mesos","Consolidate 'foo', 'bar', ... string constants in test and example code","We are using 'foo', 'bar', ... string constants and pairs in src/tests/master_tests.cpp, src/tests/slave_tests.cpp, src/tests/hook_tests.cpp and src/examples/test_hook_module.cpp for label and hooks tests. These values should be stored in local variables to avoid the possibility of assignment getting out of sync with checking for that same value.",Bug,Major,Accepted,"2015-04-20 21:50:28","2015-04-20 20:50:28",2
"Apache Mesos","Segfault in inline Try<IP> getIP(const std::string& hostname, int family)","We saw a segfault in production. Attaching the coredump, we see:  Core was generated by `/usr/local/sbin/mesos-slave --port=5051 --resources=cpus:23;mem:70298;ports:[31'. Program terminated with signal 11, Segmentation fault. #0  0x00007f639867c77e in free () from /lib64/libc.so.6 (gdb) bt #0  0x00007f639867c77e in free () from /lib64/libc.so.6 #1  0x00007f63986c25d0 in freeaddrinfo () from /lib64/libc.so.6 #2  0x00007f6399deeafa in net::getIP (hostname=<redacted>, family=2) at ./3rdparty/stout/include/stout/net.hpp:201 #3  0x00007f6399e1f273 in process::initialize (delegate=Unhandled dwarf expression opcode 0xf3 ) at src/process.cpp:837 #4  0x000000000042342f in main ()",Bug,Major,Resolved,"2015-04-18 01:11:43","2015-04-18 00:11:43",1
"Apache Mesos","Move implementations of Framework struct functions out of master.hpp.","To help reduce compile time and keep the header easy to read, let's move the implementations of the Framework struct functions out of master.hpp",Task,Trivial,Resolved,"2015-04-16 23:04:41","2015-04-16 22:04:41",1
"Apache Mesos","Remove capture by reference of temporaries in libprocess",,Task,Major,Resolved,"2015-04-16 22:48:23","2015-04-16 21:48:23",1
"Apache Mesos","Remove capture by reference of temporaries in Stout",,Task,Major,Resolved,"2015-04-16 22:47:35","2015-04-16 21:47:35",1
"Apache Mesos","Update style guide to disallow capture by reference of temporaries","We modify the style guide to disallow constant references to temporaries as a whole. This means disallowing both (1) and (2) below.  h3. Background 1. Constant references to simple expression temporaries do extend the lifetime of the temporary till end of function scope: * Temporary returned by function:    * Temporary constructed as simple expression:     2. Constant references to expressions that result in a reference to a temporary do not extend the lifetime of the temporary:   * Temporary returned by function:      * Temporary constructed as simple expression:     h3. Mesos Case   - In Mesos we use Future<T> a lot. Many of our functions return Futures by value:      - Sometimes we capture these Futures:      - Sometimes we chain these Futures:      - Sometimes we do both:     h3. Reasoning - Although (1) is ok, and considered a [feature|http://herbsutter.com/2008/01/01/gotw-88-a-candidate-for-the-most-important-const/], (2) is extremely dangerous and leads to hard to track bugs. - If we explicitly allow (1), but disallow (2), then my worry is that someone coming along to maintain the code later on may accidentally turn (1) into (2), without recognizing the severity of this mistake. For example:  - If we disallow both cases: it will be easier to catch these mistakes early on in code reviews (and avoid these painful bugs), at the same cost of introducing a new style guide rule.  h3. Performance Implications - BenH suggests c++ developers are commonly taught to capture by constant reference to hint to the compiler that the copy can be elided. - Modern compilers use a Data Flow Graph to make optimizations such as   - *In-place-construction*: leveraged by RVO and NRVO to construct the object in place on the stack. Similar to *Placement new*: http://en.wikipedia.org/wiki/Placement_syntax   - *RVO* (Return Value Optimization): http://en.wikipedia.org/wiki/Return_value_optimization   - *NRVO* (Named Return Value Optimization): https://msdn.microsoft.com/en-us/library/ms364057%28v=vs.80%29.aspx - Since modern compilers perform these optimizations, we no longer need to 'hint' to the compiler that the copies can be elided.  h3. Example program  Output: ",Task,Major,Resolved,"2015-04-16 22:46:42","2015-04-16 21:46:42",1
"Apache Mesos","ExamplesTest.PersistentVolumeFramework is flaky","This just failed for the first time on our OS X Bot (Far less frequent flaky than the other ExamplesTest, but still flaky) while compiling master at commit f6620f851f635b3346c6ebf878152f38b3932ad9. There weren't any commits which touched / changed anything in the test in the set.  ",Bug,Major,Resolved,"2015-04-16 19:35:44","2015-04-16 18:35:44",1
"Apache Mesos","Document the semantic change in decorator return values","In order to enable decorator modules to _remove_ metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks.  The Result<T> return values means:  ||State||Before||After|| |Error|Error is propagated to the call-site|No change| |None|The result of the decorator is not applied|No change| |Some|The result of the decorator is *appended*|The result of the decorator *overwrites* the final labels/environment object|",Documentation,Major,Resolved,"2015-04-16 00:36:51","2015-04-15 23:36:51",1
"Apache Mesos","Pipe 'updateFramework' path from master to Allocator to support framework re-registration","Pipe the 'updateFramework' call from the master through the allocator, as described in the design doc in the epic: MESOS-703",Task,Major,Resolved,"2015-04-14 03:22:39","2015-04-14 02:22:39",1
"Apache Mesos","Change docker rm command","Right now it seems Mesos is using „docker rm –f ID“ to delete containers so bind mounts are not deleted. This means thousands of dirs in /var/lib/docker/vfs/dir   I would like to have the option to change it to „docker rm –f –v ID“ This deletes bind mounts but not persistant volumes.  Best,  Mike",Improvement,Minor,Resolved,"2015-04-13 19:34:27","2015-04-13 18:34:27",2
"Apache Mesos","Notify dev / user mailing list of the upcoming mem stat renames in 0.23.0 ",,Task,Major,Resolved,"2015-04-10 02:07:02","2015-04-10 01:07:02",2
"Apache Mesos","Add /reserve and /unreserve endpoints on the master for dynamic reservation","Enable operators to manage dynamic reservations by Introducing the {{/reserve}} and {{/unreserve}} HTTP endpoints on the master.",Task,Critical,Resolved,"2015-04-08 19:50:57","2015-04-08 18:50:57",5
"Apache Mesos","Slave state.json frameworks.executors.queued_tasks wrong format?","queued_tasks.executor_id is expected to be a string and not a complete json object. It should have the very same format as the tasks array on the same level.  Example, directly taken from slave  ",Bug,Minor,Resolved,"2015-04-07 17:30:27","2015-04-07 16:30:27",3
"Apache Mesos","Update allocator docs","Once Allocator interface changes, so does the way of writing new allocators. This should be reflected in Mesos docs. The modules doc should mention how to write and use allocator modules. Configuration doc should mention the new {{--allocator}} flag.",Task,Major,Resolved,"2015-04-07 14:53:44","2015-04-07 13:53:44",2
"Apache Mesos","Create docker executor","Currently we're reusing the command executor to wait on the progress of the docker executor, but has the following drawback:  - We need to launch a seperate docker log process just to forward logs, where we can just simply reattach stdout/stderr if we create a specific executor for docker - In general, Mesos slave is assuming that the executor is the one starting the actual task. But the current docker containerizer, the containerizer is actually starting the docker container first then launches the command executor to wait on it. This can cause problems if the container failed before the command executor was able to launch, as slave will try to update the limits of the containerizer on executor registration but then the docker containerizer will fail to do so since the container failed.   Overall it's much simpler to tie the container lifecycle with the executor and simplfies logic and log management.",Improvement,Major,Resolved,"2015-04-07 01:34:32","2015-04-07 00:34:32",8
"Apache Mesos","Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse","Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse",Improvement,Minor,Resolved,"2015-04-03 23:35:31","2015-04-03 22:35:31",2
"Apache Mesos","Let the slave control the duration of the perf sampler instead of relying on a sleep command.","Right now, we use a sleep command to control the duration of perf sampling:   This causes an additional process (i.e., the sleep process) to be forked and causes troubles for us to terminate the perf sampler once the slave exits (See MESOS-2462).  Seems that the additional sleep process is not necessary. The slave can just monitor the duration and send a SIGINT to the perf process when duration elapsed. This will cause the perf process to output the stats and terminate.",Improvement,Major,Resolved,"2015-04-02 19:13:53","2015-04-02 18:13:53",3
"Apache Mesos","Create optional release step: update PyPi repositories","One of the build artifacts for a release is the python package `mesos.interface`. That needs to be uploaded to PyPi along with a release to allow for users of python frameworks to use that version of mesos.",Documentation,Major,Resolved,"2015-03-31 22:40:55","2015-03-31 21:40:55",2
"Apache Mesos","Document tips, best practices, guidelines for doing code reviews.","We currently have a [Committers Guide|https://github.com/apache/mesos/blob/0.22.0/docs/committers-guide.md], however most of this information is relevant to all contributors looking to be participating in the code review process.  I'm proposing we extract much of this information into a more general Code Reviewing document, and include additional tips, best practices, lessons learned from members of the community.  This would be a great pre-requisite for on-boarding more committers and adding [MAINTAINERS|http://mail-archives.apache.org/mod_mbox/mesos-dev/201502.mbox/%3CCA+8RcoReugMVqoOpsnB8WGYBELa5fHwPA=J=<EMAIL>%3E].  The committers guide can be more specific to our expectations of committers, so we may want to make this into a committership document to help set expectations for contributors looking to become committers.",Improvement,Major,Resolved,"2015-03-31 19:51:16","2015-03-31 18:51:16",3
"Apache Mesos","0.22.1 release",,Task,Major,Resolved,"2015-03-31 19:04:32","2015-03-31 18:04:32",1
"Apache Mesos","Add '{' on newline for function declarations in style checker","Similar to MESOS-2577; another common style mistake is to not move curly braces on a newline for function and class declarations:    vs    This should be easy to check with our style checker too.",Improvement,Trivial,Accepted,"2015-03-31 17:26:38","2015-03-31 16:26:38",1
"Apache Mesos","Namespace handle symlinks in port_mapping isolator should not be under /var/run/netns","Consider putting symlinks under /var/run/messo/netns. This is because 'ip' command assumes all files under /var/run/netns are valid namespaces without duplication and it has command like:  ip -all netns exec ip link  to list all links for each network namespace.",Bug,Major,Resolved,"2015-03-30 23:51:16","2015-03-30 22:51:16",3
"Apache Mesos","Use Memory Test Helper to improve some test code.",,Improvement,Minor,Resolved,"2015-03-30 21:58:06","2015-03-30 20:58:06",2
"Apache Mesos","Add memory statistics tests.",,Task,Major,Reviewable,"2015-03-30 19:03:11","2015-03-30 18:03:11",5
"Apache Mesos","Expose Memory Pressure in MemIsolator",,Improvement,Major,Resolved,"2015-03-30 19:00:19","2015-03-30 18:00:19",3
"Apache Mesos","0.24.0 release","The main feature of this release is going to be v1 (beta) release of the HTTP scheduler API (part of MESOS-2288 epic).  Unresolved issues tracker: https://issues.apache.org/jira/issues/?jql=project%20%3D%20MESOS%20AND%20status%20!%3D%20Resolved%20AND%20%22Target%20Version%2Fs%22%20%3D%200.24.0%20ORDER%20BY%20status%20DESC",Task,Major,Resolved,"2015-03-26 20:31:10","2015-03-26 20:31:10",5
"Apache Mesos","Do not use RunTaskMessage.framework_id.","Assume that FrameworkInfo.id is always set and so need to read/set RunTaskMessage.framework_id.  This should land after https://issues.apache.org/jira/browse/MESOS-2558 has been shipped.",Bug,Major,Resolved,"2015-03-26 19:21:30","2015-03-26 19:21:30",1
"Apache Mesos","Document issue with slave recovery when using systemd.","As the problem encountered in MESOS-2419 is a common problem with the default systemd configuration it would make sense to document this in the upgrade guide or somewhere else in the documentation.",Documentation,Critical,Resolved,"2015-03-26 15:29:46","2015-03-26 15:29:46",1
"Apache Mesos","C++ Scheduler library should send HTTP Calls to master","Once the scheduler library sends Call messages, we should update it to send Calls as HTTP requests to /call endpoint on master.",Bug,Major,Resolved,"2015-03-25 22:58:14","2015-03-25 22:58:14",3
"Apache Mesos","C++ Scheduler library should send Call messages to Master","Currently, the C++ library sends different messages to Master instead of a single Call message. To vet the new Call API it should send Call messages. Master should be updated to handle all types of Calls.",Story,Major,Resolved,"2015-03-25 22:56:41","2015-03-25 22:56:41",8
"Apache Mesos","new `make distcheck` failures inside a docker container","After the commits:   Numerous tests inside our internal CI started failing:     Docker Info: Docker is run with `--privileged` and `-v /sys/fs/cgroup:/sys/fs/cgroup:RW`  Dockerfile   Comands it runs: ",Bug,Blocker,Resolved,"2015-03-25 19:05:40","2015-03-25 19:05:40",1
"Apache Mesos","Cleanup stale bind mounts for port mapping isolator during slave recovery.","Leaked bind mount under /var/run/netns for port mapping isolator is a known issue. There are many ways it can get leaked. For example, if the slave crashes after creating the bind mount but before creating the veth, the bind mount will be leaked. Also, if the detached unmount does not finish in time and the subsequent os::rm fails, the bind mount will be leaked as well.  Since leaked bind mount is inevitable, we need to clean them up during startup (slave recovery).",Improvement,Major,Resolved,"2015-03-25 17:48:14","2015-03-25 17:48:14",2
"Apache Mesos","Developer guide for libprocess","Create a developer guide for libprocess that explains the philosophy behind it and explains the most important features as well as the prevalent use patterns in Mesos with examples.   This could be similar to stout/README.md. ",Documentation,Major,Resolved,"2015-03-25 12:33:06","2015-03-25 12:33:06",2
"Apache Mesos","Remove unnecessary default flags from PortMappingMesosTest.","As all the explicitly set flags are defaults, we can remove them and simplify the code.  MESOS-2375 removed other occurrences of these default flags.",Bug,Minor,Resolved,"2015-03-24 18:36:16","2015-03-24 18:36:16",1
"Apache Mesos","PerfTest.ROOT_SampleInit test fails.","From MESOS-2300 as well, it looks like this test is not reliable:    It looks like this test samples PID 1, which is either {{init}} or {{systemd}}. Per a chat with [~<USER> this should probably sample something that is guaranteed to be consuming cycles.",Bug,Major,Resolved,"2015-03-23 23:28:53","2015-03-23 23:28:53",2
"Apache Mesos","Support HTTP checks in Mesos.","Currently, only commands are supported but our health check protobuf enables users to encode HTTP checks as well. We should wire up this in the health check program or remove the http field from the protobuf.",Improvement,Major,Resolved,"2015-03-23 23:22:57","2015-03-23 23:22:57",8
"Apache Mesos","Symlink the namespace handle with ContainerID for the port mapping isolator.","This serves two purposes: 1) Allows us to enter the network namespace using container ID (instead of pid): ip netns exec <ContainerID> [commands] [args]. 2) Allows us to get container ID for orphan containers during recovery. This will be helpful for solving MESOS-2367.  The challenge here is to solve it in a backward compatible way. I propose to create symlinks under /var/run/netns. For example: /var/run/netns/containeridxxxx --> /var/run/netns/12345 (12345 is the pid)  The old code will only remove the bind mounts and leave the symlinks, which I think is fine since containerid is globally unique (uuid).",Improvement,Major,Resolved,"2015-03-20 23:58:44","2015-03-20 23:58:44",3
"Apache Mesos","Log IP addresses from HTTP requests","Querying /master/state.json is an expensive operation when a cluster is large, and it's possible to DOS the master via frequent and repeated queries (which is a separate problem). Querying the endpoint results in a log entry being written, but the entry lacks useful information, such as an IP address, response code and response size. These details are useful for tracking down who/what is querying the endpoint. Consider adding these details to the log entry, or even writing a separate [access|https://httpd.apache.org/docs/trunk/logs.html#accesslog] [log|https://httpd.apache.org/docs/trunk/logs.html#common]. Also consider writing log entries for _all_ HTTP requests (/metrics/snapshot produces no log entries).  {noformat:title=sample log entry} I0319 18:06:18.824846 10521 http.cpp:478] HTTP request for '/master/state.json' {noformat}",Improvement,Minor,Resolved,"2015-03-19 19:22:33","2015-03-19 19:22:33",3
"Apache Mesos","Change the default leaf qdisc to fq_codel inside containers","When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:  `tc qd add dev eth0 parent 1:1 fq_codel`  I can no longer see packet drops after this change.",Bug,Major,Resolved,"2015-03-18 21:35:29","2015-03-18 21:35:29",1
"Apache Mesos","FetcherTest.ExtractNotExecutable is flaky","Observed in our internal CI.  ",Bug,Major,Resolved,"2015-03-17 19:20:44","2015-03-17 19:20:44",2
"Apache Mesos","Performance issue in the master when a large number of slaves are registering.","For large clusters, when a lot of slaves are registering, the master gets backlogged processing registration requests. {{perf}} revealed the following:    This is likely because we loop over all the slaves for each registration:  ",Improvement,Major,Resolved,"2015-03-17 01:13:51","2015-03-17 01:13:51",5
"Apache Mesos","Doxygen style for libprocess","Create a description of the Doxygen style to use for libprocess documentation.   It is expected that this will later also become the Doxygen style for stout and Mesos, but we are working on libprocess only for now.  Possible outcome: a file named docs/doxygen-style.md  We hope for much input and expect a lot of discussion! ",Documentation,Major,Resolved,"2015-03-16 15:49:57","2015-03-16 15:49:57",1
"Apache Mesos","Doxygen setup for libprocess","Goals:  - Initial doxygen setup.  - Enable interested developers to generate already available doxygen content locally in their workspace and view it. - Form the basis for future contributions of more doxygen content.  1. Devise a way to use Doxygen with Mesos source code. (For example, solve this by adding optional brew/apt-get installation to the Getting Started doc.) 2. Create a make target for libprocess documentation that can be manually triggered. 3. Create initial library top level documentation. 4. Enhance one header file with Doxygen. Make sure the generated output has all necessary links to navigate from the lib to the file and back, etc. ",Documentation,Major,Resolved,"2015-03-16 15:45:33","2015-03-16 15:45:33",2
"Apache Mesos","Create synchronous validations for Calls","/call endpoint will return a 202 accepted code but has to do some basic validations before. In case of invalidation it will return a 4xx code. We have to create a mechanism that will validate the 'request' and send back the appropriate code.",Bug,Major,Resolved,"2015-03-13 19:03:32","2015-03-13 19:03:32",8
"Apache Mesos","Persist the reservation state on the slave","h3. Goal  The goal for this task is to persist the reservation state stored on the master on the corresponding slave. The {{needCheckpointing}} predicate is used to capture the condition for which a resource needs to be checkpointed. Currently the only condition is {{isPersistentVolume}}. We'll update this to include dynamically reserved resources.  h3. Expected Outcome  * The dynamically reserved resources will be persisted on the slave.",Task,Major,Resolved,"2015-03-13 08:17:29","2015-03-13 08:17:29",5
"Apache Mesos","Enable a framework to perform reservation operations.","h3. Goal  This is the first step to supporting dynamic reservations. The goal of this task is to enable a framework to reply to a resource offer with *Reserve* and *Unreserve* offer operations as defined by {{Offer::Operation}} in {{mesos.proto}}.  h3. Overview  It's divided into a few subtasks so that it's clear what the small chunks to be addressed are. In summary, we need to introduce the {{Resource::ReservationInfo}} protobuf message to encapsulate the reservation information, enable the C++ {{Resources}} class to handle it then enable the master to handle reservation operations.  h3. Expected Outcome  * The framework will be able to send back reservation operations to (un)reserve resources. * The reservations are kept only in the master since we don't send the {{CheckpointResources}} message to checkpoint the reservations on the slave yet. * The reservations are considered to be reserved for the framework's role.",Task,Major,Resolved,"2015-03-13 07:55:56","2015-03-13 07:55:56",4
"Apache Mesos","Add ability to distinguish slave removals metrics by reason.","Currently we only expose a single removal metric ({{master/slave_removals}}) which makes it difficult to distinguish between removal reasons in the alerting.  Currently, a slave can be removed for the following reasons:  # Health checks failed. # Slave unregistered. # Slave was replaced by a new slave (on the same endpoint).  In the case of (2), we expect this to be due to maintenance and don't want to be notified as strongly as with health check failures.",Improvement,Major,Resolved,"2015-03-12 19:44:00","2015-03-12 19:44:00",3
"Apache Mesos","Enable Resources::apply to handle reservation operations.","{{Resources::apply}} currently only handles {{Create}} and {{Destroy}} operations which exist for persistent volumes. We need to handle the {{Reserve}} and {{Unreserve}} operations for dynamic reservations as well.",Task,Major,Resolved,"2015-03-11 04:22:02","2015-03-11 04:22:02",3
"Apache Mesos","Enable Resources to handle Resource::ReservationInfo","After [MESOS-2475|https://issues.apache.org/jira/browse/MESOS-2475], our C++ {{Resources}} class needs to know how to handle {{Resource}} protobuf messages that have the {{reservation}} field set.",Task,Major,Resolved,"2015-03-11 04:15:13","2015-03-11 04:15:13",2
"Apache Mesos","Add the Resource::ReservationInfo protobuf message","The {{Resource::ReservationInfo}} protobuf message encapsulates information needed to keep track of reservations. It's named {{ReservationInfo}} rather than {{Reservation}} to keep consistency with {{Resource::DiskInfo}}.  Here's what it will look like:  ",Task,Major,Resolved,"2015-03-11 03:48:37","2015-03-11 03:48:37",2
"Apache Mesos","Mesos master/slave should be able to bind to 127.0.0.1 if explicitly requested","With the current refactoring to IP it looks like master and slave can no longer bind to 127.0.0.1 even if explicitly requested via --ip flag.   Among other things, this breaks the balloon framework test which uses this flag.",Bug,Major,Resolved,"2015-03-09 23:40:00","2015-03-09 23:40:00",1
"Apache Mesos","Allow --resources flag to take JSON.","Currently, we used a customized format for --resources flag. As we introduce more and more stuffs (e.g., persistence, reservation) in Resource object, we need a more generic way to specify --resources.  For backward compatibility, we can scan the first character. If it is '[', then we invoke the JSON parser. Otherwise, we use the existing parser.",Improvement,Major,Resolved,"2015-03-09 16:36:45","2015-03-09 16:36:45",3
"Apache Mesos","Write documentation for all the LIBPROCESS_* environment variables.","libprocess uses a set of environment variables to modify its behaviour; however, these variables are not documented anywhere, nor it is defined where the documentation should be.  What would be needed is a decision whether the environment variables should be documented (a new doc file or reusing an existing one), and then add the documentation there.  After searching in the code, these are the variables which need to be documented:  # {{LIBPROCESS_IP}} # {{LIBPROCESS_PORT}} # {{LIBPROCESS_ADVERTISE_IP}} # {{LIBPROCESS_ADVERTISE_PORT}}",Documentation,Major,Resolved,"2015-03-09 10:43:57","2015-03-09 10:43:57",2
"Apache Mesos","Authentication failure may lead to slave crash","When slave authentication fails, the following attempt to transmit a {{UnregisterSlaveMessage}} may cause a crash within the slave.    The problem here is the following code:    Authentication happens before registration. {{info.id}} is an optional member (of {{SlaveInfo}}) and not known yet. It is set later, while registering. So {{slave_id}} will remain unset.",Bug,Major,Resolved,"2015-03-09 00:44:54","2015-03-09 00:44:54",1
"Apache Mesos","Add option for Subprocess to set a death signal for the forked child","Currently, children forked by the slave, including those through Subprocess, will continue running if the slave exits. For some processes, including helper processes like the fetcher, du, or perf, we'd like them to be terminated when the slave exits.  Add support to Subprocess to optionally set a DEATHSIG for the child, e.g., setting SIGTERM would mean the child would get SIGTERM when the slave terminates.  This can be done (*after forking*) with PR_SET_DEATHSIG. See man prctl. It is preserved through an exec call.",Improvement,Minor,Resolved,"2015-03-06 22:25:46","2015-03-06 22:25:46",3
"Apache Mesos","Slave should provide details on processes running in its cgroups","The slave can optionally be put into its own cgroups for a list of subsystems, e.g., for monitoring of memory and cpu. See the slave flag: --slave_subsystems  It currently refuses to start if there are any processes in its cgroups - this could be another slave or some subprocess started by a previous slave - and only logs the pids of those processes.  Improve this to log details about the processes: suggest at least the process command, uid running it, and perhaps its start time.",Improvement,Minor,Resolved,"2015-03-06 21:27:40","2015-03-06 21:27:40",1
"Apache Mesos","Add operator endpoints to create/destroy persistent volumes.","Persistent volumes will not be released automatically.  So we probably need an endpoint for operators to forcefully release persistent volumes. We probably need to add principal to Persistence struct and use ACLs to control who can release what.  Additionally, it would be useful to have an endpoint for operators to create persistent volumes.",Task,Critical,Resolved,"2015-03-05 23:38:32","2015-03-05 23:38:32",3
"Apache Mesos","Add support for /proc/self/mountinfo on Linux","/proc/self/mountinfo provides mount information specific to the calling process. This includes information on optional fields describing mount propagation, e.g., shared/slave mounts.   Initially, add this to linux/fs then perhaps move existing users of MountTable to use the mountinfo, deprecating and removing the mostly (but not entirely) redundant code.",Improvement,Major,Resolved,"2015-03-05 20:21:01","2015-03-05 20:21:01",3
"Apache Mesos","The recovered executor directory points to the meta directory.","The bug was introduced in this review: https://reviews.apache.org/r/29687   RunState.directory points to the metadata directory.  This would cause the PosixDiskIsolator to report incorrect disk usages after slave recovery.  We also need a test to test the slave recovery path for the PosixDiskIsolator.",Bug,Critical,Resolved,"2015-03-05 01:28:09","2015-03-05 01:28:09",2
"Apache Mesos","Mesos replicated log does not log the Action type name.","This is a regression introduced during the internal namespace refactor.  0.21.0 master:   0.22.0 master: ",Bug,Major,Resolved,"2015-03-04 01:27:43","2015-03-04 01:27:43",1
"Apache Mesos","Improve support for streaming HTTP Responses in libprocess.","Currently libprocess' HTTP::Response supports a PIPE construct for doing streaming responses:    This interface is too low level and difficult to program against:  * Connection closure is signaled with SIGPIPE, which is difficult for callers to deal with (must suppress SIGPIPE locally or globally in order to get EPIPE instead). * Pipes are generally for inter-process communication, and the pipe has finite size. With a blocking pipe the caller must deal with blocking when the pipe's buffer limit is exceeded. With a non-blocking pipe, the caller must deal with retrying the write.  We'll want to consider a few use cases: # Sending an HTTP::Response with streaming data. # Making a request with http::get and http::post in which the data is returned in a streaming manner. # Making a request in which the request content is streaming.  This ticket will focus on 1 as it is required for the HTTP API.",Improvement,Major,Resolved,"2015-03-03 22:25:53","2015-03-03 22:25:53",8
"Apache Mesos","Add Python bindings for the acceptOffers API.","We introduced the new acceptOffers API in C++ driver. We need to provide Python binding for this API as well.",Task,Major,Resolved,"2015-03-02 22:52:42","2015-03-02 22:52:42",2
"Apache Mesos","Add Java binding for the acceptOffers API.","We introduced the new acceptOffers API in C++ driver. We need to provide Java binding for this API as well.",Task,Major,Resolved,"2015-03-02 22:51:28","2015-03-02 22:51:28",2
"Apache Mesos","Use fq_codel qdisc for egress network traffic isolation",,Task,Major,Resolved,"2015-02-27 21:37:36","2015-02-27 21:37:36",8
"Apache Mesos","Slave should reclaim storage for destroyed persistent volumes.","At present, destroying a persistent volume does not cleanup any filesystem space that was used by the volume (it just removes the Mesos-level metadata about the volume). This effectively leads to a storage leak, which is bad. For task sandboxes, we do garbage collection to remove the sandbox at a later time to facilitate debugging failed tasks; for volumes, because they are explicitly deleted and are not tied to the lifecycle of a task, removing the associated storage immediately seems best.  To implement this safely, we'll either need to ensure that libprocess messages are delivered in-order, or else add some extra safe-guards to ensure that out-of-order {{CheckpointResources}} messages don't lead to accidental data loss.",Task,Major,Resolved,"2015-02-26 00:53:46","2015-02-26 00:53:46",5
"Apache Mesos","Add user doc for using persistent volumes.",,Task,Critical,Resolved,"2015-02-26 00:48:49","2015-02-26 00:48:49",2
"Apache Mesos","Add an example framework to test persistent volumes.","This serves two purposes:  1) testing the new persistence feature 2) served as an example for others to use the new feature",Task,Major,Resolved,"2015-02-26 00:48:00","2015-02-26 00:48:00",3
"Apache Mesos","MasterAllocatorTest/0.FrameworkReregistersFirst is flaky",,Bug,Major,Resolved,"2015-02-25 20:06:37","2015-02-25 20:06:37",2
"Apache Mesos","MesosContainerizerDestroyTest.LauncherDestroyFailure is flaky","Failed to os::execvpe in childMain. Never seen this one before.  ",Bug,Major,Resolved,"2015-02-25 07:03:12","2015-02-25 07:03:12",2
"Apache Mesos","MasterTest.ShutdownFrameworkWhileTaskRunning is flaky","Looks like the executorShutdownTimeout() was called immediately after executorShutdown() was called!  ",Bug,Major,Resolved,"2015-02-25 06:57:45","2015-02-25 06:57:45",1
"Apache Mesos","Improve NsTest.ROOT_setns","- Use symbol NAME directly to launch the subprocess instead of the hard-coded string.  - Replaced the static string with char[]. ",Improvement,Minor,Resolved,"2015-02-24 20:12:45","2015-02-24 20:12:45",1
"Apache Mesos","Create styleguide for documentation","As of right now different pages in our documentation use quite different styles. Consider for example the different emphasis for NOTE: *  *     Would be great to establish a common style for the documentation!",Documentation,Minor,Resolved,"2015-02-24 14:17:21","2015-02-24 14:17:21",2
"Apache Mesos","Rate limit slaves removals during master recovery.","Much like we rate limit slave removals in the common path (MESOS-1148), we need to rate limit slave removals that occur during master recovery. When a master recovers and is using a strict registry, slaves that do not re-register within a timeout will be removed.  Currently there is a safeguard in place to abort when too many slaves have not re-registered. However, in the case of a transient partition, we don't want to remove large sections of slaves without rate limiting.",Improvement,Major,Resolved,"2015-02-24 02:22:48","2015-02-24 02:22:48",3
"Apache Mesos","Provide user doc for the new posix disk isolator in Mesos containerizer","We introduced a posix disk isolator for Mesos containerizer in 0.22.0. This isolator allows us to get container disk usage as well as enforcing container disk quota. It's based on 'du'. We need to document this feature.",Documentation,Major,Resolved,"2015-02-24 00:48:53","2015-02-24 00:48:53",2
"Apache Mesos","GroupTest.LabelledGroup segfaults","Observed this on internal CI. Not sure if it is due to GroupTest.LabelledGroup or an earlier test.  ",Bug,Major,Open,"2015-02-23 21:52:43","2015-02-23 21:52:43",2
"Apache Mesos","SlaveTest.TaskLaunchContainerizerUpdateFails is flaky","Observed on internal CI  ",Bug,Major,Resolved,"2015-02-23 20:22:21","2015-02-23 20:22:21",1
"Apache Mesos","replace unsafe find | xargs with find -exec","The problem exists in  1194:src/Makefile.am  47:src/tests/balloon_framework_test.sh  The current find | xargs rm -rf in the Makefile could potentially destroy data if mesos source was in a folder with a space in the name. E.g. if you for some reason checkout mesos to / mesos the command in src/Makefile.am would turn into a rm -rf /  find | xargs should be NUL delimited with find -print0 | xargs -0 for safer execution or can just be replaced with the find build-in option find -exec '{}' \+ which behaves similar to xargs.  There was a second occurrence of this in a test script, though in that case it would only rmdir empty folders, so is less critical.  I submitted a PR here: https://github.com/apache/mesos/pull/36 ",Bug,Major,Resolved,"2015-02-23 10:51:10","2015-02-23 10:51:10",1
"Apache Mesos","DRFSorter needs to distinguish resources from different slaves.","Currently the {{DRFSorter}} aggregates total and allocated resources across multiple slaves, which only works for scalar resources. We need to distinguish resources from different slaves.  Suppose we have 2 slaves and 1 framework. The framework is allocated all resources from both slaves.    To provide some context, this issue came up while trying to reserve all unreserved resources from every offer.    Suppose the slave resources are the same as above:  {quote} Slave1: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}} Slave2: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}} {quote}  Initial (incorrect) total resources in the DRFSorter is:  {quote} {{cpus(\*):4; mem(\*):1024; ports(\*):\[31000-32000\]}} {quote}  We receive 2 offers, 1 from each slave:  {quote} Offer1: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}} Offer2: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}} {quote}  At this point, the resources allocated for the framework is:  {quote} {{cpus(\*):4; mem(\*):1024; ports(\*):\[31000-32000\]}} {quote}  After first {{RESERVE}} operation with Offer1:  The allocated resources for the framework becomes:  {quote} {{cpus(\*):2; mem(\*):512; cpus(role):2; mem(role):512; ports(role):\[31000-32000\]}} {quote}  During second {{RESERVE}} operation with Offer2:  {code:title=HierarchicalAllocatorProcess::updateAllocation}   // ...    FrameworkSorter* frameworkSorter =     frameworkSorters[frameworks\[frameworkId\].role];    Resources allocation = frameworkSorter->allocation(frameworkId.value());    // Update the allocated resources.   Try<Resources> updatedAllocation = allocation.apply(operations);   CHECK_SOME(updatedAllocation);    // ... {code}  {{allocation}} in the above code is:  {quote} {{cpus(\*):2; mem(\*):512; cpus(role):2; mem(role):512; ports(role):\[31000-32000\]}} {quote}  We try to {{apply}} a {{RESERVE}} operation and we fail to find {{ports(\*):\[31000-32000\]}} which leads to the {{CHECK}} fail at {{CHECK_SOME(updatedAllocation);}}",Bug,Major,Resolved,"2015-02-18 23:14:00","2015-02-18 23:14:00",2
"Apache Mesos","Test script for verifying compatibility between Mesos components","While our current unit/integration test suite catches functional bugs, it doesn't catch compatibility bugs (e.g, MESOS-2371). This is really crucial to provide operators the ability to do seamless upgrades on live clusters.  We should have a test suite / framework (ideally running on CI vetting each review on RB) that tests upgrade paths between master, slave, scheduler and executor.",Improvement,Major,Resolved,"2015-02-18 20:34:31","2015-02-18 20:34:31",2
"Apache Mesos","Improve slave resiliency in the face of orphan containers ","Right now there's a case where a misbehaving executor can cause a slave process to flap:  {panel:title=Quote From [~<USER>} {quote} 1) User tries to kill an instance 2) Slave sends {{KillTaskMessage}} to executor 3) Executor sends kill signals to task processes 4) Executor sends {{TASK_KILLED}} to slave 5) Slave updates container cpu limit to be 0.01 cpus 6) A user-process is still processing the kill signal 7) the task process cannot exit since it has too little cpu share and is throttled 8) Executor itself terminates 9) Slave tries to destroy the container, but cannot because the user-process is stuck in the exit path. 10) Slave restarts, and is constantly flapping because it cannot kill orphan containers {quote} {panel}  The slave's orphan container handling should be improved to deal with this case despite ill-behaved users (framework writers).",Bug,Critical,Resolved,"2015-02-18 02:35:38","2015-02-18 02:35:38",5
"Apache Mesos","MasterSlaveReconciliationTest.ReconcileLostTask is flaky","https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2746/changes  ",Task,Major,Resolved,"2015-02-18 02:12:26","2015-02-18 02:12:26",1
"Apache Mesos","Improve performance of the state.json endpoint for large clusters.","The master's state.json endpoint consistently takes a long time to compute the JSON result, for large clusters:    This can cause the master to get backlogged if there are many state.json requests in flight.  Looking at {{perf}} data, it seems most of the time is spent doing memory allocation / de-allocation. This ticket will try to capture any low hanging fruit to speed this up. Possibly we can leverage moves if they are not already being used by the compiler.",Improvement,Major,Resolved,"2015-02-14 01:20:40","2015-02-14 01:20:40",5
"Apache Mesos","Add support for MesosContainerizerLaunch to chroot to a specified path","In preparation for the MesosContainerizer to support a filesystem isolator the MesosContainerizerLauncher must support chrooting. Optionally, it should also configure the chroot environment by (re-)mounting special filesystems such as /proc and /sys and making device nodes such as /dev/zero, etc., such that the chroot environment is functional.",Improvement,Major,Resolved,"2015-02-12 05:44:17","2015-02-12 05:44:17",5
"Apache Mesos","Provide a way to execute an arbitrary process in a MesosContainerizer container context","Include a separate binary that when provided with a container_id, path to an executable, and optional arguments will find the container context, enter it, and exec the executable.  e.g.,   This need only support (initially) containers created with the MesosContainerizer and will support all isolators shipped with Mesos, i.e., it should find and enter the cgroups and namespaces for the running executor of the specified container.",Improvement,Major,Resolved,"2015-02-12 05:40:17","2015-02-12 05:40:17",5
"Apache Mesos","Add ability for schedulers to explicitly acknowledge status updates on the driver.","In order for schedulers to be able to handle status updates in a scalable manner, they need the ability to send acknowledgements through the driver. This enables optimizations in schedulers (e.g. process status updates asynchronously w/o backing up the driver, processing/acking updates in batch).  Without this, an implicit reconciliation can overload a scheduler (hence the motivation for MESOS-2308).",Improvement,Major,Resolved,"2015-02-12 00:34:25","2015-02-12 00:34:25",8
"Apache Mesos","Add ability to decode JSON serialized MasterInfo from ZK","Currently to discover the master a client needs the ZK node location and access to the MasterInfo protobuf so it can deserialize the binary blob in the node.  I think it would be nice to publish JSON (like Twitter's ServerSets) so clients are not tied to protobuf to do service discovery.  This ticket is an intermediate (compatibility) step: we add in {{0.23}} the ability for the {{Detector}} to understand JSON **alongside** Protobuf serialized format; this makes it compatible with both earlier versions, as well a future one (most likely, {{0.24}}) that will write the {{MasterInfo}} information in JSON format.",Improvement,Major,Resolved,"2015-02-10 21:57:42","2015-02-10 21:57:42",5
"Apache Mesos","__init__.py not getting installed in $PREFIX/lib/pythonX.Y/site-packages/mesos","When doing a {{make install}}, the src/python/native/src/mesos/__init__.py file is not getting installed in {{$PREFIX/lib/pythonX.Y/site-packages/mesos/}}.    This makes it impossible to do the following import when {{PYTHONPATH}} is set to the {{site-packages}} directory.    The directories {{$PREFIX/lib/pythonX.Y/site-packages/mesos/interface, native}} do have their corresponding {{__init__.py}} files.  Reproducing the bug: ",Bug,Critical,Resolved,"2015-02-10 18:25:37","2015-02-10 18:25:37",2
"Apache Mesos","Mesos Lifecycle Modules","A new kind of module that receives callbacks at significant life cycle events of its host libprocess process. Typically the latter is a Mesos slave or master and the life time of the libprocess process coincides with the underlying OS process.   h4. Motivation and Use Cases  We want to add customized and experimental capabilities that concern the life time of Mesos components without protruding into Mesos source code and without creating new build process dependencies for everybody.   Example use cases: 1. A slave or master life cycle module that gathers fail-over incidents and reports summaries thereof to a remote data sink. 2. A slave module that observes host computer metrics and correlates these with task activity. This can be used to find resources leaks and to prevent, respectively guide, oversubscription. 3. Upgrades and provisioning that require shutdown and restart.  h4. Specifics  The specific life cycle events that we want to get notified about and want to be able to act upon are:  - Process is spawning/initializing - Process is terminating/finalizing  In all these cases, a reference to the process is passed as a parameter, giving the module access for inspection and reaction.   h4. Module Classification  Unlike other named modules, a life cycle module does not directly replace or provide essential Mesos functionality (such as an Isolator module does). Unlike a decorator module it does not directly add or inject data into Mesos core either.",Improvement,Major,Resolved,"2015-02-10 16:57:45","2015-02-10 16:57:45",0.5
"Apache Mesos","Report per-container metrics for network bandwidth throttling","Export metrics from the network isolation to identify scope and duration of container throttling.    Packet loss can be identified from the overlimits and requeues fields of the htb qdisc report for the virtual interface, e.g.    Note that since a packet can be examined multiple times before transmission, overlimits can exceed total packets sent.    Add to the port_mapping isolator usage() and the container statistics protobuf. Carefully consider the naming (esp tx/rx) + commenting of the protobuf fields so it's clear what these represent and how they are different to the existing dropped packet counts from the network stack.",Improvement,Major,Resolved,"2015-02-10 01:53:38","2015-02-10 01:53:38",5
"Apache Mesos","MasterAllocatorTest/0.OutOfOrderDispatch is flaky"," {noformat:title=} [ RUN      ] MasterAllocatorTest/0.OutOfOrderDispatch Using temporary directory '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_kjLb9b' I0206 07:55:44.084333 15065 leveldb.cpp:175] Opened db in 25.006293ms I0206 07:55:44.089635 15065 leveldb.cpp:182] Compacted db in 5.256332ms I0206 07:55:44.089695 15065 leveldb.cpp:197] Created db iterator in 23534ns I0206 07:55:44.089710 15065 leveldb.cpp:203] Seeked to beginning of db in 2175ns I0206 07:55:44.089720 15065 leveldb.cpp:272] Iterated through 0 keys in the db in 417ns I0206 07:55:44.089781 15065 replica.cpp:743] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0206 07:55:44.093750 15086 recover.cpp:448] Starting replica recovery I0206 07:55:44.094044 15086 recover.cpp:474] Replica is in EMPTY status I0206 07:55:44.095473 15086 replica.cpp:640] Replica in EMPTY status received a broadcasted recover request I0206 07:55:44.095724 15086 recover.cpp:194] Received a recover response from a replica in EMPTY status I0206 07:55:44.096097 15086 recover.cpp:565] Updating replica status to STARTING I0206 07:55:44.106575 15086 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 10.289939ms I0206 07:55:44.106613 15086 replica.cpp:322] Persisted replica status to STARTING I0206 07:55:44.108144 15086 recover.cpp:474] Replica is in STARTING status I0206 07:55:44.109122 15086 replica.cpp:640] Replica in STARTING status received a broadcasted recover request I0206 07:55:44.110879 15091 recover.cpp:194] Received a recover response from a replica in STARTING status I0206 07:55:44.117267 15087 recover.cpp:565] Updating replica status to VOTING I0206 07:55:44.124771 15087 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 2.66794ms I0206 07:55:44.124814 15087 replica.cpp:322] Persisted replica status to VOTING I0206 07:55:44.124948 15087 recover.cpp:579] Successfully joined the Paxos group I0206 07:55:44.125095 15087 recover.cpp:463] Recover process terminated I0206 07:55:44.126204 15087 master.cpp:344] Master 20150206-075544-16842879-38895-15065 (utopic) started on 127.0.1.1:38895 I0206 07:55:44.126268 15087 master.cpp:390] Master only allowing authenticated frameworks to register I0206 07:55:44.126281 15087 master.cpp:395] Master only allowing authenticated slaves to register I0206 07:55:44.126307 15087 credentials.hpp:35] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_kjLb9b/credentials' I0206 07:55:44.126683 15087 master.cpp:439] Authorization enabled I0206 07:55:44.129329 15086 master.cpp:1350] The newly elected leader is master@127.0.1.1:38895 with id 20150206-075544-16842879-38895-15065 I0206 07:55:44.129361 15086 master.cpp:1363] Elected as the leading master! I0206 07:55:44.129389 15086 master.cpp:1181] Recovering from registrar I0206 07:55:44.129653 15088 registrar.cpp:312] Recovering registrar I0206 07:55:44.130859 15088 log.cpp:659] Attempting to start the writer I0206 07:55:44.132334 15088 replica.cpp:476] Replica received implicit promise request with proposal 1 I0206 07:55:44.135187 15088 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 2.825465ms I0206 07:55:44.135390 15088 replica.cpp:344] Persisted promised to 1 I0206 07:55:44.138062 15091 coordinator.cpp:229] Coordinator attemping to fill missing position I0206 07:55:44.139576 15091 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2 I0206 07:55:44.142156 15091 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 2.545543ms I0206 07:55:44.142189 15091 replica.cpp:678] Persisted action at 0 I0206 07:55:44.143414 15091 replica.cpp:510] Replica received write request for position 0 I0206 07:55:44.143468 15091 leveldb.cpp:437] Reading position from leveldb took 28872ns I0206 07:55:44.145982 15091 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 2.480277ms I0206 07:55:44.146015 15091 replica.cpp:678] Persisted action at 0 I0206 07:55:44.147050 15089 replica.cpp:657] Replica received learned notice for position 0 I0206 07:55:44.154364 15089 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 7.281644ms I0206 07:55:44.154400 15089 replica.cpp:678] Persisted action at 0 I0206 07:55:44.154422 15089 replica.cpp:663] Replica learned NOP action at position 0 I0206 07:55:44.155506 15091 log.cpp:675] Writer started with ending position 0 I0206 07:55:44.156746 15091 leveldb.cpp:437] Reading position from leveldb took 30248ns I0206 07:55:44.173681 15091 registrar.cpp:345] Successfully fetched the registry (0B) in 43.977984ms I0206 07:55:44.173821 15091 registrar.cpp:444] Applied 1 operations in 30768ns; attempting to update the 'registry' I0206 07:55:44.176213 15086 log.cpp:683] Attempting to append 119 bytes to the log I0206 07:55:44.176426 15086 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 1 I0206 07:55:44.177608 15088 replica.cpp:510] Replica received write request for position 1 I0206 07:55:44.180059 15088 leveldb.cpp:342] Persisting action (136 bytes) to leveldb took 2.415145ms I0206 07:55:44.180094 15088 replica.cpp:678] Persisted action at 1 I0206 07:55:44.181324 15084 replica.cpp:657] Replica received learned notice for position 1 I0206 07:55:44.183831 15084 leveldb.cpp:342] Persisting action (138 bytes) to leveldb took 2.473724ms I0206 07:55:44.183866 15084 replica.cpp:678] Persisted action at 1 I0206 07:55:44.183887 15084 replica.cpp:663] Replica learned APPEND action at position 1 I0206 07:55:44.185510 15084 registrar.cpp:489] Successfully updated the 'registry' in 11.619072ms I0206 07:55:44.185678 15086 log.cpp:702] Attempting to truncate the log to 1 I0206 07:55:44.186111 15086 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 2 I0206 07:55:44.186944 15086 replica.cpp:510] Replica received write request for position 2 I0206 07:55:44.187492 15084 registrar.cpp:375] Successfully recovered registrar I0206 07:55:44.188016 15087 master.cpp:1208] Recovered 0 slaves from the Registry (83B) ; allowing 10mins for slaves to re-register I0206 07:55:44.189678 15086 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 2.702559ms I0206 07:55:44.189713 15086 replica.cpp:678] Persisted action at 2 I0206 07:55:44.190620 15086 replica.cpp:657] Replica received learned notice for position 2 I0206 07:55:44.193383 15086 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 2.737088ms I0206 07:55:44.193455 15086 leveldb.cpp:400] Deleting ~1 keys from leveldb took 37762ns I0206 07:55:44.193475 15086 replica.cpp:678] Persisted action at 2 I0206 07:55:44.193496 15086 replica.cpp:663] Replica learned TRUNCATE action at position 2 I0206 07:55:44.200028 15065 containerizer.cpp:102] Using isolation: posix/cpu,posix/mem I0206 07:55:44.212924 15088 slave.cpp:172] Slave started on 46)@127.0.1.1:38895 I0206 07:55:44.213762 15088 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_RuNyVQ/credential' I0206 07:55:44.214251 15088 slave.cpp:281] Slave using credential for: test-principal I0206 07:55:44.214653 15088 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] I0206 07:55:44.214918 15088 slave.cpp:328] Slave hostname: utopic I0206 07:55:44.215116 15088 slave.cpp:329] Slave checkpoint: false W0206 07:55:44.215332 15088 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0206 07:55:44.217061 15090 state.cpp:32] Recovering state from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_RuNyVQ/meta' I0206 07:55:44.235409 15088 status_update_manager.cpp:196] Recovering status update manager I0206 07:55:44.235601 15088 containerizer.cpp:299] Recovering containerizer I0206 07:55:44.236486 15088 slave.cpp:3526] Finished recovery I0206 07:55:44.237709 15087 status_update_manager.cpp:170] Pausing sending status updates I0206 07:55:44.237890 15088 slave.cpp:620] New master detected at master@127.0.1.1:38895 I0206 07:55:44.241575 15088 slave.cpp:683] Authenticating with master master@127.0.1.1:38895 I0206 07:55:44.247459 15088 slave.cpp:688] Using default CRAM-MD5 authenticatee I0206 07:55:44.248617 15089 authenticatee.hpp:137] Creating new client SASL connection I0206 07:55:44.249099 15089 master.cpp:3788] Authenticating slave(46)@127.0.1.1:38895 I0206 07:55:44.249137 15089 master.cpp:3799] Using default CRAM-MD5 authenticator I0206 07:55:44.249728 15089 authenticator.hpp:169] Creating new server SASL connection I0206 07:55:44.250285 15089 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5 I0206 07:55:44.250496 15089 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 07:55:44.250452 15088 slave.cpp:656] Detecting new master I0206 07:55:44.251063 15091 authenticator.hpp:275] Received SASL authentication start I0206 07:55:44.251124 15091 authenticator.hpp:397] Authentication requires more steps I0206 07:55:44.251256 15089 authenticatee.hpp:274] Received SASL authentication step I0206 07:55:44.251451 15090 authenticator.hpp:303] Received SASL authentication step I0206 07:55:44.251575 15090 authenticator.hpp:389] Authentication success I0206 07:55:44.251687 15090 master.cpp:3846] Successfully authenticated principal 'test-principal' at slave(46)@127.0.1.1:38895 I0206 07:55:44.253306 15089 authenticatee.hpp:314] Authentication success I0206 07:55:44.258015 15089 slave.cpp:754] Successfully authenticated with master master@127.0.1.1:38895 I0206 07:55:44.258468 15089 master.cpp:2913] Registering slave at slave(46)@127.0.1.1:38895 (utopic) with id 20150206-075544-16842879-38895-15065-S0 I0206 07:55:44.259028 15089 registrar.cpp:444] Applied 1 operations in 88902ns; attempting to update the 'registry' I0206 07:55:44.269492 15065 sched.cpp:149] Version: 0.22.0 I0206 07:55:44.270539 15090 sched.cpp:246] New master detected at master@127.0.1.1:38895 I0206 07:55:44.270614 15090 sched.cpp:302] Authenticating with master master@127.0.1.1:38895 I0206 07:55:44.270634 15090 sched.cpp:309] Using default CRAM-MD5 authenticatee I0206 07:55:44.270900 15090 authenticatee.hpp:137] Creating new client SASL connection I0206 07:55:44.272300 15089 log.cpp:683] Attempting to append 285 bytes to the log I0206 07:55:44.272552 15089 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 3 I0206 07:55:44.273609 15086 master.cpp:3788] Authenticating scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.273643 15086 master.cpp:3799] Using default CRAM-MD5 authenticator I0206 07:55:44.273955 15086 authenticator.hpp:169] Creating new server SASL connection I0206 07:55:44.274617 15090 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5 I0206 07:55:44.274813 15090 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 07:55:44.275171 15088 authenticator.hpp:275] Received SASL authentication start I0206 07:55:44.275215 15088 authenticator.hpp:397] Authentication requires more steps I0206 07:55:44.275408 15090 authenticatee.hpp:274] Received SASL authentication step I0206 07:55:44.275696 15084 authenticator.hpp:303] Received SASL authentication step I0206 07:55:44.275774 15084 authenticator.hpp:389] Authentication success I0206 07:55:44.275876 15084 master.cpp:3846] Successfully authenticated principal 'test-principal' at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.277593 15090 authenticatee.hpp:314] Authentication success I0206 07:55:44.278201 15086 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:38895 I0206 07:55:44.278548 15086 master.cpp:1568] Received registration request for framework 'framework1' at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.278642 15086 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*' I0206 07:55:44.279157 15086 master.cpp:1632] Registering framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.280081 15086 sched.cpp:440] Framework registered with 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.280320 15086 hierarchical_allocator_process.hpp:318] Added framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.281411 15089 replica.cpp:510] Replica received write request for position 3 I0206 07:55:44.282289 15085 master.cpp:2901] Ignoring register slave message from slave(46)@127.0.1.1:38895 (utopic) as admission is already in progress I0206 07:55:44.284984 15089 leveldb.cpp:342] Persisting action (304 bytes) to leveldb took 3.368213ms I0206 07:55:44.285020 15089 replica.cpp:678] Persisted action at 3 I0206 07:55:44.285893 15089 replica.cpp:657] Replica received learned notice for position 3 I0206 07:55:44.288350 15089 leveldb.cpp:342] Persisting action (306 bytes) to leveldb took 2.430449ms I0206 07:55:44.288384 15089 replica.cpp:678] Persisted action at 3 I0206 07:55:44.288405 15089 replica.cpp:663] Replica learned APPEND action at position 3 I0206 07:55:44.290154 15089 registrar.cpp:489] Successfully updated the 'registry' in 31.046912ms I0206 07:55:44.290307 15085 log.cpp:702] Attempting to truncate the log to 3 I0206 07:55:44.290671 15085 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 4 I0206 07:55:44.291482 15085 replica.cpp:510] Replica received write request for position 4 I0206 07:55:44.292559 15087 master.cpp:2970] Registered slave 20150206-075544-16842879-38895-15065-S0 at slave(46)@127.0.1.1:38895 (utopic) with cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] I0206 07:55:44.292940 15087 slave.cpp:788] Registered with master master@127.0.1.1:38895; given slave ID 20150206-075544-16842879-38895-15065-S0 I0206 07:55:44.293298 15087 hierarchical_allocator_process.hpp:450] Added slave 20150206-075544-16842879-38895-15065-S0 (utopic) with cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] available) I0206 07:55:44.293684 15087 status_update_manager.cpp:177] Resuming sending status updates I0206 07:55:44.294085 15087 master.cpp:3730] Sending 1 offers to framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.299957 15085 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 8.442691ms I0206 07:55:44.300165 15085 replica.cpp:678] Persisted action at 4 I0206 07:55:44.300698 15065 sched.cpp:1468] Asked to stop the driver I0206 07:55:44.301127 15090 sched.cpp:806] Stopping framework '20150206-075544-16842879-38895-15065-0000' I0206 07:55:44.301503 15090 master.cpp:1892] Asked to unregister framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.301535 15090 master.cpp:4158] Removing framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.302376 15090 slave.cpp:1592] Asked to shut down framework 20150206-075544-16842879-38895-15065-0000 by master@127.0.1.1:38895 W0206 07:55:44.302407 15090 slave.cpp:1607] Cannot shut down unknown framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.302814 15090 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.302947 15090 hierarchical_allocator_process.hpp:351] Removed framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.309281 15086 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]) on slave 20150206-075544-16842879-38895-15065-S0 from framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.310158 15084 replica.cpp:657] Replica received learned notice for position 4 I0206 07:55:44.313246 15084 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 3.055049ms I0206 07:55:44.313328 15084 leveldb.cpp:400] Deleting ~2 keys from leveldb took 45270ns I0206 07:55:44.313349 15084 replica.cpp:678] Persisted action at 4 I0206 07:55:44.313374 15084 replica.cpp:663] Replica learned TRUNCATE action at position 4 I0206 07:55:44.329591 15065 sched.cpp:149] Version: 0.22.0 I0206 07:55:44.330258 15088 sched.cpp:246] New master detected at master@127.0.1.1:38895 I0206 07:55:44.330346 15088 sched.cpp:302] Authenticating with master master@127.0.1.1:38895 I0206 07:55:44.330368 15088 sched.cpp:309] Using default CRAM-MD5 authenticatee I0206 07:55:44.330652 15088 authenticatee.hpp:137] Creating new client SASL connection I0206 07:55:44.331403 15088 master.cpp:3788] Authenticating scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.331717 15088 master.cpp:3799] Using default CRAM-MD5 authenticator I0206 07:55:44.332293 15088 authenticator.hpp:169] Creating new server SASL connection I0206 07:55:44.332655 15088 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5 I0206 07:55:44.332684 15088 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 07:55:44.332792 15088 authenticator.hpp:275] Received SASL authentication start I0206 07:55:44.332835 15088 authenticator.hpp:397] Authentication requires more steps I0206 07:55:44.332903 15088 authenticatee.hpp:274] Received SASL authentication step I0206 07:55:44.332983 15088 authenticator.hpp:303] Received SASL authentication step I0206 07:55:44.333056 15088 authenticator.hpp:389] Authentication success I0206 07:55:44.333153 15088 authenticatee.hpp:314] Authentication success I0206 07:55:44.333297 15091 master.cpp:3846] Successfully authenticated principal 'test-principal' at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.334326 15087 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:38895 I0206 07:55:44.334645 15087 master.cpp:1568] Received registration request for framework 'framework2' at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.334722 15087 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*' I0206 07:55:44.335153 15087 master.cpp:1632] Registering framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.336019 15087 sched.cpp:440] Framework registered with 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.336156 15087 hierarchical_allocator_process.hpp:318] Added framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.336796 15087 master.cpp:3730] Sending 1 offers to framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.337725 15065 sched.cpp:1468] Asked to stop the driver I0206 07:55:44.338002 15086 sched.cpp:806] Stopping framework '20150206-075544-16842879-38895-15065-0001' I0206 07:55:44.338297 15090 master.cpp:1892] Asked to unregister framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.338353 15090 master.cpp:4158] Removing framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 ../../src/tests/master_allocator_tests.cpp:300: Failure Mock function called more times than expected - taking default action specified at: ../../src/tests/mesos.hpp:713:     Function call: deactivateFramework(@0x7fdb74008d70 20150206-075544-16842879-38895-15065-0001)          Expected: to be called once            Actual: called twice - over-saturated and active ../../src/tests/master_allocator_tests.cpp:312: Failure Mock function called more times than expected - taking default action specified at: ../../src/tests/mesos.hpp:753:     Function call: recoverResources(@0x7fdb74013040 20150206-075544-16842879-38895-15065-0001, @0x7fdb74013060 20150206-075544-16842879-38895-15065-S0, @0x7fdb74013080 { cpus(*):2, mem(*):1024, disk(*):24988, ports(*):[31000-32000] }, @0x7fdb74013098 16-byte object <01-00 00-00 DB-7F 00-00 00-00 00-00 00-00 00-00>)          Expected: to be called once            Actual: called twice - over-saturated and active I0206 07:55:44.339527 15090 slave.cpp:1592] Asked to shut down framework 20150206-075544-16842879-38895-15065-0001 by master@127.0.1.1:38895 W0206 07:55:44.339558 15090 slave.cpp:1607] Cannot shut down unknown framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.339954 15090 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.340095 15090 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]) on slave 20150206-075544-16842879-38895-15065-S0 from framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.340181 15090 hierarchical_allocator_process.hpp:351] Removed framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.340852 15085 master.cpp:781] Master terminating I0206 07:55:44.345564 15086 slave.cpp:2680] master@127.0.1.1:38895 exited W0206 07:55:44.345593 15086 slave.cpp:2683] Master disconnected! Waiting for a new master to be elected I0206 07:55:44.393707 15065 slave.cpp:502] Slave terminating [  FAILED  ] MasterAllocatorTest/0.OutOfOrderDispatch, where TypeParam = mesos::master::allocator::HierarchicalAllocatorProcess<mesos::master::allocator::DRFSorter, mesos::master::allocator::DRFSorter> (360 ms) {noformat}",Bug,Major,Resolved,"2015-02-06 18:59:21","2015-02-06 18:59:21",1
"Apache Mesos","Unable to set --work_dir to a non /tmp device","When starting mesos-slave with --work_dir set to a directory which is not the same device as /tmp results in mesos-slave throwing a core dump:   Removing the --work_dir option results in the slave starting successfully.",Bug,Major,Resolved,"2015-02-04 18:35:02","2015-02-04 18:35:02",2
"Apache Mesos","Remove deprecated checkpoint=false code","Cody's plan from MESOS-444 was: 1) -Make it so the flag can't be changed at the command line- 2) -Remove the checkpoint variable entirely from slave/flags.hpp. This is a fairly involved change since a number of unit tests depend on manually setting the flag, as well as the default being non-checkpointing.- 3) -Remove logic around checkpointing in the slave, remove logic inside the master.- 4) Drop the flag from the SlaveInfo struct (Will require a deprecation cycle). ",Epic,Major,"In Progress","2015-02-04 00:05:44","2015-02-04 00:05:44",3
"Apache Mesos","Deprecate / Remove CommandInfo::ContainerInfo","IIUC this has been deprecated and all current code (except examples/docker_no_executor_framework.cpp) uses the top-level ContainerInfo?",Task,Minor,Resolved,"2015-02-02 21:40:49","2015-02-02 21:40:49",2
"Apache Mesos","remove unnecessary constants","In {{src/slave/paths.cpp}} a number of string constants are defined to describe the formats of various paths. However, given there is a 1:1 mapping between the string constant and the functions that build the paths, the code would be more readable if the format strings were inline in the functions.  In the cases where one constant depends on another (see the {{EXECUTOR_INFO_PATH, EXECUTOR_PATH, FRAMEWORK_PATH, SLAVE_PATH, ROOT_PATH}} chain, for example) the function calls can just be chained together.  This will have the added benefit of removing some statically constructed string constants, which are dangerous.",Improvement,Minor,Resolved,"2015-02-02 18:39:00","2015-02-02 18:39:00",2
"Apache Mesos","Mesos rejects ExecutorInfo as incompatible when there is no functional difference","In AURORA-1076 it was discovered that if an ExecutorInfo was changed such that a previously unset optional field with a default value was changed to have the field set with the default value, it would be rejected as not compatible.  For example if we have an ExecutorInfo with a CommandInfo with the {{shell}} attribute unset and then we change the CommandInfo to set the {{shell}} attribute to true Mesos will reject the task with:    This is not intuitive because the default value of the {{shell}} attribute is true. There should be no difference between not setting an optional field with a default value and setting that field to the default value.",Bug,Minor,Resolved,"2015-01-30 23:46:20","2015-01-30 23:46:20",3
"Apache Mesos","MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky.","Good run:    Bad run:  ",Bug,Major,Resolved,"2015-01-29 22:58:05","2015-01-29 22:58:05",1
"Apache Mesos","Refactor validators in Master.","There are several motivation for this. We are in the process of adding dynamic reservations and persistent volumes support in master. To do that, master needs to validate relevant operations from the framework (See Offer::Operation in mesos.proto). The existing validator style in master is hard to extend, compose and re-use.  Another motivation for this is for unit testing (MESOS-1064). Right now, we write integration tests for those validators which is unfortunate.",Bug,Major,Resolved,"2015-01-29 22:27:45","2015-01-29 22:27:45",3
"Apache Mesos","FaultToleranceTest.SchedulerFailoverFrameworkMessage is flaky.","Bad Run:   Good Run: ",Bug,Major,Resolved,"2015-01-29 19:40:42","2015-01-29 19:40:42",1
"Apache Mesos","Provide a Java library for master detection","When schedulers start interacting with Mesos master via HTTP endpoints, they need a way to detect masters.   Mesos should provide a master detection Java library to make this easy for frameworks.",Task,Major,Resolved,"2015-01-28 23:06:00","2015-01-28 23:06:00",5
"Apache Mesos","Add authentication support for HTTP API","Since most of the communication between mesos components will happen through HTTP with the arrival of the [HTTP API|https://issues.apache.org/jira/browse/MESOS-2288], it makes sense to use HTTP standard mechanisms to authenticate this communication.",Epic,Major,Resolved,"2015-01-28 23:03:29","2015-01-28 23:03:29",1
"Apache Mesos","Implement the Events stream on slave for Call endpoint",,Task,Major,Resolved,"2015-01-28 23:02:48","2015-01-28 23:02:48",8
"Apache Mesos","Implement the Call endpoint on Slave",,Task,Major,Resolved,"2015-01-28 23:02:24","2015-01-28 23:02:24",8
"Apache Mesos","Implement the Events stream on master for Call endpoint",,Task,Major,Resolved,"2015-01-28 23:01:58","2015-01-28 23:01:58",8
"Apache Mesos","Implement the scheduler endpoint on master",,Story,Major,Resolved,"2015-01-28 23:01:23","2015-01-28 23:01:23",8
"Apache Mesos","Move all scheduler driver validations to master","With HTTP API, the scheduler driver will no longer exist and hence all the validations should move to the master.",Task,Major,Resolved,"2015-01-28 22:54:43","2015-01-28 22:54:43",3
"Apache Mesos","Design doc for the HTTP API","This tracks the design of the HTTP API.",Task,Major,Resolved,"2015-01-28 22:53:24","2015-01-28 22:53:24",13
"Apache Mesos","SlaveRecoveryTest.ReconcileKillTask is flaky.","Saw this on an internal CI:  ",Bug,Major,Resolved,"2015-01-28 02:31:41","2015-01-28 02:31:41",1
"Apache Mesos","Deprecate plain text Credential format.","Currently two formats of credentials are supported: JSON    And a new line file:   We should deprecate the new line format and remove support for the old format.",Improvement,Major,Resolved,"2015-01-27 22:33:03","2015-01-27 22:33:03",3
"Apache Mesos","Future callbacks should be cleared once the future has transitioned.","For example, when a future has transitioned into READY state, all onDiscard callbacks should be cleared to avoid potential cyclic dependency and memory leak. For instance:    The above code has a cyclic dependency because f.data has a reference to the future inside an std::function which has a reference to f.data.",Bug,Major,Resolved,"2015-01-27 21:38:11","2015-01-27 21:38:11",2
"Apache Mesos","Document header include rules in style guide","We have several ways of sorting, grouping and ordering headers includes in Mesos. We should agree on a rule set and do a style scan.",Improvement,Trivial,Resolved,"2015-01-27 18:56:18","2015-01-27 18:56:18",3
"Apache Mesos","Add tests target to Makefile for building-but-not-running tests.","'make check' allows one to build and run the test suite. However, often we just want to build the tests.  Currently, this is done by setting GTEST_FILTER to an empty string.  It will be nice to have a dedicated target such as 'make tests' that allows one to build the test suite without running it.",Improvement,Major,Resolved,"2015-01-26 23:07:56","2015-01-26 23:07:56",1
"Apache Mesos","Version the Operator/Admin API","As a consumer of the Mesos HTTP API, it is necessary for us to determine the current version of Mesos so that we can parse the JSON documents returned correctly (since they change from version to version).   Currently we're doing this by fetching state.json, parsing it and pulling out the version field. A more idiomatic way to do this would be to filter on the content-type in the header itself.  To give a more concrete example, currently the JSON documents returned by the HTTP API return the following headers:   Something like the following (e.g. for master/state.json) would be easy to switch upon:   The vnd prefix is typically used for vendor specific file types (see: http://en.wikipedia.org/wiki/Internet_media_type#Prefix_vnd). Charset=utf-8 is required for JSON documents and is currently being omitted.  This content-type would change for each document type, for example:   Alternatively, the version could be appended as an extra field:   Thanks!",Task,Minor,Resolved,"2015-01-23 21:53:17","2015-01-23 21:53:17",13
"Apache Mesos","DiskUsageCollectorTest.SymbolicLink test is flaky","Observed this on a local machine running linux w/ sudo.  ",Bug,Major,Resolved,"2015-01-21 22:59:46","2015-01-21 22:59:46",1
"Apache Mesos","Run ASF CI mesos builds inside docker","There are several limitations to mesos projects current state of CI, which is run on builds.a.o  --> Only runs on Ubuntu --> Doesn't run any tests that deal with cgroups --> Doesn't run any tests that need root permissions  Now that ASF CI supports docker (https://issues.apache.org/jira/browse/BUILDS-25), it would be great for the Mesos project to use it.",Task,Major,Resolved,"2015-01-16 21:59:45","2015-01-16 21:59:45",5
"Apache Mesos","Suppress MockAllocator::transformAllocation() warnings.","After transforming allocated resources feature was added to allocator, a number of warnings are popping out for allocator tests. Commits leading to this behaviour: {{dacc88292cc13d4b08fe8cda4df71110a96cb12a}} {{5a02d5bdc75d3b1149dcda519016374be06ec6bd}} corresponding reviews: https://reviews.apache.org/r/29083 https://reviews.apache.org/r/29084  Here is an example: ",Bug,Minor,Resolved,"2015-01-16 15:45:41","2015-01-16 15:45:41",3
"Apache Mesos","Update RateLimiter to allow the acquired future to be discarded","Currently there is no way for the future returned by RateLimiter's acquire() to be discarded by the user of the limiter. This is useful in cases where the user is no longer interested in the permit. See MESOS-1148 for an example use case.",Improvement,Major,Resolved,"2015-01-16 01:00:53","2015-01-16 01:00:53",3
"Apache Mesos","SlaveTest.MesosExecutorGracefulShutdown is flaky","Observed this on internal CI  ",Bug,Major,Resolved,"2015-01-15 21:39:56","2015-01-15 21:39:56",3
"Apache Mesos","HookTest.VerifySlaveLaunchExecutorHook is flaky","Observed this on internal CI  ",Bug,Major,Resolved,"2015-01-15 20:25:07","2015-01-15 20:25:07",3
"Apache Mesos","FaultToleranceTest.ReregisterFrameworkExitedExecutor is flaky","Observed this on internal CI.  ",Bug,Major,Resolved,"2015-01-15 20:23:02","2015-01-15 20:23:02",2
"Apache Mesos","Add ACLs for the maintenance HTTP endpoints.","In order to authorize the HTTP endpoints for maintenance (to be added in MESOS-2067), we will need to add an ACL definition for performing maintenance operations.",Task,Major,Accepted,"2015-01-15 02:43:30","2015-01-15 02:43:30",3
"Apache Mesos","The Docker containerizer attempts to recover any task when checkpointing is enabled, not just docker tasks.","Once the slave restarts and recovers the task, I see this error in the log for all tasks that were recovered every second or so.  Note, these were NOT docker tasks:  W0113 16:01:00.790323 773142 monitor.cpp:213] Failed to get resource usage for  container 7b729b89-dc7e-4d08-af97-8cd1af560a21 for executor thermos-1421085237813-slipstream-prod-agent-3-8f769514-1835-4151-90d0-3f55dcc940dd of framework 20150109-161713-715350282-5050-290797-0000: Failed to 'docker inspect mesos-7b729b89-dc7e-4d08-af97-8cd1af560a21': exit status = exited with status 1 stderr = Error: No such image or container: mesos-7b729b89-dc7e-4d08-af97-8cd1af560a21 However the tasks themselves are still healthy and running.  The slave was launched with --containerizers=mesos,docker  ----- More info: it looks like the docker containerizer is a little too ambitious about recovering containers, again this was not a docker task: I0113 15:59:59.476145 773142 docker.cpp:814] Recovering container '7b729b89-dc7e-4d08-af97-8cd1af560a21' for executor 'thermos-1421085237813-slipstream-prod-agent-3-8f769514-1835-4151-90d0-3f55dcc940dd' of framework 20150109-161713-715350282-5050-290797-0000  Looking into the source, it looks like the problem is that the ComposingContainerize runs recover in parallel, but neither the docker containerizer nor mesos containerizer check if they should recover the task or not (because they were the ones that launched it).  Perhaps this needs to be written into the checkpoint somewhere?",Bug,Major,Resolved,"2015-01-13 16:06:17","2015-01-13 16:06:17",8
"Apache Mesos","Disallow special characters in role.","As we introduce persistent volumes in MESOS-1524, we will use roles as directory names on the slave (https://reviews.apache.org/r/28562/). As a result, the master should disallow special characters (like space and slash) in role.",Task,Major,Resolved,"2015-01-08 23:01:25","2015-01-08 23:01:25",2
"Apache Mesos","Add user documentation for reservations","Add a user guide for reservations which describes basic usage of them, how ACLs are used to specify who can unreserve whose resources, and few advanced usage cases.",Documentation,Critical,Resolved,"2014-12-30 02:01:38","2014-12-30 02:01:38",2
"Apache Mesos","ReplicaTest.Restore fails with leveldb greater than v1.7.","I wanted to configure Mesos with system provided leveldb libraries when I ran into this issue. Apparently,  if one does {{../configure --with-leveldb=/path/to/leveldb}}, compilation succeeds, however the ReplicaTest_Restore test fails with the following back trace:    The bundled version of leveldb is v1.4. I tested version 1.5 and that seems to work.  However, v1.6 had some build issues and us unusable with Mesos. The next version v1.7, allows Mesos to compile fine but results in the above error.",Bug,Minor,Resolved,"2014-12-22 22:22:37","2014-12-22 22:22:37",3
"Apache Mesos","bogus docker images result in bad error message to scheduler","When a scheduler specifies a bogus image in ContainerInfo mesos doesn't tell the scheduler that the docker pull failed or why.  This error is logged in the mesos-slave log, but it isn't given to the scheduler (as far as I can tell):    If the docker image is not in the registry, the scheduler should give the user an error message.  If docker pull failed because of networking issues, it should be retried.  Mesos should give the scheduler enough information to be able to make that decision.",Bug,Major,Accepted,"2014-12-19 20:49:03","2014-12-19 20:49:03",2
"Apache Mesos","Failing test: SlaveTest.ROOT_RunTaskWithCommandInfoWithUser","Appears that running the executor as {{nobody}} is not supported.    [~<USER> can you take a look?    Executor log:      Test output:  ",Bug,Major,Resolved,"2014-12-19 01:31:57","2014-12-19 01:31:57",2
"Apache Mesos","Add ContainerId to the TaskStatus message","{{TaskStatus}} provides the frameworks with certain information ({{executorId}}, {{slaveId}}, etc.) which is useful when collecting statistics about cluster performance; however, it is difficult to associate tasks to the container it is executed since this information stays always within mesos itself. Therefore it would be good to provide the framework scheduler with this information, adding a new field in the {{TaskStatus}} message.  See comments for a use case.",Wish,Major,Resolved,"2014-12-12 05:12:22","2014-12-12 05:12:22",3
"Apache Mesos","deprecate unused flag 'cgroups_subsystems'","cgroups_subsystems is a slave flag that is no longer used and should be deprecated.",Task,Minor,Resolved,"2014-12-09 22:24:37","2014-12-09 22:24:37",1
"Apache Mesos","Performance issue in libprocess SocketManager.","Noticed an issue in production under which the master is slow to respond after failover for ~15 minutes.  After looking at some perf data, the top offender is:    It appears that in the SocketManager, whenever an internal Process exits, we loop over all the links unnecessarily:    On clusters with 10,000s of slaves, this means we hold the socket manager lock for a very expensive loop erasing nothing from a set! This is because, the master contains links from the Master Process to each slave. However, when a random ephemeral Process terminates, we don't need to loop over each slave link.  While we hold this lock, the following calls will block:    As a result, the slave observers and the master can block calling send()!  Short term, we will try to fix this issue by removing the unnecessary looping. Longer term, it would be nice to avoid all this locking when sending on independent sockets.",Bug,Blocker,Resolved,"2014-12-09 01:04:04","2014-12-09 01:04:04",3
"Apache Mesos","Hierarchical allocator inconsistently accounts for reserved resources. ","Looking through the allocator code for MESOS-2099, I see an issue with respect to accounting reserved resources in the sorters:  Within {{HierarchicalAllocatorProcess::allocate}}, only unreserved resources are accounted for in the sorters, whereas everywhere else (add/remove framework, add/remove slave) we account for both reserved and unreserved.  From git blame, it looks like this issue was introduced over a long course of refactoring and fixes to the allocator. My guess is that this was never caught due to the lack of unit-testability of the allocator (unnecessarily requires a master PID to use an allocator).  From my understanding, the two levels of the hierarchical sorter should have the following semantics:  # Level 1 sorts across roles. Only unreserved resources are shared across roles, and therefore the role sorter for level 1 should only account for the unreserved resource pool. # Level 2 sorts across frameworks, within a role. Both unreserved and reserved resources are shared across frameworks within a role, and therefore the framework sorters for level 2 should each account for the reserved resource pool for the role, as well as the unreserved resources _allocated_ inside the role.",Bug,Major,Resolved,"2014-12-05 19:36:39","2014-12-05 19:36:39",5
"Apache Mesos","Consolidate all fetcher env vars into one that holds a JSON object","There are 5 env vars for that transport parameters into the fetcher program. Once we have a fetcher cache, it might be 7. The env var holding the CommandInfo  already uses JSON parsing to simplify the source code. Let's extend this benefit to all of them and bundle them together for simpler processing at both ends.",Improvement,Minor,Resolved,"2014-12-04 02:03:08","2014-12-04 02:03:08",0.2
"Apache Mesos","Refactor fetcher namespace into a class","Fetching will have state once we introduce caching. In preparation for that, let's refactor the fetcher routines into a class for a singleton fetcher object per slave that will be able to hold the cache state.",Improvement,Minor,Resolved,"2014-12-04 01:58:22","2014-12-04 01:58:22",0.1
"Apache Mesos"," PerfEventIsolatorTest.ROOT_CGROUPS_Sample requires 'perf' to be installed","The perf::valid() relies on the 'perf' command being installed. This isn't always the case. Configure should probably check for the perf command exists.",Bug,Major,Resolved,"2014-12-02 23:51:47","2014-12-02 23:51:47",1
"Apache Mesos","Add /master/slaves and /master/frameworks/{framework}/tasks/{task} endpoints","master/state.json exports the entire state of the cluster and can, for large clusters, become massive (tens of megabytes of JSON). Often, a client only need information about subsets of the entire state, for example all connected slaves, or information (registration info, tasks, etc) belonging to a particular framework.  We can partition state.json into many smaller endpoints, but for starters, being able to get slave information and tasks information per framework would be useful.",Task,Trivial,Accepted,"2014-11-25 19:11:42","2014-11-25 19:11:42",5
"Apache Mesos","Large number of connections slows statistics.json responses.","We observed that in our production environment with network monitoring being turned on.  If there are many connections (> 10^4) in a container, getting socket information is expensive. It might take 1min to process all the socket information.  One of the reason is that the library we are using (libnl) is not so optimized. <USER>has already submitted a patch: http://lists.infradead.org/pipermail/libnl/2014-November/001715.html",Bug,Major,Resolved,"2014-11-20 22:19:53","2014-11-20 22:19:53",2
"Apache Mesos","Segmentation Fault in ExamplesTest.LowLevelSchedulerPthread","Occured on review bot review of: https://reviews.apache.org/r/28262/#review62333  The review doesn't touch code related to the test (And doesn't break libprocess in general)  [ RUN      ] ExamplesTest.LowLevelSchedulerPthread ../../src/tests/script.cpp:83: Failure Failed low_level_scheduler_pthread_test.sh terminated with signal Segmentation fault [  FAILED  ] ExamplesTest.LowLevelSchedulerPthread (7561 ms)  The test ",Bug,Minor,Resolved,"2014-11-20 18:56:20","2014-11-20 18:56:20",8
"Apache Mesos","Enable the master to handle reservation operations","master's {{_accept}} function currently only handles {{Create}} and {{Destroy}} operations which exist for persistent volumes. We need to handle the {{Reserve}} and {{Unreserve}} operations for dynamic reservations as well.  In addition, we need to add {{validate}} functions for the reservation operations.",Task,Major,Resolved,"2014-11-20 00:13:59","2014-11-20 00:13:59",5
"Apache Mesos","Expose per-cgroup memory pressure","The cgroup memory controller can provide information on the memory pressure of a cgroup. This is in the form of an event based notification where events of (low, medium, critical) are generated when the kernel makes specific actions to allocate memory. This signal is probably more informative than comparing memory usage to memory limit. ",Improvement,Major,Resolved,"2014-11-19 22:21:22","2014-11-19 22:21:22",5
"Apache Mesos","Support DiskInfo in C++ Resources","We need to change the following functions: 1) addable 2) subtractable 3) validate  We probably shouldn't add two disk resources with the same persistence id because they must come from different namespaces. We can add more checks in the validate functions (for protobufs).",Task,Major,Resolved,"2014-11-19 19:57:17","2014-11-19 19:57:17",3
"Apache Mesos","Turning on cgroups_limit_swap effectively disables memory isolation","Our test runs show that enabling cgroups_limit_swap effectively disables memory isolation altogether.  Per: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html  It is important to set the memory.limit_in_bytes parameter before setting the memory.memsw.limit_in_bytes parameter: attempting to do so in the reverse order results in an error. This is because memory.memsw.limit_in_bytes becomes available only after all memory limitations (previously set in memory.limit_in_bytes) are exhausted.  Looks like the flag sets memory.memsw.limit_in_bytes if true and memory.limit_in_bytes if false, but should always set memory.limit_in_bytes and in addition set memory.memsw.limit_in_bytes if true. Otherwise the limits won't be set and enforced.  See: https://github.com/apache/mesos/blob/c8598f7f5a24a01b6a68e0f060b79662ee97af89/src/slave/containerizer/isolators/cgroups/mem.cpp#L365 ",Bug,Major,Resolved,"2014-11-19 01:18:43","2014-11-19 01:18:43",2
"Apache Mesos","killTask() should perform reconciliation for unknown tasks.","Currently, {{killTask}} uses its own reconciliation logic, which has diverged from the {{reconcileTasks}} logic. Specifically, when the task is unknown and a non-strict registry is in use, {{killTask}} will not send TASK_LOST whereas {{reconcileTask}} will.  We should make these consistent. ",Improvement,Major,Resolved,"2014-11-18 22:56:01","2014-11-18 22:56:01",3
"Apache Mesos","Document changes in C++ Resources API in CHANGELOG.","With the refactor introduced in MESOS-1974, we need to document those API changes in CHANGELOG. ",Task,Major,Resolved,"2014-11-18 01:20:50","2014-11-18 01:20:50",2
"Apache Mesos","Add Socket tests","Add more Socket specific tests to get coverage while doing libev to libevent (w and wo SSL) move",Task,Major,Accepted,"2014-11-17 19:42:35","2014-11-17 19:42:35",5
"Apache Mesos","Configurable Ping Timeouts","After a series of ping-failures, the master considers the slave lost and calls shutdownSlave, requiring such a slave that reconnects to kill its tasks and re-register as a new slaveId. On the other side, after a similar timeout, the slave will consider the master lost and try to detect a new master. These timeouts are currently hardcoded constants (5 * 15s), which may not be well-suited for all scenarios. - Some clusters may tolerate a longer slave process restart period, and wouldn't want tasks to be killed upon reconnect. - Some clusters may have higher-latency networks (e.g. cross-datacenter, or for volunteer computing efforts), and would like to tolerate longer periods without communication.  We should provide flags/mechanisms on the master to control its tolerance for non-communicative slaves, and (less importantly?) on the slave to tolerate missing masters.",Improvement,Major,Resolved,"2014-11-14 02:11:08","2014-11-14 02:11:08",8
"Apache Mesos","Add configure flag or environment variable to enable SSL/libevent Socket",,Task,Major,Resolved,"2014-11-14 01:17:04","2014-11-14 01:17:04",1
"Apache Mesos","Correct naming of cgroup memory statistics","mem_rss_bytes is *not* RSS but is the total memory usage (memory.usage_in_bytes) of the cgroup, including file cache etc. Actual RSS is reported as mem_anon_bytes. These, and others, should be consistently named.",Improvement,Major,Resolved,"2014-11-13 22:50:21","2014-11-13 22:50:21",3
"Apache Mesos","Expose number of processes and threads in a container","The CFS cpu statistics (cpus_nr_throttled, cpus_nr_periods, cpus_throttled_time) are difficult to interpret. 1) nr_throttled is the number of intervals where *any* throttling occurred 2) throttled_time is the aggregate time *across all runnable tasks* (tasks in the Linux sense).  For example, in a typical 60 second sampling interval: nr_periods = 600, nr_throttled could be 60, i.e., 10% of intervals, but throttled_time could be much higher than (60/600) * 60 = 6 seconds if there is more than one task that is runnable but throttled. *Each* throttled task contributes to the total throttled time.  Small test to demonstrate throttled_time > nr_periods * quota_interval:  5 x {{'openssl speed'}} running with quota=100ms:  All 10 intervals throttled (100%) for total time of 2.8 seconds in 1 second (more than 100% of the time interval)   It would be helpful to expose the number of processes and tasks in the container cgroup. This would be at a very coarse granularity but would give some guidance.",Improvement,Major,Resolved,"2014-11-13 19:17:47","2014-11-13 19:17:47",2
"Apache Mesos","Add the persistent resources release primitive to the framework API","We are thinking about introducing a Release protobuf message which specifies persistent disk resources (w/ DiskInfo) to release. The Release message could be piggybacked on the Launch/Decline message.  This probably will overlap with the dynamic reservation work (MESOS-2018).",Task,Major,Resolved,"2014-11-13 01:06:40","2014-11-13 01:06:40",3
"Apache Mesos","Implement master to slave protocol for persistent disk resources.","We need to do the following: 1) Slave needs to send persisted resources when registering (or re-registering). 2) Master needs to send total persisted resources to slave by either re-using RunTask/UpdateFrameworkInfo or introduce new type of messages (like UpdateResources).",Task,Major,Resolved,"2014-11-13 01:00:36","2014-11-13 01:00:36",8
"Apache Mesos","Support acquiring/releasing resources with DiskInfo in allocator.","The allocator needs to be changed because the resources are changing while we acquiring or releasing persistent disk resources (resources with DiskInfo). For example, when we release a persistent disk resource, we are changing the release with DiskInfo to a resource with the DiskInfo.",Task,Major,Resolved,"2014-11-13 00:57:07","2014-11-13 00:57:07",8
"Apache Mesos","Update task validation to be after task authorization.","So that we can simply the task validation because we no longer need to check with pendingTasks.",Task,Major,Resolved,"2014-11-13 00:46:14","2014-11-13 00:46:14",3
"Apache Mesos","Update Resource protobuf with DiskInfo",,Task,Major,Resolved,"2014-11-13 00:40:04","2014-11-13 00:40:04",1
"Apache Mesos","Add support encrypted and non-encrypted communication in parallel for cluster upgrade","During cluster upgrade from non-encrypted to encrypted communication, we need to support an interim where: 1) A master can have connections to both encrypted and non-encrypted slaves 2) A slave that supports encrypted communication connects to a master that has not yet been upgraded. 3) Frameworks are encrypted but the master has not been upgraded yet. 4) Master has been upgraded but frameworks haven't. 5) A slave process has upgraded but running executor processes haven't.",Task,Critical,Resolved,"2014-11-12 01:04:22","2014-11-12 01:04:22",13
"Apache Mesos","Add documentation for maintenance primitives.","We should provide some guiding documentation around the upcoming maintenance primitives in Mesos.  Specifically, we should ensure that general users, framework developers, and operators understand the notion of maintenance in Mesos. Some guidance and recommendations for the latter two audiences will be necessary.",Documentation,Major,Resolved,"2014-11-12 00:43:52","2014-11-12 00:43:52",8
"Apache Mesos","Update the webui to include maintenance information.","The simplest thing here would probably be to include another tab in the header for maintenance information.  We could also consider adding maintenance information inline to the slaves table. Depending on how this is done, the maintenance tab could actually be a subset of the slaves table; only those slaves for which there is maintenance information.",Task,Major,Resolved,"2014-11-12 00:01:19","2014-11-12 00:01:19",5
"Apache Mesos","Add safety constraints for maintenance primitives.","In order to ensure that the maintenance primitives can be used safely by operators, we want to put a few safety mechanisms in place. Some ideas from the [design doc|https://docs.google.com/a/twitter.com/document/d/16k0lVwpSGVOyxPSyXKmGC-gbNmRlisNEe4p-fAUSojk/]:  # Prevent bad schedules from being constructed: schedules with more than x% overlap in slaves are rejected. # Prevent bad maintenance from proceeding unchecked: if x% of the slaves are not being unscheduled, or are not re-registering, cancel the schedule.  These will likely be configurable via flags.",Task,Major,Accepted,"2014-11-11 23:49:04","2014-11-11 23:49:04",8
"Apache Mesos","Add master metrics for maintenance.","We'll need metrics in order to gain visibility into the maintenance functionality. This will also allow operators to add alerting on these metrics, in particular:  # Number of scheduled hosts. # Number of active windows. # Number of expired windows. # Number of successful drains. # Number of failed drains.  As an example of an alert guideline, we would want to know the number of expired windows as a gauge to ensure that it is not growing excessively. This allows alerting to catch when operators are not properly unscheduling maintenance once it is complete.",Task,Major,"In Progress","2014-11-11 23:41:56","2014-11-11 23:41:56",3
"Apache Mesos","Scheduler driver may ACK status updates when the scheduler threw an exception","[~<USER> discovered that this can happen if the scheduler calls {{SchedulerDriver#stop}} before or while handling {{Scheduler#statusUpdate}}.  In src/sched/sched.cpp: The driver invokes {{statusUpdate}} and later checks the {{aborted}} flag to determine whether to send an ACK.   In src/java/jni/org_apache_mesos_MesosSchedulerDriver.cpp: The {{statusUpdate}} implementation checks for an exception and invokes {{driver->abort()}}.   In src/sched/sched.cpp: The {{abort()}} implementation exits early if {{status != DRIVER_RUNNING}}, and *does not set the aborted flag*.   As a result, the code will ACK despite an exception being thrown.",Bug,Critical,Resolved,"2014-11-11 21:07:20","2014-11-11 21:07:20",3
"Apache Mesos","Ensure that TASK_LOSTs for a hard slave drain (SIGUSR1) include a Reason.","For maintenance, sometimes operators will force the drain of a slave (via SIGUSR1), when deemed safe (e.g. non-critical tasks running) and/or necessary (e.g. bad hardware).  To eliminate alerting noise, we'd like to add a 'Reason' that expresses the forced drain of the slave, so that these are not considered to be a generic slave removal TASK_LOST.",Improvement,Major,Reviewable,"2014-11-11 20:52:24","2014-11-11 20:52:24",3
"Apache Mesos","Implement maintenance primitives in the Master.","The master will need to do a number of things to implement the maintenance primitives:  # For machines that have a maintenance window: #* Disambiguate machines to agents. #* For unused resources, offers must be augmented with an Unavailability. #* For used resources, inverse offers must be sent. # For inverse offers: #* Filter them before sending them again. #* For declined inverse offers, do something with the reason (store or log). # Recover the maintenance information upon failover.  Note: Some amount of this logic will need to be placed in the allocator.",Task,Major,Resolved,"2014-11-11 20:42:54","2014-11-11 20:42:54",13
"Apache Mesos","Add maintenance information to the replicated registry.","To achieve fault-tolerance for the maintenance primitives, we will need to add the maintenance information to the registry.  The registry currently stores all of the slave information, which is quite large (~ 17MB for 50,000 slaves from my testing), which results in a protobuf object that is extremely expensive to copy.  As far as I can tell, reads / writes to maintenance information is independent of reads / writes to the existing 'registry' information. So there are two approach here:  h4. Add maintenance information to 'maintenance' key:  # The advantage of this approach is that we don't further grow the large Registry object. # This approach assumes that writes to 'maintenance' are independent of writes to the 'registry'. -If these writes are not independent, this approach requires that we add transactional support to the State abstraction.- # -This approach requires adding compaction to LogStorage.- # This approach likely requires some refactoring to the Registrar.  h4. Add maintenance information to 'registry' key: (This is the chosen method.) # The advantage of this approach is that it's the easiest to implement. # This will further grow the single 'registry' object, but doesn't preclude it being split apart in the future. # This approach may require using the diff support in LogStorage and/or adding compression support to LogStorage snapshots to deal with the increased size of the registry.",Task,Major,Resolved,"2014-11-11 20:14:43","2014-11-11 20:14:43",13
"Apache Mesos","Fetcher cache test fixture","To accelerate providing good test coverage for the fetcher cache (MESOS-336), we can provide a framework that canonicalizes creating and running a number of tasks and allows easy parametrization with combinations of the following: - whether to cache or not - whether make what has been downloaded executable or not - whether to extract from an archive or not - whether to download from a file system, http, or...  We can create a simple HHTP server in the test fixture to support the latter.  Furthermore, the tests need to be robust wrt. varying numbers of StatusUpdate messages. An accumulating update message sink that reports the final state is needed.  All this has already been programmed in this patch, just needs to be rebased: https://reviews.apache.org/r/21316/",Improvement,Major,Resolved,"2014-11-11 16:59:11","2014-11-11 16:59:11",5
"Apache Mesos","Fetcher cache eviction","Delete files from the fetcher cache so that a given cache size is never exceeded. Succeed in doing so while concurrent downloads are on their way and new requests are pouring in.  Idea: measure the size of each download before it begins, make enough room before the download. This means that only download mechanisms that divulge the size before the main download will be supported. AFAWK, those in use so far have this property.   The calculation of how much space to free needs to be under concurrency control, accumulating all space needed for competing, incomplete download requests. (The Python script that performs fetcher caching for Aurora does not seem to implement this. See https://gist.github.com/<USER>f41df77510ef9d00265a, imagine several of these programs running concurrently, each one's _cache_eviction() call succeeding, each perceiving the SAME free space being available.)  Ultimately, a conflict resolution strategy is needed if just the downloads underway already exceed the cache capacity. Then, as a fallback, direct download into the work directory will be used for some tasks. TBD how to pick which task gets treated how.   At first, only support copying of any downloaded files to the work directory for task execution. This isolates the task life cycle after starting a task from cache eviction considerations.   (Later, we can add symbolic links that avoid copying. But then eviction of fetched files used by ongoing tasks must be blocked, which adds complexity. another future extension is MESOS-1667 Extract from URI while downloading into work dir). ",Improvement,Major,Resolved,"2014-11-11 15:00:34","2014-11-11 15:00:34",8
"Apache Mesos","Implement simple slave recovery behavior for fetcher cache","Clean the fetcher cache completely upon slave restart/recovery. This implements correct, albeit not ideal behavior. More efficient schemes that restore knowledge about cached files or even resume downloads can be added later. ",Improvement,Major,Resolved,"2014-11-11 11:58:34","2014-11-11 11:58:34",2
"Apache Mesos","Basic fetcher cache functionality","Add a flag to CommandInfo URI protobufs that indicates that files downloaded by the fetcher shall be cached in a repository. To be followed by MESOS-2057 for concurrency control.  Also see MESOS-336 for the overall goals for the fetcher cache.",Improvement,Major,Resolved,"2014-11-11 11:58:21","2014-11-11 11:58:21",8
"Apache Mesos","Add HTTP API to the master for maintenance operations.","Based on MESOS-1474, we'd like to provide an HTTP API on the master for the maintenance primitives in mesos.  For the MVP, we'll want something like this for manipulating the schedule:  (Note: The slashes in URLs might not be supported yet.)  A schedule might look like:   There should be firewall settings such that only those with access to master can use these endpoints.",Task,Major,Resolved,"2014-11-11 02:19:30","2014-11-11 02:19:30",8
"Apache Mesos","Add optional 'Unavailability' to resource offers to provide maintenance awareness.","In order to inform frameworks about upcoming maintenance on offered resources, per MESOS-1474, we'd like to add an optional 'Unavailability' information to offers:  ",Task,Major,Resolved,"2014-11-11 00:36:09","2014-11-11 00:36:09",3
"Apache Mesos","Add InverseOffer to Python Scheduler API.","The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add these to the Python Scheduler API is to add a new callback:    Egg / libmesos compatibility will need to be figured out here.  We may want to leave the Python binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers.",Task,Major,Resolved,"2014-11-11 00:07:27","2014-11-11 00:07:27",5
"Apache Mesos","Add InverseOffer to Java Scheduler API.","The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add these to the Java Scheduler API is to add a new callback:    JAR / libmesos compatibility will need to be figured out here.  We may want to leave the Java binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers.",Task,Major,Resolved,"2014-11-11 00:07:25","2014-11-11 00:07:25",5
"Apache Mesos","Add InverseOffer to C++ Scheduler API.","The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add these to the C++ Scheduler API is to add a new callback:    libmesos compatibility will need to be figured out here.  We may want to leave the C++ binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers.",Task,Major,Resolved,"2014-11-11 00:07:23","2014-11-11 00:07:23",5
"Apache Mesos","Add InverseOffer to Event/Call API.","The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add this is to tack it on to the OFFERS Event:  ",Task,Major,Resolved,"2014-11-10 23:57:09","2014-11-10 23:57:09",3
"Apache Mesos","Add InverseOffer protobuf message.","InverseOffer was defined as part of the maintenance work in MESOS-1474, design doc here: https://docs.google.com/document/d/16k0lVwpSGVOyxPSyXKmGC-gbNmRlisNEe4p-fAUSojk/edit?usp=sharing    This ticket is to capture the addition of the InverseOffer protobuf to mesos.proto, the necessary API changes for Event/Call and the language bindings will be tracked separately.",Task,Major,Resolved,"2014-11-10 23:51:29","2014-11-10 23:51:29",3
"Apache Mesos","Deprecate stats.json endpoints for Master and Slave","With the introduction of the libprocess {{/metrics/snapshot}} endpoint, metrics are now duplicated in the Master and Slave between this and {{stats.json}}. We should deprecate the {{stats.json}} endpoints.  Manual inspection of {{stats.json}} shows that all metrics are now covered by the new endpoint for Master and Slave.",Task,Major,Resolved,"2014-11-10 21:29:03","2014-11-10 21:29:03",1
"Apache Mesos","Concurrency control for fetcher cache","Having added a URI flag to CommandInfo messages (in MESOS-2069) that indicates caching, caching files downloaded by the fetcher in a repository, now ensure that when a URI is cached, it is only ever downloaded once for the same user on the same slave as long as the slave keeps running.   This even holds if multiple tasks request the same URI concurrently. If multiple requests for the same URI occur, perform only one of them and reuse the result. Make concurrent requests for the same URI wait for the one download.   Different URIs from different CommandInfos can be downloaded concurrently.  No cache eviction, cleanup or failover will be handled for now. Additional tickets will be filed for these enhancements. (So don't use this feature in production until the whole epic is complete.)  Note that implementing this does not suffice for production use. This ticket contains the main part of the fetcher logic, though. See the epic MESOS-336 for the rest of the features that lead to a fully functional fetcher cache.  The proposed general approach is to keep all bookkeeping about what is in which stage of being fetched and where it resides in the slave's MesosContainerizerProcess, so that all concurrent access is disambiguated and controlled by an actor (aka libprocess process).  Depends on MESOS-2056 and MESOS-2069. ",Improvement,Major,Resolved,"2014-11-10 17:32:28","2014-11-10 17:32:28",8
"Apache Mesos","Refactor fetcher code in preparation for fetcher cache","Refactor/rearrange fetcher-related code so that cache functionality can be dropped in. One could do both together in one go. This is splitting up reviews into smaller chunks. It will not immediately be obvious how this change will be used later, but it will look better-factored and still do the exact same thing as before. In particular, a download routine to be reused several times in launcher/fetcher will be factored out and the remainder of fetcher-related code can be moved from the containerizer realm into fetcher.cpp.",Improvement,Minor,Resolved,"2014-11-10 17:30:21","2014-11-10 17:30:21",1
"Apache Mesos","MesosContainerizerExecuteTest.IoRedirection test is flaky","Observed this on ASF CI:  ",Bug,Major,Resolved,"2014-11-08 01:27:58","2014-11-08 01:27:58",1
"Apache Mesos","RunState::recover should always recover 'completed'","RunState::recover() will return partial state if it cannot find or open the libprocess pid file. Specifically, it does not recover the 'completed' flag.  However, if the slave has removed the executor (because launch failed or the executor failed to register) the sentinel flag will be set and this fact should be recovered. This ensures that container recovery is not attempted later.  This was discovered when the LinuxLauncher failed to recover because it was asked to recover two containers with the same forkedPid. Investigation showed the executors both OOM'ed before registering, i.e., no libprocess pid file was present. However, the containerizer had detected the OOM, destroyed the container, and notified the slave which cleaned everything up: failing the task and calling removeExecutor (which writes the completed sentinel file.)",Bug,Major,Resolved,"2014-11-07 19:16:54","2014-11-07 19:16:54",1
"Apache Mesos","Pull Metrics struct out of Master and Slave to improve readability",,Improvement,Minor,Resolved,"2014-11-07 17:44:27","2014-11-07 17:44:27",2
"Apache Mesos","Use one IP address per container for network isolation","If there are enough IP addresses, either IPv4 or IPv6, we should use one IP address per container, instead of the ugly port range based solution. One problem with this is the IP address management, usually it is managed by a DHCP server, maybe we need to manage them in mesos master/slave.  Also, maybe use macvlan instead of veth for better isolation.",Epic,Major,Resolved,"2014-11-04 22:39:49","2014-11-04 22:39:49",40
"Apache Mesos","Framework auth fail with timeout error and never get authenticated","I'm facing this issue in master as of https://github.com/apache/mesos/commit/74ea59e144d131814c66972fb0cc14784d3503d4  As [~<USER> mentioned in IRC, this sounds similar to MESOS-1866. I'm running 1 master and 1 scheduler (aurora). The framework authentication fail due to time out:  error on mesos master:    scheduler error:   Looks like 2 instances {{scheduler-20f88a53-5945-4977-b5af-28f6c52d3c94}} & {{scheduler-d2d4437b-d375-4467-a583-362152fe065a}} of same framework is trying to authenticate and fail.   Restarting master and scheduler didn't fix it.   This particular issue happen with 1 master and 1 scheduler after MESOS-1866 is fixed.",Bug,Critical,Resolved,"2014-11-04 20:16:12","2014-11-04 20:16:12",5
"Apache Mesos","Add reason to containerizer proto Termination","When an isolator kills a task, the reason is unknown. As part of MESOS-1830, the reason is set to a general one but ideally we would have the termination reason to pass through to the status update.",Improvement,Major,Resolved,"2014-11-03 22:29:27","2014-11-03 22:29:27",5
"Apache Mesos","Documentation for isolator namespaces/pid.",,Documentation,Major,Resolved,"2014-11-03 20:00:21","2014-11-03 20:00:21",1
"Apache Mesos","Documentation for isolator filesystem/shared.",,Documentation,Major,Resolved,"2014-11-03 19:59:51","2014-11-03 19:59:51",1
"Apache Mesos","Update Maintenance design to account for persistent resources.","With persistent resources and dynamic reservations, frameworks need to know how long the resources will be unavailable for maintenance operations.  This is because for persistent resources, the framework needs to understand how long the persistent resource will be unavailable. For example, if there will be a 10 minute reboot for a kernel upgrade, the framework will not want to re-replicate all of it's persistent data on the machine. Rather, tolerating one unavailable replica for the maintenance window would be preferred.  I'd like to do a revisit of the design to ensure it works well for persistent resources as well.",Task,Major,Resolved,"2014-11-03 19:29:08","2014-11-03 19:29:08",13
"Apache Mesos","Manage persistent directories on slave.","Whenever a slave sees a persistent disk resource (in ExecutorInfo or TaskInfo) that is new to it, it will create a persistent directory which is for tasks to store persistent data.  The slave needs to do the following after it's created: 1) symlink into the executor sandbox so that tasks/executor can see it 2) garbage collect it once it is released by the framework",Task,Major,Resolved,"2014-11-03 18:25:59","2014-11-03 18:25:59",5
"Apache Mesos","Maintain persistent disk resources in master memory.","Maintain an in-memory data structure to track persistent disk resources on each slave. Update this data structure when slaves register/re-register/disconnect, etc.",Task,Major,Resolved,"2014-11-03 18:16:30","2014-11-03 18:16:30",3
"Apache Mesos","Allow slave to checkpoint resources.","The checkpointed resources are independent of the slave lifecycle. In other words, even if the slave host reboots, it'll still recover the checkpointed resources (unlike other checkpointed data). The slave needs to verify during startup that the checkpointed resources are compatible with the resources of the slave (specified using --resources flag).",Task,Major,Resolved,"2014-11-03 18:05:29","2014-11-03 18:05:29",5
"Apache Mesos","Segfault with Pure virtual method called when tests fail","The most recent one:  {noformat:title=DRFAllocatorTest.DRFAllocatorProcess} [ RUN      ] DRFAllocatorTest.DRFAllocatorProcess Using temporary directory '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j' I1030 05:55:06.934813 24459 leveldb.cpp:176] Opened db in 3.175202ms I1030 05:55:06.935925 24459 leveldb.cpp:183] Compacted db in 1.077924ms I1030 05:55:06.935976 24459 leveldb.cpp:198] Created db iterator in 16460ns I1030 05:55:06.935995 24459 leveldb.cpp:204] Seeked to beginning of db in 2018ns I1030 05:55:06.936005 24459 leveldb.cpp:273] Iterated through 0 keys in the db in 335ns I1030 05:55:06.936039 24459 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1030 05:55:06.936705 24480 recover.cpp:437] Starting replica recovery I1030 05:55:06.937023 24480 recover.cpp:463] Replica is in EMPTY status I1030 05:55:06.938158 24475 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1030 05:55:06.938859 24482 recover.cpp:188] Received a recover response from a replica in EMPTY status I1030 05:55:06.939486 24474 recover.cpp:554] Updating replica status to STARTING I1030 05:55:06.940249 24489 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 591981ns I1030 05:55:06.940274 24489 replica.cpp:320] Persisted replica status to STARTING I1030 05:55:06.940752 24481 recover.cpp:463] Replica is in STARTING status I1030 05:55:06.940820 24489 master.cpp:312] Master 20141030-055506-3142697795-40429-24459 (pomona.apache.org) started on 67.195.81.187:40429 I1030 05:55:06.940871 24489 master.cpp:358] Master only allowing authenticated frameworks to register I1030 05:55:06.940891 24489 master.cpp:363] Master only allowing authenticated slaves to register I1030 05:55:06.940908 24489 credentials.hpp:36] Loading credentials for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j/credentials' I1030 05:55:06.941215 24489 master.cpp:392] Authorization enabled I1030 05:55:06.941751 24475 master.cpp:120] No whitelist given. Advertising offers for all slaves I1030 05:55:06.942227 24474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1030 05:55:06.942401 24476 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40429 I1030 05:55:06.942895 24483 recover.cpp:188] Received a recover response from a replica in STARTING status I1030 05:55:06.943035 24474 master.cpp:1242] The newly elected leader is master@67.195.81.187:40429 with id 20141030-055506-3142697795-40429-24459 I1030 05:55:06.943063 24474 master.cpp:1255] Elected as the leading master! I1030 05:55:06.943079 24474 master.cpp:1073] Recovering from registrar I1030 05:55:06.943313 24480 registrar.cpp:313] Recovering registrar I1030 05:55:06.943455 24475 recover.cpp:554] Updating replica status to VOTING I1030 05:55:06.944144 24474 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 536365ns I1030 05:55:06.944172 24474 replica.cpp:320] Persisted replica status to VOTING I1030 05:55:06.944355 24489 recover.cpp:568] Successfully joined the Paxos group I1030 05:55:06.944576 24489 recover.cpp:452] Recover process terminated I1030 05:55:06.945155 24486 log.cpp:656] Attempting to start the writer I1030 05:55:06.947013 24473 replica.cpp:474] Replica received implicit promise request with proposal 1 I1030 05:55:06.947854 24473 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 806463ns I1030 05:55:06.947883 24473 replica.cpp:342] Persisted promised to 1 I1030 05:55:06.948547 24481 coordinator.cpp:230] Coordinator attemping to fill missing position I1030 05:55:06.950269 24479 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1030 05:55:06.950933 24479 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 603843ns I1030 05:55:06.950961 24479 replica.cpp:676] Persisted action at 0 I1030 05:55:06.952180 24476 replica.cpp:508] Replica received write request for position 0 I1030 05:55:06.952239 24476 leveldb.cpp:438] Reading position from leveldb took 28437ns I1030 05:55:06.952896 24476 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 623980ns I1030 05:55:06.952926 24476 replica.cpp:676] Persisted action at 0 I1030 05:55:06.953543 24485 replica.cpp:655] Replica received learned notice for position 0 I1030 05:55:06.954082 24485 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 511807ns I1030 05:55:06.954107 24485 replica.cpp:676] Persisted action at 0 I1030 05:55:06.954128 24485 replica.cpp:661] Replica learned NOP action at position 0 I1030 05:55:06.954710 24473 log.cpp:672] Writer started with ending position 0 I1030 05:55:06.956215 24478 leveldb.cpp:438] Reading position from leveldb took 33085ns I1030 05:55:06.959481 24475 registrar.cpp:346] Successfully fetched the registry (0B) in 16.11904ms I1030 05:55:06.959616 24475 registrar.cpp:445] Applied 1 operations in 28239ns; attempting to update the 'registry' I1030 05:55:06.962514 24487 log.cpp:680] Attempting to append 139 bytes to the log I1030 05:55:06.962646 24474 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1030 05:55:06.964146 24486 replica.cpp:508] Replica received write request for position 1 I1030 05:55:06.964962 24486 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 743389ns I1030 05:55:06.964993 24486 replica.cpp:676] Persisted action at 1 I1030 05:55:06.965895 24473 replica.cpp:655] Replica received learned notice for position 1 I1030 05:55:06.966531 24473 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 607242ns I1030 05:55:06.966555 24473 replica.cpp:676] Persisted action at 1 I1030 05:55:06.966578 24473 replica.cpp:661] Replica learned APPEND action at position 1 I1030 05:55:06.967706 24481 registrar.cpp:490] Successfully updated the 'registry' in 8.036096ms I1030 05:55:06.967895 24481 registrar.cpp:376] Successfully recovered registrar I1030 05:55:06.967993 24482 log.cpp:699] Attempting to truncate the log to 1 I1030 05:55:06.968258 24479 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1030 05:55:06.968268 24475 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1030 05:55:06.969156 24476 replica.cpp:508] Replica received write request for position 2 I1030 05:55:06.969678 24476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 491913ns I1030 05:55:06.969703 24476 replica.cpp:676] Persisted action at 2 I1030 05:55:06.970459 24478 replica.cpp:655] Replica received learned notice for position 2 I1030 05:55:06.971060 24478 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 573076ns I1030 05:55:06.971124 24478 leveldb.cpp:401] Deleting ~1 keys from leveldb took 35339ns I1030 05:55:06.971145 24478 replica.cpp:676] Persisted action at 2 I1030 05:55:06.971168 24478 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1030 05:55:06.980211 24459 containerizer.cpp:100] Using isolation: posix/cpu,posix/mem I1030 05:55:06.984153 24473 slave.cpp:169] Slave started on 203)@67.195.81.187:40429 I1030 05:55:07.055308 24473 credentials.hpp:84] Loading credential for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/credential' I1030 05:55:06.988750 24459 sched.cpp:137] Version: 0.21.0 I1030 05:55:07.055521 24473 slave.cpp:276] Slave using credential for: test-principal I1030 05:55:07.055726 24473 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):0; ports(*):[31000-32000] I1030 05:55:07.055865 24473 slave.cpp:318] Slave hostname: pomona.apache.org I1030 05:55:07.055881 24473 slave.cpp:319] Slave checkpoint: false W1030 05:55:07.055889 24473 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I1030 05:55:07.056172 24485 sched.cpp:233] New master detected at master@67.195.81.187:40429 I1030 05:55:07.056222 24485 sched.cpp:283] Authenticating with master master@67.195.81.187:40429 I1030 05:55:07.056717 24485 state.cpp:33] Recovering state from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/meta' I1030 05:55:07.056851 24475 authenticatee.hpp:133] Creating new client SASL connection I1030 05:55:07.057003 24473 status_update_manager.cpp:197] Recovering status update manager I1030 05:55:07.057252 24488 master.cpp:3853] Authenticating scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.057502 24489 containerizer.cpp:281] Recovering containerizer I1030 05:55:07.057524 24475 authenticator.hpp:161] Creating new server SASL connection I1030 05:55:07.057688 24475 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1030 05:55:07.057719 24475 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1030 05:55:07.057919 24481 authenticator.hpp:267] Received SASL authentication start I1030 05:55:07.057968 24481 authenticator.hpp:389] Authentication requires more steps I1030 05:55:07.058070 24473 authenticatee.hpp:270] Received SASL authentication step I1030 05:55:07.058199 24485 authenticator.hpp:295] Received SASL authentication step I1030 05:55:07.058223 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1030 05:55:07.058233 24485 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1030 05:55:07.058259 24485 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1030 05:55:07.058290 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1030 05:55:07.058302 24485 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.058307 24485 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.058320 24485 authenticator.hpp:381] Authentication success I1030 05:55:07.058467 24480 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.058493 24485 slave.cpp:3456] Finished recovery I1030 05:55:07.058593 24478 authenticatee.hpp:310] Authentication success I1030 05:55:07.058838 24478 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40429 I1030 05:55:07.058861 24478 sched.cpp:476] Sending registration request to master@67.195.81.187:40429 I1030 05:55:07.058969 24475 slave.cpp:602] New master detected at master@67.195.81.187:40429 I1030 05:55:07.058969 24487 status_update_manager.cpp:171] Pausing sending status updates I1030 05:55:07.059026 24475 slave.cpp:665] Authenticating with master master@67.195.81.187:40429 I1030 05:55:07.059061 24481 master.cpp:1362] Received registration request for framework 'framework1' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.059131 24481 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role 'role1' I1030 05:55:07.059171 24475 slave.cpp:638] Detecting new master I1030 05:55:07.059214 24482 authenticatee.hpp:133] Creating new client SASL connection I1030 05:55:07.059550 24481 master.cpp:3853] Authenticating slave(203)@67.195.81.187:40429 I1030 05:55:07.059787 24487 authenticator.hpp:161] Creating new server SASL connection I1030 05:55:07.059922 24481 master.cpp:1426] Registering framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.059996 24474 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1030 05:55:07.060034 24474 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1030 05:55:07.060117 24474 authenticator.hpp:267] Received SASL authentication start I1030 05:55:07.060165 24474 authenticator.hpp:389] Authentication requires more steps I1030 05:55:07.060377 24476 hierarchical_allocator_process.hpp:329] Added framework 20141030-055506-3142697795-40429-24459-0000 I1030 05:55:07.060394 24488 sched.cpp:407] Framework registered with 20141030-055506-3142697795-40429-24459-0000 I1030 05:55:07.060403 24476 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1030 05:55:07.060431 24476 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 29857ns I1030 05:55:07.060443 24488 sched.cpp:421] Scheduler::registered took 19407ns I1030 05:55:07.060545 24478 authenticatee.hpp:270] Received SASL authentication step I1030 05:55:07.060645 24478 authenticator.hpp:295] Received SASL authentication step I1030 05:55:07.060673 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1030 05:55:07.060685 24478 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1030 05:55:07.060714 24478 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1030 05:55:07.060740 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1030 05:55:07.060760 24478 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.060770 24478 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.060788 24478 authenticator.hpp:381] Authentication success I1030 05:55:07.060920 24474 authenticatee.hpp:310] Authentication success I1030 05:55:07.060945 24485 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(203)@67.195.81.187:40429 I1030 05:55:07.061388 24489 slave.cpp:722] Successfully authenticated with master master@67.195.81.187:40429 I1030 05:55:07.061504 24489 slave.cpp:1050] Will retry registration in 4.778336ms if necessary I1030 05:55:07.061718 24480 master.cpp:3032] Registering slave at slave(203)@67.195.81.187:40429 (pomona.apache.org) with id 20141030-055506-3142697795-40429-24459-S0 I1030 05:55:07.062119 24489 registrar.cpp:445] Applied 1 operations in 53691ns; attempting to update the 'registry' I1030 05:55:07.065182 24479 log.cpp:680] Attempting to append 316 bytes to the log I1030 05:55:07.065337 24487 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I1030 05:55:07.066359 24474 replica.cpp:508] Replica received write request for position 3 I1030 05:55:07.066643 24474 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 249579ns I1030 05:55:07.066671 24474 replica.cpp:676] Persisted action at 3 I../../src/tests/allocator_tests.cpp:120: Failure Failed to wait 10secs for offers1 1030 05:55:07.067101 24477 slave.cpp:1050] Will retry registration in 24.08243ms if necessary I1030 05:55:07.067140 24473 master.cpp:3020] Ignoring register slave message from slave(203)@67.195.81.187:40429 (pomona.apache.org) as admission is already in progress I1030 05:55:07.067395 24488 replica.cpp:655] Replica received learned notice for position 3 I1030 05:55:07.943416 24478 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1030 05:55:19.804687 24478 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 11.861261123secs I1030 05:55:11.942713 24474 master.cpp:120] No whitelist given. Advertising offers for all slaves I1030 05:55:19.805850 24488 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 1.067224ms I1030 05:55:19.806012 24488 replica.cpp:676] Persisted action at 3 ../../src/tests/allocator_tests.cpp:115: Failure Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(_, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1030 05:55:19.806144 24488 replica.cpp:661] Replica learned APPEND action at position 3 I1030 05:55:19.806695 24473 master.cpp:768] Framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 disconnected I1030 05:55:19.806726 24473 master.cpp:1731] Disconnecting framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.806751 24473 master.cpp:1747] Deactivating framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.806967 24473 master.cpp:790] Giving framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 0ns to failover ../../src/tests/allocator_tests.cpp:94: Failure Actual function call count doesn't match EXPECT_CALL(allocator, slaveAdded(_, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active F1030 05:55:19.806967 24480 logging.cpp:57] RAW: Pure virtual method called I1030 05:55:19.807348 24488 master.cpp:3665] Framework failover timeout, removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.807370 24488 master.cpp:4201] Removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 *** Aborted at 1414648519 (unix time) try date -d @1414648519 if you are using GNU date *** PC: @           0x91bc86 process::PID<>::PID() *** SIGSEGV (@0x0) received by PID 24459 (TID 0x2b86c919a700) from PID 0; stack trace: *** I1030 05:55:19.808631 24489 registrar.cpp:490] Successfully updated the 'registry' in 12.746377984secs     @     0x2b86c55fc340 (unknown) I1030 05:55:19.808938 24473 log.cpp:699] Attempting to truncate the log to 3     @     0x2b86c3327174  google::LogMessage::Fail() I1030 05:55:19.809084 24481 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4     @           0x91bc86 process::PID<>::PID()     @     0x2b86c332c868  google::RawLog__() I1030 05:55:19.810191 24479 replica.cpp:508] Replica received write request for position 4 I1030 05:55:19.810899 24479 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 678090ns I1030 05:55:19.810919 24479 replica.cpp:676] Persisted action at 4     @           0x91bf24 process::Process<>::self() I1030 05:55:19.811635 24485 replica.cpp:655] Replica received learned notice for position 4 I1030 05:55:19.812180 24485 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 523927ns I1030 05:55:19.812228 24485 leveldb.cpp:401] Deleting ~2 keys from leveldb took 29523ns I1030 05:55:19.812242 24485 replica.cpp:676] Persisted action at 4 I    @     0x2b86c29d2a36  __cxa_pure_virtual 1030 05:55:19.812258 24485 replica.cpp:661] Replica learned TRUNCATE action at position 4     @          0x1046936  testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith() I1030 05:55:19.829655 24474 slave.cpp:1050] Will retry registration in 31.785967ms if necessary     @           0x9c0633  testing::internal::FunctionMockerBase<>::InvokeWith()     @           0x9b6152  testing::internal::FunctionMocker<>::Invoke()     @           0x9abdeb  mesos::internal::tests::MockAllocatorProcess<>::frameworkDeactivated()     @           0x91c78f  _ZZN7process8dispatchIN5mesos8internal6master9allocator16AllocatorProcessERKNS1_11FrameworkIDES6_EEvRKNS_3PIDIT_EEMSA_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESJ_     @           0x959ad7  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal6master9allocator16AllocatorProcessERKNS5_11FrameworkIDESA_EEvRKNS0_3PIDIT_EEMSE_FvT0_ET1_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_     @     0x2b86c32d174f  std::function<>::operator()()     @     0x2b86c32b2a17  process::ProcessBase::visit()     @     0x2b86c32bd34c  process::DispatchEvent::visit()     @           0x8e0812  process::ProcessBase::serve()     @     0x2b86c32aec8c  process::ProcessManager::resume() I1030 05:55:22.050081 24478 slave.cpp:1050] Will retry registration in 25.327301ms if necessary     @     0x2b86c32a5351  process::schedule()     @     0x2b86c55f4182  start_thread     @     0x2b86c5904fbd  (unknown) {noformat}",Bug,Major,Resolved,"2014-10-30 17:16:19","2014-10-30 17:16:19",5
"Apache Mesos","MasterAuthorizationTest.DuplicateReregistration is flaky","{noformat:title=} [ RUN      ] MasterAuthorizationTest.DuplicateReregistration Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX' I1029 08:25:26.021766 32232 leveldb.cpp:176] Opened db in 3.066621ms I1029 08:25:26.022734 32232 leveldb.cpp:183] Compacted db in 935019ns I1029 08:25:26.022766 32232 leveldb.cpp:198] Created db iterator in 4350ns I1029 08:25:26.022785 32232 leveldb.cpp:204] Seeked to beginning of db in 902ns I1029 08:25:26.022799 32232 leveldb.cpp:273] Iterated through 0 keys in the db in 387ns I1029 08:25:26.022831 32232 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1029 08:25:26.023305 32248 recover.cpp:437] Starting replica recovery I1029 08:25:26.023598 32248 recover.cpp:463] Replica is in EMPTY status I1029 08:25:26.025059 32260 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1029 08:25:26.025320 32247 recover.cpp:188] Received a recover response from a replica in EMPTY status I1029 08:25:26.025585 32256 recover.cpp:554] Updating replica status to STARTING I1029 08:25:26.026546 32249 master.cpp:312] Master 20141029-082526-3142697795-40696-32232 (pomona.apache.org) started on 67.195.81.187:40696 I1029 08:25:26.026561 32261 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 694444ns I1029 08:25:26.026592 32249 master.cpp:358] Master only allowing authenticated frameworks to register I1029 08:25:26.026592 32261 replica.cpp:320] Persisted replica status to STARTING I1029 08:25:26.026605 32249 master.cpp:363] Master only allowing authenticated slaves to register I1029 08:25:26.026639 32249 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX/credentials' I1029 08:25:26.026877 32249 master.cpp:392] Authorization enabled I1029 08:25:26.026901 32260 recover.cpp:463] Replica is in STARTING status I1029 08:25:26.027498 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:26.027541 32248 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40696 I1029 08:25:26.028055 32252 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1029 08:25:26.028451 32247 recover.cpp:188] Received a recover response from a replica in STARTING status I1029 08:25:26.028733 32249 master.cpp:1242] The newly elected leader is master@67.195.81.187:40696 with id 20141029-082526-3142697795-40696-32232 I1029 08:25:26.028764 32249 master.cpp:1255] Elected as the leading master! I1029 08:25:26.028781 32249 master.cpp:1073] Recovering from registrar I1029 08:25:26.028904 32246 recover.cpp:554] Updating replica status to VOTING I1029 08:25:26.029163 32257 registrar.cpp:313] Recovering registrar I1029 08:25:26.029556 32251 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 485711ns I1029 08:25:26.029588 32251 replica.cpp:320] Persisted replica status to VOTING I1029 08:25:26.029726 32253 recover.cpp:568] Successfully joined the Paxos group I1029 08:25:26.029932 32253 recover.cpp:452] Recover process terminated I1029 08:25:26.030436 32250 log.cpp:656] Attempting to start the writer I1029 08:25:26.032152 32248 replica.cpp:474] Replica received implicit promise request with proposal 1 I1029 08:25:26.032778 32248 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 597030ns I1029 08:25:26.032807 32248 replica.cpp:342] Persisted promised to 1 I1029 08:25:26.033481 32254 coordinator.cpp:230] Coordinator attemping to fill missing position I1029 08:25:26.035429 32247 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1029 08:25:26.036154 32247 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 690208ns I1029 08:25:26.036181 32247 replica.cpp:676] Persisted action at 0 I1029 08:25:26.037344 32249 replica.cpp:508] Replica received write request for position 0 I1029 08:25:26.037395 32249 leveldb.cpp:438] Reading position from leveldb took 22607ns I1029 08:25:26.038074 32249 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 647429ns I1029 08:25:26.038105 32249 replica.cpp:676] Persisted action at 0 I1029 08:25:26.038683 32247 replica.cpp:655] Replica received learned notice for position 0 I1029 08:25:26.039378 32247 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 664911ns I1029 08:25:26.039407 32247 replica.cpp:676] Persisted action at 0 I1029 08:25:26.039433 32247 replica.cpp:661] Replica learned NOP action at position 0 I1029 08:25:26.040045 32252 log.cpp:672] Writer started with ending position 0 I1029 08:25:26.041378 32251 leveldb.cpp:438] Reading position from leveldb took 25625ns I1029 08:25:26.044642 32246 registrar.cpp:346] Successfully fetched the registry (0B) in 15.433984ms I1029 08:25:26.044742 32246 registrar.cpp:445] Applied 1 operations in 16444ns; attempting to update the 'registry' I1029 08:25:26.047538 32256 log.cpp:680] Attempting to append 139 bytes to the log I1029 08:25:26.156330 32247 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1029 08:25:26.158460 32261 replica.cpp:508] Replica received write request for position 1 I1029 08:25:26.159277 32261 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 782308ns I1029 08:25:26.159328 32261 replica.cpp:676] Persisted action at 1 I1029 08:25:26.160267 32255 replica.cpp:655] Replica received learned notice for position 1 I1029 08:25:26.161070 32255 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 750259ns I1029 08:25:26.161100 32255 replica.cpp:676] Persisted action at 1 I1029 08:25:26.161125 32255 replica.cpp:661] Replica learned APPEND action at position 1 I1029 08:25:26.162199 32253 registrar.cpp:490] Successfully updated the 'registry' in 117.40416ms I1029 08:25:26.162400 32253 registrar.cpp:376] Successfully recovered registrar I1029 08:25:26.162724 32249 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1029 08:25:26.162757 32253 log.cpp:699] Attempting to truncate the log to 1 I1029 08:25:26.162919 32256 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1029 08:25:26.163949 32250 replica.cpp:508] Replica received write request for position 2 I1029 08:25:26.164589 32250 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 603175ns I1029 08:25:26.164618 32250 replica.cpp:676] Persisted action at 2 I1029 08:25:26.165385 32251 replica.cpp:655] Replica received learned notice for position 2 I1029 08:25:26.166007 32251 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 594003ns I1029 08:25:26.166056 32251 leveldb.cpp:401] Deleting ~1 keys from leveldb took 23309ns I1029 08:25:26.166077 32251 replica.cpp:676] Persisted action at 2 I1029 08:25:26.166100 32251 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1029 08:25:26.178493 32232 sched.cpp:137] Version: 0.21.0 I1029 08:25:26.179029 32256 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:26.179078 32256 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:26.179424 32246 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:26.179678 32259 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.179970 32250 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:26.180165 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:26.180191 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:26.180272 32250 authenticator.hpp:267] Received SASL authentication start I1029 08:25:26.180378 32250 authenticator.hpp:389] Authentication requires more steps I1029 08:25:26.180557 32260 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:26.180704 32254 authenticator.hpp:295] Received SASL authentication step I1029 08:25:26.180737 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:26.180748 32254 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:26.180780 32254 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:26.180804 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:26.180816 32254 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.180824 32254 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.180841 32254 authenticator.hpp:381] Authentication success I1029 08:25:26.180937 32259 authenticatee.hpp:310] Authentication success I1029 08:25:26.180991 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.181422 32259 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:26.181449 32259 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:26.181697 32260 master.cpp:1362] Received registration request for framework 'default' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.181758 32260 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:26.182063 32260 master.cpp:1426] Registering framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.182430 32248 hierarchical_allocator_process.hpp:329] Added framework 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:26.182462 32248 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:26.182462 32261 sched.cpp:407] Framework registered with 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:26.182473 32248 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 15372ns I1029 08:25:26.182554 32261 sched.cpp:421] Scheduler::registered took 60059ns I1029 08:25:26.185515 32260 sched.cpp:227] Scheduler::disconnected took 16607ns I1029 08:25:26.185538 32260 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:26.185567 32260 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:26.185783 32246 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:26.186218 32250 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.186456 32247 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:26.186594 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:26.186621 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:26.186745 32259 authenticator.hpp:267] Received SASL authentication start I1029 08:25:26.186800 32259 authenticator.hpp:389] Authentication requires more steps I1029 08:25:26.186936 32260 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:26.187062 32249 authenticator.hpp:295] Received SASL authentication step I1029 08:25:26.187095 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:26.187108 32249 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:26.187137 32249 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:26.187162 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:26.187175 32249 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.187182 32249 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.187199 32249 authenticator.hpp:381] Authentication success I1029 08:25:26.187327 32249 authenticatee.hpp:310] Authentication success I1029 08:25:26.187366 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.187631 32249 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:26.187659 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:27.028445 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:28.045682 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.017231941secs I1029 08:25:28.045760 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:28.045900 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.045989 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:28.046455 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.046529 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:28.050155 32247 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:28.050217 32247 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:28.050405 32252 master.cpp:1552] Re-registering framework 20141029-082526-3142697795-40696-32232-0000 (default)  at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.050509 32253 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:28.050566 32252 master.cpp:1592] Allowing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 to re-register with an already used id I1029 08:25:28.051084 32257 sched.cpp:449] Framework re-registered with 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:28.051151 32252 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.051167 32257 sched.cpp:463] Scheduler::reregistered took 52801ns I1029 08:25:28.051723 32261 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:28.052042 32249 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:28.052077 32249 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:28.052170 32249 master.cpp:1534] Dropping re-registration request of framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 because new authentication attempt is in progress I1029 08:25:28.052218 32257 authenticator.hpp:267] Received SASL authentication start I1029 08:25:28.052325 32257 authenticator.hpp:389] Authentication requires more steps I1029 08:25:28.052428 32257 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:28.052641 32246 authenticator.hpp:295] Received SASL authentication step I1029 08:25:28.052685 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:28.052701 32246 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:28.052739 32246 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:28.052767 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:28.052779 32246 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:28.052788 32246 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:28.052804 32246 authenticator.hpp:381] Authentication success I1029 08:25:28.052947 32252 authenticatee.hpp:310] Authentication success I1029 08:25:28.053020 32246 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.053462 32247 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:29.046855 32261 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:29.046880 32261 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 35632ns I1029 08:25:30.047458 32253 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:30.047487 32253 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 43031ns I1029 08:25:31.028373 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:31.048673 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:31.048702 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 44769ns I1029 08:25:32.049576 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:32.049604 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 51919ns I1029 08:25:33.050864 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:33.050896 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38019ns I1029 08:25:34.051961 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:34.051993 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 64619ns I1029 08:25:35.052196 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:35.052223 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 34475ns I1029 08:25:36.029101 32259 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:36.053067 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:36.053095 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38354ns I1029 08:25:37.053506 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:37.053536 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38249ns tests/master_authorization_tests.cpp:877: Failure Failed to wait 10secs for frameworkReregisteredMessage I1029 08:25:38.053241 32259 master.cpp:768] Framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 disconnected I1029 08:25:38.053375 32259 master.cpp:1731] Disconnecting framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.053426 32259 master.cpp:1747] Deactivating framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.053932 32259 master.cpp:790] Giving framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 0ns to failover I1029 08:25:38.054072 32257 hierarchical_allocator_process.hpp:405] Deactivated framework 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:38.054208 32257 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:38.054236 32257 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38534ns I1029 08:25:38.054508 32258 master.cpp:3665] Framework failover timeout, removing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.054549 32258 master.cpp:4201] Removing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.055179 32252 master.cpp:677] Master terminating I1029 08:25:38.055181 32254 hierarchical_allocator_process.hpp:360] Removed framework 20141029-082526-3142697795-40696-32232-0000 ../3rdparty/libprocess/include/process/gmock.hpp:345: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <B8-BD 01-88 4A-2B 00-00>, 1-byte object <95>, 1-byte object <30>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] MasterAuthorizationTest.DuplicateReregistration (12042 ms) {noformat}",Bug,Major,Resolved,"2014-10-29 18:17:52","2014-10-29 18:17:52",2
"Apache Mesos","AllocatorTest/0.SlaveReregistersFirst is flaky","{noformat:title=} [ RUN      ] AllocatorTest/0.SlaveReregistersFirst Using temporary directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d' I1028 23:48:22.360447 31190 leveldb.cpp:176] Opened db in 2.192575ms I1028 23:48:22.361253 31190 leveldb.cpp:183] Compacted db in 760753ns I1028 23:48:22.361320 31190 leveldb.cpp:198] Created db iterator in 22188ns I1028 23:48:22.361340 31190 leveldb.cpp:204] Seeked to beginning of db in 1950ns I1028 23:48:22.361351 31190 leveldb.cpp:273] Iterated through 0 keys in the db in 345ns I1028 23:48:22.361403 31190 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1028 23:48:22.362185 31217 recover.cpp:437] Starting replica recovery I1028 23:48:22.362764 31219 recover.cpp:463] Replica is in EMPTY status I1028 23:48:22.363955 31210 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1028 23:48:22.364320 31217 recover.cpp:188] Received a recover response from a replica in EMPTY status I1028 23:48:22.364820 31211 recover.cpp:554] Updating replica status to STARTING I1028 23:48:22.365365 31215 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418991ns I1028 23:48:22.365391 31215 replica.cpp:320] Persisted replica status to STARTING I1028 23:48:22.365617 31217 recover.cpp:463] Replica is in STARTING status I1028 23:48:22.366328 31206 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043 I1028 23:48:22.366377 31206 master.cpp:358] Master only allowing authenticated frameworks to register I1028 23:48:22.366391 31206 master.cpp:363] Master only allowing authenticated slaves to register I1028 23:48:22.366402 31206 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials' I1028 23:48:22.366708 31206 master.cpp:392] Authorization enabled I1028 23:48:22.366886 31209 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1028 23:48:22.367311 31208 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:22.367312 31207 recover.cpp:188] Received a recover response from a replica in STARTING status I1028 23:48:22.367686 31211 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043 I1028 23:48:22.367863 31212 recover.cpp:554] Updating replica status to VOTING I1028 23:48:22.368477 31218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 375527ns I1028 23:48:22.368505 31218 replica.cpp:320] Persisted replica status to VOTING I1028 23:48:22.368517 31204 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190 I1028 23:48:22.368549 31204 master.cpp:1255] Elected as the leading master! I1028 23:48:22.368567 31204 master.cpp:1073] Recovering from registrar I1028 23:48:22.368621 31215 recover.cpp:568] Successfully joined the Paxos group I1028 23:48:22.368716 31219 registrar.cpp:313] Recovering registrar I1028 23:48:22.369000 31215 recover.cpp:452] Recover process terminated I1028 23:48:22.369523 31208 log.cpp:656] Attempting to start the writer I1028 23:48:22.370909 31205 replica.cpp:474] Replica received implicit promise request with proposal 1 I1028 23:48:22.371266 31205 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 325016ns I1028 23:48:22.371290 31205 replica.cpp:342] Persisted promised to 1 I1028 23:48:22.371979 31218 coordinator.cpp:230] Coordinator attemping to fill missing position I1028 23:48:22.373378 31210 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1028 23:48:22.373746 31210 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 329018ns I1028 23:48:22.373772 31210 replica.cpp:676] Persisted action at 0 I1028 23:48:22.374897 31214 replica.cpp:508] Replica received write request for position 0 I1028 23:48:22.374951 31214 leveldb.cpp:438] Reading position from leveldb took 26002ns I1028 23:48:22.375272 31214 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 289094ns I1028 23:48:22.375298 31214 replica.cpp:676] Persisted action at 0 I1028 23:48:22.375886 31204 replica.cpp:655] Replica received learned notice for position 0 I1028 23:48:22.376258 31204 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 346650ns I1028 23:48:22.376277 31204 replica.cpp:676] Persisted action at 0 I1028 23:48:22.376298 31204 replica.cpp:661] Replica learned NOP action at position 0 I1028 23:48:22.376843 31215 log.cpp:672] Writer started with ending position 0 I1028 23:48:22.378056 31205 leveldb.cpp:438] Reading position from leveldb took 28265ns I1028 23:48:22.380323 31217 registrar.cpp:346] Successfully fetched the registry (0B) in 11.55584ms I1028 23:48:22.380466 31217 registrar.cpp:445] Applied 1 operations in 50632ns; attempting to update the 'registry' I1028 23:48:22.382472 31217 log.cpp:680] Attempting to append 139 bytes to the log I1028 23:48:22.382715 31210 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1028 23:48:22.383463 31210 replica.cpp:508] Replica received write request for position 1 I1028 23:48:22.383857 31210 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 363758ns I1028 23:48:22.383875 31210 replica.cpp:676] Persisted action at 1 I1028 23:48:22.384397 31218 replica.cpp:655] Replica received learned notice for position 1 I1028 23:48:22.384840 31218 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 420161ns I1028 23:48:22.384862 31218 replica.cpp:676] Persisted action at 1 I1028 23:48:22.384882 31218 replica.cpp:661] Replica learned APPEND action at position 1 I1028 23:48:22.385684 31211 registrar.cpp:490] Successfully updated the 'registry' in 5.158144ms I1028 23:48:22.385818 31211 registrar.cpp:376] Successfully recovered registrar I1028 23:48:22.385912 31214 log.cpp:699] Attempting to truncate the log to 1 I1028 23:48:22.386101 31218 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1028 23:48:22.386124 31211 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1028 23:48:22.387398 31209 replica.cpp:508] Replica received write request for position 2 I1028 23:48:22.387758 31209 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 334969ns I1028 23:48:22.387776 31209 replica.cpp:676] Persisted action at 2 I1028 23:48:22.388272 31204 replica.cpp:655] Replica received learned notice for position 2 I1028 23:48:22.388453 31204 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 159390ns I1028 23:48:22.388501 31204 leveldb.cpp:401] Deleting ~1 keys from leveldb took 30409ns I1028 23:48:22.388516 31204 replica.cpp:676] Persisted action at 2 I1028 23:48:22.388531 31204 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1028 23:48:22.400737 31207 slave.cpp:169] Slave started on 34)@67.195.81.190:50043 I1028 23:48:22.400786 31207 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/credential' I1028 23:48:22.400996 31207 slave.cpp:276] Slave using credential for: test-principal I1028 23:48:22.401304 31207 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:22.401413 31207 slave.cpp:318] Slave hostname: pietas.apache.org I1028 23:48:22.401520 31207 slave.cpp:319] Slave checkpoint: false W1028 23:48:22.401535 31207 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I1028 23:48:22.402349 31207 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/meta' I1028 23:48:22.402678 31207 status_update_manager.cpp:197] Recovering status update manager I1028 23:48:22.403048 31211 slave.cpp:3456] Finished recovery I1028 23:48:22.403815 31215 slave.cpp:602] New master detected at master@67.195.81.190:50043 I1028 23:48:22.403852 31215 slave.cpp:665] Authenticating with master master@67.195.81.190:50043 I1028 23:48:22.403875 31206 status_update_manager.cpp:171] Pausing sending status updates I1028 23:48:22.403961 31215 slave.cpp:638] Detecting new master I1028 23:48:22.404016 31211 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:22.404230 31204 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043 I1028 23:48:22.404464 31205 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:22.404613 31211 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:22.404649 31211 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:22.404734 31211 authenticator.hpp:267] Received SASL authentication start I1028 23:48:22.404783 31211 authenticator.hpp:389] Authentication requires more steps I1028 23:48:22.404898 31215 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:22.404999 31215 authenticator.hpp:295] Received SASL authentication step I1028 23:48:22.405030 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:22.405047 31215 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:22.405086 31215 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:22.405109 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:22.405122 31215 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.405129 31215 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.405146 31215 authenticator.hpp:381] Authentication success I1028 23:48:22.405243 31213 authenticatee.hpp:310] Authentication success I1028 23:48:22.405253 31214 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043 I1028 23:48:22.405505 31213 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:22.405619 31213 slave.cpp:1050] Will retry registration in 17.050994ms if necessary I1028 23:48:22.405819 31215 master.cpp:3032] Registering slave at slave(34)@67.195.81.190:50043 (pietas.apache.org) with id 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.406262 31216 registrar.cpp:445] Applied 1 operations in 52647ns; attempting to update the 'registry' I1028 23:48:22.406697 31190 sched.cpp:137] Version: 0.21.0 I1028 23:48:22.407083 31211 sched.cpp:233] New master detected at master@67.195.81.190:50043 I1028 23:48:22.407114 31211 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:22.407290 31214 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:22.407424 31214 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.407659 31207 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:22.407757 31207 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:22.407774 31207 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:22.407830 31207 authenticator.hpp:267] Received SASL authentication start I1028 23:48:22.407868 31207 authenticator.hpp:389] Authentication requires more steps I1028 23:48:22.407927 31207 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:22.408015 31212 authenticator.hpp:295] Received SASL authentication step I1028 23:48:22.408037 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:22.408046 31212 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:22.408072 31212 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:22.408092 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:22.408100 31212 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.408105 31212 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.408116 31212 authenticator.hpp:381] Authentication success I1028 23:48:22.408192 31210 authenticatee.hpp:310] Authentication success I1028 23:48:22.408210 31217 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.408419 31210 sched.cpp:357] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:22.408460 31210 sched.cpp:476] Sending registration request to master@67.195.81.190:50043 I1028 23:48:22.408568 31217 master.cpp:1362] Received registration request for framework 'default' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.408617 31217 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1028 23:48:22.408937 31214 master.cpp:1426] Registering framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.409265 31213 sched.cpp:407] Framework registered with 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.409267 31212 hierarchical_allocator_process.hpp:329] Added framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.409312 31212 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1028 23:48:22.409324 31215 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:22.409333 31213 sched.cpp:421] Scheduler::registered took 38591ns I1028 23:48:22.409327 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 24107ns I1028 23:48:22.409518 31205 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I1028 23:48:22.410127 31206 replica.cpp:508] Replica received write request for position 3 I1028 23:48:22.410706 31206 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554098ns I1028 23:48:22.410725 31206 replica.cpp:676] Persisted action at 3 I1028 23:48:22.411151 31217 replica.cpp:655] Replica received learned notice for position 3 I1028 23:48:22.411499 31217 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 326572ns I1028 23:48:22.411519 31217 replica.cpp:676] Persisted action at 3 I1028 23:48:22.411533 31217 replica.cpp:661] Replica learned APPEND action at position 3 I1028 23:48:22.412292 31219 registrar.cpp:490] Successfully updated the 'registry' in 5.972992ms I1028 23:48:22.412518 31218 log.cpp:699] Attempting to truncate the log to 3 I1028 23:48:22.412621 31213 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I1028 23:48:22.412734 31219 slave.cpp:2522] Received ping from slave-observer(38)@67.195.81.190:50043 I1028 23:48:22.412787 31206 master.cpp:3086] Registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:22.412858 31219 slave.cpp:756] Registered with master master@67.195.81.190:50043; given slave ID 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.412994 31210 status_update_manager.cpp:178] Resuming sending status updates I1028 23:48:22.413014 31211 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available) I1028 23:48:22.413159 31211 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] on slave 20141028-234822-3193029443-50043-31190-S0 to framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.413290 31208 replica.cpp:508] Replica received write request for position 4 I1028 23:48:22.413421 31211 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 346658ns I1028 23:48:22.413650 31208 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 336067ns I1028 23:48:22.413668 31208 replica.cpp:676] Persisted action at 4 I1028 23:48:22.413797 31216 master.cpp:3795] Sending 1 offers to framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.414077 31212 replica.cpp:655] Replica received learned notice for position 4 I1028 23:48:22.414356 31212 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260401ns I1028 23:48:22.414403 31212 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28541ns I1028 23:48:22.414417 31212 replica.cpp:676] Persisted action at 4 I1028 23:48:22.414446 31212 replica.cpp:661] Replica learned TRUNCATE action at position 4 I1028 23:48:22.414422 31207 sched.cpp:544] Scheduler::resourceOffers took 310278ns I1028 23:48:22.415086 31214 master.cpp:2321] Processing reply for offers: [ 20141028-234822-3193029443-50043-31190-O0 ] on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) for framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 W1028 23:48:22.415163 31214 master.cpp:1969] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W1028 23:48:22.415186 31214 master.cpp:1980] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I1028 23:48:22.415256 31214 master.cpp:2417] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I1028 23:48:22.416033 31219 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) I1028 23:48:22.416084 31219 master.cpp:2480] Launching task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.416317 31214 slave.cpp:1081] Got assigned task 0 for framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.416679 31215 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.416721 31215 hierarchical_allocator_process.hpp:599] Framework 20141028-234822-3193029443-50043-31190-0000 filtered slave 20141028-234822-3193029443-50043-31190-S0 for 5secs I1028 23:48:22.416724 31214 slave.cpp:1191] Launching task 0 for framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.418534 31214 slave.cpp:3871] Launching executor default of framework 20141028-234822-3193029443-50043-31190-0000 in work directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.420557 31214 exec.cpp:132] Version: 0.21.0 I1028 23:48:22.420755 31213 exec.cpp:182] Executor started at: executor(22)@67.195.81.190:50043 with pid 31190 I1028 23:48:22.420903 31214 slave.cpp:1317] Queuing task '0' for executor default of framework '20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.420997 31214 slave.cpp:555] Successfully attached file '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.421058 31214 slave.cpp:1849] Got registration for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043 I1028 23:48:22.421295 31214 slave.cpp:1968] Flushing queued task 0 for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.421391 31205 exec.cpp:206] Executor registered on slave 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.421495 31214 slave.cpp:2802] Monitoring executor 'default' of framework '20141028-234822-3193029443-50043-31190-0000' in container 'd593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.422873 31205 exec.cpp:218] Executor::registered took 19148ns I1028 23:48:22.422991 31205 exec.cpp:293] Executor asked to run task '0' I1028 23:48:22.423085 31205 exec.cpp:302] Executor::launchTask took 76519ns I1028 23:48:22.424541 31205 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.424724 31205 slave.cpp:2202] Handling status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043 I1028 23:48:22.424932 31213 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.424963 31213 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425122 31213 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to the slave I1028 23:48:22.425257 31205 slave.cpp:2442] Forwarding the update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to master@67.195.81.190:50043 I1028 23:48:22.425398 31205 slave.cpp:2369] Status update manager successfully handled status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425420 31205 slave.cpp:2375] Sending acknowledgement for status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to executor(22)@67.195.81.190:50043 I1028 23:48:22.425583 31212 master.cpp:3410] Forwarding status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425621 31206 exec.cpp:339] Executor received status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425786 31212 master.cpp:3382] Status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.425832 31212 master.cpp:4617] Updating the latest state of task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to TASK_RUNNING I1028 23:48:22.425885 31208 sched.cpp:635] Scheduler::statusUpdate took 49727ns I1028 23:48:22.426082 31208 master.cpp:2882] Forwarding status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 to slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.426360 31206 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.426623 31206 slave.cpp:1789] Status update manager successfully handled status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.426893 31210 master.cpp:677] Master terminating W1028 23:48:22.427028 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING I1028 23:48:22.427397 31209 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.427512 31210 master.cpp:4705] Removing executor 'default' with resources  of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.428129 31206 slave.cpp:2607] master@67.195.81.190:50043 exited W1028 23:48:22.428153 31206 slave.cpp:2610] Master disconnected! Waiting for a new master to be elected I1028 23:48:22.434645 31190 leveldb.cpp:176] Opened db in 2.551453ms I1028 23:48:22.437157 31190 leveldb.cpp:183] Compacted db in 2.484612ms I1028 23:48:22.437203 31190 leveldb.cpp:198] Created db iterator in 19171ns I1028 23:48:22.437235 31190 leveldb.cpp:204] Seeked to beginning of db in 18300ns I1028 23:48:22.437306 31190 leveldb.cpp:273] Iterated through 3 keys in the db in 59465ns I1028 23:48:22.437347 31190 replica.cpp:741] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned I1028 23:48:22.437827 31216 recover.cpp:437] Starting replica recovery I1028 23:48:22.438127 31216 recover.cpp:463] Replica is in VOTING status I1028 23:48:22.438443 31216 recover.cpp:452] Recover process terminated I1028 23:48:22.439877 31212 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043 I1028 23:48:22.439916 31212 master.cpp:358] Master only allowing authenticated frameworks to register I1028 23:48:22.439931 31212 master.cpp:363] Master only allowing authenticated slaves to register I1028 23:48:22.439946 31212 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials' I1028 23:48:22.440142 31212 master.cpp:392] Authorization enabled I1028 23:48:22.440439 31218 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:22.440901 31213 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043 I1028 23:48:22.441395 31206 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190 I1028 23:48:22.441421 31206 master.cpp:1255] Elected as the leading master! I1028 23:48:22.441457 31206 master.cpp:1073] Recovering from registrar I1028 23:48:22.441623 31205 registrar.cpp:313] Recovering registrar I1028 23:48:22.442172 31219 log.cpp:656] Attempting to start the writer I1028 23:48:22.443235 31219 replica.cpp:474] Replica received implicit promise request with proposal 2 I1028 23:48:22.443685 31219 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 427888ns I1028 23:48:22.443703 31219 replica.cpp:342] Persisted promised to 2 I1028 23:48:22.444371 31213 coordinator.cpp:230] Coordinator attemping to fill missing position I1028 23:48:22.444687 31209 log.cpp:672] Writer started with ending position 4 I1028 23:48:22.445754 31215 leveldb.cpp:438] Reading position from leveldb took 47909ns I1028 23:48:22.445826 31215 leveldb.cpp:438] Reading position from leveldb took 30611ns I1028 23:48:22.446941 31218 registrar.cpp:346] Successfully fetched the registry (277B) in 5.213184ms I1028 23:48:22.447118 31218 registrar.cpp:445] Applied 1 operations in 42362ns; attempting to update the 'registry' I1028 23:48:22.449329 31204 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:22.449477 31218 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5 I1028 23:48:22.450187 31215 replica.cpp:508] Replica received write request for position 5 I1028 23:48:22.450767 31215 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554400ns I1028 23:48:22.450788 31215 replica.cpp:676] Persisted action at 5 I1028 23:48:22.451561 31215 replica.cpp:655] Replica received learned notice for position 5 I1028 23:48:22.451979 31215 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 397219ns I1028 23:48:22.452000 31215 replica.cpp:676] Persisted action at 5 I1028 23:48:22.452020 31215 replica.cpp:661] Replica learned APPEND action at position 5 I1028 23:48:22.452993 31213 registrar.cpp:490] Successfully updated the 'registry' in 5.816832ms I1028 23:48:22.453136 31213 registrar.cpp:376] Successfully recovered registrar I1028 23:48:22.453238 31208 log.cpp:699] Attempting to truncate the log to 5 I1028 23:48:22.453384 31214 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6 I1028 23:48:22.453518 31215 master.cpp:1100] Recovered 1 slaves from the Registry (277B) ; allowing 10mins for slaves to re-register I1028 23:48:22.454116 31207 replica.cpp:508] Replica received write request for position 6 I1028 23:48:22.454570 31207 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 427424ns I1028 23:48:22.454589 31207 replica.cpp:676] Persisted action at 6 I1028 23:48:22.455095 31219 replica.cpp:655] Replica received learned notice for position 6 I1028 23:48:22.455399 31219 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 282466ns I1028 23:48:22.455462 31219 leveldb.cpp:401] Deleting ~2 keys from leveldb took 43939ns I1028 23:48:22.455478 31219 replica.cpp:676] Persisted action at 6 I1028 23:48:22.455494 31219 replica.cpp:661] Replica learned TRUNCATE action at position 6 I1028 23:48:22.465553 31213 status_update_manager.cpp:171] Pausing sending status updates I1028 23:48:22.465566 31216 slave.cpp:602] New master detected at master@67.195.81.190:50043 I1028 23:48:22.465612 31216 slave.cpp:665] Authenticating with master master@67.195.81.190:50043 I1028 23:48:23.441506 31206 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1028 23:48:27.441004 31214 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:30.101379 31206 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 6.659877806secs I1028 23:48:30.101568 31216 slave.cpp:638] Detecting new master I1028 23:48:30.101632 31214 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:30.102021 31218 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043 I1028 23:48:30.102329 31212 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:30.102505 31216 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:30.102545 31216 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:30.102638 31216 authenticator.hpp:267] Received SASL authentication start I1028 23:48:30.102709 31216 authenticator.hpp:389] Authentication requires more steps I1028 23:48:30.102812 31216 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:30.102957 31204 authenticator.hpp:295] Received SASL authentication step I1028 23:48:30.102982 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:30.102993 31204 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:30.103032 31204 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:30.103049 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:30.103056 31204 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:30.103061 31204 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:30.103073 31204 authenticator.hpp:381] Authentication success I1028 23:48:30.103149 31209 authenticatee.hpp:310] Authentication success I1028 23:48:30.103153 31204 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043 I1028 23:48:30.103371 31209 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:30.103773 31209 slave.cpp:1050] Will retry registration in 12.861518ms if necessary I1028 23:48:30.104068 31219 master.cpp:3210] Re-registering slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:30.104760 31216 registrar.cpp:445] Applied 1 operations in 71655ns; attempting to update the 'registry' I1028 23:48:30.107877 31205 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:30.108070 31219 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7 I1028 23:48:30.109110 31211 replica.cpp:508] Replica received write request for position 7 I1028 23:48:30.109434 31211 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 281545ns I1028 23:48:30.109484 31211 replica.cpp:676] Persisted action at 7 I1028 23:48:30.110124 31219 replica.cpp:655] Replica received learned notice for position 7 I1028 23:48:30.110903 31219 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 750414ns I1028 23:48:30.110927 31219 replica.cpp:676] Persisted action at 7 I1028 23:48:30.110950 31219 replica.cpp:661] Replica learned APPEND action at position 7 I1028 23:48:30.112160 31205 registrar.cpp:490] Successfully updated the 'registry' in 7.33824ms I1028 23:48:30.112529 31217 log.cpp:699] Attempting to truncate the log to 7 I1028 23:48:30.112714 31207 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8 I1028 23:48:30.112870 31210 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) W1028 23:48:30.113136 31210 master.cpp:4394] Possibly orphaned task 0 of framework 20141028-234822-3193029443-50043-31190-0000 running on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:30.113198 31219 slave.cpp:2522] Received ping from slave-observer(39)@67.195.81.190:50043 I1028 23:48:30.113340 31210 master.cpp:3278] Re-registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:30.113499 31219 slave.cpp:824] Re-registered with master master@67.195.81.190:50043 I1028 23:48:30.113636 31219 replica.cpp:508] Replica received write request for position 8 I1028 23:48:30.113652 31210 status_update_manager.cpp:178] Resuming sending status updates I1028 23:48:30.113759 31212 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] available) I1028 23:48:30.113904 31212 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 74698ns I1028 23:48:30.114116 31219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 452165ns I1028 23:48:30.114142 31219 replica.cpp:676] Persisted action at 8 I1028 23:48:30.114786 31213 replica.cpp:655] Replica received learned notice for position 8 I1028 23:48:30.115337 31213 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 525187ns I1028 23:48:30.115399 31213 leveldb.cpp:401] Deleting ~2 keys from leveldb took 37689ns I1028 23:48:30.115418 31213 replica.cpp:676] Persisted action at 8 I1028 23:48:30.115484 31213 replica.cpp:661] Replica learned TRUNCATE action at position 8 I1028 23:48:30.116603 31212 sched.cpp:227] Scheduler::disconnected took 16969ns I1028 23:48:30.116624 31212 sched.cpp:233] New master detected at master@67.195.81.190:50043 I1028 23:48:30.116657 31212 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:30.116870 31205 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:30.117084 31207 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:30.117279 31212 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:30.117410 31210 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:30.117507 31210 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:30.117604 31214 authenticator.hpp:267] Received SASL authentication start I1028 23:48:30.117652 31214 authenticator.hpp:389] Authentication requires more steps I1028 23:48:30.117738 31210 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:30.117905 31208 authenticator.hpp:295] Received SASL authentication step I1028 23:48:30.117935 31208 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:30.117947 31208 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:30.117979 31208 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:30.118001 31208 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I../../src/tests/allocator_tests.cpp:2405: Failure 1028 23:48:30.118013 31208 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true Failed to wait 10secs for resourceOffers2 I1028 23:48:31.101976 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 124354ns I1028 23:48:58.775811 31208 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true W1028 23:48:35.117725 31214 sched.cpp:378] Authentication timed out W1028 23:48:35.117784 31219 master.cpp:3911] Authentication timed out I1028 23:48:45.114322 31213 slave.cpp:2522] Received ping from slave-observer(39)@67.195.81.190:50043 I1028 23:48:35.102212 31206 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:58.775874 31208 authenticator.hpp:381] Authentication success I1028 23:48:58.776267 31214 sched.cpp:338] Failed to authenticate with master master@67.195.81.190:50043: Authentication discarded ../../src/tests/allocator_tests.cpp:2396: Failure Actual function call count doesn't match EXPECT_CALL(allocator2, frameworkAdded(_, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1028 23:48:58.776526 31204 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:58.776626 31214 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:58.776928 31204 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:58.777194 31210 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 W1028 23:48:58.777528 31210 master.cpp:3888] Failed to authenticate scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043: Failed to communicate with authenticatee ../../src/tests/allocator_tests.cpp:2399: Failure Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(&driver, _))...          Expected: to be called once            Actual: never called - unsatisfied and active ../../src/tests/allocator_tests.cpp:2394: Failure Actual function call count doesn't match EXPECT_CALL(sched, registered(&driver, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1028 23:48:58.778053 31205 slave.cpp:591] Re-detecting master I1028 23:48:58.778084 31205 slave.cpp:638] Detecting new master I1028 23:48:58.778115 31207 status_update_manager.cpp:171] Pausing sending status updates F1028 23:48:58.778115 31205 logging.cpp:57] RAW: Pure virtual method called I1028 23:48:58.778724 31210 master.cpp:677] Master terminating W1028 23:48:58.778919 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING *** Aborted at 1414540138 (unix time) try date -d @1414540138 if you are using GNU date *** PC: @           0x91bc86 process::PID<>::PID() *** SIGSEGV (@0x0) received by PID 31190 (TID 0x2b20a6d95700) from PID 0; stack trace: ***     @     0x2b20a41ff340 (unknown)     @     0x2b20a1f2a188  google::LogMessage::Fail()     @     0x2b20a1f2f87c  google::RawLog__()     @           0x91bc86 process::PID<>::PID()     @           0x91bf24 process::Process<>::self()     @     0x2b20a15d5c06  __cxa_pure_virtual     @     0x2b20a1877752  mesos::internal::slave::Slave::detected()     @     0x2b20a1671f24 process::dispatch<>()     @     0x2b20a18b35f9  _ZZN7process8dispatchIN5mesos8internal5slave5SlaveERKNS_6FutureI6OptionINS1_10MasterInfoEEEES9_EEvRKNS_3PIDIT_EEMSD_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESM_     @     0x2b20a1663217 mesos::internal::master::allocator::Allocator::resourcesRecovered()     @     0x2b20a1650d01 mesos::internal::master::Master::removeTask()     @     0x2b20a162fb41 mesos::internal::master::Master::finalize()     @     0x2b20a1eb69a1 process::ProcessBase::visit()     @     0x2b20a1ec0464 process::TerminateEvent::visit()     @           0x8e0812 process::ProcessBase::serve()     @     0x2b20a18da89e  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal5slave5SlaveERKNS0_6FutureI6OptionINS5_10MasterInfoEEEESD_EEvRKNS0_3PIDIT_EEMSH_FvT0_ET1_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_     @     0x2b20a1eb1ca0 process::ProcessManager::resume()     @     0x2b20a1ea8365 process::schedule()     @     0x2b20a41f7182 start_thread     @     0x2b20a4507fbd (unknown) make[3]: *** [check-local] Segmentation fault {noformat}",Bug,Major,Resolved,"2014-10-29 17:42:16","2014-10-29 17:42:16",2
"Apache Mesos","Container network stats reported by the port mapping isolator is the reverse of the actual network stats.","Looks like the TX/RX network stats reported is the reverse of the actual network stats. The reason is because we simply get TX/RX data from veth on the host.  Since veth pair is a tunnel, the ingress of veth on host is the egress of eth0 in container (and vice versa). Therefore, we need to flip the data we got from veth.  ",Bug,Major,Resolved,"2014-10-24 01:57:34","2014-10-24 00:57:34",1
"Apache Mesos","Documentation for Egress Control Limit",,Documentation,Minor,Resolved,"2014-10-23 23:36:44","2014-10-23 22:36:44",1
"Apache Mesos","Refactor the C++ Resources abstraction for DiskInfo","As we introduce DiskInfo and reservation for Resource. We need to change the C++ Resources abstraction to properly deal with merge/split of resources with those additional fields.  Also, the existing C++ 'Resources' interfaces are poorly designed. Some of them are confusing and unintuitive. Some of them are overloaded with too many functionalities. For instance,    This interface in non-intuitive because A <= B doesn't imply !(B <= A).    This one is also non-intuitive because if 'left' is not compatible with 'right', the result is 'left' (why not right???). Similar for operator '-'.    This one assume Resources is flattened, but it might not be.  As we start to introduce persistent disk resources (MESOS-1554), things will get more complicated. For example, one may want to get two types of 'disk()' functions: one returns the ephemeral disk bytes (with no disk info), one returns the total disk bytes (including ones that have disk info). We may wanna introduce a concept about Resource that indicates that a resource cannot be merged or split (e.g., atomic?).  Since we need to change this class anyway. I wanna take this chance to refactor it.",Improvement,Major,Resolved,"2014-10-23 19:41:20","2014-10-23 18:41:20",8
"Apache Mesos","Move TASK_LOST generations due to invalid tasks from scheduler driver to master","As we move towards pure scheduler/executor clients, it is imperative that the scheduler driver doesn't do validation of tasks and generate TASK_LOST messages itself. All that logic should live in the master. Schedulers should reconcile dropped messages via reconciliation. ",Improvement,Major,Resolved,"2014-10-23 18:18:25","2014-10-23 17:18:25",3
"Apache Mesos","slave and offer ids are indistinguishable in the logs","It is currently impossible to tell slave ids and offer ids apart when looking at logs. Adding some differentiator will make log reading a little simpler.",Bug,Minor,Resolved,"2014-10-22 21:29:46","2014-10-22 20:29:46",1
"Apache Mesos","RBT only takes revision ranges as args for versions >= 0.6","the {{support/post-reviews.py}} script doesn't differentiate between RBT versions although the calling conventions for passing revision ranges are different. ",Bug,Major,Resolved,"2014-10-22 21:25:25","2014-10-22 20:25:25",1
"Apache Mesos","Test RoutingTest.INETSockets fails on some machine",,Bug,Major,Resolved,"2014-10-22 19:10:00","2014-10-22 18:10:00",2
"Apache Mesos","0.21.0 release","Mesos release 0.21.0 will include the following major feature(s):  - Provide state reconciliation for frameworks. [(MESOS-1407)|https://issues.apache.org/jira/browse/MESOS-1407]  Possible features to include: - Isolation of system directories (/tmp) for Mesos containers [(MESOS-1586)|https://issues.apache.org/jira/browse/MESOS-1586] - Expose reason for TASK_KILLED [(1930)|https://issues.apache.org/jira/browse/MESOS-1930]  This ticket will be used to track blockers to this release. ",Task,Major,Resolved,"2014-10-22 05:00:54","2014-10-22 04:00:54",5
"Apache Mesos","Specification for Executor and Task life cycles in Slave","We should create a precise specification of what the Mesos source code is supposed to be implementing wrt. the life cycle of executors and tasks. And in addition, we should document why certain design decisions have been made one way or another, to provide guidance for future code changes.  With such a source code-independent specification, we could write unbiased regression and scale tests, which would be instrumental in maintaining high quality.  Furthermore, this would make the source code more amenable.  Why pick this particular area of the source code? Shouldn't more of Mesos have a thorough specification? Probably so. But we need to start somewhere and this area seems to be a good choice, given both its intricacy and its importance. ",Documentation,Major,Open,"2014-10-21 17:07:09","2014-10-21 16:07:09",5
"Apache Mesos","Add event queue size metrics to scheduler driver","In the master process, we expose metrics for event queue sizes for various event types. We should do the same for the scheduler driver process.",Task,Minor,Resolved,"2014-10-17 18:30:13","2014-10-17 17:30:13",2
"Apache Mesos","Make executor's user owner of executor's cgroup directory","Currently, when cgroups are enabled, and executor is spawned, it's mounted under, for ex: /sys/fs/cgroup/cpu/mesos/<mesos-id>. This directory in current implementation is only writable by root user. This prevents process launched by executor to mount its child processes under this cgroup, because the cgroup directory is only writable by root.  To enable a executor spawned process to mount it's child processes under it's cgroup directory, the cgroup directory should be made writable by the user which spawns the executor.",Improvement,Minor,Resolved,"2014-10-16 23:52:35","2014-10-16 22:52:35",3
"Apache Mesos","Create libevent/SSL-backed Socket implementation",,Task,Major,Resolved,"2014-10-14 02:03:26","2014-10-14 01:03:26",13
"Apache Mesos","Add backoff to framework re-registration retries","To avoid so many duplicate framework re-registration attempts (and thus offer rescinds) we should add backoff to re-registration retries.",Task,Major,Resolved,"2014-10-10 19:43:48","2014-10-10 18:43:48",3
"Apache Mesos","Slave resources obtained from localhost:5051/state.json is not correct.","The 'resources' field in Slave is uninitialized.  Also, seems that 'attributes' field in Slave is redundant as we store slave info. ",Bug,Major,Resolved,"2014-10-10 19:33:41","2014-10-10 18:33:41",2
"Apache Mesos","os::killtree() incorrectly returns early if pid has terminated","If groups == true and/or sessions == true then os::killtree() should continue to signal all processes in the process group and/or session, even if the leading pid has terminated.",Bug,Major,Resolved,"2014-10-08 18:17:37","2014-10-08 17:17:37",2
"Apache Mesos","UpdateFramework message might reach the slave before Reregistered message and get dropped","In reregisterSlave() we send 'SlaveReregisteredMessage' before we link the slave pid, which means a temporary socket will be created and used.  Subsequently, after linking, we send the UpdateFrameworkMessage, which creates and uses a persistent socket.  This might lead to out-of-order delivery, resulting in UpdateFrameworkMessage reaching the slave before the SlaveReregisteredMessage and getting dropped because the slave is not yet (re-)registered.",Bug,Major,Resolved,"2014-10-06 18:37:21","2014-10-06 17:37:21",1
"Apache Mesos","Race between ~Authenticator() and Authenticator::authenticate() can lead to schedulers/slaves to never get authenticated","The master might get a duplicate authenticate() request while a previous authentication attempt is in progress. Depending on what the AuthenticatorProcess is executing at the time, there are 2 possible race conditions which will cause scheduler/slave to continuously retry authentication but never succeed.  We have seen both the race conditions in a heavily loaded production cluster.  Race1: ---------- --> An authenticate() event was dispatched to AuthenticatorProcess (Master::authenticate() called Authenticator::authenticate())  --> A terminate() event was then injected into the front of the AuthenticatorProcess queue (duplicate Master::authenticate() did ~Authenticator) before the above authenticate() event was executed.  --> Due to the bug in libprocess, the future returned by Master::authenticate() was never transitioned to discarded (Master::_authenticate() was never called).  --> This caused all the subsequent authentication retries to be enqueued on the master waiting for Master::_authenticate() to be executed.  Fix: Transition the dispatched future to discarded if the libprocess is terminated (https://reviews.apache.org/r/25945/)  Race 2: ----------- --> An authenticate() event was dispatched to AuthenticatorProcess (Master::authenticate() called Authenticator::authenticate())  --> AuthenticatorProcess::authenticate() executed and set promise.onDiscard(defer(self, Self::discarded)). NOTE: The internal promise of AuthenticatorProcess is discarded in AuthenticatorProcess::discarded()  --> A terminate() event was then injected into the front of the AuthenticatorProcess queue (duplicate Master::authenticate() did  ~Authenticator) before the above discarded() event was executed)  --> ~AuthenticatorProcess is destructed without ever discarding the internal promise (Master::_authenticate() was never called).  --> This caused all the subsequent authentication retries to be enqueued on the master waiting for Master::_authenticate() to be executed.  Fix: The fix here is to discard the internal promise when the AuthenticatorProcess is destructed.",Bug,Critical,Resolved,"2014-10-04 00:58:27","2014-10-03 23:58:27",2
"Apache Mesos","Redirect to the leader master when current master is not a leader","Some of the API endpoints, for example /master/tasks.json, will return bogus information if you query a non-leading master:    This is very hard for end-users to work around.  For example if I query which master is leading followed by leader: which tasks are running it is possible that the leader fails over in between, leaving me with an incorrect answer and no way to know that this happened.  In my opinion the API should return the correct response (by asking the current leader?) or an error (500 Not the leader?) but it's unacceptable to return a successful wrong answer. ",Bug,Major,Resolved,"2014-10-04 00:09:23","2014-10-03 23:09:23",3
"Apache Mesos","Split launch tasks and decline offers metrics","Both launchTasks() and declineOffers() scheduler driver calls end up in messages_launch_tasks metric on the master. It would be nice to split them for differentiating these two calls.",Improvement,Major,Resolved,"2014-10-03 22:02:24","2014-10-03 21:02:24",1
"Apache Mesos","Performance regression in the Master's http metrics.","As part of the change to hold on to terminal unacknowledged tasks in the master, we introduced a performance regression during the following patch:  https://github.com/apache/mesos/commit/0760b007ad65bc91e8cea377339978c78d36d247   Rather than keeping a running count of allocated resources, we now compute resources on-demand. This was done in order to ignore terminal task's resources.  As a result of this change, the /stats.json and /metrics/snapshot endpoints on the master have slowed down substantially on large clusters.    {{perf top}} reveals some of the resource computation during a request to stats.json: {noformat: perf top} Events: 36K cycles  10.53%  libc-2.5.so             [.] _int_free   9.90%  libc-2.5.so             [.] malloc   8.56%  libmesos-0.21.0.so  [.] std::_Rb_tree<process::ProcessBase*, process::ProcessBase*, std::_Identity<process::ProcessBase*>, std::less<process::ProcessBase*>, std::allocator<process::ProcessBase*> >::   8.23%  libc-2.5.so             [.] _int_malloc   5.80%  libstdc++.so.6.0.8      [.] std::_Rb_tree_increment(std::_Rb_tree_node_base*)   5.33%  [kernel]                [k] _raw_spin_lock   3.13%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&)   2.95%  libmesos-0.21.0.so  [.] process::SocketManager::exited(process::ProcessBase*)   2.43%  libmesos-0.21.0.so  [.] mesos::Resource::MergeFrom(mesos::Resource const&)   1.88%  libmesos-0.21.0.so  [.] mesos::internal::master::Slave::used() const   1.48%  libstdc++.so.6.0.8      [.] __gnu_cxx::__atomic_add(int volatile*, int)   1.45%  [kernel]                [k] find_busiest_group   1.41%  libc-2.5.so             [.] free   1.38%  libmesos-0.21.0.so  [.] mesos::Value_Range::MergeFrom(mesos::Value_Range const&)   1.13%  libmesos-0.21.0.so  [.] mesos::Value_Scalar::MergeFrom(mesos::Value_Scalar const&)   1.12%  libmesos-0.21.0.so  [.] mesos::Resource::SharedDtor()   1.07%  libstdc++.so.6.0.8      [.] __gnu_cxx::__exchange_and_add(int volatile*, int)   0.94%  libmesos-0.21.0.so  [.] google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)   0.92%  libstdc++.so.6.0.8      [.] operator new(unsigned long)   0.88%  libmesos-0.21.0.so  [.] mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&)   0.75%  libmesos-0.21.0.so  [.] mesos::matches(mesos::Resource const&, mesos::Resource const&) {noformat}",Bug,Blocker,Resolved,"2014-10-03 20:25:58","2014-10-03 19:25:58",3
"Apache Mesos","Leaked file descriptors in StatusUpdateStream.","https://github.com/apache/mesos/blob/master/src/slave/status_update_manager.hpp#L180  We should set cloexec for 'fd'.",Bug,Major,Resolved,"2014-10-02 19:18:44","2014-10-02 18:18:44",1
"Apache Mesos","Support specifying libnl3 install location.","LIBNL_CFLAGS uses a hard-coded path in the configure script, instead of detecting the location.",Task,Blocker,Resolved,"2014-10-01 23:10:01","2014-10-01 22:10:01",2
"Apache Mesos","Mesos 0.20.1 doesn't compile","The compilation of Mesos 0.20.1 fails on Ubuntu Trusty with the following error -  slave/containerizer/mesos/containerizer.cpp  -fPIC -DPIC -o slave/containerizer/mesos/.libs/libmesos_no_3rdparty_la-containerizer.o In file included from ./linux/routing/filter/ip.hpp:36:0,                  from ./slave/containerizer/isolators/network/port_mapping.hpp:42,                  from slave/containerizer/mesos/containerizer.cpp:44: ./linux/routing/filter/filter.hpp:29:43: fatal error: linux/routing/filter/handle.hpp: No such file or directory  #include linux/routing/filter/handle.hpp                                            ^",Bug,Major,Resolved,"2014-10-01 21:24:54","2014-10-01 20:24:54",1
"Apache Mesos","Remove /proc and /sys remounts from port_mapping isolator","/proc/net reflects a new network namespace regardless and remount doesn't actually do what we expected anyway, i.e., it's not sufficient for a new pid namespace and a new mount is required.",Bug,Major,Resolved,"2014-10-01 18:36:47","2014-10-01 17:36:47",3
"Apache Mesos","AllocatorTest/0.SlaveLost is flaky",,Bug,Major,Resolved,"2014-09-30 02:31:25","2014-09-30 01:31:25",1
"Apache Mesos","Expose master stats differentiating between master-generated and slave-generated LOST tasks","The master exports a monotonically-increasing counter of tasks transitioned to TASK_LOST.  This loses fidelity of the source of the lost task.  A first step in exposing the source of lost tasks might be to just differentiate between TASK_LOST transitions initiated by the master vs the slave (and maybe bad input from the scheduler).",Story,Minor,Resolved,"2014-09-25 19:36:45","2014-09-25 18:36:45",5
"Apache Mesos","Completed tasks remains in TASK_RUNNING when framework is disconnected","We have run into a problem that cause tasks which completes, when a framework is disconnected and has a fail-over time, to remain in a running state even though the tasks actually finishes. This hogs the cluster and gives users a inconsistent view of the cluster state. Going to the slave, the task is finished. Going to the master, the task is still in a non-terminal state. When the scheduler reattaches or the failover timeout expires, the tasks finishes correctly. The current workflow of this scheduler has a long fail-over timeout, but may on the other hand never reattach.  Here is a test framework we have been able to reproduce the issue with: https://gist.github.com/nqn/9b9b1de9123a6e836f54 It launches many short-lived tasks (1 second sleep) and when killing the framework instance, the master reports the tasks as running even after several minutes: http://cl.ly/image/2R3719461e0t/Screen%20Shot%202014-09-10%20at%203.19.39%20PM.png  When clicking on one of the slaves where, for example, task 49 runs; the slave knows that it completed: http://cl.ly/image/2P410L3m1O1N/Screen%20Shot%202014-09-10%20at%203.21.29%20PM.png  Here is the log of a mesos-local instance where I reproduced it: https://gist.github.com/nqn/f7ee20601199d70787c0 (Here task 10 to 19 are stuck in running state). There is a lot of output, so here is a filtered log for task 10: https://gist.github.com/nqn/a53e5ea05c5e41cd5a7d  The problem turn out to be an issue with the ack-cycle of status updates: If the framework disconnects (with a failover timeout set), the status update manage on the slaves will keep trying to send the front of status update stream to the master (which in turn forwards it to the framework). If the first status update after the disconnect is terminal, things work out fine; the master pick the terminal state up, removes the task and release the resources. If, on the other hand, one non-terminal status is in the stream. The master will never know that the task finished (or failed) before the framework reconnects.  During a discussion on the dev mailing list (http://mail-archives.apache.org/mod_mbox/mesos-dev/201409.mbox/%3cCADKthhAVR5mrq1s9HXw1BB_XFALXWWxjutp7MV4y3wP-Bh=<EMAIL>%3e) we enumerated a couple of options to solve this problem.  First off, having two ack-cycles: one between masters and slaves and one between masters and frameworks, would be ideal. We would be able to replay the statuses in order while keeping the master state current. However, this requires us to persist the master state in a replicated storage.  As a first pass, we can make sure that the tasks caught in a running state doesn't hog the cluster when completed and the framework being disconnected.  Here is a proof-of-concept to work out of: https://github.com/nqn/mesos/tree/niklas/status-update-disconnect/  A new (optional) field have been added to the internal status update message: https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/messages/messages.proto#L68  Which makes it possible for the status update manager to set the field, if the latest status was terminal: https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/slave/status_update_manager.cpp#L501  I added a test which should high-light the issue as well: https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/tests/fault_tolerance_tests.cpp#L2478  I would love some input on the approach before moving on. There are rough edges in the PoC which (of course) should be addressed before bringing it for up review.",Bug,Major,Resolved,"2014-09-19 01:17:19","2014-09-19 00:17:19",2
"Apache Mesos","Create a guide to becoming a committer","We have a committer's guide, but the process by which one becomes a committer is unclear. We should set some guidelines and a process by which we can grow contributors into committers.",Documentation,Major,Resolved,"2014-09-18 18:33:46","2014-09-18 17:33:46",3
"Apache Mesos","Task attempted to use more offers than requested in example jave and python frameworks",,Bug,Major,Resolved,"2014-09-18 17:42:28","2014-09-18 16:42:28",2
"Apache Mesos","Fail fast in example frameworks if task goes into unexpected state","Most of the example frameworks launch a bunch of tasks and exit if *all* of them reach FINISHED state. But if there is a bug in the code resulting in TASK_LOST, the framework waits forever. Instead the framework should abort if an un-expected task state is encountered.",Improvement,Major,Resolved,"2014-09-18 04:42:48","2014-09-18 03:42:48",1
"Apache Mesos","Reconcile disconnected/deactivated semantics in the master code","Currently the master code treats a deactivated and disconnected slave similarly, by setting 'disconnected' variable in the slave struct. This causes us to disconnect() a slave in cases where we really only want to deactivate() the slave (e.g., authentication).  It would be nice to differentiate these semantics by adding a new variable active in the Slave struct.  We might want to do the same with the Framework struct for consistency.",Improvement,Major,Resolved,"2014-09-18 02:20:46","2014-09-18 01:20:46",3
"Apache Mesos","Expose RTT in container stats","As we expose the bandwidth, so we should expose the RTT as a measure of latency each container is experiencing.  We can use {{ss}} to get the per-socket statistics and filter and aggregate accordingly to get a measure of RTT.",Task,Major,Resolved,"2014-09-17 23:59:19","2014-09-17 22:59:19",3
"Apache Mesos","Disallow executors with cpu only or memory only resources","Currently master allows executors to be launched with either only cpus or only memory but we shouldn't allow that. This is because executor is an actual unix process that is launched by the slave. If an executor doesn't specify cpus, what should the cpu limits be for that executor when there are no tasks running on it? If no cpu limits are set then it might starve other executors/tasks on the slave violating isolation guarantees. Same goes with memory. Moreover, the current containerizer/isolator code will throw failures when using such an executor, e.g., when the last task on the executor finishes and Containerizer::update() is called with 0 cpus or 0 mem.  According to a source code [TODO | https://github.com/apache/mesos/blob/0226620747e1769434a1a83da547bfc3470a9549/src/master/validation.cpp#L400] this should also include checking whether requested resources are greater than  MIN_CPUS/MIN_BYTES.",Improvement,Major,Accepted,"2014-09-17 19:31:35","2014-09-17 18:31:35",3
"Apache Mesos","HealthCheckTest.HealthStatusChange is flaky on jenkins.","https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2374/consoleFull  ",Bug,Minor,Resolved,"2014-09-17 01:32:35","2014-09-17 00:32:35",5
"Apache Mesos","Reconciliation can send out-of-order updates.","When a slave re-registers with the master, it currently sends the latest task state for all tasks that are not both terminal and acknowledged.  However, reconciliation assumes that we always have the latest unacknowledged state of the task represented in the master.  As a result, out-of-order updates are possible, e.g.  (1) Slave has task T in TASK_FINISHED, with unacknowledged updates: [TASK_RUNNING, TASK_FINISHED]. (2) Master fails over. (3) New master re-registers the slave with T in TASK_FINISHED. (4) Reconciliation request arrives, master sends TASK_FINISHED. (5) Slave sends TASK_RUNNING to master, master sends TASK_RUNNING.  I think the fix here is to preserve the task state invariants in the master, namely, that the master has the latest unacknowledged state of the task. This means when the slave re-registers, it should instead send the latest acknowledged state of each task.",Bug,Major,Resolved,"2014-09-16 21:04:42","2014-09-16 20:04:42",3
"Apache Mesos","Add chown option to CommandInfo.URI","Mesos fetcher always chown()s the extracted executor URIs as the executor user but sometimes this is not desirable, e.g., setuid bit gets lost during chown() if slave/fetcher is running as root.   It would be nice to give frameworks the ability to skip the chown.",Improvement,Major,Reviewable,"2014-09-13 19:06:32","2014-09-13 18:06:32",2
"Apache Mesos","Design the semantics for updating FrameworkInfo","Currently, there is no easy way for frameworks to update their FrameworkInfo., resulting in issues like MESOS-703 and MESOS-1218.  This ticket captures the design for doing FrameworkInfo update without having to roll masters/slaves/tasks/executors.",Documentation,Major,Resolved,"2014-09-10 20:50:17","2014-09-10 19:50:17",3
"Apache Mesos","AllocatorTest/0.FrameworkExited is flaky","{noformat:title=} [ RUN      ] AllocatorTest/0.FrameworkExited Using temporary directory '/tmp/AllocatorTest_0_FrameworkExited_B6WZng' I0909 08:02:35.116555 18112 leveldb.cpp:176] Opened db in 31.64686ms I0909 08:02:35.126065 18112 leveldb.cpp:183] Compacted db in 9.449823ms I0909 08:02:35.126118 18112 leveldb.cpp:198] Created db iterator in 5858ns I0909 08:02:35.126137 18112 leveldb.cpp:204] Seeked to beginning of db in 1136ns I0909 08:02:35.126150 18112 leveldb.cpp:273] Iterated through 0 keys in the db in 560ns I0909 08:02:35.126178 18112 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0909 08:02:35.126502 18133 recover.cpp:425] Starting replica recovery I0909 08:02:35.126601 18133 recover.cpp:451] Replica is in EMPTY status I0909 08:02:35.127012 18133 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I0909 08:02:35.127094 18133 recover.cpp:188] Received a recover response from a replica in EMPTY status I0909 08:02:35.127223 18133 recover.cpp:542] Updating replica status to STARTING I0909 08:02:35.226631 18133 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 99.308134ms I0909 08:02:35.226690 18133 replica.cpp:320] Persisted replica status to STARTING I0909 08:02:35.226812 18131 recover.cpp:451] Replica is in STARTING status I0909 08:02:35.227246 18131 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I0909 08:02:35.227308 18131 recover.cpp:188] Received a recover response from a replica in STARTING status I0909 08:02:35.227409 18131 recover.cpp:542] Updating replica status to VOTING I0909 08:02:35.228540 18129 master.cpp:286] Master 20140909-080235-16842879-44005-18112 (precise) started on 127.0.1.1:44005 I0909 08:02:35.228593 18129 master.cpp:332] Master only allowing authenticated frameworks to register I0909 08:02:35.228607 18129 master.cpp:337] Master only allowing authenticated slaves to register I0909 08:02:35.228620 18129 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_FrameworkExited_B6WZng/credentials' I0909 08:02:35.228754 18129 master.cpp:366] Authorization enabled I0909 08:02:35.229560 18129 master.cpp:120] No whitelist given. Advertising offers for all slaves I0909 08:02:35.229933 18129 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@127.0.1.1:44005 I0909 08:02:35.230057 18127 master.cpp:1212] The newly elected leader is master@127.0.1.1:44005 with id 20140909-080235-16842879-44005-18112 I0909 08:02:35.230129 18127 master.cpp:1225] Elected as the leading master! I0909 08:02:35.230144 18127 master.cpp:1043] Recovering from registrar I0909 08:02:35.230257 18127 registrar.cpp:313] Recovering registrar I0909 08:02:35.232461 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 4.999384ms I0909 08:02:35.232489 18131 replica.cpp:320] Persisted replica status to VOTING I0909 08:02:35.232544 18131 recover.cpp:556] Successfully joined the Paxos group I0909 08:02:35.232611 18131 recover.cpp:440] Recover process terminated I0909 08:02:35.232727 18131 log.cpp:656] Attempting to start the writer I0909 08:02:35.233012 18131 replica.cpp:474] Replica received implicit promise request with proposal 1 I0909 08:02:35.238785 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.749504ms I0909 08:02:35.238818 18131 replica.cpp:342] Persisted promised to 1 I0909 08:02:35.244056 18131 coordinator.cpp:230] Coordinator attemping to fill missing position I0909 08:02:35.244580 18131 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I0909 08:02:35.250143 18131 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.382351ms I0909 08:02:35.250319 18131 replica.cpp:676] Persisted action at 0 I0909 08:02:35.250901 18131 replica.cpp:508] Replica received write request for position 0 I0909 08:02:35.251137 18131 leveldb.cpp:438] Reading position from leveldb took 18689ns I0909 08:02:35.256597 18131 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.274169ms I0909 08:02:35.256764 18131 replica.cpp:676] Persisted action at 0 I0909 08:02:35.263712 18126 replica.cpp:655] Replica received learned notice for position 0 I0909 08:02:35.269613 18126 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.417225ms I0909 08:02:35.351641 18126 replica.cpp:676] Persisted action at 0 I0909 08:02:35.351655 18126 replica.cpp:661] Replica learned NOP action at position 0 I0909 08:02:35.351889 18126 log.cpp:672] Writer started with ending position 0 I0909 08:02:35.352165 18126 leveldb.cpp:438] Reading position from leveldb took 25215ns I0909 08:02:35.353163 18126 registrar.cpp:346] Successfully fetched the registry (0B) I0909 08:02:35.353185 18126 registrar.cpp:422] Attempting to update the 'registry' I0909 08:02:35.354152 18126 log.cpp:680] Attempting to append 120 bytes to the log I0909 08:02:35.354195 18126 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0909 08:02:35.354416 18126 replica.cpp:508] Replica received write request for position 1 I0909 08:02:35.351579 18127 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.354558 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 2.984795ms I0909 08:02:35.360254 18126 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.811986ms I0909 08:02:35.360285 18126 replica.cpp:676] Persisted action at 1 I0909 08:02:35.364126 18132 replica.cpp:655] Replica received learned notice for position 1 I0909 08:02:35.369856 18132 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 5.702756ms I0909 08:02:35.369899 18132 replica.cpp:676] Persisted action at 1 I0909 08:02:35.369910 18132 replica.cpp:661] Replica learned APPEND action at position 1 I0909 08:02:35.370209 18132 registrar.cpp:479] Successfully updated 'registry' I0909 08:02:35.370311 18132 registrar.cpp:372] Successfully recovered registrar I0909 08:02:35.370477 18132 log.cpp:699] Attempting to truncate the log to 1 I0909 08:02:35.370553 18132 master.cpp:1070] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register I0909 08:02:35.370594 18132 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0909 08:02:35.371201 18127 replica.cpp:508] Replica received write request for position 2 I0909 08:02:35.376760 18127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.264501ms I0909 08:02:35.377105 18127 replica.cpp:676] Persisted action at 2 I0909 08:02:35.377770 18127 replica.cpp:655] Replica received learned notice for position 2 I0909 08:02:35.383363 18127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.272769ms I0909 08:02:35.383818 18127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 28148ns I0909 08:02:35.384137 18127 replica.cpp:676] Persisted action at 2 I0909 08:02:35.384399 18127 replica.cpp:661] Replica learned TRUNCATE action at position 2 I0909 08:02:35.396512 18127 slave.cpp:167] Slave started on 64)@127.0.1.1:44005 I0909 08:02:35.654770 18131 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.654847 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 104933ns I0909 08:02:35.654974 18127 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/credential' I0909 08:02:35.655097 18127 slave.cpp:274] Slave using credential for: test-principal I0909 08:02:35.655203 18127 slave.cpp:287] Slave resources: cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] I0909 08:02:35.655274 18127 slave.cpp:315] Slave hostname: precise I0909 08:02:35.655285 18127 slave.cpp:316] Slave checkpoint: false I0909 08:02:35.655804 18127 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/meta' I0909 08:02:35.655913 18127 status_update_manager.cpp:193] Recovering status update manager I0909 08:02:35.656005 18127 slave.cpp:3202] Finished recovery I0909 08:02:35.656251 18127 slave.cpp:598] New master detected at master@127.0.1.1:44005 I0909 08:02:35.656285 18127 slave.cpp:672] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.656325 18127 slave.cpp:645] Detecting new master I0909 08:02:35.656358 18127 status_update_manager.cpp:167] New master detected at master@127.0.1.1:44005 I0909 08:02:35.656389 18127 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.656563 18127 master.cpp:3653] Authenticating slave(64)@127.0.1.1:44005 I0909 08:02:35.656651 18127 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.656770 18127 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.656796 18127 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.656822 18127 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.656858 18127 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.656883 18127 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.656924 18127 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.656960 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.656971 18127 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.656982 18127 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.656997 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.657004 18127 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.657008 18127 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.657019 18127 authenticator.hpp:376] Authentication success I0909 08:02:35.657047 18127 authenticatee.hpp:305] Authentication success I0909 08:02:35.657073 18127 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(64)@127.0.1.1:44005 I0909 08:02:35.657145 18127 slave.cpp:729] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.657183 18127 slave.cpp:980] Will retry registration in 19.238717ms if necessary I0909 08:02:35.657276 18128 master.cpp:2843] Registering slave at slave(64)@127.0.1.1:44005 (precise) with id 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.657389 18128 registrar.cpp:422] Attempting to update the 'registry' I0909 08:02:35.658382 18130 log.cpp:680] Attempting to append 295 bytes to the log I0909 08:02:35.658432 18130 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0909 08:02:35.658635 18130 replica.cpp:508] Replica received write request for position 3 I0909 08:02:35.660959 18112 sched.cpp:137] Version: 0.21.0 I0909 08:02:35.661093 18126 sched.cpp:233] New master detected at master@127.0.1.1:44005 I0909 08:02:35.661111 18126 sched.cpp:283] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.661175 18126 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.661306 18126 master.cpp:3653] Authenticating scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661376 18126 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.661466 18126 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.661483 18126 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.661504 18126 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.661530 18126 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.661552 18126 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.661579 18126 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.661592 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.661598 18126 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.661607 18126 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.661613 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.661619 18126 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.661623 18126 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.661633 18126 authenticator.hpp:376] Authentication success I0909 08:02:35.661653 18126 authenticatee.hpp:305] Authentication success I0909 08:02:35.661672 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661730 18126 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.661741 18126 sched.cpp:476] Sending registration request to master@127.0.1.1:44005 I0909 08:02:35.661782 18126 master.cpp:1331] Received registration request from scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661798 18126 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*' I0909 08:02:35.661917 18126 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0000 at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.662017 18126 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.662039 18126 sched.cpp:421] Scheduler::registered took 9070ns I0909 08:02:35.662119 18126 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.662130 18126 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.662135 18126 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5558ns I0909 08:02:35.672230 18130 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 13.567526ms I0909 08:02:35.672268 18130 replica.cpp:676] Persisted action at 3 I0909 08:02:35.672483 18130 replica.cpp:655] Replica received learned notice for position 3 I0909 08:02:35.677322 18132 slave.cpp:980] Will retry registration in 14.890338ms if necessary I0909 08:02:35.677399 18132 master.cpp:2831] Ignoring register slave message from slave(64)@127.0.1.1:44005 (precise) as admission is already in progress I0909 08:02:35.680881 18130 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 8.376798ms I0909 08:02:35.680908 18130 replica.cpp:676] Persisted action at 3 I0909 08:02:35.680917 18130 replica.cpp:661] Replica learned APPEND action at position 3 I0909 08:02:35.681252 18130 registrar.cpp:479] Successfully updated 'registry' I0909 08:02:35.681330 18130 log.cpp:699] Attempting to truncate the log to 3 I0909 08:02:35.681385 18130 master.cpp:2883] Registered slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.681399 18130 master.cpp:4126] Adding slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] I0909 08:02:35.681504 18130 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0909 08:02:35.681570 18130 slave.cpp:763] Registered with master master@127.0.1.1:44005; given slave ID 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.681689 18130 slave.cpp:2329] Received ping from slave-observer(50)@127.0.1.1:44005 I0909 08:02:35.681753 18130 hierarchical_allocator_process.hpp:442] Added slave 20140909-080235-16842879-44005-18112-0 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] (and cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] available) I0909 08:02:35.681808 18130 hierarchical_allocator_process.hpp:734] Offering cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.681892 18130 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140909-080235-16842879-44005-18112-0 in 109580ns I0909 08:02:35.681968 18130 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.682014 18130 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.682443 18130 sched.cpp:544] Scheduler::resourceOffers took 254258ns I0909 08:02:35.682633 18130 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.682684 18130 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-0 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.682708 18130 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I0909 08:02:35.682971 18130 replica.cpp:508] Replica received write request for position 4 I0909 08:02:35.683132 18132 master.hpp:833] Adding task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.683159 18132 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0000 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.683363 18132 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.683580 18132 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.684833 18133 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.684864 18133 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0000 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs I0909 08:02:35.686401 18132 exec.cpp:132] Version: 0.21.0 I0909 08:02:35.686848 18128 exec.cpp:182] Executor started at: executor(8)@127.0.1.1:44005 with pid 18112 I0909 08:02:35.687095 18132 slave.cpp:1231] Queuing task '0' for executor executor-1 of framework '20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.687302 18132 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-18112-0000/executors/executor-1/runs/c4458e43-94ee-4b5e-bd74-5d39a09deff6' I0909 08:02:35.687568 18132 slave.cpp:1741] Got registration for executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.687893 18127 exec.cpp:206] Executor registered on slave 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.688789 18127 exec.cpp:218] Executor::registered took 15015ns I0909 08:02:35.688977 18132 slave.cpp:1859] Flushing queued task 0 for executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.689260 18133 exec.cpp:293] Executor asked to run task '0' I0909 08:02:35.689441 18133 exec.cpp:302] Executor::launchTask took 24599ns I0909 08:02:35.691651 18112 sched.cpp:137] Version: 0.21.0 I0909 08:02:35.691946 18131 sched.cpp:233] New master detected at master@127.0.1.1:44005 I0909 08:02:35.692126 18131 sched.cpp:283] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.692399 18131 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.692791 18131 master.cpp:3653] Authenticating scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.693068 18131 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.693351 18131 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.693532 18131 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.693739 18131 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.693979 18131 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.694202 18131 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.694449 18131 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.694633 18131 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.694792 18131 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.694980 18131 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.695158 18131 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.695369 18131 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.695724 18131 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.695907 18131 authenticator.hpp:376] Authentication success I0909 08:02:35.696117 18128 authenticatee.hpp:305] Authentication success I0909 08:02:35.698509 18130 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 15.520863ms I0909 08:02:35.698698 18130 replica.cpp:676] Persisted action at 4 I0909 08:02:35.698940 18128 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.699095 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.699354 18130 replica.cpp:655] Replica received learned notice for position 4 I0909 08:02:35.699973 18128 sched.cpp:476] Sending registration request to master@127.0.1.1:44005 I0909 08:02:35.700265 18128 master.cpp:1331] Received registration request from scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.700515 18128 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*' I0909 08:02:35.700809 18128 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0001 at scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.701037 18133 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701211 18133 sched.cpp:421] Scheduler::registered took 11991ns I0909 08:02:35.701488 18131 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701728 18131 hierarchical_allocator_process.hpp:734] Offering cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701992 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 297969ns I0909 08:02:35.702229 18128 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-1 with resources cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.702481 18128 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.702901 18129 sched.cpp:544] Scheduler::resourceOffers took 127949ns I0909 08:02:35.703305 18128 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-1 with resources cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.703629 18128 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-1 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.703908 18128 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I0909 08:02:35.703789 18132 slave.cpp:2542] Monitoring executor 'executor-1' of framework '20140909-080235-16842879-44005-18112-0000' in container 'c4458e43-94ee-4b5e-bd74-5d39a09deff6' I0909 08:02:35.704763 18128 master.hpp:833] Adding task 0 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.704951 18128 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0001 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.705255 18129 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.705582 18129 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.707756 18129 exec.cpp:132] Version: 0.21.0 I0909 08:02:35.708035 18130 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.127072ms I0909 08:02:35.708281 18130 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28817ns I0909 08:02:35.708459 18130 replica.cpp:676] Persisted action at 4 I0909 08:02:35.708632 18130 replica.cpp:661] Replica learned TRUNCATE action at position 4 I0909 08:02:35.708869 18133 exec.cpp:182] Executor started at: executor(9)@127.0.1.1:44005 with pid 18112 I0909 08:02:35.709120 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 35083ns I0909 08:02:35.709511 18129 slave.cpp:1231] Queuing task '0' for executor executor-2 of framework '20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.709707 18129 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-18112-0001/executors/executor-2/runs/7654870b-fd36-40b2-aac7-37b1bcfa821e' I0909 08:02:35.709913 18129 slave.cpp:1741] Got registration for executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.710188 18129 slave.cpp:1859] Flushing queued task 0 for executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.710516 18129 slave.cpp:2542] Monitoring executor 'executor-2' of framework '20140909-080235-16842879-44005-18112-0001' in container '7654870b-fd36-40b2-aac7-37b1bcfa821e' I0909 08:02:35.710321 18130 exec.cpp:206] Executor registered on slave 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.711678 18130 exec.cpp:218] Executor::registered took 14355ns I0909 08:02:35.711987 18130 exec.cpp:293] Executor asked to run task '0' I0909 08:02:35.715551 18130 exec.cpp:302] Executor::launchTask took 3.40476ms I0909 08:02:35.716006 18131 sched.cpp:745] Stopping framework '20140909-080235-16842879-44005-18112-0000' I0909 08:02:35.716292 18128 master.cpp:1640] Asked to unregister framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.716490 18127 hierarchical_allocator_process.hpp:563] Recovered mem(*):256; disk(*):25116; ports(*):[31000-32000] (total allocatable: mem(*):256; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.716792 18127 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0001 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs I0909 08:02:35.717018 18128 master.cpp:3976] Removing framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.717269 18128 master.hpp:851] Removing task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise) W0909 08:02:35.717607 18128 master.cpp:4419] Removing task 0 of framework 20140909-080235-16842879-44005-18112-0000 and slave 20140909-080235-16842879-44005-18112-0 in non-terminal state TASK_STAGING I0909 08:02:35.717470 18131 hierarchical_allocator_process.hpp:405] Deactivated framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718065 18131 hierarchical_allocator_process.hpp:563] Recovered cpus(*):2; mem(*):512 (total allocatable: mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.717438 18132 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0000 by master@127.0.1.1:44005 I0909 08:02:35.718444 18132 slave.cpp:1439] Shutting down framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718621 18132 slave.cpp:2882] Shutting down executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718843 18133 exec.cpp:379] Executor asked to shutdown I0909 08:02:35.719022 18133 exec.cpp:394] Executor::shutdown took 13745ns I0909 08:02:35.722009 18128 hierarchical_allocator_process.hpp:360] Removed framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.830785 18131 hierarchical_allocator_process.hpp:734] Offering mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.830940 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 218030ns I0909 08:02:35.831056 18127 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-2 with resources mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.831115 18127 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.831248 18127 sched.cpp:544] Scheduler::resourceOffers took 18178ns I0909 08:02:35.831387 18112 master.cpp:650] Master terminating I0909 08:02:35.831441 18112 master.hpp:851] Removing task 0 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 (precise) W0909 08:02:35.831488 18112 master.cpp:4419] Removing task 0 of framework 20140909-080235-16842879-44005-18112-0001 and slave 20140909-080235-16842879-44005-18112-0 in non-terminal state TASK_STAGING I0909 08:02:35.831573 18112 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-2 with resources mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.832608 18112 slave.cpp:475] Slave terminating I0909 08:02:35.832630 18112 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0000 by @0.0.0.0:0 W0909 08:02:35.832643 18112 slave.cpp:1435] Ignoring shutdown framework 20140909-080235-16842879-44005-18112-0000 because it is terminating I0909 08:02:35.832648 18112 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0001 by @0.0.0.0:0 I0909 08:02:35.832654 18112 slave.cpp:1439] Shutting down framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.832664 18112 slave.cpp:2882] Shutting down executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 tests/allocator_tests.cpp:1444: Failure Actual function call count doesn't match EXPECT_CALL(this->allocator, resourcesRecovered(_, _, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] AllocatorTest/0.FrameworkExited, where TypeParam = mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> (756 ms) {noformat}",Bug,Major,Resolved,"2014-09-10 00:15:48","2014-09-09 23:15:48",1
"Apache Mesos","Provide an option to validate flag value in stout/flags. ","Currently we can provide the default value for a flag, but cannot check if the flag is set to a reasonable value and, e.g., issue a warning. Passing an optional lambda checker to {{FlagBase::add()}} can be a possible solution.",Improvement,Minor,Resolved,"2014-09-09 16:19:34","2014-09-09 15:19:34",3
"Apache Mesos","Design persistent resources",,Task,Major,Resolved,"2014-09-08 22:06:45","2014-09-08 21:06:45",13
"Apache Mesos","introduce unique_ptr","* add unique_ptr to the configure check * document use of unique_ptr in style guide ** use when possible, use std::move when necessary * move raw pointers to Owned to establish ownership * deprecate Owned in favour of unique_ptr ",Improvement,Major,Resolved,"2014-09-06 00:01:41","2014-09-05 23:01:41",1
"Apache Mesos","MasterAuthorizationTest.DuplicateRegistration test is flaky",,Bug,Major,Resolved,"2014-09-05 18:24:32","2014-09-05 17:24:32",2
"Apache Mesos","Use PID namespace to avoid freezing cgroup","There is some known kernel issue when we freeze the whole cgroup upon OOM. Mesos probably can just use PID namespace so that we will only need to kill the init of the pid namespace, instead of freezing all the processes and killing them one by one. But I am not quite sure if this would break the existing code.",Story,Major,Resolved,"2014-09-05 17:54:39","2014-09-05 16:54:39",5
"Apache Mesos","MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky","Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2355/changes  ",Bug,Major,Resolved,"2014-09-04 00:12:43","2014-09-03 23:12:43",1
"Apache Mesos","Freezer failure leads to lost task during container destruction.","In the past we've seen numerous issues around the freezer. Lately, on the 2.6.44 kernel, we've seen issues where we're unable to freeze the cgroup:  (1) An oom occurs. (2) No indication of oom in the kernel logs. (3) The slave is unable to freeze the cgroup. (4) The task is marked as lost.    We should consider avoiding the freezer entirely in favor of a kill(2) loop. We don't have to wait for pid namespaces to remove the freezer dependency.  At the very least, when the freezer fails, we should proceed with a kill(2) loop to ensure that we destroy the cgroup.",Bug,Major,Resolved,"2014-09-04 00:08:44","2014-09-03 23:08:44",2
"Apache Mesos","Allow variadic templates","Add variadic templates to the C++11 configure check. Once there, we can start using them in the code-base.",Improvement,Minor,Resolved,"2014-09-02 18:54:19","2014-09-02 17:54:19",1
"Apache Mesos","Request for stats.json cannot be fulfilled after stopping the framework ","Request for stats.json to master from a test case doesn't work after calling frameworks' {{driver.stop()}}. However, it works for state.json. I think the problem is related to {{stats()}} continuation {{_stats()}}. The following test illustrates the issue: {code:title=TestCase.cpp|borderStyle=solid} TEST_F(MasterTest, RequestAfterDriverStop) {   Try<PID<Master> > master = StartMaster();   ASSERT_SOME(master);    Try<PID<Slave> > slave = StartSlave();   ASSERT_SOME(slave);    MockScheduler sched;   MesosSchedulerDriver driver(       &sched, DEFAULT_FRAMEWORK_INFO, master.get(), DEFAULT_CREDENTIAL);    driver.start();      Future<process::http::Response> response_before =       process::http::get(master.get(), stats.json);   AWAIT_READY(response_before);    driver.stop();    Future<process::http::Response> response_after =       process::http::get(master.get(), stats.json);   AWAIT_READY(response_after);    driver.join();    Shutdown();  // Must shutdown before 'containerizer' gets deallocated. } {code}",Bug,Minor,Accepted,"2014-09-02 17:26:39","2014-09-02 16:26:39",5
"Apache Mesos","SlaveRecoveryTest.ShutdownSlave is flaky",,Bug,Major,Resolved,"2014-08-29 19:07:34","2014-08-29 18:07:34",2
"Apache Mesos","MasterZooKeeperTest.LostZooKeeperCluster is flaky","{noformat:title=} tests/master_tests.cpp:1795: Failure Failed to wait 10secs for slaveRegisteredMessage {noformat}  Should have placed the FUTURE_MESSAGE that attempts to capture this messages before the slave starts...",Bug,Major,Resolved,"2014-08-29 18:53:52","2014-08-29 17:53:52",1
"Apache Mesos","Allow slave reconfiguration on restart","Make it so that either via a slave restart or a out of process reconfigure ping, the attributes and resources of a slave can be updated to be a superset of what they used to be.",Epic,Major,Resolved,"2014-08-27 20:37:05","2014-08-27 19:37:05",12
"Apache Mesos","Change the stout path utility to declare a single, variadic 'join' function instead of several separate declarations of various discrete arities",,Improvement,Major,Resolved,"2014-08-26 22:02:22","2014-08-26 21:02:22",5
"Apache Mesos","Libprocess: report bind parameters on failure","When you attempt to start slave or master and there's another one already running there, it is nice to report what are the actual parameters to {{bind}} call that failed.",Improvement,Trivial,Resolved,"2014-08-21 17:37:03","2014-08-21 16:37:03",1
"Apache Mesos","Configure fails with ../configure: line 18439: syntax error near unexpected token `PROTOBUFPREFIX,'","I followed the Getting started documentation and did:  which aborts with ",Bug,Major,Resolved,"2014-08-21 12:56:11","2014-08-21 11:56:11",2
"Apache Mesos","Slave should send exited executor message when the executor is never launched.","When the slave sends TASK_LOST before launching an executor for a task, the slave does not send an exited executor message to the master.    Since the master receives no exited executor message, it still thinks the executor's resources are consumed on the slave.    One possible fix for this would be to send the exited executor message to the master in these cases.",Bug,Major,Resolved,"2014-08-19 02:44:35","2014-08-19 01:44:35",8
"Apache Mesos","Command executor can overcommit the agent.","Currently we give a small amount of resources to the command executor, in addition to resources used by the command task:  https://github.com/apache/mesos/blob/0.20.0-rc1/src/slave/slave.cpp#L2448 {code: title=} ExecutorInfo Slave::getExecutorInfo(     const FrameworkID& frameworkId,     const TaskInfo& task) {   ...     // Add an allowance for the command executor. This does lead to a     // small overcommit of resources.     executor.mutable_resources()->MergeFrom(         Resources::parse(           cpus: + stringify(DEFAULT_EXECUTOR_CPUS) + ; +           mem: + stringify(DEFAULT_EXECUTOR_MEM.megabytes())).get());   ... } {code}  This leads to an overcommit of the slave. Ideally, for command tasks we can transfer all of the task resources to the executor at the slave / isolation level.",Improvement,Critical,Accepted,"2014-08-19 00:50:35","2014-08-18 23:50:35",3
"Apache Mesos","The slave does not show pending tasks in the JSON endpoints.","The slave does not show pending tasks in the /state.json endpoint.  This is a bit tricky to add since we rely on knowing the executor directory.",Bug,Major,Open,"2014-08-19 00:17:23","2014-08-18 23:17:23",1
"Apache Mesos","The slave does not send pending tasks during re-registration.","In what looks like an oversight, the pending tasks and executors in the slave (Framework::pending) are not sent in the re-registration message.  For tasks, this can lead to spurious TASK_LOST notifications being generated by the master when it falsely thinks the tasks are not present on the slave.",Bug,Major,Resolved,"2014-08-19 00:13:38","2014-08-18 23:13:38",3
"Apache Mesos","Automate disallowing of commits mixing mesos/libprocess/stout","For various reasons, we don't want to mix mesos/libprocess/stout changes into a single commit. Typically, it is up to the reviewee/reviewer to catch this.   It wold be nice to automate this via the pre-commit hook .",Bug,Major,Resolved,"2014-08-18 20:43:00","2014-08-18 19:43:00",2
"Apache Mesos","SubprocessTest.Status sometimes flakes out","It's a pretty rare event, but happened more then once.    [ RUN      ] SubprocessTest.Status *** Aborted at 1408023909 (unix time) try date -d @1408023909 if you are using GNU date *** PC: @       0x35700094b1 (unknown) *** SIGTERM (@0x3e8000041d8) received by PID 16872 (TID 0x7fa9ea426780) from PID 16856; stack trace: ***     @       0x3570435cb0 (unknown)     @       0x35700094b1 (unknown)     @       0x3570009d9f (unknown)     @       0x357000e726 (unknown)     @       0x3570015185 (unknown)     @           0x5ead42 process::childMain()     @           0x5ece8d std::_Function_handler<>::_M_invoke()     @           0x5eac9c process::defaultClone()     @           0x5ebbd4 process::subprocess()     @           0x55a229 process::subprocess()     @           0x55a846 process::subprocess()     @           0x54224c SubprocessTest_Status_Test::TestBody()     @     0x7fa9ea460323 (unknown)     @     0x7fa9ea455b67 (unknown)     @     0x7fa9ea455c0e (unknown)     @     0x7fa9ea455d15 (unknown)     @     0x7fa9ea4593a8 (unknown)     @     0x7fa9ea459647 (unknown)     @           0x422466 main     @       0x3570421d65 (unknown)     @           0x4260bd (unknown) [       OK ] SubprocessTest.Status (153 ms)",Bug,Minor,Resolved,"2014-08-15 16:15:17","2014-08-15 15:15:17",2
"Apache Mesos","better error message when replicated log hasn't been initialized","Aurora uses the mesos replicated log.    If you don't run mesos-log initialize before starting aurora you'll get INFO messages in your aurora log:    It is has been deemed too dangerous to automatically run mesos-log initialize for the user (see AURORA-243).   It would be helpful if that error message was made more friendly and at the ERROR level.  The message could explain what the user should do and the implications of doing so.  Links to the docs would be helpful.  See http://wilderness.apache.org/channels/?f=aurora/2014-08-14#1408055261 for context",Bug,Minor,Resolved,"2014-08-15 00:46:13","2014-08-14 23:46:13",1
"Apache Mesos","Add document for network monitoring.","The doc should tell the user how to use the new network monitoring feature.",Documentation,Major,Resolved,"2014-08-15 00:14:46","2014-08-14 23:14:46",2
"Apache Mesos","make check segfaults","Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2331/consoleFull  It looks like the segfault happens before any tests are run. So I suspect somewhere in the setup phase of the tests.  ",Bug,Major,Resolved,"2014-08-13 18:35:18","2014-08-13 17:35:18",2
"Apache Mesos","Improve reconciliation between master and slave.","As we update the Master to keep tasks in memory until they are both terminal and acknowledged (MESOS-1410), the lifetime of tasks in Mesos will look as follows:    In the current form of reconciliation, the slave sends to the master all tasks that are not both terminal and acknowledged. At any point in the above lifecycle, the slave's re-registration message can reach the master.  Note the following properties:  *(1)* The master may have a non-terminal task, not present in the slave's re-registration message. *(2)* The master may have a non-terminal task, present in the slave's re-registration message but in a different state. *(3)* The slave's re-registration message may contain a terminal unacknowledged task unknown to the master.  In the current master / slave [reconciliation|https://github.com/apache/mesos/blob/0.19.1/src/master/master.cpp#L3146] code, the master assumes that case (1) is because a launch task message was dropped, and it sends TASK_LOST. We've seen above that (1) can happen even when the task reaches the slave correctly, so this can lead to inconsistency!  After chatting with [~<USER>, we're considering updating the reconciliation to occur as follows:   → Slave sends all tasks that are not both terminal and acknowledged, during re-registration. This is the same as before.  → If the master sees tasks that are missing in the slave, the master sends the tasks that need to be reconciled to the slave for the tasks. This can be piggy-backed on the re-registration message.  → The slave will send TASK_LOST if the task is not known to it. Preferably in a retried manner, unless we update socket closure on the slave to force a re-registration.",Bug,Major,Resolved,"2014-08-12 20:01:49","2014-08-12 19:01:49",3
"Apache Mesos","The stats.json endpoint on the slave exposes registered as a string.","The slave is currently exposing a string value for the registered statistic, this should be a number:    Should be a pretty straightforward fix, looks like this first originated back in 2013:  ",Bug,Minor,Resolved,"2014-08-12 19:01:28","2014-08-12 18:01:28",1
"Apache Mesos","Future::failure should return a const string&",,Task,Minor,Resolved,"2014-08-12 00:13:46","2014-08-11 23:13:46",1
"Apache Mesos","Expose metric for container destroy failures","Increment counter when container destroy fails.",Bug,Major,Resolved,"2014-08-08 23:08:29","2014-08-08 22:08:29",3
"Apache Mesos","Create user doc for framework rate limiting feature","Create a Markdown doc under /docs",Task,Major,Resolved,"2014-08-07 19:59:28","2014-08-07 18:59:28",2
"Apache Mesos","AllocatorTest.FrameworkReregistersFirst is flaky.",,Bug,Major,Resolved,"2014-08-06 20:23:12","2014-08-06 19:23:12",2
"Apache Mesos","ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession is flaky","{noformat:title=} [ RUN      ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession I0806 01:18:37.648684 17458 zookeeper_test_server.cpp:158] Started ZooKeeperTestServer on port 42069 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1682db0 flags=0 2014-08-06 01:18:37,656:17458(0x2b468638b700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:37,669:17458(0x2b468638b700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0000, negotiated timeout=6000 I0806 01:18:37.671725 17486 group.cpp:313] Group process (group(37)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:37.671758 17486 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:37.671771 17486 group.cpp:385] Trying to create path '/mesos' in ZooKeeper 2014-08-06 01:18:39,101:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:42,441:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0806 01:18:42.656673 17481 contender.cpp:131] Joining the ZK group I0806 01:18:42.662484 17484 contender.cpp:247] New candidate (id='0') has entered the contest for leadership I0806 01:18:42.663754 17481 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:42.663884 17481 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:42.664788 17483 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x15c00f0 flags=0 2014-08-06 01:18:42,668:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:42,672:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000 I0806 01:18:42.673542 17485 group.cpp:313] Group process (group(38)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:42.673570 17485 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:42.673580 17485 group.cpp:385] Trying to create path '/mesos' in ZooKeeper 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 131ms) 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 115ms) 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms 2014-08-06 01:18:46,799:17458(0x2b4687394700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 1025ms 2014-08-06 01:18:46,800:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0806 01:18:46.806895 17486 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:46.807857 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:47.669064 17482 contender.cpp:131] Joining the ZK group 2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2989ms 2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:47,671:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000 I0806 01:18:47.682868 17485 contender.cpp:247] New candidate (id='1') has entered the contest for leadership I0806 01:18:47.683404 17482 group.cpp:313] Group process (group(38)@127.0.1.1:55561) reconnected to ZooKeeper I0806 01:18:47.683445 17482 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:47.685998 17482 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:47.686142 17482 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:47.687289 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c0421c0 flags=0 2014-08-06 01:18:47,699:17458(0x2b4687de6700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:47,712:17458(0x2b4687de6700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0002, negotiated timeout=6000 I0806 01:18:47.712846 17479 group.cpp:313] Group process (group(39)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:47.712873 17479 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:47.712882 17479 group.cpp:385] Trying to create path '/mesos' in ZooKeeper I0806 01:18:47.714648 17479 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:47.714759 17479 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:47.716130 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:47,718:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response I0806 01:18:47.718889 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... 2014-08-06 01:18:47,720:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response I0806 01:18:47.720788 17484 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:47.724663 17458 zookeeper_test_server.cpp:122] Shutdown ZooKeeperTestServer on port 42069 2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 4133ms 2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:49,720:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 33ms 2014-08-06 01:18:49,721:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:49,722:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:50,136:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:50,800:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:51,723:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:51,723:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:52,801:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:52.842553 17481 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0000) expiration I0806 01:18:52.842911 17481 group.cpp:472] ZooKeeper session expired I0806 01:18:52.843468 17485 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:52.843483 17485 detector.cpp:138] Detected a new leader: None I0806 01:18:52.843618 17485 contender.cpp:196] Membership cancelled: 0 2014-08-06 01:18:52,843:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0000  2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1349ad0 flags=0 2014-08-06 01:18:52,844:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:53,473:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:53.720684 17480 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0001) expiration I0806 01:18:53.721132 17480 group.cpp:472] ZooKeeper session expired I0806 01:18:53.721516 17479 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:53.721534 17479 detector.cpp:138] Detected a new leader: None I0806 01:18:53.721696 17479 contender.cpp:196] Membership cancelled: 1 2014-08-06 01:18:53,721:17458(0x2b46798a3700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0001  2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x16a0550 flags=0 2014-08-06 01:18:53,723:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:53,726:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:53.730258 17479 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0002) expiration I0806 01:18:53.730736 17479 group.cpp:472] ZooKeeper session expired I0806 01:18:53.731081 17481 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:53.731132 17481 detector.cpp:138] Detected a new leader: None 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0002  2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c035f30 flags=0 2014-08-06 01:18:53,733:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:54,512:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:55,393:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:55,403:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:56,301:17458(0x2b468698f700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 122ms 2014-08-06 01:18:56,302:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:56,809:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:57,939:17458(0x2b4686f92700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 879ms 2014-08-06 01:18:57,940:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 870ms 2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client tests/master_contender_detector_tests.cpp:574: Failure Failed to wait 10secs for leaderReconnecting 2014-08-06 01:18:57,941:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  I0806 01:18:57.949972 17458 contender.cpp:186] Now cancelling the membership: 1 2014-08-06 01:18:57,950:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  I0806 01:18:57.950731 17458 contender.cpp:186] Now cancelling the membership: 0 2014-08-06 01:18:57,951:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  ../3rdparty/libprocess/include/process/gmock.hpp:298: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const DispatchEvent&>()))...     Expected args: dispatch matcher (1, 16-byte object <50-20 4A-00 00-00 00-00 00-00 00-00 00-00 00-00>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession (20308 ms) {noformat}",Bug,Major,Resolved,"2014-08-06 19:21:32","2014-08-06 18:21:32",1
"Apache Mesos","Kill private_resources and treat 'ephemeral_ports' as a resource.","As the first step to solve MESOS-1654, we need to kill private_resources in SlaveInfo and add a 'ephemeral_ports' resource.  For now, the slave and the port mapping isolator will simply ignore the 'ephemeral_ports' resource in ExecutorInfo and TaskInfo, and make allocation by itself. We will revisit this once the overcommit race (MESOS-1466) is fixed.",Task,Major,Resolved,"2014-08-06 17:44:25","2014-08-06 16:44:25",3
"Apache Mesos","The value of MASTER_PING_TIMEOUT is non-deterministic","Right now, it is declared as follows:   Since static initialization order in C++ is undefined, MASTER_PING_TIMEOUT's value is non-deterministic. We've already observed that in tests (where MASTER_PING_TIMEOUT == 0).",Bug,Critical,Resolved,"2014-08-05 20:17:58","2014-08-05 19:17:58",1
"Apache Mesos","Add filter to allocator resourcesRecovered method","The allocator already allows filters to be added when resources are unused. It is useful to also allow the same behaviour in {{resourcesRecovered}}.",Task,Major,Resolved,"2014-08-05 19:32:48","2014-08-05 18:32:48",2
"Apache Mesos","Expose executor metrics for slave.","Expose the following metrics:  slave/executors_registering slave/executors_running slave/executors_terminating slave/executors_terminated",Task,Minor,Resolved,"2014-08-05 19:23:53","2014-08-05 18:23:53",2
"Apache Mesos","Handle a temporary one-way master --> slave socket closure.","In MESOS-1529, we realized that it's possible for a slave to remain disconnected in the master if the following occurs:  → Master and Slave connected operating normally. → Temporary one-way network failure, master→slave link breaks. → Master marks slave as disconnected. → Network restored and health checking continues normally, slave is not removed as a result. Slave does not attempt to re-register since it is receiving pings once again. → Slave remains disconnected according to the master, and the slave does not try to re-register. Bad!  We were originally thinking of using a failover timeout in the master to remove these slaves that don't re-register. However, it can be dangerous when ZooKeeper issues are preventing the slave from re-registering with the master; we do not want to remove a ton of slaves in this situation.  Rather, when the slave is health checking correctly but does not re-register within a timeout, we could send a registration request from the master to the slave, telling the slave that it must re-register. This message could also be used when receiving status updates (or other messages) from slaves that are disconnected in the master.",Bug,Minor,Resolved,"2014-08-04 23:04:18","2014-08-04 22:04:18",2
"Apache Mesos","Set maximum executors per slave to avoid overcommit of ephemeral ports","With network isolation, we statically assign ephemeral port ranges. As such there is a upper bound on the number of containers each slave can support.  We should avoid sending offers for slaves that have hit that limit as any tasks will fail to launch and will be LOST. ",Task,Major,Resolved,"2014-08-04 22:42:55","2014-08-04 21:42:55",1
"Apache Mesos","Inform framework when rate limiting is active","When we rate-limit messages from a framework, we should let them know so they can proactively back-off to avoid putting extra pressure on the master.",Improvement,Major,Accepted,"2014-08-04 20:07:54","2014-08-04 19:07:54",3
"Apache Mesos","Network isolator should tolerate slave crashes while doing isolate/cleanup.","A slave may crash while we are installing/removing filters. The slave recovery for the network isolator should tolerate those partially installed filters. Also, we want to avoid leaking a filter on host eth0 and host lo.  The current code cannot tolerate that, thus may cause the following error:  ",Bug,Major,Resolved,"2014-07-29 06:43:16","2014-07-29 05:43:16",3
"Apache Mesos","0.20.0 Release","I would like to volunteer to be the release manager for 0.20.0, which will be releasing the following major features:  - Docker support in Mesos (MESOS-1524)  - Container level network monitoring for mesos containerizer (MESOS-1228)  - Authorization (MESOS-1342)  - Framework rate limiting (MESOS-1306)  - Enable building against installed third-party dependencies (MESOS-1071)  I would like to track blockers for the release on this ticket.",Task,Major,Resolved,"2014-07-28 22:41:05","2014-07-28 21:41:05",5
"Apache Mesos","GLOG Initialized twice if the Framework Scheduler also uses GLOG",,Bug,Major,Resolved,"2014-07-22 23:24:36","2014-07-22 22:24:36",2
"Apache Mesos","Installed protobuf header files include wrong path to mesos header file","Playing with installed mesos headers, realized that we expect users to include the path to mesos directory (e.g., /usr/local/include/mesos) even though it is on the system path. This is because scheduler.pb.h etc include mesos.pb.h instead of mesos/mesos.pb.h.",Bug,Major,Resolved,"2014-07-22 22:30:09","2014-07-22 21:30:09",2
"Apache Mesos","Apache Jenkins build fails due to -lsnappy is set when building leveldb","The failed build: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2261/consoleFull {noformat:title=the log where -lsnappy is used when compiling leveldb} gzip -d -c ../../3rdparty/leveldb.tar.gz | tar xf - test ! -e ../../3rdparty/leveldb.patch || patch -d leveldb -p1 <../../3rdparty/leveldb.patch touch leveldb-stamp cd leveldb && \           make  CC=gcc CXX=g++ OPT=-g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC make[5]: Entering directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_build/3rdparty/leveldb' g++ -pthread -lsnappy -shared -Wl,-soname -Wl,/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_build/3rdparty/leveldb/libleveldb.so.1 -I. -I./include -fno-builtin-memcmp -pthread -DOS_LINUX -DLEVELDB_PLATFORM_POSIX -DSNAPPY -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC -fPIC db/builder.cc db/c.cc db/db_impl.cc db/db_iter.cc db/dbformat.cc db/filename.cc db/log_reader.cc db/log_writer.cc db/memtable.cc db/repair.cc db/table_cache.cc db/version_edit.cc db/version_set.cc db/write_batch.cc table/block.cc table/block_builder.cc table/filter_block.cc table/format.cc table/iterator.cc table/merger.cc table/table.cc table/table_builder.cc table/two_level_iterator.cc util/arena.cc util/bloom.cc util/cache.cc util/coding.cc util/comparator.cc util/crc32c.cc util/env.cc util/env_posix.cc util/filter_policy.cc util/hash.cc util/histogram.cc util/logging.cc util/options.cc util/status.cc  port/port_posix.cc -o libleveldb.so.1.4 ln -fs libleveldb.so.1.4 libleveldb.so ln -fs libleveldb.so.1.4 libleveldb.so.1 g++ -I. -I./include -fno-builtin-memcmp -pthread -DOS_LINUX -DLEVELDB_PLATFORM_POSIX -DSNAPPY -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC -c db/builder.cc -o db/builder.o ",Bug,Major,Resolved,"2014-07-22 05:54:25","2014-07-22 04:54:25",1
"Apache Mesos","Reconciliation does not send back tasks pending validation / authorization.","Per Vinod's feedback on https://reviews.apache.org/r/23542/, we do not send back TASK_STAGING for those tasks that are pending in the Master (validation / authorization still in progress).  For both implicit and explicit task reconciliation, the master could reply with TASK_STAGING for these tasks, as this provides additional information to the framework.",Improvement,Major,Resolved,"2014-07-21 20:38:39","2014-07-21 19:38:39",3
"Apache Mesos","Create design document for Optimistic Offers","As a first step toward Optimistic Offers, take the description from the epic and build an implementation design doc that can be shared for comments.  Note: the links to the working group notes and design doc are located in the [JIRA Epic|MESOS-1607].",Documentation,Major,Resolved,"2014-07-18 00:04:41","2014-07-17 23:04:41",8
"Apache Mesos","Cleanup stout build setup","While investigating stout build setup for making it installable, I came across some discrepancies.  stout tests are included in libprocess's Makefile instead of stout Makefile.  stout's 3rd party dependencies (e.g., picojson) live in libprocess's 3rdparty directory instead of living in stout's (non-existent) 3rd party directory.  It would be nice to fix these issues before making stout installable.",Improvement,Major,Open,"2014-07-16 05:54:41","2014-07-16 04:54:41",3
"Apache Mesos","SlaveRecoveryTest/0.ReconcileKillTask is flaky","Observed this on Jenkins.  ",Bug,Major,Resolved,"2014-07-15 02:01:29","2014-07-15 01:01:29",1
"Apache Mesos","Design inverse resource offer support","An inverse resource offer means that Mesos is requesting resources back from the framework, possibly within some time interval.  This can be leveraged initially to provide more automated cluster maintenance, by offering schedulers the opportunity to move tasks to compensate for planned maintenance. Operators can set a time limit on how long to wait for schedulers to relocate tasks before the tasks are forcibly terminated.  Inverse resource offers have many other potential uses, as it opens the opportunity for the allocator to attempt to move tasks in the cluster through the co-operation of the framework, possibly providing better over-subscription, fairness, etc.",Task,Major,Resolved,"2014-07-14 22:56:05","2014-07-14 21:56:05",5
"Apache Mesos","Allow LoadGeneratorFramework to read password from a file","It currently just reads the flag as the value of the password.",Improvement,Major,Resolved,"2014-07-14 22:22:39","2014-07-14 21:22:39",1
"Apache Mesos","Report disk usage from MesosContainerizer","We should report disk usage for the executor work directory from MesosContainerizer and include in the ResourceStatistics protobuf.",Improvement,Major,Resolved,"2014-07-14 18:28:25","2014-07-14 17:28:25",5
"Apache Mesos","Isolate system directories, e.g., per-container /tmp","Ideally, tasks should not write outside their sandbox (executor work directory) but pragmatically they may need to write to /tmp, /var/tmp, or some other directory.  1) We should include any such files in disk usage and quota. 2) We should make these shared directories private, i.e., each container has their own. 3) We should make the lifetime of any such files the same as the executor work directory.",Improvement,Major,Resolved,"2014-07-14 18:26:47","2014-07-14 17:26:47",3
"Apache Mesos","Improve framework rate limiting by imposing the max number of outstanding messages per framework principal","* Rate limits config takes a configurable *capacity* for each principal. * To ensure that Master maintain the message order of a framework it's important that Master sends an FrameworkErrorMessage back to the scheduler to ask it to abort.",Bug,Major,Resolved,"2014-07-11 20:05:00","2014-07-11 19:05:00",5
"Apache Mesos","Signal escalation timeout is not configurable.","Even though the executor shutdown grace period is set to a larger interval, the signal escalation timeout will still be 3 seconds. It should either be configurable or dependent on EXECUTOR_SHUTDOWN_GRACE_PERIOD.  Thoughts?",Improvement,Major,Resolved,"2014-07-08 20:11:39","2014-07-08 19:11:39",8
"Apache Mesos","Add logging of the user uid when receiving SIGTERM.","We currently do not log the user id when receiving a SIGTERM, this makes debugging a bit difficult. It's easy to get this information through sigaction.",Improvement,Major,Resolved,"2014-07-08 04:04:29","2014-07-08 03:04:29",1
"Apache Mesos","Allow jenkins build machine to dump stack traces of all threads when timeout","Many of the time, when jenkins build times out, we know that some test freezes at some place. However, most of the time, it's very hard to reproduce the deadlock on dev machines.  I would be cool if we can dump the stack traces of all threads when jenkins build times out. Some command like the following:  ",Improvement,Minor,Resolved,"2014-07-02 22:33:50","2014-07-02 21:33:50",5
"Apache Mesos","SlaveRecoveryTest/0.MultipleFrameworks is flaky",,Bug,Major,Resolved,"2014-06-26 01:42:48","2014-06-26 00:42:48",1
"Apache Mesos","Handle a network partition between Master and Slave","If a network partition occurs between a Master and Slave, the Master will remove the Slave (as it fails health check) and mark the tasks being run there as LOST. However, the Slave is not aware that it has been removed so the tasks will continue to run.  (To clarify a little bit: neither the master nor the slave receives 'exited' event, indicating that the connection between the master and slave is not closed).  There are at least two possible approaches to solving this issue:  1. Introduce a health check from Slave to Master so they have a consistent view of a network partition. We may still see this issue should a one-way connection error occur.  2. Be less aggressive about marking tasks and Slaves as lost. Wait until the Slave reappears and reconcile then. We'd still need to mark Slaves and tasks as potentially lost (zombie state) but maybe the Scheduler can make a more intelligent decision.",Bug,Major,Resolved,"2014-06-23 00:42:16","2014-06-22 23:42:16",5
"Apache Mesos","Choose containerizer at runtime","Currently you have to choose the containerizer at mesos-slave start time via the --isolation option.  I'd like to be able to specify the containerizer in the request to launch the job. This could be specified by a new Provider field in the ContainerInfo proto buf.",Improvement,Major,Resolved,"2014-06-21 01:05:00","2014-06-21 00:05:00",3
"Apache Mesos","Update Rate Limiting Design doc to reflect the latest changes","- Usage - Design - Implementation Notes",Improvement,Major,Resolved,"2014-06-19 22:27:29","2014-06-19 21:27:29",2
"Apache Mesos","Improve child exit if slave dies during executor launch in MC","When restarting many slaves there's a reasonable chance that a slave will be restarted between the fork and exec stages of launching an executor in the MesosContainerizer.  The forked child correctly detects this however rather than abort it should safely log and then exit non-zero cleanly.",Bug,Major,Resolved,"2014-06-11 23:29:10","2014-06-11 22:29:10",1
"Apache Mesos","Document replicated log design/internals","The replicated log could benefit from some documentation. In particular, how does it work? What do operators need to know? Possibly there is some overlap with our future maintenance documentation in MESOS-1470.  I believe [~<USER> has some unpublished work that could be leveraged here!",Documentation,Major,Resolved,"2014-06-11 04:48:02","2014-06-11 03:48:02",5
"Apache Mesos","No output from review bot on timeout","When the mesos review build times out, likely due to a long-running failing test, we have no output to debug. We should find a way to stream the output from the build instead of waiting for the build to finish.",Bug,Minor,Resolved,"2014-06-10 18:35:42","2014-06-10 17:35:42",2
"Apache Mesos","Race between executor exited event and launch task can cause overcommit of resources","The following sequence of events can cause an overcommit  --> Launch task is called for a task whose executor is already running  --> Executor's resources are not accounted for on the master  --> Executor exits and the event is enqueued behind launch tasks on the master  --> Master sends the task to the slave which needs to commit for resources for task and the (new) executor.  --> Master processes the executor exited event and re-offers the executor's resources causing an overcommit of resources.",Bug,Major,Resolved,"2014-06-10 01:32:37","2014-06-10 00:32:37",8
"Apache Mesos","Build failure: Ubuntu 13.10/clang due to missing virtual destructor","In file included from launcher/main.cpp:19: In file included from ./launcher/launcher.hpp:24: In file included from ../3rdparty/libprocess/include/process/future.hpp:23: ../3rdparty/libprocess/include/process/owned.hpp:188:5: error: delete called on 'mesos::internal::launcher::Operation' that is abstract but has non-virtual destructor [-Werror,-Wdelete-non-virtual-dtor]     delete t;     ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:456:8: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Data::~Data' requested here               delete __p;               ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:768:24: note: in instantiation of function template specialization 'std::__shared_count<2>::__shared_count<process::Owned<mesos::internal::launcher::Operation>::Data *>' requested here         : _M_ptr(__p), _M_refcount(__p)                        ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:919:4: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here           __shared_ptr(__p).swap(*this);           ^ ../3rdparty/libprocess/include/process/owned.hpp:68:10: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::reset<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here     data.reset(new Data(t));          ^ ./launcher/launcher.hpp:101:7: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Owned' requested here   add(process::Owned<Operation>(new T()));       ^ launcher/main.cpp:26:3: note: in instantiation of function template specialization 'mesos::internal::launcher::add<mesos::internal::launcher::ShellOperation>' requested here   launcher::add<launcher::ShellOperation>();   ^ 1 error generated.",Bug,Major,Resolved,"2014-06-06 18:34:38","2014-06-06 17:34:38",1
"Apache Mesos","Improve Master::removeOffer to avoid further resource accounting bugs.","Per comments on this review: https://reviews.apache.org/r/21750/  We've had numerous bugs around resource accounting in the master due to the trickiness of removing offers in the Master code.  There are a few ways to improve this:  1. Add multiple offer methods to differentiate semantics:   2. Add an enum to removeOffer to differentiate removal semantics: ",Improvement,Major,Resolved,"2014-06-03 02:44:52","2014-06-03 01:44:52",3
"Apache Mesos","Add new tests for framework rate limiting",,Task,Major,Resolved,"2014-05-31 00:54:01","2014-05-30 23:54:01",3
"Apache Mesos","Integrate rate limiter into the master",,Task,Major,Resolved,"2014-05-31 00:52:48","2014-05-30 23:52:48",5
"Apache Mesos","Create a protobuf for framework rate limit configuration and load it as JSON through master flags",,Task,Major,Resolved,"2014-05-31 00:51:13","2014-05-30 23:51:13",2
"Apache Mesos","LogZooKeeperTest.WriteRead test is flaky",,Bug,Major,Resolved,"2014-05-28 02:01:16","2014-05-28 01:01:16",1
"Apache Mesos","Mesos tests should not rely on echo","Triggered by MESOS-1413 I would like to propose changing our tests to not rely on {{echo}} but to use {{printf}} instead.  This seems to be useful as {{echo}} is introducing an extra linefeed after the supplied string whereas {{printf}} does not. The {{-n}} switch preventing that extra linefeed is unfortunately not portable - it is not supported by the builtin {{echo}} of the BSD / OSX {{/bin/sh}}. ",Improvement,Minor,Resolved,"2014-05-27 23:31:20","2014-05-27 22:31:20",1
"Apache Mesos","Keep terminal unacknowledged tasks in the master's state.","Once we are sending acknowledgments through the master as per MESOS-1409, we need to keep terminal tasks that are *unacknowledged* in the Master's memory.  This will allow us to identify these tasks to frameworks when we haven't yet forwarded them an update. Without this, we're susceptible to MESOS-1389.",Task,Major,Resolved,"2014-05-22 20:33:18","2014-05-22 19:33:18",5
"Apache Mesos","Document perf isolator flags","Document interval, duration and the event flags. Document event name normalization for the protobuf.",Improvement,Major,Resolved,"2014-05-20 23:10:59","2014-05-20 22:10:59",1
"Apache Mesos","Rename ResourceStatistics for containers","Rename ContainerStatistics which includes optional ResourceStatistics and optional PerfStatistics.",Improvement,Major,Open,"2014-05-20 23:10:14","2014-05-20 22:10:14",8
"Apache Mesos","Introduce a PerfStatistics protobuf","Field names from `perf list` normalized to convert hyphens to underscores and down-cased. Start with just the hardware and software events, not raw hardware, breakpoints or tracepoints,   All fields should be optional. Include as an optional field to ResourceStatistics.",Improvement,Major,Resolved,"2014-05-20 23:09:27","2014-05-20 22:09:27",2
"Apache Mesos","Test perf isolator for slave roll forward/roll back","Test that changes to add/remove perf isolator will be handled through slave recovery, e.g., containers started without the perf isolator continue to report resource statistics and containers started with the perf isolator will include perf statistics.",Improvement,Major,Resolved,"2014-05-20 23:07:17","2014-05-20 22:07:17",2
"Apache Mesos","Test different versions of perf","Test across different kernel versions (at least 2.6.XX and 3.X) and across different distributions.  Test input flags and parsing output.",Improvement,Major,Resolved,"2014-05-20 23:05:38","2014-05-20 22:05:38",3
"Apache Mesos","Write parser for perf output.","1. Should support output from pid and cgroup targets. 2. Should support output for the same events from >= 1 cgroup 3. Should return as PerfStatistics protobuf. ",Improvement,Major,Resolved,"2014-05-20 23:04:03","2014-05-20 22:04:03",3
"Apache Mesos","Failure when znode is removed before we can read its contents.","Looks like the following can occur when a znode goes away right before we can read it's contents:  {noformat: title=Slave exit} I0520 16:33:45.721727 29155 group.cpp:382] Trying to create path '/home/mesos/test/master' in ZooKeeper I0520 16:33:48.600837 29155 detector.cpp:134] Detected a new leader: (id='2617') I0520 16:33:48.601428 29147 group.cpp:655] Trying to get '/home/mesos/test/master/info_0000002617' in ZooKeeper Failed to detect a master: Failed to get data for ephemeral node '/home/mesos/test/master/info_0000002617' in ZooKeeper: no node Slave Exit Status: 1 {noformat}",Bug,Major,Resolved,"2014-05-20 19:51:25","2014-05-20 18:51:25",3
"Apache Mesos","Verify static libprocess scheduler port works with Mesos Master",,Task,Major,Resolved,"2014-05-15 00:45:33","2014-05-14 23:45:33",5
"Apache Mesos","Keep track of the principals for authenticated pids in Master.","Need to add a 'principal' field to FrameworkInfo and verify if the Framework has the claimed principal during registration.",Task,Major,Resolved,"2014-05-15 00:29:39","2014-05-14 23:29:39",3
"Apache Mesos","Expose libprocess queue length from scheduler driver to metrics endpoint","We expose the master's event queue length and we should do the same for the scheduler driver.",Task,Major,Resolved,"2014-05-14 21:59:15","2014-05-14 20:59:15",1
"Apache Mesos","SlaveRecoveryTest/0.MultipleFrameworks is flaky","--gtest_repeat=-1 --gtest_shuffle --gtest_break_on_failure  ",Bug,Minor,Resolved,"2014-05-13 23:57:15","2014-05-13 22:57:15",1
"Apache Mesos","Show when the leading master was elected in the webui","This would be nice to have during debugging.",Improvement,Minor,Resolved,"2014-05-13 21:58:48","2014-05-13 20:58:48",1
"Apache Mesos","GarbageCollectorIntegrationTest.DiskUsage is flaky.","From Jenkins: https://builds.apache.org/job/Mesos-Ubuntu-distcheck/79/consoleFull  ",Bug,Major,Resolved,"2014-05-12 20:14:42","2014-05-12 19:14:42",2
"Apache Mesos","Add flags support for JSON",,Improvement,Major,Resolved,"2014-05-10 18:07:01","2014-05-10 17:07:01",2
"Apache Mesos","Add per-framework-principal counters for all messages from a scheduler on Master","Framework::principal is used identify one or more frameworks. If multiple frameworks use the same principal they'll have one counter showing their combined message count.",Improvement,Major,Resolved,"2014-05-09 22:03:18","2014-05-09 21:03:18",3
"Apache Mesos","Improve Master and Slave metric names","As we move the metrics to a new endpoint, we should consider revisiting the names of some of the current metrics to make them clearer.  It may also be worth considering changing some existing counter-style metrics to gauges. ",Improvement,Major,Resolved,"2014-05-09 00:40:37","2014-05-08 23:40:37",3
"Apache Mesos","Implement decent unit test coverage for the mesos-fetcher tool","There are current no tests that cover the {{mesos-fetcher}} tool itself, and hence bugs like MESOS-1313 have accidentally slipped though.",Improvement,Major,Resolved,"2014-05-06 22:43:00","2014-05-06 21:43:00",2
"Apache Mesos","Authorize offer allocations","When frameworks register or reregister they should authorize their roles.    Split register framework / reregister framework.   ",Task,Major,Resolved,"2014-05-05 21:46:31","2014-05-05 20:46:31",8
"Apache Mesos","ExamplesTest.{TestFramework, NoExecutorFramework} flaky","I'm having trouble reproducing this but I did observe it once on my OSX system:    when investigating a failed make check for https://reviews.apache.org/r/20971/ ",Bug,Major,Resolved,"2014-05-05 20:05:43","2014-05-05 19:05:43",1
"Apache Mesos","stout's os::ls should return a Try<>","stout's os::ls returns a list that can be empty - instead it should return a Try<list...> to be consistent.",Improvement,Minor,Resolved,"2014-04-23 22:13:48","2014-04-23 21:13:48",2
"Apache Mesos","stout's os module uses a mix of Try<Nothing> and bool returns","stout's os module should use Try<Nothing> for return values throughout.",Improvement,Minor,Resolved,"2014-04-23 22:11:46","2014-04-23 21:11:46",2
"Apache Mesos","Master should disallow frameworks that reconnect after failover timeout.","When a scheduler reconnects after the failover timeout has exceeded, the framework id is usually reused because the scheduler doesn't know that the timeout exceeded and it is actually handled as a new framework.  The /framework/:framework_id route of the Web UI doesn't handle those cases very well because its key is reused. It only shows the terminated one.  Would it make sense to ignore the provided framework id when a scheduler reconnects to a terminated framework and generate a new id to make sure it's unique?",Bug,Major,Resolved,"2014-04-17 06:49:07","2014-04-17 05:49:07",2
"Apache Mesos","Subprocess is slow -> gated by process::reap poll interval","Subprocess uses process::reap to wait on the subprocess pid and set the exit status. However, process::reap polls with a one second interval resulting in a delay up to the interval duration before the status future is set.  This means if you need to wait for the subprocess to complete you get hit with E(delay) = 0.5 seconds, independent of the execution time. For example, the MesosContainerizer uses mesos-fetcher in a Subprocess to fetch the executor during launch. At Twitter we fetch a local file, i.e., a very fast operation, but the launch is blocked until the mesos-fetcher pid is reaped -> adding 0 to 1 seconds for every launch!  The problem is even worse with a chain of short Subprocesses because after the first Subprocess completes you'll be synchronized with the reap interval and you'll see nearly the full interval before notification, i.e., 10 Subprocesses each of << 1 second duration with take ~10 seconds!  This has become particularly apparent in some new tests I'm working on where test durations are now greatly extended with each taking several seconds.",Improvement,Major,Resolved,"2014-04-08 19:20:27","2014-04-08 18:20:27",1
"Apache Mesos","systemd.slice + cgroup enablement fails in multiple ways. ","When attempting to configure mesos to use systemd slices on a 'rawhide/f21' machine, it fails creating the isolator:   I0407 12:39:28.035354 14916 containerizer.cpp:180] Using isolation: cgroups/cpu,cgroups/mem Failed to create a containerizer: Could not create isolator cgroups/cpu: Failed to create isolator: The cpu subsystem is co-mounted at /sys/fs/cgroup/cpu with other subsytems  ------ details ------ /sys/fs/cgroup total 0 drwxr-xr-x. 12 root root 280 Mar 18 08:47 . drwxr-xr-x.  6 root root   0 Mar 18 08:47 .. drwxr-xr-x.  2 root root   0 Mar 18 08:47 blkio lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpu -> cpu,cpuacct lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpuacct -> cpu,cpuacct drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpu,cpuacct drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpuset drwxr-xr-x.  2 root root   0 Mar 18 08:47 devices drwxr-xr-x.  2 root root   0 Mar 18 08:47 freezer drwxr-xr-x.  2 root root   0 Mar 18 08:47 hugetlb drwxr-xr-x.  3 root root   0 Apr  3 11:26 memory drwxr-xr-x.  2 root root   0 Mar 18 08:47 net_cls drwxr-xr-x.  2 root root   0 Mar 18 08:47 perf_event drwxr-xr-x.  4 root root   0 Mar 18 08:47 systemd ",Bug,Major,Resolved,"2014-04-07 19:24:53","2014-04-07 18:24:53",3
"Apache Mesos","Add support for rate limiting slave removal","To safeguard against unforeseen bugs leading to widespread slave removal, it would be nice to allow for rate limiting of the decision to remove slaves and/or send TASK_LOST messages for tasks on those slaves.  Ideally this would allow an operator to be notified soon enough to intervene before causing cluster impact.",Improvement,Major,Resolved,"2014-03-26 20:48:43","2014-03-26 20:48:43",3
"Apache Mesos","Add a TASK_ERROR task status.","During task validation we drop tasks that have errors and send TASK_LOST status updates. In most circumstances a framework will want to relaunch a task that has gone lost, and in the event the task is actually malformed (thus invalid) this will result in an infinite loop of sending a task and having it go lost.",Improvement,Major,Resolved,"2014-03-24 16:13:43","2014-03-24 16:13:43",2
"Apache Mesos","Implement the protobufs for the scheduler API","The default scheduler/executor interface and implementation in Mesos have a few drawbacks:  (1) The interface is fairly high-level which makes it hard to do certain things, for example, handle events (callbacks) in batch. This can have a big impact on the performance of schedulers (for example, writing task updates that need to be persisted).  (2) The implementation requires writing a lot of boilerplate JNI and native Python wrappers when adding additional API components.  The plan is to provide a lower-level API that can easily be used to implement the higher-level API that is currently provided. This will also open the door to more easily building native-language Mesos libraries (i.e., not needing the C++ shim layer) and building new higher-level abstractions on top of the lower-level API.",Task,Major,Resolved,"2014-03-20 17:23:16","2014-03-20 17:23:16",8
"Apache Mesos","HTTP auth for CLI","Integrate HTTP auth into the CLI programs",Improvement,Minor,Open,"2014-03-18 20:00:10","2014-03-18 20:00:10",3
"Apache Mesos","Allocator should make an allocation decision per slave instead of per framework/role.","Currently the Allocator::allocate() code loops through roles and frameworks (based on DRF sort) and allocates *all* slaves resources to the first framework.  This logic should be a bit inversed. Instead, the slave should go through each slave, allocate it a role/framework and update the DRF shares.",Bug,Major,Resolved,"2014-03-18 18:25:15","2014-03-18 18:25:15",2
"Apache Mesos","Authorize task/executor launches",,Task,Major,Resolved,"2014-03-17 22:31:28","2014-03-17 22:31:28",8
"Apache Mesos","Master should not deactivate authenticated framework/slave on new AuthenticateMessage unless new authentication succeeds.","Master should not deactivate an authenticated framework/slave upon receiving a new AuthenticateMessage unless new authentication succeeds. As it stands now, a malicious user could spoof the pid of an authenticated framework/slave and send an AuthenticateMessage to knock a valid framework/slave off the authenticated list, forcing the valid framework/slave to re-authenticate and re-register. This could be used in a DoS attack. But how should we handle the scenario when the actual authenticated framework/slave sends an AuthenticateMessage that fails authentication?",Bug,Major,Resolved,"2014-03-11 21:48:01","2014-03-11 21:48:01",1
"Apache Mesos","ExamplesTest.JavaLog is flaky","The {{ExamplesTest.JavaLog}} test framework is flaky, possibly related to a race condition between mutexes.   Full logs attached.",Bug,Major,Resolved,"2014-02-18 21:34:32","2014-02-18 21:34:32",2
"Apache Mesos","Python extension build is broken if gflags-dev is installed","In my environment mesos build from master results in broken python api module {{_mesos.so}}:  Unmangled version of symbol looks like this:  During {{./configure}} step {{glog}} finds {{gflags}} development files and starts using them, thus *implicitly* adding dependency on {{libgflags.so}}. This breaks Python extensions module and perhaps can break other mesos subsystems when moved to hosts without {{gflags}} installed.  This task is done when the ExamplesTest.PythonFramework test will pass on a system with gflags installed.",Bug,Major,Resolved,"2014-02-17 21:24:16","2014-02-17 21:24:16",3
"Apache Mesos","Slave should wait() and start executor registration timeout after launch ","The current code will start launch a container and wait on it before the launch is complete. We should do this only after the container has successfully launched. Likewise for the executor registration timeout.",Bug,Minor,Accepted,"2014-02-12 23:57:59","2014-02-12 23:57:59",3
"Apache Mesos","Slave should wait until Containerizer::update() completes successfully","Container resources are updated in several places in the slave and we don't check the update was successful or even wait until it completes.",Bug,Major,Resolved,"2014-02-12 23:54:29","2014-02-12 23:54:29",5
"Apache Mesos","ExamplesTest.PythonFramework is flaky","Looks like a SEGFAULT during shutdown.  ",Bug,Major,Resolved,"2014-02-11 22:25:26","2014-02-11 22:25:26",3
"Apache Mesos","SlaveRecoveryTest/1.SchedulerFailover is flaky","[==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from SlaveRecoveryTest/1, where TypeParam = mesos::internal::slave::CgroupsIsolator [ RUN      ] SlaveRecoveryTest/1.SchedulerFailover I0206 20:18:31.525116 56447 master.cpp:239] Master ID: 2014-02-06-20:18:31-1740121354-55566-56447 Hostname: smfd-bkq-03-sr4.devel.twitter.com I0206 20:18:31.525295 56481 master.cpp:321] Master started on 10.37.184.103:55566 I0206 20:18:31.525315 56481 master.cpp:324] Master only allowing authenticated frameworks to register! I0206 20:18:31.527093 56481 master.cpp:756] The newly elected leader is master@10.37.184.103:55566 I0206 20:18:31.527122 56481 master.cpp:764] Elected as the leading master! I0206 20:18:31.530642 56473 slave.cpp:112] Slave started on 9)@10.37.184.103:55566 I0206 20:18:31.530802 56473 slave.cpp:212] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:31.531203 56473 slave.cpp:240] Slave hostname: smfd-bkq-03-sr4.devel.twitter.com I0206 20:18:31.531221 56473 slave.cpp:241] Slave checkpoint: true I0206 20:18:31.531991 56482 cgroups_isolator.cpp:225] Using /tmp/mesos_test_cgroup as cgroups hierarchy root I0206 20:18:31.532470 56478 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta' I0206 20:18:31.532698 56469 status_update_manager.cpp:188] Recovering status update manager I0206 20:18:31.533962 56472 sched.cpp:265] Authenticating with master master@10.37.184.103:55566 I0206 20:18:31.534102 56472 sched.cpp:234] Detecting new master I0206 20:18:31.534124 56484 authenticatee.hpp:124] Creating new client SASL connection I0206 20:18:31.534299 56473 master.cpp:2317] Authenticating framework at scheduler(9)@10.37.184.103:55566 I0206 20:18:31.534459 56461 authenticator.hpp:140] Creating new server SASL connection I0206 20:18:31.534572 56466 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0206 20:18:31.534595 56466 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 20:18:31.534667 56474 authenticator.hpp:243] Received SASL authentication start I0206 20:18:31.534732 56474 authenticator.hpp:325] Authentication requires more steps I0206 20:18:31.534814 56468 authenticatee.hpp:258] Received SASL authentication step I0206 20:18:31.534946 56466 authenticator.hpp:271] Received SASL authentication step I0206 20:18:31.535007 56466 authenticator.hpp:317] Authentication success I0206 20:18:31.535084 56471 authenticatee.hpp:298] Authentication success I0206 20:18:31.535107 56461 master.cpp:2357] Successfully authenticated framework at scheduler(9)@10.37.184.103:55566 I0206 20:18:31.535392 56476 sched.cpp:339] Successfully authenticated with master master@10.37.184.103:55566 I0206 20:18:31.535512 56465 master.cpp:812] Received registration request from scheduler(9)@10.37.184.103:55566 I0206 20:18:31.535570 56465 master.cpp:830] Registering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at scheduler(9)@10.37.184.103:55566 I0206 20:18:31.535856 56465 hierarchical_allocator_process.hpp:332] Added framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.537802 56482 cgroups_isolator.cpp:840] Recovering isolator I0206 20:18:31.538462 56472 slave.cpp:2760] Finished recovery I0206 20:18:31.538910 56472 slave.cpp:508] New master detected at master@10.37.184.103:55566 I0206 20:18:31.539036 56478 status_update_manager.cpp:162] New master detected at master@10.37.184.103:55566 I0206 20:18:31.539223 56464 master.cpp:1834] Attempting to register slave on smfd-bkq-03-sr4.devel.twitter.com at slave(9)@10.37.184.103:55566 I0206 20:18:31.539271 56472 slave.cpp:533] Detecting new master I0206 20:18:31.539330 56464 master.cpp:2804] Adding slave 2014-02-06-20:18:31-1740121354-55566-56447-0 at smfd-bkq-03-sr4.devel.twitter.com with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:31.539454 56472 slave.cpp:551] Registered with master master@10.37.184.103:55566; given slave ID 2014-02-06-20:18:31-1740121354-55566-56447-0 I0206 20:18:31.539620 56472 slave.cpp:564] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/slave.info' I0206 20:18:31.539834 56475 hierarchical_allocator_process.hpp:445] Added slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0206 20:18:31.540341 56472 master.cpp:2272] Sending 1 offers to framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.543433 56472 master.cpp:1568] Processing reply for offers: [ 2014-02-06-20:18:31-1740121354-55566-56447-0 ] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.543642 56472 master.hpp:411] Adding task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) I0206 20:18:31.543781 56472 master.cpp:2441] Launching task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) I0206 20:18:31.544002 56484 slave.cpp:736] Got assigned task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.544097 56484 slave.cpp:2899] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.info' I0206 20:18:31.544272 56484 slave.cpp:2906] Checkpointing framework pid 'scheduler(9)@10.37.184.103:55566' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.pid' I0206 20:18:31.544617 56484 slave.cpp:845] Launching task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.546721 56484 slave.cpp:3169] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/executor.info' I0206 20:18:31.547317 56484 slave.cpp:3257] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/tasks/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/task.info' I0206 20:18:31.547514 56484 slave.cpp:955] Queuing task 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework '2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.547590 56481 cgroups_isolator.cpp:517] Launching d045a0bd-2ed2-410a-bd1f-5bd9219896e3 (/home/vinod/mesos/build/src/mesos-executor) in /tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 in cgroup mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 I0206 20:18:31.548408 56481 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:31.548833 56481 cgroups_isolator.cpp:1007] Updated 'cpu.shares' to 2048 for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.549294 56481 cgroups_isolator.cpp:1117] Updated 'memory.soft_limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.550107 56481 cgroups_isolator.cpp:1147] Updated 'memory.limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.550571 56481 cgroups_isolator.cpp:1174] Started listening for OOM events for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.551553 56481 cgroups_isolator.cpp:569] Forked executor at = 56671 Checkpointing executor's forked pid 56671 to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/pids/forked.pid' I0206 20:18:31.552222 56472 slave.cpp:2098] Monitoring executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 forked at pid 56671 Fetching resources into '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986' I0206 20:18:31.604012 56472 slave.cpp:1431] Got registration for executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.604167 56472 slave.cpp:1516] Checkpointing executor pid 'executor(1)@10.37.184.103:46181' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/pids/libprocess.pid' I0206 20:18:31.605183 56472 slave.cpp:1552] Flushing queued task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 Registered executor on smfd-bkq-03-sr4.devel.twitter.com Starting task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 sh -c 'sleep 1000' Forked command at 56712 I0206 20:18:31.613098 56481 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from executor(1)@10.37.184.103:46181 I0206 20:18:31.613628 56469 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.614006 56469 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.795529 56469 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to master@10.37.184.103:55566 I0206 20:18:31.795992 56480 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to executor(1)@10.37.184.103:46181 I0206 20:18:31.796131 56471 master.cpp:2020] Status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from slave(9)@10.37.184.103:55566 I0206 20:18:31.797099 56483 status_update_manager.cpp:392] Received status update acknowledgement (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.797165 56483 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.882767 56481 slave.cpp:394] Slave terminating I0206 20:18:31.883112 56481 master.cpp:641] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) disconnected I0206 20:18:31.883200 56476 hierarchical_allocator_process.hpp:484] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 disconnected I0206 20:18:31.888206 56473 sched.cpp:265] Authenticating with master master@10.37.184.103:55566 I0206 20:18:31.888473 56473 sched.cpp:234] Detecting new master I0206 20:18:31.888556 56469 authenticatee.hpp:124] Creating new client SASL connection I0206 20:18:31.888978 56484 master.cpp:2317] Authenticating framework at scheduler(10)@10.37.184.103:55566 I0206 20:18:31.889348 56469 authenticator.hpp:140] Creating new server SASL connection I0206 20:18:31.889925 56469 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0206 20:18:31.889989 56469 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 20:18:31.890059 56469 authenticator.hpp:243] Received SASL authentication start I0206 20:18:31.890233 56469 authenticator.hpp:325] Authentication requires more steps I0206 20:18:31.890399 56468 authenticatee.hpp:258] Received SASL authentication step I0206 20:18:31.890554 56484 authenticator.hpp:271] Received SASL authentication step I0206 20:18:31.890630 56484 authenticator.hpp:317] Authentication success I0206 20:18:31.890728 56470 authenticatee.hpp:298] Authentication success I0206 20:18:31.890748 56484 master.cpp:2357] Successfully authenticated framework at scheduler(10)@10.37.184.103:55566 I0206 20:18:31.892210 56469 sched.cpp:339] Successfully authenticated with master master@10.37.184.103:55566 I0206 20:18:31.892410 56473 master.cpp:900] Re-registering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at scheduler(10)@10.37.184.103:55566 I0206 20:18:31.892460 56473 master.cpp:926] Framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 failed over W0206 20:18:31.892691 56465 master.cpp:1048] Ignoring deactivate framework message for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from 'scheduler(9)@10.37.184.103:55566' because it is not from the registered framework 'scheduler(10)@10.37.184.103:55566' I0206 20:18:31.897049 56466 slave.cpp:112] Slave started on 10)@10.37.184.103:55566 I0206 20:18:31.897207 56466 slave.cpp:212] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:31.897536 56466 slave.cpp:240] Slave hostname: smfd-bkq-03-sr4.devel.twitter.com I0206 20:18:31.897554 56466 slave.cpp:241] Slave checkpoint: true I0206 20:18:31.898388 56463 cgroups_isolator.cpp:225] Using /tmp/mesos_test_cgroup as cgroups hierarchy root I0206 20:18:31.898936 56472 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta' I0206 20:18:31.901702 56465 slave.cpp:2828] Recovering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.901759 56465 slave.cpp:3020] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.902716 56464 status_update_manager.cpp:188] Recovering status update manager I0206 20:18:31.902884 56464 status_update_manager.cpp:196] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.475915 56463 cgroups_isolator.cpp:840] Recovering isolator I0206 20:18:34.476066 56463 cgroups_isolator.cpp:847] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.477478 56463 cgroups_isolator.cpp:1174] Started listening for OOM events for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.478728 56463 slave.cpp:2700] Sending reconnect request to executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at executor(1)@10.37.184.103:46181 I0206 20:18:34.480114 56476 slave.cpp:1597] Re-registering executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.480566 56476 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:34.481370 56476 cgroups_isolator.cpp:1007] Updated 'cpu.shares' to 2048 for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.481827 56476 cgroups_isolator.cpp:1117] Updated 'memory.soft_limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 Re-registered executor on smfd-bkq-03-sr4.devel.twitter.com I0206 20:18:34.489497 56471 slave.cpp:1713] Cleaning up un-reregistered executors I0206 20:18:34.489588 56471 slave.cpp:2760] Finished recovery I0206 20:18:34.490048 56463 slave.cpp:508] New master detected at master@10.37.184.103:55566 I0206 20:18:34.490257 56475 status_update_manager.cpp:162] New master detected at master@10.37.184.103:55566 I0206 20:18:34.490357 56463 slave.cpp:533] Detecting new master W0206 20:18:34.490603 56480 master.cpp:1878] Slave at slave(10)@10.37.184.103:55566 (smfd-bkq-03-sr4.devel.twitter.com) is being allowed to re-register with an already in use id (2014-02-06-20:18:31-1740121354-55566-56447-0) I0206 20:18:34.490927 56479 slave.cpp:601] Re-registered with master master@10.37.184.103:55566 I0206 20:18:34.491322 56461 hierarchical_allocator_process.hpp:498] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 reconnected I0206 20:18:34.491421 56468 slave.cpp:1312] Updating framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 pid to scheduler(10)@10.37.184.103:55566 I0206 20:18:34.491444 56480 master.cpp:1673] Asked to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.491488 56468 slave.cpp:1320] Checkpointing framework pid 'scheduler(10)@10.37.184.103:55566' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.pid' I0206 20:18:34.491497 56480 master.cpp:1707] Telling slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.491657 56468 slave.cpp:1013] Asked to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 Shutting down Killing process tree at pid 56712 Killed the following process trees: [  --- 56712 sleep 1000  ] Command terminated with signal Killed (pid: 56712) I0206 20:18:34.615216 56463 slave.cpp:1765] Handling status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from executor(1)@10.37.184.103:46181 I0206 20:18:34.615556 56483 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources  I0206 20:18:34.615624 56476 status_update_manager.cpp:314] Received status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.615701 56476 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.706945 56476 status_update_manager.cpp:367] Forwarding status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to master@10.37.184.103:55566 I0206 20:18:34.707263 56476 slave.cpp:1890] Sending acknowledgement for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to executor(1)@10.37.184.103:46181 I0206 20:18:34.707352 56469 master.cpp:2020] Status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from slave(10)@10.37.184.103:55566 I0206 20:18:34.707620 56469 master.hpp:429] Removing task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) I0206 20:18:34.708348 56466 hierarchical_allocator_process.hpp:637] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 from framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.708673 56469 status_update_manager.cpp:392] Received status update acknowledgement (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.708749 56469 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.709411 56470 master.cpp:2272] Sending 1 offers to framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.809782 56447 master.cpp:583] Master terminating I0206 20:18:34.810066 56447 master.cpp:246] Shutting down master I0206 20:18:34.810134 56482 slave.cpp:1965] master@10.37.184.103:55566 exited W0206 20:18:34.810184 56482 slave.cpp:1968] Master disconnected! Waiting for a new master to be elected I0206 20:18:34.810652 56447 master.cpp:289] Removing slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) I0206 20:18:34.813144 56447 slave.cpp:394] Slave terminating I0206 20:18:34.821583 56467 cgroups.cpp:1209] Trying to freeze cgroup /tmp/mesos_test_cgroup/mesos_test I0206 20:18:34.821652 56467 cgroups.cpp:1248] Successfully froze cgroup /tmp/mesos_test_cgroup/mesos_test after 1 attempts I0206 20:18:34.823129 56471 cgroups.cpp:1224] Trying to thaw cgroup /tmp/mesos_test_cgroup/mesos_test I0206 20:18:34.823247 56471 cgroups.cpp:1334] Successfully thawed /tmp/mesos_test_cgroup/mesos_test I0206 20:18:34.923945 56470 cgroups.cpp:1209] Trying to freeze cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 I0206 20:18:34.924018 56470 cgroups.cpp:1248] Successfully froze cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 after 1 attempts I0206 20:18:34.925506 56461 cgroups.cpp:1224] Trying to thaw cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 I0206 20:18:34.925580 56461 cgroups.cpp:1334] Successfully thawed /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 [       OK ] SlaveRecoveryTest/1.SchedulerFailover (3408 ms) [----------] 1 test from SlaveRecoveryTest/1 (3409 ms total)  [----------] Global test environment tear-down ../../src/tests/environment.cpp:247: Failure Failed Tests completed with child processes remaining: -+- 56447 /home/vinod/mesos/build/src/.libs/lt-mesos-tests --verbose --gtest_filter=*SlaveRecoveryTest/1.SchedulerFailover* --gtest_repeat=10   \--- 56671 () ",Bug,Major,Resolved,"2014-02-06 20:25:18","2014-02-06 20:25:18",1
"Apache Mesos","'Logging and Debugging' document is out-of-date.","The following is no longer correct: http://mesos.apache.org/documentation/latest/logging-and-debugging/  We should either delete this document or re-write it entirely.",Bug,Major,Resolved,"2014-01-21 18:43:59","2014-01-21 18:43:59",1
"Apache Mesos","Set GLOG_drop_log_memory=false in environment prior to logging initialization.","We've observed issues where the masters are slow to respond. Two perf traces collected while the masters were slow to respond:      These have been found to be attributed to the posix_fadvise calls made by glog. We can disable these via the environment:      We should set GLOG_drop_log_memory=false prior to making our call to google::InitGoogleLogging, to avoid others running into this issue.",Improvement,Blocker,Resolved,"2014-01-17 19:09:51","2014-01-17 19:09:51",2
"Apache Mesos","Add Kerberos Authentication support","MESOS-704 added basic authentication support using CRAM-MD5 through SASL. Now we should integrate Kerberos authentication using GSS-API, which is already supported by SASL. Kerberos is a widely-used industry standard authentication service, and integration with Mesos will make it easier for customers to integrate their existing security process with Mesos.",Story,Major,Resolved,"2014-01-15 00:53:28","2014-01-15 00:53:28",2
"Apache Mesos","ExamplesTest.JavaFramework is flaky","Identify the cause of the following test failure:  [ RUN      ] ExamplesTest.JavaFramework Using temporary directory '/tmp/ExamplesTest_JavaFramework_wSc7u8' Enabling authentication for the framework I1120 15:13:39.820032 1681264640 master.cpp:285] Master started on 172.25.133.171:52576 I1120 15:13:39.820180 1681264640 master.cpp:299] Master ID: 201311201513-2877626796-52576-3234 I1120 15:13:39.820194 1681264640 master.cpp:302] Master only allowing authenticated frameworks to register! I1120 15:13:39.821197 1679654912 slave.cpp:112] Slave started on 1)@172.25.133.171:52576 I1120 15:13:39.821795 1679654912 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.822855 1682337792 slave.cpp:112] Slave started on 2)@172.25.133.171:52576 I1120 15:13:39.823652 1682337792 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.825330 1679118336 master.cpp:744] The newly elected leader is master@172.25.133.171:52576 I1120 15:13:39.825445 1679118336 master.cpp:748] Elected as the leading master! I1120 15:13:39.825907 1681264640 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/meta' I1120 15:13:39.826127 1681264640 status_update_manager.cpp:180] Recovering status update manager I1120 15:13:39.826331 1681801216 process_isolator.cpp:317] Recovering isolator I1120 15:13:39.826738 1682874368 slave.cpp:2743] Finished recovery I1120 15:13:39.827747 1682337792 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/meta' I1120 15:13:39.827945 1680191488 slave.cpp:112] Slave started on 3)@172.25.133.171:52576 I1120 15:13:39.828415 1682337792 status_update_manager.cpp:180] Recovering status update manager I1120 15:13:39.828608 1680728064 sched.cpp:260] Authenticating with master master@172.25.133.171:52576 I1120 15:13:39.828606 1680191488 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.828680 1682874368 slave.cpp:497] New master detected at master@172.25.133.171:52576 I1120 15:13:39.828765 1682337792 process_isolator.cpp:317] Recovering isolator I1120 15:13:39.829828 1680728064 sched.cpp:229] Detecting new master I1120 15:13:39.830288 1679654912 authenticatee.hpp:100] Initializing client SASL I1120 15:13:39.831635 1680191488 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/meta' I1120 15:13:39.831991 1679118336 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576 I1120 15:13:39.832042 1682874368 slave.cpp:524] Detecting new master I1120 15:13:39.832314 1682337792 slave.cpp:2743] Finished recovery I1120 15:13:39.832309 1681264640 master.cpp:1266] Attempting to register slave on vkone.local at slave(1)@172.25.133.171:52576 I1120 15:13:39.832929 1680728064 status_update_manager.cpp:180] Recovering status update manager I1120 15:13:39.833371 1681801216 slave.cpp:497] New master detected at master@172.25.133.171:52576 I1120 15:13:39.833273 1681264640 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-0 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.833595 1680728064 process_isolator.cpp:317] Recovering isolator I1120 15:13:39.833859 1681801216 slave.cpp:524] Detecting new master I1120 15:13:39.833861 1682874368 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576 I1120 15:13:39.834092 1680191488 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-0 I1120 15:13:39.834486 1681264640 master.cpp:1266] Attempting to register slave on vkone.local at slave(2)@172.25.133.171:52576 I1120 15:13:39.834549 1681264640 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-1 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.834750 1680191488 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/meta/slaves/201311201513-2877626796-52576-3234-0/slave.info' I1120 15:13:39.834875 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-0 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available) I1120 15:13:39.835155 1680728064 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-1 I1120 15:13:39.835458 1679118336 slave.cpp:2743] Finished recovery I1120 15:13:39.835739 1680728064 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/meta/slaves/201311201513-2877626796-52576-3234-1/slave.info' I1120 15:13:39.835922 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-1 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available) I1120 15:13:39.836120 1681264640 slave.cpp:497] New master detected at master@172.25.133.171:52576 I1120 15:13:39.836340 1679118336 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576 I1120 15:13:39.836436 1681264640 slave.cpp:524] Detecting new master I1120 15:13:39.836629 1682874368 master.cpp:1266] Attempting to register slave on vkone.local at slave(3)@172.25.133.171:52576 I1120 15:13:39.836653 1682874368 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-2 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.836804 1680728064 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-2 I1120 15:13:39.837190 1680728064 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/meta/slaves/201311201513-2877626796-52576-3234-2/slave.info' I1120 15:13:39.837569 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-2 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available) I1120 15:13:39.852011 1679654912 authenticatee.hpp:124] Creating new client SASL connection I1120 15:13:39.852219 1680191488 master.cpp:1734] Authenticating framework at scheduler(1)@172.25.133.171:52576 I1120 15:13:39.852577 1682337792 authenticator.hpp:83] Initializing server SASL I1120 15:13:39.856160 1682337792 authenticator.hpp:140] Creating new server SASL connection I1120 15:13:39.856334 1681264640 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5 I1120 15:13:39.856360 1681264640 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I1120 15:13:39.856421 1681264640 authenticator.hpp:243] Received SASL authentication start I1120 15:13:39.856487 1681264640 authenticator.hpp:325] Authentication requires more steps I1120 15:13:39.856531 1681264640 authenticatee.hpp:258] Received SASL authentication step I1120 15:13:39.856576 1681264640 authenticator.hpp:271] Received SASL authentication step I1120 15:13:39.856643 1681264640 authenticator.hpp:317] Authentication success I1120 15:13:39.856724 1681264640 authenticatee.hpp:298] Authentication success I1120 15:13:39.856768 1681264640 master.cpp:1774] Successfully authenticated framework at scheduler(1)@172.25.133.171:52576 I1120 15:13:39.857028 1681264640 sched.cpp:334] Successfully authenticated with master master@172.25.133.171:52576 I1120 15:13:39.857139 1681264640 master.cpp:798] Received registration request from scheduler(1)@172.25.133.171:52576 I1120 15:13:39.857306 1681264640 master.cpp:816] Registering framework 201311201513-2877626796-52576-3234-0000 at scheduler(1)@172.25.133.171:52576 I1120 15:13:39.862296 1680191488 hierarchical_allocator_process.hpp:332] Added framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.863867 1680191488 master.cpp:1700] Sending 3 offers to framework 201311201513-2877626796-52576-3234-0000 Registered! ID = 201311201513-2877626796-52576-3234-0000 Launching task 0 Launching task 1 Launching task 2 I1120 15:13:39.905390 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-0 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.905825 1680191488 master.hpp:400] Adding task 0 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:39.905886 1680191488 master.cpp:2150] Launching task 0 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:39.906422 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-1 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.906664 1680191488 master.hpp:400] Adding task 1 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:39.906721 1680191488 master.cpp:2150] Launching task 1 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:39.907171 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-2 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.907419 1680191488 master.hpp:400] Adding task 2 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) I1120 15:13:39.907480 1680191488 master.cpp:2150] Launching task 2 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) I1120 15:13:39.907938 1680191488 slave.cpp:722] Got assigned task 0 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.908473 1680191488 slave.cpp:833] Launching task 0 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.914427 1682874368 slave.cpp:722] Got assigned task 1 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.914594 1680728064 slave.cpp:722] Got assigned task 2 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.914844 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-1 for 1secs I1120 15:13:39.915292 1682874368 slave.cpp:833] Launching task 1 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.915424 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-2 for 1secs I1120 15:13:39.915685 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-0 for 1secs I1120 15:13:39.915828 1680728064 slave.cpp:833] Launching task 2 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.917840 1680191488 slave.cpp:943] Queuing task '0' for executor default of framework '201311201513-2877626796-52576-3234-0000 I1120 15:13:39.917935 1679118336 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/1/slaves/201311201513-2877626796-52576-3234-1/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/375b31a9-7093-4db1-964d-e6b425b1e4b4 with resources ' for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.922019 1679118336 process_isolator.cpp:163] Forked executor at 3268 I1120 15:13:39.922703 1679118336 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3268 I1120 15:13:39.929134 1682874368 slave.cpp:943] Queuing task '1' for executor default of framework '201311201513-2877626796-52576-3234-0000 I1120 15:13:39.929323 1682874368 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/2/slaves/201311201513-2877626796-52576-3234-2/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/2bd0e75d-a2b9-4ae6-be08-9782612309a5 with resources ' for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.931243 1682874368 process_isolator.cpp:163] Forked executor at 3269 I1120 15:13:39.931612 1681801216 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3269 E1120 15:13:39.931836 1681801216 slave.cpp:2099] Failed to watch executor default of framework 201311201513-2877626796-52576-3234-0000: Already watched I1120 15:13:39.936460 1680728064 slave.cpp:943] Queuing task '2' for executor default of framework '201311201513-2877626796-52576-3234-0000 I1120 15:13:39.936619 1681801216 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/0/slaves/201311201513-2877626796-52576-3234-0/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/16d600da-da86-4614-91cb-58a7b27ab534 with resources ' for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.941299 1681801216 process_isolator.cpp:163] Forked executor at 3270 I1120 15:13:39.942179 1681801216 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3270 E1120 15:13:39.942395 1681801216 slave.cpp:2099] Failed to watch executor default of framework 201311201513-2877626796-52576-3234-0000: Already watched Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/slaves/201311201513-2877626796-52576-3234-2/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/2bd0e75d-a2b9-4ae6-be08-9782612309a5' Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/slaves/201311201513-2877626796-52576-3234-1/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/375b31a9-7093-4db1-964d-e6b425b1e4b4' Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/slaves/201311201513-2877626796-52576-3234-0/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/16d600da-da86-4614-91cb-58a7b27ab534' I1120 15:13:40.372573 1681801216 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.373258 1681801216 slave.cpp:1527] Flushing queued task 1 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.388317 1681801216 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.388983 1681801216 slave.cpp:1527] Flushing queued task 0 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.398084 1679654912 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.399344 1679654912 slave.cpp:1527] Flushing queued task 2 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 Registered executor on vkone.local I1120 15:13:40.491843 1679654912 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577 I1120 15:13:40.492202 1679654912 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.492424 1679654912 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 Registered executor on vkone.local I1120 15:13:40.492671 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576 I1120 15:13:40.492735 1682337792 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577 Status update: task 1 is in state TASK_RUNNING I1120 15:13:40.502235 1679654912 status_update_manager.cpp:380] Received status update acknowledgement (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 Registered executor on vkone.local I1120 15:13:40.531292 1679654912 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52579 I1120 15:13:40.532091 1680728064 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.532305 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.532776 1682874368 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52579 I1120 15:13:40.532951 1681801216 master.cpp:1452] Status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from slave(1)@172.25.133.171:52576 Status update: task 2 is in state TASK_RUNNING I1120 15:13:40.538895 1682874368 status_update_manager.cpp:380] Received status update acknowledgement (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.541267 1682874368 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578 I1120 15:13:40.541555 1682874368 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.541725 1682874368 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.542196 1682874368 master.cpp:1452] Status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576 I1120 15:13:40.542251 1682874368 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578 Status update: task 0 is in state TASK_RUNNING I1120 15:13:40.545537 1682874368 status_update_manager.cpp:380] Received status update acknowledgement (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 Running task value: 1  I1120 15:13:40.764219 1682337792 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577 I1120 15:13:40.764629 1682337792 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.764698 1682337792 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.765043 1682337792 master.cpp:1452] Status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576 I1120 15:13:40.765192 1682337792 master.hpp:418] Removing task 1 with resources cpus(*):1; mem(*):128 on slave 2Status update: task 1 is in state TASK_FINISHED Finished tasks: 1 01311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:40.765363 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577 I1120 15:13:40.772738 1682337792 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-2 from framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.773190 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 Running task value: 0  Running task value: 2  I1120 15:13:40.790068 1679118336 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578 I1120 15:13:40.790411 1680728064 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.790493 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.790674 1679118336 master.cpp:1452] Status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576 I1120 15:13:40.790798 1679118336 master.hpp:418] Removing task 0 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:40.790928 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578 Status update: task 0 is in state TASK_FINISHED Finished tasks: 2 I1120 15:13:40.791225 1680191488 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-1 from framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.794234 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.795830 1681801216 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52579 I1120 15:13:40.796111 1679118336 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.796182 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.796352 1680728064 master.cpp:1452] Status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from slave(1)@172.25.133.171:52576 I1120 15:13:40.796398 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52579 I1120 15:13:40.796466 1680728064 master.hpp:418] Removing task 2 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) I1120 15:13:40.796707 1679118336 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-0 from framework 201311201513-2877626796-52576-3234-0000 Status update: task 2 is in state TASK_FINISHED Finished tasks: 3 I1120 15:13:40.797384 1680728064 status_update_manager.cpp:380] Received status update acknowledgement (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.824383 1681801216 master.cpp:1700] Sending 3 offers to framework 201311201513-2877626796-52576-3234-0000 Launching task 3 Launching task 4 I1120 15:13:40.826971 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-3 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.827268 1679118336 master.hpp:400] Adding task 3 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:40.827348 1679118336 master.cpp:2150] Launching task 3 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:40.827487 1680728064 slave.cpp:722] Got assigned task 3 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.827857 1680728064 slave.cpp:833] Launching task 3 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.827913 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-4 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.827986 1680728064 slave.cpp:968] Sending task '3' to executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.828126 1679118336 master.hpp:400] Adding task 4 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:40.828187 1679118336 master.cpp:2150] Launching task 4 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:40.828632 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-5 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.828655 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-1 for 1secs I1120 15:13:40.829005 1679118336 slave.cpp:722] Got assigned task 4 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.829027 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-2 for 1secs I1120 15:13:40.829260 1679118336 slave.cpp:833] Launching task 4 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.829273 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-0 for 1secs I1120 15:13:40.829390 1679118336 slave.cpp:968] Sending task '4' to executor 'default' of framework 201311201513-2877626796-52576-3234-0000 Running task value: 3  Running task value: 4  I1120 15:13:40.839279 1682337792 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578 I1120 15:13:40.839534 1679118336 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.839705 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.839944 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576 Status update: task 3 is in state TASK_RUNNING I1120 15:13:40.839947 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578 I1120 15:13:40.856334 1679118336 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578 I1120 15:13:40.856650 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.856818 1679118336 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.856875 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.857105 1679118336 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577 I1120 15:13:40.857369 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578 I1120 15:13:40.857498 1680728064 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.857518 1682337792 master.cpp:1452] Status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576 I1120 15:13:40.857635 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.857630 1682337792 master.hpp:418] Removing task 3 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:40.857843 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576 I1120 15:13:40.858043 1680728064 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-1 from framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.858098 1680728064 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577 Status update: task 3 is in state TASK_FINISHED Finished tasks: 4 Status update: task 4 is in state TASK_RUNNING I1120 15:13:40.858896 1682337792 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.858957 1680728064 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.859905 1679654912 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577 I1120 15:13:40.860174 1680728064 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.860245 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.860437 1679654912 master.cpp:1452] Status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576 I1120 15:13:40.860486 1680728064 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577 I1120 15:13:40.860550 1679654912 master.hpp:418] Removing task 4 with resources cpus(*):1; mem(Status update: task 4 is in state TASK_FINISHED Finished tasks: 5 *):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:40.863689 1679654912 master.cpp:996] Asked to unregister framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.863750 1679654912 master.cpp:2385] Removing framework 201311201513-2877626796-52576-3234-0000 ../../src/tests/script.cpp:81: Failure Failed java_framework_test.sh terminated with signal 'Abort trap: 6' [  FAILED  ] ExamplesTest.JavaFramework (2688 ms) [----------] 1 test from ExamplesTest (2688 ms total)  [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (2692 ms total) [  PASSED  ] 0 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] ExamplesTest.JavaFramework ",Bug,Major,Resolved,"2013-11-20 23:14:32","2013-11-20 23:14:32",8
"Apache Mesos","Update semantics of when framework registered()/reregistered() get called","Current semantics:  1) Framework connects w/ master very first time --> registered() 2) Framework reconnects w/ same master after a zk blip --> reregistered() 3) Framework reconnects w/ failed over master --> registered() 4) Failed over framework connects w/ same master --> registered() 5) Failed over framework connects w/ failed over master --> registered()   Updated semantics:  Everything same except  3) Framework reconnects w/ failed over master --> reregistered()",Bug,Major,Resolved,"2013-10-30 19:39:15","2013-10-30 19:39:15",3
"Apache Mesos","SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave test is flaky","[ RUN      ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave Checkpointing executor's forked pid 32281 to '/tmp/SlaveRecoveryTest_0_ReconcileTasksMissingFromSlave_NT1btb/meta/slaves/201310151913-16777343-35153-31491-0/frameworks/201310151913-16777343-35153-31491-0000/executors/0514b52f-3c17-4ee5-ba16-635198701ca2/runs/97c9e2cc-ceea-40a8-a915-aed5fed1dcb3/pids/forked.pid' Fetching resources into '/tmp/SlaveRecoveryTest_0_ReconcileTasksMissingFromSlave_NT1btb/slaves/201310151913-16777343-35153-31491-0/frameworks/201310151913-16777343-35153-31491-0000/executors/0514b52f-3c17-4ee5-ba16-635198701ca2/runs/97c9e2cc-ceea-40a8-a915-aed5fed1dcb3' Registered executor on localhost.localdomain Starting task 0514b52f-3c17-4ee5-ba16-635198701ca2 Forked command at 32317 sh -c 'sleep 10' tests/slave_recovery_tests.cpp:1927: Failure Mock function called more times than expected - returning directly.     Function call: statusUpdate(0x7fffae636eb0, @0x7f1590027a00 64-byte object <F0-2F D0-A1 15-7F 00-00 00-00 00-00 00-00 00-00 40-E9 01-90 15-7F 00-00 20-6B 03-90 15-7F 00-00 48-91 C3-00 00-00 00-00 B0-3B 01-90 15-7F 00-00 05-00 00-00 00-00 00-00 17-00 00-00 00-00 00-00>)          Expected: to be called once            Actual: called twice - over-saturated and active Command exited with status 0 (pid: 32317) ",Bug,Major,Resolved,"2013-10-18 23:09:38","2013-10-18 22:09:38",1
"Apache Mesos","Expose total number of resources allocated to the slave in its endpoint","This could be useful information if there are bugs in master/slave that causes slaves to overcommit its resources.",Improvement,Major,Resolved,"2013-10-06 02:35:56","2013-10-06 01:35:56",2
"Apache Mesos","Static files missing Last-Modified HTTP headers","Static assets served by the Mesos master don't return Last-Modified HTTP headers. That means clients receive a 200 status code and re-download assets on every page request even if the assets haven't changed. Because Angular JS does most of the work, the downloading happens only when you navigate to Mesos master in your browser or use the browser's refresh.  Example header for mesos.css:      HTTP/1.1 200 OK     Date: Thu, 26 Sep 2013 17:18:52 GMT     Content-Length: 1670     Content-Type: text/css  Clients sometimes use the Date header for the same effect as Last-Modified, but the date is always the time of the response from the server, i.e. it changes on every request and makes the assets look new every time.  The Last-Modified header should be added and should be the last modified time of the file. On subsequent requests for the same files, the master should return 304 responses with no content rather than 200 with the full files. It could save clients a lot of download time since Mesos assets are rather heavyweight.",Improvement,Major,Accepted,"2013-09-26 18:27:00","2013-09-26 17:27:00",2
"Apache Mesos","`HierarchicalAllocatorProcess::removeSlave` doesn't properly handle framework allocations/resources","Currently a slaveRemoved() simply removes the slave from 'slaves' map and slave's resources from 'roleSorter'. Looking at resourcesRecovered(), more things need to be done when a slave is removed (e.g., framework unallocations).    It would be nice to fix this and have a test for this.",Bug,Major,Resolved,"2013-08-06 23:49:54","2013-08-06 22:49:54",5
"Apache Mesos","Also check 'git diff --shortstat --staged' in post-reviews.py.","We current check if you have any changes before we run post-reviews.py but we don't check for staged changes which IIUC could get lost.",Bug,Major,Resolved,"2013-07-30 02:33:23","2013-07-30 01:33:23",1
"Apache Mesos","Update Contribution Documentation","Our contribution guide is currently fairly verbose, and it focuses on the ReviewBoard workflow for making code contributions. It would be helpful for new contributors to have a first-time contribution guide which focuses on using GitHub PRs to make small contributions, since that workflow has a smaller barrier to entry for new users.",Improvement,Major,Resolved,"2013-07-17 20:41:59","2013-07-17 19:41:59",3
"Apache Mesos","Balloon framework fails to run due to bad flags","I suspect this has to do with the latest flags refactor.  [vinod@smfd-bkq-03-sr4 build]$  sudo GLOG_v=1 ./bin/mesos-tests.sh --gtest_filter=*Balloon* --verbose WARNING: Logging before InitGoogleLogging() is written to STDERR I0529 22:28:13.094351 31506 process.cpp:1426] libprocess is initialized on 10.37.184.103:53425 for 24 cpus I0529 22:28:13.095010 31506 logging.cpp:91] Logging to STDERR Source directory: /home/vinod/mesos Build directory: /home/vinod/mesos/build ------------------------------------------------------------- We cannot run any cgroups tests that require mounting hierarchies because you have the following hierarchies mounted: /cgroup We'll disable the CgroupsNoHierarchyTest test fixture for now. ------------------------------------------------------------- Note: Google Test filter = *Balloon*-CgroupsNoHierarchyTest.ROOT_CGROUPS_NOHIERARCHY_MountUnmountHierarchy: [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from CgroupsIsolatorTest [ RUN      ] CgroupsIsolatorTest.ROOT_CGROUPS_BalloonFramework Using temporary directory '/tmp/CgroupsIsolatorTest_ROOT_CGROUPS_BalloonFramework_pWWdE1' Launched master at 31574 Failed to load unknown flag 'build_dir' Usage: lt-mesos-master [...]  Supported options:   --allocation_interval=VALUE     Amount of time to wait between performing                                    (batch) allocations (e.g., 500ms, 1sec, etc) (default: 1secs)   --cluster=VALUE                 Human readable name for the cluster,                                   displayed in the webui   --framework_sorter=VALUE        Policy to use for allocating resources                                   between a given user's frameworks. Options                                   are the same as for user_allocator (default: drf)   --[no-]help                     Prints this help message (default: false)   --ip=VALUE                      IP address to listen on   --log_dir=VALUE                 Location to put log files (no default, nothing                                   is written to disk unless specified;                                   does not affect logging to stderr)   --logbufsecs=VALUE              How many seconds to buffer log messages for (default: 0)   --port=VALUE                    Port to listen on (default: 5050)   --[no-]quiet                    Disable logging to stderr (default: false)   --[no-]root_submissions         Can root submit frameworks? (default: true)   --slaves=VALUE                  Initial slaves that should be                                   considered part of this cluster                                   (or if using ZooKeeper a URL) (default: *)   --user_sorter=VALUE             Policy to use for allocating resources                                   between users. May be one of:                                     dominant_resource_fairness (drf) (default: drf)   --webui_dir=VALUE               Location of the webui files/assets (default: /usr/local/share/mesos/webui)   --whitelist=VALUE               Path to a file with a list of slaves                                   (one per line) to advertise offers for;                                   should be of the form: file://path/to/file (default: *)   --zk=VALUE                      ZooKeeper URL (used for leader election amongst masters)                                   May be one of:                                     zk://host1:port1,host2:port2,.../path                                     zk://username:password@host1:port1,host2:port2,.../path                                     file://path/to/file (where file contains one of the above) (default: ) {RED}Master crashed; failing test /home/vinod/mesos/src/tests/balloon_framework_test.sh: line 31: kill: (31574) - No such process ../../src/tests/script.cpp:76: Failure Failed balloon_framework_test.sh exited with status 2 [  FAILED  ] CgroupsIsolatorTest.ROOT_CGROUPS_BalloonFramework (2031 ms) [----------] 1 test from CgroupsIsolatorTest (2031 ms total)  [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (2031 ms total) [  PASSED  ] 0 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] CgroupsIsolatorTest.ROOT_CGROUPS_BalloonFramework   1 FAILED TEST ",Bug,Major,Resolved,"2013-05-29 23:29:33","2013-05-29 22:29:33",1
"Apache Mesos","Expose TASK_FAILED reason to Frameworks.","We now have a message string inside TaskStatus that provides human readable information about TASK_FAILED.  It would be good to add some structure to the failure reasons, for framework schedulers to act on programmatically.  E.g.  enum TaskFailure {   EXECUTOR_OOM;   EXECUTOR_OUT_OF_DISK;   EXECUTOR_TERMINATED;   SLAVE_LOST;   etc.. }",Story,Minor,Resolved,"2013-02-04 18:27:52","2013-02-04 18:27:52",8
"Apache Mesos","Mesos slave should cache executors","The slave should be smarter about how it handles pulling down executors.  In our environment, executors rarely change but the slave will always pull it down from regardless HDFS.  This puts undue stress on our HDFS clusters, and is not resilient to reduced HDFS availability.",Epic,Major,Resolved,"2013-01-18 19:19:37","2013-01-18 19:19:37",5
"Apache Mesos","Report executor terminations to framework schedulers.","The Scheduler interface has a callback for executorLost, but currently it is never called.",Improvement,Major,Resolved,"2012-11-19 21:12:23","2012-11-19 21:12:23",2
