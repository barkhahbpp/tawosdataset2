Name,Title,Description,Type,Priority,Status,Creation_Date,Estimation_Date,Story_Point
"The Mongo Java driver","Java driver 3.11 page still says 3.10","h2. Description  Java driver 3.11 page still says 3.10         [https://mongodb.github.io/mongo-java-driver/3.11/]    h2. Scope of changes    h2. Impact to Other Docs    h2. MVP (Work and Date)    h2. Resources (Scope or Design Docs, Invision, etc.)    ",Improvement,"Minor - P4",Closed,"2019-06-27 11:37:37","2019-06-27 10:37:37",1
"The Mongo Java driver","Connection string is displayed with password in logs if it contains an invalid key","Sample Code to recreate:    String  mongoUriString= mongodb://username123:password123@abcmongo1.cloud,abcmongo2.cloud,abcmongo3.cloud/database123?replicaSet=mongorepl1&adsada=1000    MongoClientURI mongoClientURI = new MongoClientURI(mongoUriString)         The above code will log,    2018-11-14 15:18:53.692 WARN docgen --- [ost-startStop-1] org.mongodb.driver.uri : Unsupported option 'adsada' in the connection string 'mongodb://username123:password123@abcmongo1.cloud,abcmongo2.cloud,abcmongo3.cloud/database123?replicaSet=mongorepl1&adsada=1000'.         I think we should not be logging the connection string in the log.",Bug,"Major - P3",Closed,"2018-11-14 22:37:09","2018-11-14 22:37:09",1
"The Mongo Java driver","Allow unencoded subdelimiters in usernames and passwords","Spec change is here: [https://github.com/mongodb/specifications/commit/c32a5d326c624764dccb6fc38bea057a72be56a4]    Drivers need to resync their connection string tests.",Improvement,"Major - P3",Closed,"2018-06-05 00:14:00","2018-06-04 23:14:00",3
"The Mongo Java driver","Only send bypassDocumentValidation if it's true","See spec changes [here|https://github.com/mongodb/specifications/pull/266/files]. ",Improvement,"Major - P3",Closed,"2018-06-04 17:35:42","2018-06-04 16:35:42",1
"The Mongo Java driver","Spring MongoTemplate through exception while accessing through spring @async(multithreading) method ","Hi,    Got this error while trying to aggregate through spring data mongoDB in a Spring async method.    ",Bug,"Major - P3",Closed,"2018-02-27 11:10:26","2018-02-27 11:10:26",1
"The Mongo Java driver","Aggregation, runCommand, and index management examples for Docs","Here are the examples. They should be delimited with        as has been done for the CRUD examples and such.  I've attached the mongoexported datasets I used for the agg examples if one wants to make sure everything is working.      || Delimiter || Shell Snippet || Notes ||  | Aggregation Example 1 | |Simple agg example|  | Aggregation Example 2 | | Aggregation - $match, $group, $project, $unwind, $sum, $sort, $dayOfWeek |  | Aggregation Example 3 | | Aggregation - {{$unwind, $group, $sum, $dayOfWeek, $multiply, $project, $cond}}.   If you're testing this one, change $revenue, 250 to a smaller number since the dataset I've attached doesn't have that many documents currently |  | Aggregation Example 4 | | Aggregation - $lookup, $filter, $match|  |runCommand Example 1|| |  |runCommand Example 2|| |  |Index Example 1||Indexes - builds simple ascending index |  |Index Example 2||Indexes - builds multikey index with partial filter expression|  ",Task,"Major - P3",Closed,"2018-02-09 19:08:32","2018-02-09 19:08:32",2
"The Mongo Java driver","Resync connection string spec tests for percent encoding in userinfo",,Improvement,"Major - P3",Closed,"2018-01-17 19:37:53","2018-01-17 19:37:53",2
"The Mongo Java driver","Implement fallback to IP v4/IP v6 vice-versa","The driver should try to connect to all IP addresses returned from {{java.net.InetAddress#getAllByName(java.lang.String)}} instead of just the first one as returned from {{java.net.InetAddress#getByName(java.lang.String)}} before giving up trying to connect.    ","New Feature","Major - P3",Closed,"2017-12-11 19:39:59","2017-12-11 19:39:59",3
"The Mongo Java driver","mongo throws IllegalStateException: state should be: open","I meet IllegalStateException: state should be: open and wonder how to solve it. Below is more context:    [mongo context]   mongo replica set: 3 nodes  version: 3.4 with WiredTiger  mongo driver in the app: java driver    [app context]  We have 12 workers to process log and write/read related information to mongo. Each worker has multiple threads.    [operation]  MongoCursor<BasicDBObject> cursor = getArrayCollection().aggregate(aggregateParams, BasicDBObject.class).iterator();    [log]  2017-09-15 17:57:17.505 ERROR 14373 [SQSConsumerFixed-18] --- c.p.s.worker.sqs.DiagLogWorkerHandler   :[2118482-264832996-317561309355740509, OSDiagLogWorkerHandler] Got exception that will be reprocessed   java.lang.IllegalStateException: state should be: open          at com.mongodb.assertions.Assertions.isTrue(Assertions.java:70)          at com.mongodb.connection.BaseCluster.selectServer(BaseCluster.java:82)          at com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.<init>(ClusterBinding.java:75)          at com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.<init>(ClusterBinding.java:71)          at com.mongodb.binding.ClusterBinding.getReadConnectionSource(ClusterBinding.java:63)          at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:402)          at com.mongodb.operation.AggregateOperation.execute(AggregateOperation.java:253)          at com.mongodb.operation.AggregateOperation.execute(AggregateOperation.java:67)          at com.mongodb.Mongo.execute(Mongo.java:836)          at com.mongodb.Mongo$2.execute(Mongo.java:823)          at com.mongodb.OperationIterable.iterator(OperationIterable.java:47)          at com.mongodb.AggregateIterableImpl.iterator(AggregateIterableImpl.java:123)          at com.purestorage.spog.array.ArrayInfoDaoImpl.getArrays_aroundBody0(ArrayInfoDaoImpl.java:122)          at com.purestorage.spog.array.ArrayInfoDaoImpl.getArrays_aroundBody1$advice(ArrayInfoDaoImpl.java:31)          at com.purestorage.spog.array.ArrayInfoDaoImpl.getArrays(ArrayInfoDaoImpl.java:1)          at com.purestorage.spog.worker.diagnostics.FreqDiagLogProcessorBase.isLatestDiag(FreqDiagLogProcessorBase.java:127)          at com.purestorage.spog.worker.diagnostics.OSFreqDiagLogProcessor.processDiagnosticLog_aroundBody0(OSFreqDiagLogProcessor.java:129)          at com.purestorage.spog.worker.diagnostics.OSFreqDiagLogProcessor.processDiagnosticLog_aroundBody1$advice(OSFreqDiagLogProcessor.java:31)          at com.purestorage.spog.worker.diagnostics.OSFreqDiagLogProcessor.processDiagnosticLog(OSFreqDiagLogProcessor.java:1)          at com.purestorage.spog.worker.sqs.DiagLogWorkerHandler.processDiagnosticsLog(DiagLogWorkerHandler.java:255)          at com.purestorage.spog.worker.sqs.DiagLogWorkerHandler.onHandle(DiagLogWorkerHandler.java:173)          at com.purestorage.spog.worker.sqs.OSDiagLogWorkerHandler.onHandle_aroundBody0(OSDiagLogWorkerHandler.java:43)          at com.purestorage.spog.worker.sqs.OSDiagLogWorkerHandler.onHandle_aroundBody1$advice(OSDiagLogWorkerHandler.java:31)          at com.purestorage.spog.worker.sqs.OSDiagLogWorkerHandler.onHandle(OSDiagLogWorkerHandler.java:1)          at com.purestorage.sqshandler.SQSConsumerFixed$SQSWorkerRunnable.lambda$processMessage$0(SQSConsumerFixed.java:220)          at java.util.concurrent.FutureTask.run(FutureTask.java:266)          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)          at java.lang.Thread.run(Thread.java:745)",Task,"Major - P3",Closed,"2017-09-21 19:49:34","2017-09-21 18:49:34",2
"The Mongo Java driver","Multiple $match filters in BSON for aggregator","I'm running with below aggregation where in match condition will be added depending upon selected filter   Bson detailsUnwind = unwind($details);    Bson empIdMatch = new BasicDBObject();  Bson deptIdMatch = new BasicDBObject();  Bson gradeMatch = new BasicDBObject();     empIdMatch =  match(eq(empId,1234567)); // mandatory value - empId  if(deptId != null) // optional value - deptId   deptIdMatch =  match(eq(details.deptId,20));  if(grade != null) // optional value - grade   gradeMatch =  match(eq(details.grade,L1));    List<Bson> pipeline = asList(detailsUnwind,empIdMatch,deptIdMatch,gradeMatch);    AggregateIterable<Document> aggresult = collection.aggregate(pipeline);    for (Document documet : aggresult) {   Document doc = (Document) documet.get(details);    if (doc != null) {     System.out.println(Testing);    }  }    If i level is not available, than i'm getting -   Command failed with error 16435: 'exception: A pipeline stage specification object must contain exactly one field.' on server <hostname>:<port>. The full response is { code : 16435, ok : 0.0, errmsg : exception: A pipeline stage specification object must contain exactly one field. }    This is what i want to achieve    Case 1) When both Department and Level details are available  collection.aggregate(         [                {$unwind: '$details'},                {      $match:                              {'empId': 123456,                             'details.deptId' : 20,                             'details.grade' : 'L1'                             }}                ]         )    Case 2) When only Grade is available  collection.aggregate(         [                {$unwind: '$details'},                {      $match:                              {'empId': 123456,                             'details.grade' : 'L1'                             }}                ]         )            ",Task,"Critical - P2",Closed,"2016-06-29 08:07:48","2016-06-29 07:07:48",16
"The Mongo Java driver","Ensure slaveOk property is set correctly for all uses of query protocol","Review use of `slaveOk` in 3.0  * URI String * DBCursor.addOption - when does it impact readPreference?",Task,"Major - P3",Closed,"2014-09-26 15:28:10","2014-09-26 14:28:10",1
"The Mongo Java driver","Deprecate DropDups","Its being removed in 3.0.0 - JAVA-1412",Task,"Major - P3",Closed,"2014-09-26 15:04:48","2014-09-26 14:04:48",1
"The Mongo Java driver","Remove BSONByteBuffer","The BSONByteBuffer class is not used in any meaningful way. Assuming its deprecation in the 2.x code (JAVA-1482), this class should be removed in 3.x",Task,"Major - P3",Closed,"2014-09-25 17:01:16","2014-09-25 16:01:16",1
"The Mongo Java driver","Deprecate BSONByteBuffer","The BSONByteBuffer is not used in any meaningful way - it's passed in to the Lazy classes, but all they do is extract the underlying buffer and use that directly, none of the specific functionality in BSONByteBuffer is used (or tested).  We should deprecate this in 2.x so it can be removed in 3.x",Improvement,"Major - P3",Closed,"2014-09-25 16:59:51","2014-09-25 15:59:51",1
"The Mongo Java driver","Stop relying on PBKDF2WithHmacSHA1 for SCRAM-SHA-1 authenticator","PBKDF2WithHmacSHA1 only became available in Java 6, and 2.x still supports Java 5.  Also, implementing this ourselves will allow us to substitute in a SHA algorithm with more bits.",Improvement,"Major - P3",Closed,"2014-09-25 15:22:26","2014-09-25 14:22:26",1
"The Mongo Java driver","CommandFailureException and WriteConcernException should have errorMessage properties","This is the property that existing users of the now deprecated commandResult property will need, in addition to the code property that already exists on MongoException ",Improvement,"Major - P3",Closed,"2014-09-24 22:41:54","2014-09-24 21:41:54",1
"The Mongo Java driver","Server selection exception should chain exceptions from server monitors","Currently BaseCluster.selectServer throws a MongoTimeoutException if no matching servers can be found for the selector.  But if the reason there are no matching servers is due to one or more exceptions in the server monitors, these exceptions are not communicated to the caller in any way.  For example, if the server monitor fails to connect due to an authentication issue or an IOException, that will not be part of the exception thrown to the caller.",Improvement,"Major - P3",Closed,"2014-09-24 16:29:32","2014-09-24 15:29:32",1
"The Mongo Java driver","Make DBCollection insert(List...) methods covariant","Instead of     do   ",Improvement,"Major - P3",Closed,"2014-09-24 14:47:50","2014-09-24 13:47:50",1
"The Mongo Java driver","Every connection should determine for itself the version of the server","If a Connection relies on the containing Server instance to determine its description, there is a race whereby the MongoDB server could have been restarted since the last refresh of the Server's description, and the Connection could end up reporting an incorrect version.  This is important because clients use the server description to determine what operation to send to a server (e.g., write command vs. opcodes), so it's important that this information always be correct.","New Feature","Major - P3",Closed,"2014-09-23 21:48:37","2014-09-23 20:48:37",1
"The Mongo Java driver","Add ability to run single operations with different timeout (e.g. Datastore.ensureIndexes())","We use Datastore.ensureIndexes(true) to let Morphia create the indexes. We also use a 30 second operation timeout (MongoOptions. socketTimeout(30000)) to make the application more robust against random problems (e.g. network issues).  Now if Morphia needs to create an index on a huge collection, ensureIndexes(true) will block even though background index build is enabled, which in itself is ok, since we don't want to start the application before the indexes are ready. However, it also obeys the timeout of 30 seconds (again not a bug per se), so the operation will eventually fail.   At this point the initialization of our entire application fails, because the ensureIndexes() call is part of the initialization, and we have to try to start it again, possibly several times. This is annoying.  I can see several ways around this. - A workaround would be to create an additional MongoDB connection and Morphia session without timeout just to create the indexes. This is possible, but a bit cumbersome.  - Better: Being able to re-configure the MongoClient object. Currently it can only return the current options (MongoClient.getMongoClientOptions()), but there's no way to set them. If this were possible we could disable the timeouts for this one operation. However it would be a bit nasty to reconfigure the Mongo client globally, affecting the entire application, while the application is running (not an issue for ensureIndexes() though).  - Best: Enable a single operation to use different timeout settings. This way the ensureIndexes() call could use different timeouts.   Apart from the Datastore.ensureIndexes() feature, the ability to run operations with different timeouts would also be useful for statistics/reporting or any sort of batch jobs.   I realize that we do not run the lastest version of the driver, but I have not found any change in this area in the changelogs nor the API docs.",Improvement,"Minor - P4",Closed,"2014-09-23 10:23:43","2014-09-23 09:23:43",1
"The Mongo Java driver","Add equals / hashcode to MongoClientURI","PR for 2.x - https://github.com/mongodb/mongo-java-driver/pull/255  For 3.0.x {{ConnectionString}} is proxied and needs its own implementation.",Improvement,"Major - P3",Closed,"2014-09-22 15:34:08","2014-09-22 14:34:08",1
"The Mongo Java driver","Deprecate objectStart(boolean array) from BSONCallback","The method      void objectStart(boolean array);  on BSONCallback is never used.  This means it can probably be deprecated in the 2.x code, and maybe removed in 3.0",Improvement,"Major - P3",Closed,"2014-09-22 09:14:07","2014-09-22 08:14:07",1
"The Mongo Java driver","Support authMechanismProperties on connection string","gssapiServiceName should be deprecated and replaced by authMechanismProperties:SERVICE_NAME:  CANONICALIZE_HOST_NAME should also be support in the connection string",Improvement,"Major - P3",Closed,"2014-09-19 20:46:26","2014-09-19 19:46:26",1
"The Mongo Java driver","Should not authenticate to an arbiter","This applies to the socket managed by the server monitor as well as all pooled sockets.",Improvement,"Major - P3",Closed,"2014-09-19 20:41:49","2014-09-19 19:41:49",1
"The Mongo Java driver","Invalidate all servers if authentication fails","Effectively, an authentication failure on any socket means that the client can no longer connect to the cluster at all.  Therefore, all connection pools and server monitors should be invalidated.",Improvement,"Major - P3",Closed,"2014-09-19 20:38:01","2014-09-19 19:38:01",1
"The Mongo Java driver","Support authentication mechanism negotiation","To support authentication upgrades from older versions of MongoDB to 2.8 the driver will use the following algorithm: - If the application specifies a particular authMechanism (e.g. MONGODB-CR) the driver will continue to honor it. - If the application provides a user and password but provides no explicit authMechanism, or calls the DB.authenticate method, the following applies: -- If connecting to a server whose version is >= 2.8, the driver will use SCRAM-SHA-1 (i.e. the driver's default mechanism is SCRAM-SHA-1) -- Otherwise the driver will use MONGODB-CR (i.e. the driver's default mechanism remains MONGODB-CR)  MongoDB 2.8 will *always* support SCRAM-SHA-1 if at least MONGODB-CR was specified in --authenticationMechanisms, so drivers do not have to try and fall back. If SCRAM credentials don't yet exist for a user they will be created on-the-fly when the driver uses SCRAM-SHA-1 for mechanism.","New Feature","Major - P3",Closed,"2014-09-19 19:38:24","2014-09-19 18:38:24",1
"The Mongo Java driver","Bump maxWireVersion to whatever it is for server 3.0",,Improvement,"Major - P3",Closed,"2014-09-19 18:36:47","2014-09-19 17:36:47",1
"The Mongo Java driver","Aggregation problem with Enterprise version","We are getting an error on our new enterprise server that does not appear on the open source version.  We use the same code but change the pointer to the servers.  Both servers are MongodB 2.6.4.   We are using Spring Data 1.6.0-RELEASE to load the MongoDB java driver. When we execute an aggregation using only the $out operation we see using only the $out operator to copy the collection, we get an error (below).  Here is the code:         // Get the collection         DBCollection sourceCollection = mongoOperations.getCollection(sourceCollectionName);         // Set up a pipeline         List<DBObject> ops = new ArrayList<DBObject>();         // Write to the target collection         ops.add(new BasicDBObject($out, targetCollectionName));         // Do the copy         sourceCollection.aggregate(ops);  When the code runs, it makes sure the target collection does not exist.  Here is the important part of the error:  Caused by: java.lang.IllegalArgumentException: result undefined  at com.mongodb.AggregationOutput.<init>(AggregationOutput.java:80)  at com.mongodb.DBCollection.aggregate(DBCollection.java:1554)  at com.mongodb.DBCollection.aggregate(DBCollection.java:1532)  at com.bankofamerica.risk.aml.lms.service.ListService.copyCollection(ListService.java:978)    ",Task,"Major - P3",Closed,"2014-09-19 14:51:22","2014-09-19 13:51:22",1
"The Mongo Java driver","Support changes to explain in MongoDB 2.8","The following operations are scheduled to be explainable: Count, Distinct, Update, Delete, Find and Modify, MapReduce, Group, Aggregate, Find",Task,"Major - P3",Closed,"2014-09-19 13:58:28","2014-09-19 12:58:28",1
"The Mongo Java driver","Add support for listIndexes command","Spec here: https://github.com/mongodb/specifications/blob/master/source/enumerate-indexes.rst ",Improvement,"Major - P3",Closed,"2014-09-18 16:41:19","2014-09-18 15:41:19",1
"The Mongo Java driver","Use listCollections command for helper method","Spec here: https://github.com/mongodb/specifications/blob/master/source/enumerate-collections.rst",Improvement,"Major - P3",Closed,"2014-09-18 16:30:31","2014-09-18 15:30:31",1
"The Mongo Java driver","Update 2.13 Javadoc","The legacy API documentation is in the process of being updated for 3.0, and these updates need to be ported back to 2.13 where appropriate.",Improvement,"Major - P3",Closed,"2014-09-17 15:46:48","2014-09-17 14:46:48",1
"The Mongo Java driver","Unused/untested methods on BasicBSONObject","The following methods are unused, and might need testing:   - public int getInt(final String key, final int def)  - public long getLong(final String key)  - public double getDouble(final String key)  - public String getString(final String key)  - public boolean getBoolean(final String key)  ",Task,"Major - P3",Closed,"2014-09-17 09:11:21","2014-09-17 08:11:21",1
"The Mongo Java driver","ByteBuf should be reference counted instead of Closeable",,Improvement,"Major - P3",Closed,"2014-09-17 01:53:36","2014-09-17 00:53:36",1
"The Mongo Java driver","Make driver module compliant with checkstyle.xml","Currently it uses checkstyle-lite.xml",Improvement,"Major - P3",Closed,"2014-09-16 16:29:51","2014-09-16 15:29:51",1
"The Mongo Java driver","Implement cluster monitoring specification tests","The Server Monitoring and Discovery specification defines a set of test specifications in YAML.  The Java driver needs a harness that can run these tests, and then should pass all the tests.",Improvement,"Major - P3",Closed,"2014-09-15 15:42:08","2014-09-15 14:42:08",1
"The Mongo Java driver","Make it easier to derive one CodecRegistry from another","We should have a default RootCodecRegistry that includes  * DBObjectCodecProvider * DocumentCodecProvider * BsonCodecProvider  and then allow easier customization from there, e.g.  adding a single new Codec for one type.  A good use case for this is how easy it is to change the UuidRepresentation for the UuidCodec.",Improvement,"Major - P3",Closed,"2014-09-12 18:34:03","2014-09-12 17:34:03",1
"The Mongo Java driver","FindOperation should have boolean properties instead of EnumSet<CursorFlag>","Ideally cursor flags would only be used at the lowest level, in the QueryMessage.",Improvement,"Major - P3",Closed,"2014-09-12 18:05:14","2014-09-12 17:05:14",1
"The Mongo Java driver","Make RawBsonDocument implement Closeable","Currently RawBsonDocument just has a byte[] so there is no need to close it.  But in the future it might be given a slice of a larger buffer that is checked out of the buffer pool.  In that case, making RawBsonDocument Closeable will allow it to release its reference to the slice.",Improvement,"Major - P3",Closed,"2014-09-12 17:56:18","2014-09-12 16:56:18",1
"The Mongo Java driver","Link all 2.8 new features to manual","Reminder ticket.  Any new features that are released in 2.8 eg: {{mongodb.server.release 2.8}} will need links to the mongodb docs site.  For example any operations: ListIndexOperation, CreateIndexesOperation, ListCollectionOperation etc..",Improvement,"Minor - P4",Closed,"2014-09-12 11:04:32","2014-09-12 10:04:32",1
"The Mongo Java driver","GridFS not well tested","There are unused methods on the GridFS API, which suggests they are not tested:   - public DBCursor getFileList(final DBObject query)  - public DBCursor getFileList(final DBObject query, final DBObject sort)  - public GridFSInputFile createFile(final InputStream in)  In addition, the GridFSFile class is referenced in any test classes at all - granted it is an abstract class, but I would expect it to be tested as a side effect of testing the GridFS functionality.  There are also untested methods on GridFSInputFile:   - public int saveChunks()",Task,"Major - P3",Closed,"2014-09-12 09:51:01","2014-09-12 08:51:01",1
"The Mongo Java driver","Ensure contract assertions create meaningful errors","Contract assertions should provide clear and meaningful error messages as the tracebacks can be confusing.",Improvement,"Major - P3",Closed,"2014-09-11 09:43:25","2014-09-11 08:43:25",1
"The Mongo Java driver","Iterable Codec","Need to provide a codec that supports decoding into an iterable.      For example an array is a valid top level JSON object and we should be able to take a JSON array string and decode it correctly.",Improvement,"Major - P3",Closed,"2014-09-08 11:48:19","2014-09-08 10:48:19",1
"The Mongo Java driver","slaveOK URI","{{SlaveOK}} is no longer part of the [Connection String|http://docs.mongodb.org/manual/reference/connection-string/#connection-string-uri-format] however previous users may be reliant on it.  Need to either deprecate it and document it or continue to support it.",Task,"Major - P3",Closed,"2014-09-08 11:44:38","2014-09-08 10:44:38",1
"The Mongo Java driver","Auto Reconnect","I'm currently using the java driver, we've noticed that this is not auto reconnecting to mongodb by default. The below says autoConnectRetry has been deprecated, how can I use the connectTimeout property to have auto retry?  @Deprecated public MongoClientOptions.Builder autoConnectRetry(boolean autoConnectRetry)  Deprecated. There is no replacement for this method. Use the connectTimeout property to control connection timeout. Sets whether auto connect retry is enabled.  Parameters:     autoConnectRetry - auto connect retry",Task,"Trivial - P5",Closed,"2014-09-08 04:23:37","2014-09-08 03:23:37",1
"The Mongo Java driver","DBCollection test coverage","The following public/protected methods are unused on DBCollection, and therefore probably require test coverage:   - WriteResult updateMulti(final DBObject query, final DBObject update)  - long getCount(final ReadPreference readPreference)  - List distinct(final String fieldName, final DBObject query)  - AggregationOutput aggregate(final DBObject firstOp, final DBObject... additionalOps)  - void setHintFields(final List<DBObject> indexes)  - DBObject findAndModify(final DBObject query, final DBObject sort, final DBObject update)  - void resetOptions()  - protected Class<? extends DBObject> getInternalClass(final String path) ",Task,"Major - P3",Closed,"2014-09-05 11:05:38","2014-09-05 10:05:38",1
"The Mongo Java driver","DBCollection unused code","The DBCollection methods   - getDocumentCodec  - getOptionHolder  Are package-private and unused - should they be deleted?  The getCount method getCount(DBObject, DBObject, long, long, ReadPreference, long, TimeUnit, BsonValue) has an unused parameter, DBObject projection. Since this method is package-level, and even the javadoc says it's ignored, this could be deleted.  The field NAMESPACE_KEY_NAME is never used, and private, and can be deleted. ",Task,"Major - P3",Closed,"2014-09-05 11:01:03","2014-09-05 10:01:03",1
"The Mongo Java driver","Document Lazy classes","The following classes need updated Javadoc:   - LazyBSONCallback  - LazyBSONObject  - LazyBSONList  - LazyDBCallback  - LazyDBDecoder  - LazyDBList  - LazyDBObject  - LazyWriteableDBCallback  - LazyWriteableDBDecoder  - LazyWriteableDBObject",Improvement,"Major - P3",Closed,"2014-09-04 17:14:29","2014-09-04 16:14:29",1
"The Mongo Java driver","Rename MyConnectionSource classes","I'd like to use a more appropriate name for the inner classes called MyConnectionSource.  This class name sounds poorly thought out, like demo code or toy applications.  I know we're not keen on Impl suffixes, but I think that's still better than a My prefix, and more consistent with the driver since we have MongoClientImpl (for example).",Task,"Major - P3",Closed,"2014-09-04 11:54:18","2014-09-04 10:54:18",1
"The Mongo Java driver","PatternCodec should handle unsupported regex flags better.","PatternCodec has a number of TODOs in it related to unsupported regex flags.  On top of that, there seems to be a lot of reasonably complex logic in the class, and it does not have a unit test.",Improvement,"Major - P3",Open,"2014-09-04 10:47:41","2014-09-04 09:47:41",1
"The Mongo Java driver","Make IDGenerator generic","The IDGenerator currently returns an Object, when it would be better practice to make this generic:  public interface IdGenerator<T> {     T generate(); }  There are places where we need to resort to using Objects, but this is not one of them.",Improvement,"Major - P3",Closed,"2014-09-04 10:29:38","2014-09-04 09:29:38",1
"The Mongo Java driver","Unused QueryBuilder methods","The withinBox and withinCenter methods on query builder are never used. These methods should be tested.",Task,"Major - P3",Closed,"2014-09-01 15:24:31","2014-09-01 14:24:31",1
"The Mongo Java driver","com.mongodb.ParallelScanOptions unused methods","ParallelScanOptions methods:      public int getNumCursors() {     public int getBatchSize() {   are never used. Since this is an options class, does this mean the user can set options that are not actually used? Should they be called somewhere from inside the driver code?",Task,"Major - P3",Closed,"2014-09-01 15:22:24","2014-09-01 14:22:24",1
"The Mongo Java driver","MongoClientOptions unused method","        public Builder codecRegistry(final CodecRegistry codecRegistry) {  Builder method is not used - it should be tested.",Task,"Major - P3",Closed,"2014-09-01 15:17:59","2014-09-01 14:17:59",1
"The Mongo Java driver","Unused methods on Mongo","The method     public List<ServerAddress> getAllAddress() {  on Mongo is unused.  It either needs to be tested, or deprecated and removed.  Since there's a TODO note on there, it looks like deprecation might be the correct approach.  The methods     public Collection<DB> getUsedDatabases() {     public String getConnectPoint() {  area also not used.  They should be tested or deprecated.  The decoder parameter on     <T> T execute(final WriteOperation<T> operation, final Decoder<DBObject> decoder) {  is never used, and it's a package-level method - should it just be removed?  ",Task,"Major - P3",Closed,"2014-09-01 14:46:49","2014-09-01 13:46:49",1
"The Mongo Java driver","Unused methods in DB need testing","Public methods in DB appear unused, which implies they need test coverage:  - resetOptions  And the deprecated methods:  - slaveOk  - addUser  While I could be sold on the unused deprecated methods (although I'm not sure that just because they're deprecated means we can't make sure they work the way we expect, at least until a deletion date is decided) the reset method definitely needs testing.  A test would ensure we don't accidentally introduce regression bugs.",Task,"Major - P3",Closed,"2014-08-28 14:16:15","2014-08-28 13:16:15",1
"The Mongo Java driver","BulkWriteError needs testing","While documenting the legacy API, I noticed that BulkWriteError has unused public fields. A search for usages shows this class is not tested, which implies there are missing unhappy-path tests for Buik write.",Task,"Major - P3",Closed,"2014-08-28 12:35:46","2014-08-28 11:35:46",1
"The Mongo Java driver","Mongo Cleaner thread not stopping"," I have made mongoClient connection as singleton.As written in mongo docs Mongo will take care of closing threads but Mongo Cleaner thread is not stopping.How to stop it.?",Task,"Major - P3",Closed,"2014-08-28 02:13:31","2014-08-28 01:13:31",1
"The Mongo Java driver","Remove dropDups support from index building",,Sub-task,"Major - P3",Closed,"2014-08-26 19:24:35","2014-08-26 18:24:35",1
"The Mongo Java driver","Complete Javadoc for new API","The driver module is made up of two APIs, the existing API (DBObject, DBCollection, DB) and the new CRUD API.  This ticket is to Javadoc all public classes and methods on the new CRUD API.",Sub-task,"Major - P3",Closed,"2014-08-26 17:23:13","2014-08-26 16:23:13",1
"The Mongo Java driver","Complete Javadoc for Legacy API","The driver module is made up of two APIs, the existing API (DBObject, DBCollection, DB) and the new CRUD API.  This ticket is to Javadoc all public classes and methods on the existing (legacy) API.",Sub-task,"Major - P3",Closed,"2014-08-26 17:22:29","2014-08-26 16:22:29",1
"The Mongo Java driver","Clarify in Javadoc that $snapshot only prevents duplicates","From [~<USER>:  {quote} Per SERVER-14703, setting snapshot on a cursor doesn't prevent it from omitting documents in the resultset that are updated during the lifetime of the query, they *only* prevent dups. As such, the 'or objects missed' language needs to be changed. The parenthetical '(if an object is new during the query, or deleted during the query, it may or may not be returned, even with snapshot mode)' should become a first-class sentence something like 'If an object is created, deleted, or updated during the lifetime of a query, it may or may not be returned, even with snapshot [set to true (or lang-specific equivalent)].' {quote}",Improvement,"Major - P3",Closed,"2014-08-25 18:46:09","2014-08-25 17:46:09",1
"The Mongo Java driver","List parameters should be generalized","The Java driver function {{DBCollection.aggregate}} takes in a {{List<DBObject>}}, which has the consequence that you'll get a compiler error if you pass in a {{List<BasicDBObject>}} instead.  This issue could be eliminated by changing the function to take {{List<? extends DBObject>}} instead.  It appears that similar issues exist for {{DBCollection.explainAggregate}} and {{DBCollection.insert}}.",Improvement,"Minor - P4",Closed,"2014-08-25 16:53:02","2014-08-25 15:53:02",1
"The Mongo Java driver","Enable Java 1.7 SSL builds.","Currently failing on Jenkins which needs investigation.",Improvement,"Major - P3",Closed,"2014-08-20 17:24:55","2014-08-20 16:24:55",1
"The Mongo Java driver","Migrate QuickTour and QuickTourAdmin","Migrate the QuickTour and QuickTourAdmin examples over to 3.0.0",Improvement,"Major - P3",Closed,"2014-08-19 15:12:06","2014-08-19 14:12:06",1
"The Mongo Java driver","Java driver example with SSL glosses over required truststore steps","If one were to take the [Java example provided|http://docs.mongodb.org/manual/tutorial/configure-ssl/#java] and run it, you'll be greeted by a ~100 line-long exception chain at runtime.  Relevant snippet:   Essentially the example glosses over the required steps to import and use a custom Java trustStore (and potentially the trustStore password) that contains the MongoDB server certificate. These steps are approximately discussed in [this mongodb-user comment|https://groups.google.com/forum/#!topic/mongodb-user/Ro4PmoLBZWQ]. (Although I believe the keystore usage _may_ have been unnecessary - I'm not certain myself.)  (Note: it is _possible_ that this example could work as is, but only in the case where the user starts their MongoDB node using a certificate that's been signed by a trusted 3rd party, e.g., Verisign, which are already bundled within the default JRE. However I suspect this usage would be exceedingly rare in practice.)      ",Improvement,"Major - P3",Closed,"2014-04-02 20:58:40","2014-04-02 19:58:40",1
"The Mongo Java driver","Move write result-related classes out of org.mongodb",,Improvement,"Major - P3",Closed,"2014-08-19 14:51:17","2014-08-19 13:51:17",1
"The Mongo Java driver","Move Document class out of org.mongodb","Document class is generally useful enough to move into the bson module as a replacement for BSONObject.",Improvement,"Major - P3",Closed,"2014-08-19 14:50:04","2014-08-19 13:50:04",1
"The Mongo Java driver","Complete Javadoc for bson module","Ensure all public classes in bson module have proper Javadoc",Improvement,"Major - P3",Closed,"2014-08-19 14:48:39","2014-08-19 13:48:39",1
"The Mongo Java driver","Complete Javadoc for driver module","Ensure all public methods on all public classes have proper Javadoc",Improvement,"Major - P3",Closed,"2014-08-19 14:47:58","2014-08-19 13:47:58",1
"The Mongo Java driver","Complete Javadoc for driver-core","Ensure that all public methods in all public classes in driver-core have proper Javadoc",Improvement,"Major - P3",Closed,"2014-08-19 14:46:50","2014-08-19 13:46:50",1
"The Mongo Java driver","Create release process for 3.0 Gradle build","Resolved by using the nexus plugin, and doing the other steps manu",Improvement,"Major - P3",Closed,"2014-08-19 14:42:07","2014-08-19 13:42:07",1
"The Mongo Java driver","Comment on: mongodb-ecosystem/tutorial/getting-started-with-java-driver.txt","boolean auth = db.authenticate(myUserName, myPassword);  This method is deprecated.  *Reporter*: Looly *E-mail*: [mailto:<EMAIL>]",Improvement,,Closed,"2014-05-27 00:46:26","2014-05-26 23:46:26",1
"The Mongo Java driver","Add async server connection / selection support",,Improvement,"Major - P3",Closed,"2014-08-18 11:44:36","2014-08-18 10:44:36",1
"The Mongo Java driver","Add Async Authenticator support","Enable async authenticators support",Improvement,"Major - P3",Closed,"2014-08-18 11:39:24","2014-08-18 10:39:24",1
"The Mongo Java driver","Async ConnectionInitializer","Make the {{PipelinedConnectionInitializer.initialize( initializationFuture )}} non-blocking.  ",Improvement,"Major - P3",Closed,"2014-08-18 11:37:49","2014-08-18 10:37:49",1
"The Mongo Java driver","Comment on: mongodb java driver JSON.parse($date) problem","1.java code:    Why not 2012-06-18T02:37:43.942 or Wed Jun 18 02:37:43 CST 2014? zone: Asia/Shanghai    2.mongo java driver source code:    the need to add format yyyy-MM-dd'T'HH:mm:ss.SSSZ and yyyy-MM-dd'T'HH:mm:ssZ or Joda Time or JDK 8 datatime class ?  example:     *Reporter*: oo *E-mail*: [mailto:<EMAIL>]",Improvement,,Closed,"2014-08-05 04:27:11","2014-08-05 03:27:11",1
"The Mongo Java driver","Discrimination in Jackson-Codec","so we can create a collection of classes and each document can be a subclass","New Feature","Minor - P4",Closed,"2014-08-08 18:10:14","2014-08-08 17:10:14",1
"The Mongo Java driver","Support encoding embedded Map within Document","Given   {code:java} Map myJson = ...  Document doc = new Document(myJson) {code}  and myJson contains a value which is itself a Map, encoding fails:  ",Improvement,"Major - P3",Closed,"2014-08-08 18:01:24","2014-08-08 17:01:24",1
"The Mongo Java driver","JSON date parsing fails with timezones other than Z","When the parser encounters a field with e.g. { $date : 2013-12-03T10:41:22.393+0200 } the resulting value is null.  The reason is that the format string given to SimpleDateFormat is yyyy-MM-dd'T'HH:mm:ss.SSS'Z' which requires a literal 'Z' rather than actually parsing the timezone.  Changing the string to yyyy-MM-dd'T'HH:mm:ss.SSSX should work better. ",Bug,"Minor - P4",Closed,"2014-08-07 15:01:30","2014-08-07 14:01:30",1
"The Mongo Java driver","Investigate async speed","The InternalStreamSpecification highlighted with a mocked stream that 100,000 calls to {{receiveMessageAsync}} in a thread takes ~1.3x the time it does for the sync equivalent.  Benchmark and review the futures efficiency",Task,"Major - P3",Closed,"2014-08-06 08:59:15","2014-08-06 07:59:15",1
"The Mongo Java driver","DBCursor.count() and size() return int","Though the count command supports larger values, DBCursor.count() and DBCursor.size() return int, and silently cast the long to int.  We can't really fix this without breaking binary compatibility.  Note that the getCount family of methods in DBCollection all properly return a long.",Improvement,"Major - P3",Closed,"2014-08-05 20:51:42","2014-08-05 19:51:42",1
"The Mongo Java driver","Supported sharded and nonAtomic modifiers for mapReduce","See http://docs.mongodb.org/manual/reference/command/mapReduce/#output-to-a-collection-with-an-action",Improvement,"Major - P3",Closed,"2014-08-05 20:22:32","2014-08-05 19:22:32",1
"The Mongo Java driver","For correct idle connection timeout, lastUsedAt should be set after the callback fires rather than before","UsageTrackingInternalConnection should set lastUsedAt when the callback fires, not when the message is sent.  See com.mongodb.connection.UsageTrackingInternalConnection#sendMessageAsync and com.mongodb.connection.UsageTrackingInternalConnection#receiveMessageAsync",Improvement,"Minor - P4",Closed,"2014-08-05 19:55:27","2014-08-05 18:55:27",1
"The Mongo Java driver","Handle asynchronously delivered exceptions in connection pool","DefaultConnectionPool catches and re-throws exceptions from the wrapped connection for synchronous calls, incrementing the generation count.  But it does not for sendMessageAsync or receiveMessageAsync.   ",Improvement,"Major - P3",Closed,"2014-08-05 18:50:01","2014-08-05 17:50:01",1
"The Mongo Java driver","Performance degradation when switching from 2.4.x to 2.6.x with newer Java driver","We're noticing a pretty dramatic decrease in performance on 2.6.x when using the 2.12.x Java drivers rather than the 2.11.x ones. I'm adding this here because it was pointed out to me this is likely due to the newer Java driver using a different code path for writes in which case this is a performance issue related to that code. If this is incorrect feel free to move to the java driver project.  I have attached to output of mongostat for two test runs. One is 2.11.x Java driver versus 2.6.x and the other uses 2.12.x Java driver.  There is about a 10-20% difference in raw performance depending on the operation being executed.  Given the fact that we have to provision capacity for peak usage for our customers this could affect our net hosting costs so it would be good to have a quick resolution to this issue.",Task,"Major - P3",Closed,"2014-07-17 15:47:06","2014-07-17 14:47:06",1
"The Mongo Java driver","Implement new helper API","For MongoDatabase and MongoCollection",Improvement,"Major - P3",Closed,"2014-07-31 15:56:23","2014-07-31 14:56:23",1
"The Mongo Java driver","Deprecate mongo.getDB()","{{getDatabase()}} is the future.",Improvement,"Major - P3",Closed,"2014-07-31 15:55:32","2014-07-31 14:55:32",1
"The Mongo Java driver","Final round of deprecations before 3.0 release","As part of the re-modularization of 3.0, another round of deprecations is going to be necessary for 2.x branch.",Epic,"Major - P3",Closed,"2014-07-31 15:10:28","2014-07-31 14:10:28",1
"The Mongo Java driver","Introduce API for tailable cursors that doesn't break Iterator contract","Current proposal is something like this:    Normal usage would just use the existing MongoCursor interface (which extends Iterator and adds a close() method), but for users that don't want to block waiting for the next document, the non-blocking tryNext() method can be used, e.g.  ","New Feature","Major - P3",Closed,"2014-07-31 14:28:18","2014-07-31 13:28:18",1
"The Mongo Java driver","Implement new API for queries and updates for async MongoClient ","Bring async client API in line with sync client API.","New Feature","Major - P3",Closed,"2014-07-31 14:08:32","2014-07-31 13:08:32",1
"The Mongo Java driver","Log command line friendly queries and milliseconds","We use com.mongodo.TRACE occasionally to see the queries.  Would like couple of improvements: a.) Log the output that can be copied directly to mongo command line b.) Log how long it took to run the query in milliseconds",Improvement,"Minor - P4",Closed,"2014-07-30 03:56:01","2014-07-30 02:56:01",1
"The Mongo Java driver","Write Tests for different Bson writers",,Improvement,"Major - P3",Closed,"2014-07-24 22:21:46","2014-07-24 21:21:46",1
"The Mongo Java driver","IllegalStateException: open during insert","Saw this exception in a log file:   This happened while the app was running (had been started almost exactly 20 minutes before), there was no shutdown in progress (regular application logging just continues after this). There was no IOException or similar logged.  I did have maxConnectionIdleTime assigned to 120000 ms (2 minutes). This failing request failed on the first mongo query of the webapp request, about 4 seconds after the webapp request came in the IllegalArgumentException was thrown. The webapp request before this one was about 4 minutes earlier, so probably the max connection idle time cleanup logic had run.  The application currently relies on catching MongoSocketException for retries, would be nice to receive that exception in this case too, if it was indeed a network-related problem.",Bug,"Major - P3",Closed,"2014-07-11 11:00:03","2014-07-11 10:00:03",1
"The Mongo Java driver","Optimizations to ObjectId serialization","Hi,  We do a lot of ObjectId serialization at Foursquare and want to squeeze out as much performance as possible. I created this patch to optimize byte[] and String serialization: https://github.com/mongodb/mongo-java-driver/pull/224",Improvement,"Major - P3",Closed,"2014-07-08 16:58:13","2014-07-08 15:58:13",1
"The Mongo Java driver","Gridfs chunk size should be configurable during creation","Refer: https://github.com/mongodb/mongo-java-driver/blob/master/src/main/com/mongodb/gridfs/GridFSInputFile.java#L64  The _chunksize and the buffer parameter gets created using the default value frm GridFS which is 255k.  Though we can reconfigure the chunksize later using setChunkSize, however during creation 255k chunk of memory in Java program gets reserved which feels a little inefficient use of memory if we are going to resize chunksize later.  Ideally, I think we should be able to inject the chunksize during creation: https://github.com/mongodb/mongo-java-driver/blob/master/src/main/com/mongodb/gridfs/GridFS.java#L381 as a parameter which can default to DEFAULT_CHUNKSIZE. ","New Feature","Major - P3",Closed,"2014-07-04 22:30:16","2014-07-04 21:30:16",1
"The Mongo Java driver","Ensure that an exception is thrown if a BSON string is not valid UTF-8","Currently the driver calls java.lang.String#String(byte[], int, int, java.lang.String) to decode UTF-8 byte arrays, but this method silently replaces invalid UTF-8 byte sequences with a replacement character.     The driver should instead use CharsetDecoder directly so that an exception can be thrown instead.",Improvement,"Major - P3",Closed,"2014-07-03 18:44:20","2014-07-03 17:44:20",1
"The Mongo Java driver","Support JSR-310 Dates","The current version of the Java driver only accepts java.util.Date for reading and writing Dates. Unfortunately the API of java.util.Date is broken, and so is the performance impact of creating Dates. Most projects use for that reason joda time, or since recently JSR 310.  While you could work with them already now, it would always require the creation of temporary java.util.Date objects, either bei the driver when reading, or by the developer when writing.  The driver should be configurable that joda-time or JSR 310 is directly used. Or that as a Developer I can at least provide a converter to do so (and especially avoid java.util.Date inbetween)","New Feature","Major - P3",Closed,"2014-07-01 08:02:08","2014-07-01 07:02:08",1
"The Mongo Java driver","Support binary decoding transformations","DocumentCodec hard codes a transformation from binary subtypes for UUID, but does not allow other subtypes to be mapped to different binary decoders.",Improvement,"Major - P3",Open,"2014-06-23 22:29:42","2014-06-23 21:29:42",1
"The Mongo Java driver","Support saving a document whose Codec is not a CollectibleCodec","Currently MongoCollection.save() will throw UnsupportedOperationException if this is the case.",Improvement,"Major - P3",Closed,"2014-06-20 22:13:30","2014-06-20 21:13:30",1
"The Mongo Java driver","BsonDocumentCodec should be Collectible","Currently BsonDocumentCodec does not implement CollectibleCodec.  It can't now because CollectibleCodec currently lives in the driver module.  We either need to bring that down into bson or else create a new codec in the driver module that wraps the one in bson.",Improvement,"Major - P3",Closed,"2014-06-20 22:11:57","2014-06-20 21:11:57",1
"The Mongo Java driver","java.lang.IllegalAccessError on redeployment using async driver","I have a groovy class, which runs in vert.x that makes use of the java driver:   In case vert.x redeploys the module containing this class I see the following exception:    I have not seen this error when using the synchronous driver. It only showed up when using the async driver.",Bug,"Major - P3",Closed,"2014-06-17 20:07:11","2014-06-17 19:07:11",1
"The Mongo Java driver","Create new async GridFS API","Currently there is no async support for GridFS.  We need to design a new GridFS API that works properly with async.","New Feature","Major - P3",Closed,"2014-06-17 17:23:05","2014-06-17 16:23:05",1
"The Mongo Java driver","In JsonWriter, support writing BSON Int64 values as JSON numbers rather than $numberLong","MongoDB 2.6 introduced support for the NumberLong data type in MongoDB Extended JSON using $numberLong.   In strict mode, JsonWriter will write all Int64 values using $numberLong.  But some applications will prefer to lose the type information and just writes them as JSON numbers.","New Feature","Major - P3",Closed,"2014-06-16 15:52:01","2014-06-16 14:52:01",1
"The Mongo Java driver","Create Codec implementation for POJOs","The 3.0.x branch now has pluggable support for BSON codecs (encoders/decoders), as well as a few concrete implementations for Map-like classes: DBObjectCodec, DocumentCodec, and BsonDocumentCodec.  The goal for this issue is to create a ClassModelCodec (and associated ClassModelCodecProvider) that supports encoding and decoding any POJO (Plain Old Java Object).  This codec should support the following field types:  * String * int, Integer * long, Long * boolean, Boolean * double, Double * Date * POJO (for embedded documents) * Binary * java.util.List (of any of the preceding types)  It also must provide a way to indicate a mapping for the _id property (by convention or annotation). ","New Feature","Major - P3",Closed,"2014-06-13 02:28:40","2014-06-13 01:28:40",1
"The Mongo Java driver","not master and node is recovering error messages should trigger a refresh of the server state","not master and node is recovering error messages should trigger a refresh of the server state, since it means that  the driver's current view of the state is no longer accurate (otherwise the operation would not have been sent to that server in the first place).  In addition, the connection pool for that server should be cleared.",Improvement,"Major - P3",Closed,"2014-06-12 22:34:27","2014-06-12 21:34:27",1
"The Mongo Java driver","Java driver does not clean up thread resources (sometimes)","In the case of a RuntimeException such as a ClassNotFoundException, the Java driver may not clean up all of its resources, in particular the non-daemon threads used for pool eviction (the PooledConnectionProvider.sizeMaintenanceTimer).  This can cause web containers such as Tomcat to fail to shut down properly.  The scenario is as follows:  1.  Deploy a web application which deploys a MongoClient connected to a 3 node replica set. 2.  Access the application 3.  Undeploy the application in Tomcat  Even though MongoClient.close is called by the application, some threads will remain.  Any replica set polling threads other than the primary along with threads for maintaining the pools.  What happens here is the following:  1.  MongoClient.close() is called 2.  ... 3.  ConcurrentPool.close() is called 4.  In closing the pool tries to instantiate a CLDIterator.  Since the jar containing the class has been removed a ClassNotFoundException is raised which fails termination of remaining threads.  The PooledConnectionProvider should ensure that all threads created are shut down regardless of failures in the pool.close method.  Likewise the other close methods should insulate themselves from such failures.  Also, the threads Timer threads spawned for pool eviction should probably be set as daemon threads to avoid shutdown issues.",Bug,"Minor - P4",Closed,"2014-06-11 06:06:11","2014-06-11 05:06:11",1
"The Mongo Java driver","Class WriteResult Method getN() always return 0","DBObject user = new BasicDBObject(); user.put(name, hoojo); user.put(age, 23); WriteResult wr = dbCollection.insert(user);// dbCollection.save(user);  method wr.getN() return 0 both with insert and save, but the data save success.  *Reporter*: luoch *E-mail*: [mailto:<EMAIL>]",Improvement,,Closed,"2014-06-10 07:23:58","2014-06-10 06:23:58",1
"The Mongo Java driver","Consider supporting writetimeout for sockets implementations that support it","There is no mechanism to set a write timeout on a java.net.Socket but there is for nio and netty. ",Improvement,"Major - P3",Open,"2014-06-10 15:53:13","2014-06-10 14:53:13",1
"The Mongo Java driver","Implement back pressure for asynchronous cursor","Currently there is no way to pause the firehose of documents returned by asynchronous iteration of a cursor.    One possibility is to extend MongoFuture with MongoCursorFuture, and add appropriate methods there to implement back pressure:    **Update** Should follow the http://www.reactive-streams.org/ spec once 0.4 is out of milestone ","New Feature","Major - P3",Closed,"2014-06-10 15:04:17","2014-06-10 14:04:17",1
"The Mongo Java driver","AsynchronousSocketChannel connect and socket timeout","AsynchronousSocketChannel connect essentially is blocking and you can't apply the socket timeout setting.  You can:  a) get a future from connect and then call {{get(timeout, unit)}} with the timeout the result to honour the socket timeout but that is blocking. b) pass a completion handler to connect which blocks whilst it connects and doesn't take a timeout - this is currently used.  See: http://stackoverflow.com/questions/20752756/how-to-set-java-nio-asynchronoussocketchannel-connect-timeout  essentially this functionality is not available and implementing it would be complex",Improvement,"Major - P3",Open,"2014-06-10 12:01:22","2014-06-10 11:01:22",1
"The Mongo Java driver","Support a binary bson type that can accept a byte array slice","The current Binary type contract is to pass in a byte array that represents exactly the data to be saved. It would be nice to be able to express a slice/window of a byte array by also accepting offset and length inputs that are used to calculate the relevant portion of the byte array. This is obviously a performance optimization to help avoid copies in performance sensitive parts of a system.  Justification: In streaming interfaces it is generally not known how big a binary payload will be. Thus something like a `ByteArrayOutputStream` is used, and even when an implementation provides direct access to the underlying buffer, it's not possible[1] to construct a specially sized byte array for the Binary type without a copy.  [1] I'm totally unfamiliar if java offers a way to unsafely trim a byte array, so my claim is speculation.","New Feature","Minor - P4",Closed,"2014-06-04 21:40:48","2014-06-04 20:40:48",1
"The Mongo Java driver","Ensure Async cursors can be cancelled","Currently the way to stop iterating an async cursor is to return false from the Block.    A better way might be to instead call cancel() on the Future returned from the forEach method() in the higher level API.  This models more closely the way a Subscription works in RxJava. ",Improvement,"Major - P3",Closed,"2014-05-30 16:43:18","2014-05-30 15:43:18",1
"The Mongo Java driver","DBCollection.getIndexInfo() throws java.lang.IllegalStateException: open without explanation","Sometimes... {{DBCollection.getIndexInfo()}} simply throws {{java.lang.IllegalStateException: open}} without explanation.  Please handle this case or provide a more detailed exception message.  BTW, we reuse a {{MongoClient}} connection through the lifetime of the application.  ",Bug,"Major - P3",Closed,"2014-05-19 12:30:51","2014-05-19 11:30:51",1
"The Mongo Java driver","testUTF8 test in the BSONTest fails locally","I was trying to build the java-driver myself. So I pulled it from github and switched to the r2.12.1 tag.  When I try to build it, the testUTF8 test in the BSONTest-class fails. This happened on a windows machine, so I thought there might be a problem with some weird windows specific encoding. So I tried the same on a current ubuntu machine. Same result. I looked at the test. And it is looking through all possible code points. When I altered it to just print the numbers it is failing for and not stop at the first, it fails 2047 times, starting with 55297 till 57343. ",Bug,"Minor - P4",Closed,"2014-05-13 18:58:20","2014-05-13 17:58:20",1
"The Mongo Java driver","Implement new API for all queries and updates","Implement new CRUD API in MongoDatabase and MongoCollection","New Feature","Major - P3",Closed,"2014-05-13 14:45:31","2014-05-13 13:45:31",1
"The Mongo Java driver","Using MongoDBClient with WebLogic like a Connection Pool with JNDI",,Improvement,"Trivial - P5",Closed,"2014-05-12 16:33:31","2014-05-12 15:33:31",1
"The Mongo Java driver","Incorrect ping information causes continuous updating cluster information","After updating java driver to 2.12.0 gc activity raises for a several dozen percent. In logs I can find: May 09, 2014 3:33:36 PM com.mongodb.ServerStateNotifier run FINE: Checking status of s122712.dc2:27017 May 09, 2014 3:33:36 PM com.mongodb.BaseCluster updateDescription FINE: Updating cluster description to  {type=ReplicaSet, servers=[{address=s122712.dc2:27017, type=ReplicaSetPrimary, averagePingTime=2883898000000 ms, state=Connected}]  But ping gives: ping s122712.dc2 PING s122712.dc2 (xx.xx.xx.xx) 56(84) bytes of data. 64 bytes from s122712.dc2 (xx.xx.xx.xx): icmp_req=1 ttl=64 time=0.322 ms 64 bytes from s122712.dc2 (xx.xx.xx.xx): icmp_req=2 ttl=64 time=0.359 ms  In fact there are thousands DBPort and MongoOptions objects created every second which kills servers in less than an hour.",Bug,"Blocker - P1",Closed,"2014-05-09 14:44:39","2014-05-09 13:44:39",1
"The Mongo Java driver","Supports db.grantRolesToUser() user management command","MongoDB 2.6.0 supports the [{{db.grantRolesToUser()}}|http://docs.mongodb.org/manual/reference/method/db.grantRolesToUser/#db.grantRolesToUser] command and it should be supported in the Java Driver API.  Related to JAVA-1203, SERVER-3301.",Improvement,"Major - P3",Closed,"2014-05-07 06:55:38","2014-05-07 05:55:38",1
"The Mongo Java driver","Java API should support db.createUser()","MongoDB 2.6.0 deprecates the {{db.addUser()}} and recommends the {{db.createUser()}} command.  However, the Java API deprecates the {{db.addUser()}} method and suggests using the low-level {{db.command()}} method:    MongoDB Java Driver API should support {{db.createUser()}} as a replacement to {{db.addUser()}} (taking into account server version).  Related to SERVER-3301.","New Feature","Major - P3",Closed,"2014-05-07 06:52:28","2014-05-07 05:52:28",1
"The Mongo Java driver","Refresh the tutorial","The tutorial for the Java driver needs a refresh.  Candidates for inclusion include:  # Configuration: interesting MongoClientOptions, the difference between single ServerAddress and List<ServerAddress> # Querying: Examples of common queries from the documentation, using either or both QueryBuilder and BasicDBObject # Updating: Examples of common updates from the documentation, using BasicDBObject # Bulk Insert, Update, Remove ",Task,"Major - P3",Closed,"2014-05-02 20:39:49","2014-05-02 19:39:49",1
"The Mongo Java driver","Ensure getServer and getConnection are non blocking for async bindings",,Task,"Major - P3",Closed,"2014-05-01 18:58:13","2014-05-01 17:58:13",1
"The Mongo Java driver","DBCollection.createIndex/ensureIndex deprecation notice is flipped from MongoDB Manual documentation","MongoDB Manual recommends ensureIndex :  http://docs.mongodb.org/manual//tutorial/create-an-index/ http://docs.mongodb.org/manual//reference/method/db.collection.ensureIndex/  and deprecates createIndex since very long ago:  http://docs.mongodb.org/manual//reference/method/db.collection.createIndex/  db.collection.createIndex(keys, options) Deprecated since version 1.8.  Creates indexes on collections.  Java API is the other way around, recommends createIndex:  http://api.mongodb.org/java/current/com/mongodb/DBCollection.html#createIndex(com.mongodb.DBObject) :  createIndex public void createIndex(DBObject keys) Calls createIndex(com.mongodb.DBObject, com.mongodb.DBObject) with default index options  and deprecates ensureIndex :  http://api.mongodb.org/java/current/com/mongodb/DBCollection.html#ensureIndex(com.mongodb.DBObject)  ensureIndex @Deprecated public void ensureIndex(DBObject keys) Deprecated. use createIndex(DBObject) instead Calls ensureIndex(com.mongodb.DBObject, com.mongodb.DBObject) with default options ",Improvement,"Major - P3",Closed,"2014-05-01 09:12:52","2014-05-01 08:12:52",1
"The Mongo Java driver","Add GeoJson related operators to QueryBuilder (geoIntersects, geoWithin)","Similar request to c# driver request: https://jira.mongodb.org/browse/CSHARP-683",Task,"Major - P3",Closed,"2014-04-30 13:49:44","2014-04-30 12:49:44",1
"The Mongo Java driver","Timeout while writing from multiple threads","I have a Java program that is using multiple threads to write to a stand-alone 2.6.0 server. Documents are about 1.6k each. When using a low batch size (1 or 2) and 6-8 threads, the application gets the below exception after a few minutes of writing to the database:    This does not seem to occur when the batch size is larger or less threads are used.",Task,"Major - P3",Closed,"2014-04-25 21:01:25","2014-04-25 20:01:25",1
"The Mongo Java driver","BulkWriteResult methods should have returned long rather than int","BulkWriteResult methods:   should have returned long not int.",Bug,"Major - P3",Closed,"2014-04-25 17:04:33","2014-04-25 16:04:33",1
"The Mongo Java driver","Expose com.mongodb.util.ClassMapBasedObjectSerializer to Allow Overriding of ObjectSerializers for Specific Types","I noticed that {{JSONSerializers.getStrict()}} and {{JSONSerializers.getLegacy()}} are implemented using the package-accessible {{com.mongodb.util.ClassMapBasedObjectSerializer}} class.  In the case of {{org.bson.types.Binary}}, the legacy serializer doesn't produce valid JSON; it produces a bare, unquoted string [{{<Binary Data>}}|https://github.com/mongodb/mongo-java-driver/blob/37b3771e4837245a33bc22cefa3fd848445a7f25/src/main/com/mongodb/util/JSONSerializers.java#L121].  The strict serializer produces valid JSON (quoted) string but obviously can be very, very verbose.  I'd like to be able to override the serializer and produce something in between the legacy and strict implementations for Binary: a valid JSON string that's a truncated string representation that the strict implementation outputs.  This would require making the {{com.mongodb.util.ClassMapBasedObjectSerializer}} public.",Improvement,"Major - P3",Closed,"2014-04-21 21:47:58","2014-04-21 20:47:58",1
"The Mongo Java driver","SSL connections do not verify certificate hostnames","*Overview*  By default x509 communication should validate the MongoDB's server certificate's hostname against the hostname that me the client used to connect. Roughly speaking, similar to the hostname verification that's performed by your browser; if I visit google.com and I get from the web server a certificate for acmewidgets.com, then my browser displays a warning (or outright refuses).  Currently this hostname verification is not performed when communicating with SSL-enabled MongoDB.  *Details* I looked into this a bit, and it appears the issue is that the driver's SSL connections do not specify an {{identificationAlgorithm}} on the {{SSLSocket}}.   One apparent option might be to modify the {{SSLParameters}} of the {{SSLContext}} used to create your {{SSLSocketFactory}} and specify an endpoint identification algorithm. Unfortunately, however, it appears the JDK's implementation only respects a subset of the {{SSLParameters}} at the factory level, and {{identificationAlgorithm}} is not one of them:  [SSLSocketImpl.java|http://www.docjar.com/html/api/sun/security/ssl/SSLSocketImpl.java.html] (line 2408).  Another option would be to set the algorithm on each socket created by the factory.   I did try a quick POC of this option by adding the snippet below to {{DBPort.java}} on 2.11.2.  {code:java} _socket = _options.socketFactory.createSocket(); if (_socket instanceof SSLSocket) {     final SSLSocket sslSocket = (SSLSocket) _socket;     final SSLParameters currentParams = sslSocket.getSSLParameters();      final SSLParameters newParams = new SSLParameters();     newParams.setEndpointIdentificationAlgorithm(HTTPS);     newParams.setCipherSuites(sslSocket.getEnabledCipherSuites());     newParams.setProtocols(sslSocket.getEnabledProtocols());     if (currentParams.getNeedClientAuth()) {         newParams.setNeedClientAuth(true);     } else if (currentParams.getWantClientAuth()) {         newParams.setWantClientAuth(true);     }     ((SSLSocket)_socket).setSSLParameters(newParams); } _socket.connect( _addr , _options.connectTimeout ); ... {code}  While this wouldn't be the only place requiring the code change, this did do the trick for my standalone MongoDB test. In that connecting with the matching hostname was allowed, while using an alias of the hostname was rejected:    (IMO, I think an argument could be make this is a JDK bug. Given my (limited) understanding of x.509 I would've expected the verification to be handled automatically behind the scenes and by default. Especially given the proposed workaround above relies on using the identification algorithm of HTTPS, which is clearly out of place in this context.  Relevant JDK source: [X509TrustManagerImpl.java|http://www.docjar.com/html/api/sun/security/ssl/X509TrustManagerImpl.java.html] (line 346))",Improvement,"Major - P3",Closed,"2014-04-18 16:19:03","2014-04-18 15:19:03",1
"The Mongo Java driver","Poor error message for find using sort","Performing a search in which data must be sorted and you insert  as key you get a poor error message.  -------------------------------------------------------------------- MongoClient mongo = new MongoClient(new ServerAddress(host, port),      credentialList); DB mongodb = mongo.getDB(DB_NAME) DBCollection table = mongodb.getCollection(TABLE_NAME); DBCursor cursor = table.find(); BasicDBObject orderBy = new BasicDBObject() ; orderBy.put(, 1); cursor.sort(orderBy);  --------------------------------------------------------------------  com.mongodb.MongoException: assertion d:\slave\windows_64bit_v2.4\mongo\src\mongo\db\../bson/bsonobjbuilder.h:92  at com.mongodb.MongoException.parse(MongoException.java:82)  at com.mongodb.DBApiLayer$MyCollection.__find(DBApiLayer.java:292)  at com.mongodb.DBApiLayer$MyCollection.__find(DBApiLayer.java:273)  at com.mongodb.DBCursor._check(DBCursor.java:368)  at com.mongodb.DBCursor._hasNext(DBCursor.java:459)  at com.mongodb.DBCursor.hasNext(DBCursor.java:484)",Improvement,"Minor - P4",Closed,"2014-03-12 07:12:30","2014-03-12 07:12:30",1
"The Mongo Java driver","Add modifiedCount as a property of the com.mongodb.WriteResult class","Currently the Java driver doesn't return nModified at all for the existing DBCollection.update method.  The WriteResult class doesn't expose it at all, so it's not available.      However, we should consider it.  Since the new BulkWriteResult class has the method:    * isModifiedCountAvailable  * getModifiedCount    we could add these same methods to WriteResult (or a new subclass of it, UpdateResult).","New Feature","Major - P3",Closed,"2014-03-07 17:41:01","2014-03-07 17:41:01",1
"The Mongo Java driver","Allow configuration of heartbeat background threads to be set via API","The Java driver, as of version 2.9.0, supports automatic failover of mongos instances (JAVA-381). The parameters that determine the wait time when the active mongos becomes unavailable, until choosing a new mongos, are only available as system properties. They cannot be set directly via the driver API. This ticket is to add API support for directly specifying the failover timeout value.  For the record, the relevant system properties, and their default values, are:    The maximum time to failover to a new mongos is:  Sum of: <com.mongodb.updaterIntervalMS> + max(com.mongodb.updaterSocketTimeoutMS, com.mongodb.updaterConnectTimeoutMS, ping time) for each mongos   The new options are:  * MongoClientOptions#getHeartbeatFrequency * MongoClientOptions#getHeartbeatConnectRetryFrequency * MongoClientOptions#getHeartbeatConnectTimeout * MongoClientOptions#getHeartbeatSocketTimeout * MongoClientOptions#getHeartbeatThreadCount",Epic,"Major - P3",Closed,"2014-03-07 03:17:07","2014-03-07 03:17:07",1
"The Mongo Java driver","Allow acceptable latency difference to be configured via API","Currently the only way to change the default value is via the com.mongodb.slaveAcceptableLatencyMS system property. It will be added as MongoClientOptions.acceptableLatencyDifference property.",Epic,"Major - P3",Closed,"2014-03-07 03:07:39","2014-03-07 03:07:39",1
"The Mongo Java driver","Support parallelCollectionScan command",,Epic,"Major - P3",Closed,"2014-03-07 03:05:00","2014-03-07 03:05:00",1
"The Mongo Java driver","BulkWriteResult.getModifiedCount should throw if server version < 2.6","BulkWriteResult.getModifiedCount should throw MongoInternalException if server version < 2.6 because those servers can not provide the value properly via getlasterror.",Epic,"Major - P3",Closed,"2014-03-06 22:33:38","2014-03-06 22:33:38",1
"The Mongo Java driver","A getlasterror response with jnote or wnote should throw an exception","A getlasterror response with jnote or wnote should throw a CommandFailureException, as it means that the requested write concern was not applied.  Currently the driver ignores these fields.",Epic,"Major - P3",Closed,"2014-03-06 22:29:59","2014-03-06 22:29:59",1
"The Mongo Java driver","Improve GridFS.remove(query) method ","remove(query) on GridFS is currently performed by :  - first issuing a select on files bucket  - then for each file remove it and remove the associated chunks by files_id  First I think this kind of linked removal, should be fully handled by the server and not by the client, as this is a server feature. Moreover the current implementation can result in thousands of different requests, and doesn't insure consistency anyway.  Thus the method efficiency could be improved by performing at most two requests, one on files collection using the query and the other on chunks collection using a in clause on files_id previously selected.   Tests made on a 50K files bucket have showed that the remove time for 31K files was dropping from 385000ms to 825ms only (465x improvement)  PR #171 has been issued on github : https://github.com/mongodb/mongo-java-driver/pull/171",Improvement,"Major - P3",Closed,"2014-02-27 20:40:58","2014-02-27 20:40:58",1
"The Mongo Java driver","Make JMX MBean names unique","Currently you can get duplicates because the monotonically increasing integer used for clusterId is scope to a class loader, while the JMX server is scoped to the JVM.  So we need another way to make the MBean names unique.  A guid of some sort is probably required.",Improvement,"Major - P3",Closed,"2014-02-26 12:50:27","2014-02-26 12:50:27",1
"The Mongo Java driver","Mongo Java Driver Descriptive Exception for No Primary","When a replica set primary is failing and primary reads and writes are performed, a MongoException is thrown. For example:   The only way to determine the true cause is to inspect the Exeption's message field. Ideally, we should be able to recognize this exception from its type without having to inspect/pattern match against the Exception's message field. For example, there could be a new Exception  that is only thrown in this situation.",Improvement,"Major - P3",Closed,"2014-02-19 22:21:58","2014-02-19 22:21:58",1
"The Mongo Java driver","JSON parser parses braces incorrectly","when parsing the following JSON with too many braces: {_id:{{{{{{$oid:50c5c5b0b8be290c1d00014b}}  The JSON parser from the driver parsed it in a very strange way with a key {{{{{$oid. So Looks like there is a bug in the parser when '{' are back to back. This is with the strict extended JSON parser ",Bug,"Major - P3",Closed,"2014-02-17 18:35:36","2014-02-17 18:35:36",1
"The Mongo Java driver","Connections to failed primary replica left in CLOSE_WAIT state","To recreate:  1.  Create a 3 node replica set (2 data nodes and 1 arbiter). 2.  Start up an application using the mongodb driver with connections to the replica set. 3.  Kill the primary 4.  Allow the driver to detect that the primary is down 4.  Start the primary again  Run netstat -a | grep CLOSE_WAIT.  You should see that there are a couple connections left in the CLOSE_WAIT state for the server that had failed.  In the case where a host is detected down, these connections should be closed.",Bug,"Major - P3",Closed,"2014-02-15 01:27:19","2014-02-15 01:27:19",1
"The Mongo Java driver","Driver events","An interface that would provide for the user to implement and pass back to the MongoDB Driver with methods that would get called on each type of event. For example, instead of simply logging that a command is getting executed, you could call a method in the implemented interface (e.g. onCommandExecutionBegin) with the parameters that would otherwise be getting logged (e.g. command, connection, server).  This would give the user the ability to use their own logging mechanism and their own format.","New Feature","Major - P3",Closed,"2014-02-13 15:47:05","2014-02-13 15:47:05",1
"The Mongo Java driver","Allow killing of ops","Allow operations to be killed.  A connection should have a handle that allows an operation running on that connection to be asynchronously killed.",Improvement,"Major - P3",Closed,"2014-02-12 16:43:06","2014-02-12 16:43:06",1
"The Mongo Java driver","GridFSInputFile should be able to write at a specified position","It will be great if GridFSInputFile can write at specified position, not always from the beginning of a file, i.e. can you add a method write( long position, byte[] b , int off , int len ) in MyOutputStream in file GridFSInputFile.java?","New Feature","Major - P3",Closed,"2014-02-08 07:28:21","2014-02-08 07:28:21",1
"The Mongo Java driver","Empty objects and arrays not handled correctly in results.","Values in an object returned from a query may be an empty object or an empty array.  If such cases, calling bo.keySet().isEmpty() always returns false (the result is hardwired in the method override), depsite the fact that bo.keySet().size() == 0.  A simple fix is to change the org.bson.util.StringRangeSet.isEmpty() implementation to:      @Override     public boolean isEmpty() {         return size == 0;     } ",Bug,"Minor - P4",Closed,"2014-02-05 13:36:45","2014-02-05 13:36:45",1
"The Mongo Java driver","CommandResult.getException not detecting caused by duplicate key exception","An exception will not get converted to the expected type if a duplicate key error is the underlying error:  (hosts/<USER>oids changed)    I believe this happens when the mongos adds the duplicate key error as a caused by error.",Improvement,"Minor - P4",Closed,"2014-01-31 20:26:15","2014-01-31 20:26:15",1
"The Mongo Java driver","GridFS can update file metadata incorrectly.","When using GridFS without a write concern you can set a file which fails but the metadata updates - meaning its out of sync with the chunks.  This can lead other tools to return the incorrect sized fields.  ",Bug,"Major - P3",Closed,"2014-01-24 10:09:21","2014-01-24 10:09:21",1
"The Mongo Java driver","auto retry during failover","When the driver is configured to work with a replica set, it still favors a single node in the replica set. If that node becomes unavailable, the next request will invalidate the connection and return failure. A subsequent request will attempt to reconnect to the replica set, thereby establishing a connection to a different node, and therefore succeed. Thus, in a failover scenario, one (or maybe more) requests are lost.  How we deal with this at the AKC is to use a retry aspect; it advises every interaction with MongoDB, and if the interaction returns an UncategorizedMongoException or DataAccessResourceFailureException, it resubmits the request one time. That way, in a failover, we give ourselves the chance to have zero lost requests.  While we have a code solution for this, I'd like it if this were supported by the official distribution. I've included our aspect for reference.","New Feature","Major - P3",Closed,"2014-01-16 20:17:34","2014-01-16 20:17:34",1
"The Mongo Java driver","Implement proper auto reconnect for exiting, but failed connections","It should be useful to have a feature that implements the retry logic for existing connections, e.g. when a transient network issue occurs","New Feature","Major - P3",Closed,"2014-01-10 17:31:28","2014-01-10 17:31:28",1
"The Mongo Java driver","Rhino NativeDate not serialized properly","For this object (note when field):  {code:javascript} var o = {     _id : id,     when : new Date(),     name  : MongoDB + 'def',     type  : database,     count : 1,     info  : {         x : 203,         y : 102     } }; {code}  This call:  {code:java} new BasicDBObject(o); {code}  Throws the following error:    There was a similar issue with ConsString that was fixed in the latest driver build.  The name field above should cause the ConsString instance to be passed to the function, which works.  It did not in previous version of the driver.  A fix for this would be awesome.","New Feature","Major - P3",Closed,"2014-01-02 14:11:24","2014-01-02 14:11:24",1
"The Mongo Java driver","Replace fsync, fsycAndLock, and unlock in Mongo with fsyncLock and fsyncUnlock","Replace with:    to match the shell.  The side benefit of this, and one of the motivations, is to remove the one public reference to DBObject in the Mongo API.  In 2.12 the existing methods should be deprecated and in 3.0, removed.",Improvement,"Major - P3",Closed,"2013-12-25 02:58:21","2013-12-25 02:58:21",1
"The Mongo Java driver","Memory leak in ClassAncestry","ClassAncestry stores references to my classes (like enums that I use in the queries), never releases these references, which prevent the WebappClassloader and all the classes it loaded to be reclaimed by the garbage collector when I undeploy my webapp.  I'm currently working around using reflection. ",Bug,"Major - P3",Closed,"2013-12-19 15:55:34","2013-12-19 15:55:34",1
"The Mongo Java driver","Change visibility of GridFSDBFile.getChunk(int) to public","We are using GridFS to store multipart/mixed mime messages. We would like to read these messages from GridFS via the mongodb java driver. The messages can be large up to GB size. Since the mime parser has to reposition and create sub streams on part of the stream we started to implement our own InputStream that implemented https://javamail.java.net/nonav/docs/api/javax/mail/internet/SharedInputStream.html and also added support for <USER>and reset. We used GridFSDBFile to do this but we needed to use the getChunk method which unfortunately is package private. To circumvent this we implemented a helper class in the same package as GridFSDBFile that called getChunk. Unfortunately this workaround does not work in our deployment environment since we are using an osgi platform.  So if it is possible we would like you to change the visibility of the getChunk method to public.",Improvement,"Major - P3",Closed,"2013-10-16 13:22:49","2013-10-16 12:22:49",1
"The Mongo Java driver","Ability to support multiple Kerberos keytab files","There are applications that have the desire to become multiple identities during runtime and are looking to be able to either log into different or the same cluster with multiple keytabs that each have different user principals.  Currently in order to get the driver to work with Kerberos you must kinit, which only supports a single principal","New Feature","Major - P3",Closed,"2013-10-15 06:57:32","2013-10-15 05:57:32",1
"The Mongo Java driver","Saved extra fields for Gridfs does not go in metadata sub-document","When put some metadata for a file to save with Gridfs, extradata should go into the metadata field (sub document). But it doesn't.  Following code demonstrates to issue. {code:java} public void SaveMetadata2() throws Exception {         String ConnectionString = mongodb://192.168.11.102:37017/filebox.mycoll;                 MongoClient moClient;         MongoClientURI moClientURI;                 moClientURI = new MongoClientURI(ConnectionString);         moClient = new MongoClient(moClientURI);                                DB db = moClient.getDB(moClientURI.getDatabase());                                                  GridFS gridfs = new GridFS(db, moClientURI.getCollection());                         db.requestStart();         db.requestEnsureConnection();                 File file1 = new File(e:/fbfiles/image_0001.jpg);         InputStream ins = new FileInputStream(file1);                              GridFSInputFile gif = gridfs.createFile(ins, file1.getAbsolutePath(), true);                 gif.put(XXX, YYYY);                 gif.save();                 db.requestDone();                         System.out.println(DONE!);     } {code}  The expected output of db.mycoll.files.find() is this:   But it is saved as this:  ",Bug,"Minor - P4",Closed,"2013-10-13 01:29:36","2013-10-13 00:29:36",1
"The Mongo Java driver","Providing a method to return version of the currently connected mongo instance","Currently from java driver version of the connected Mongo instance can be learn via following code snippet.  DB db = new MongoClient(127.0.0.1).getDB(test); CommandResult commandResult = db.command(buildInfo); String serverVersion = commandResult.getString(version));  However similar to CLI DB class can have a method like db.getVersion() to get version of the mongo instance.",Improvement,"Minor - P4",Closed,"2013-10-09 19:46:27","2013-10-09 18:46:27",1
"The Mongo Java driver","Cannot determine actual ReplSetStatus from singleton MongoClient getReplicaSetStatus()","Our application is such that the majority of it can work whenever the Mongo ReplicaSet is operating without a PRIMARY.  Recently I had a requirement to produce a HealthCheck service that reported the following states based upon the availability and capability of the Mongo ReplicaSet  ReadWrite:  ReplSet is operating with a PRIMARY, GET services should 20x, PUT services should 20x ReadOnly:  ReplSet is operating with only SECONDARIES, GET services should 20x, PUT services should 503 DOWN:  No members of the ReplSet can be reached, services should 503  I was surprised at how difficult it was to write this code with the 2.11.3 Java driver.  The sticking point was that my application follows the http://docs.mongodb.org/ecosystem/tutorial/getting-started-with-java-driver/ convention of establishing a Singleton MongoClient.  The ReplicaStatus object produced by this singleton provides no methods to assert the health of anything but the master (its toString(), however, does know this).  This made it impossible to discriminate between ReadOnly and DOWN.  I need the ReplicaSetStatus produced by the Singleton MongoClient to provide a direct way to interrogate the 3 states of the ReplSet as seen by the Application Server {ReadWrite, ReadOnly, DOWN}  Here is my current ugly workaround.  I am looking for a way to drive out the whole freshClient part.      @Override     public ReplSetStatus getReplSetStatus() {         ReplSetStatus rss = ReplSetStatus.DOWN;         MongoClient freshClient = null;         try {             if ( mongo != null ) {                 ReplicaSetStatus replicaSetStatus = mongo.getReplicaSetStatus();                 if ( replicaSetStatus != null ) {                     if ( replicaSetStatus.getMaster() != null ) {                         rss = ReplSetStatus.ReadWrite;                     } else {                         /*                          * When mongo.getReplicaSetStatus().getMaster() returns null, it takes a a                          * fresh client to assert whether the ReplSet is read-only or completely                          * down. What a hack.                          */                         freshClient = new MongoClient( mongo.getAllAddress(), mongo.getMongoClientOptions() );                         replicaSetStatus = freshClient.getReplicaSetStatus();                         if ( replicaSetStatus != null ) {                             rss = replicaSetStatus.getMaster() != null ? ReplSetStatus.ReadWrite : ReplSetStatus.ReadOnly;                         } else {                             log.warn( freshClient.getReplicaSetStatus() is null );                         }                     }                 } else {                     log.warn( mongo.getReplicaSetStatus() returned null );                 }             } else {                 throw new IllegalStateException( mongo is null?!? );             }         } catch ( Throwable t ) {             log.error( Ingore unexpected error, t );         } finally {             if ( freshClient != null ) {                 freshClient.close();             }         }         log.debug( getReplSetStatus(): {}, rss );         return rss;     }  Alternatively, if I could as the MongoClient can you support this ReadPreference right now? or can you support this WriteConcern currently? that would work too, or maybe even better.      /**      * @return true if current state of Client can support readPreference, false otherwise      */     boolean canDoRead( ReadPreference readPreference );      /**      * @return true if current state of Client can support writeConcern; false otherwise      */     boolean mongo.canDoWrite( WriteConcern writeConcern )   ",Improvement,"Major - P3",Closed,"2013-10-04 20:24:54","2013-10-04 19:24:54",1
"The Mongo Java driver","GridFS.findOne needs a variant that takes a ReadPreference","For our app we've setup a replica set and chosen to use a secondary-preferred read preference to help distribute requests and recover from node failures.  We have the DB configured for secondary preferred and set read preference to primary on a per-request basis - for read and update type operations.  This fails when dealing with GridFS because their is no ability to provide a read preference in the GridFS API.  I started working on a workaround, reading from the {{bucket.fs}} collection and passing the preference that way.  But now the problem is that when reading file chunks, the input stream is reading from the chunks collection with the database default read preference of secondary-preferred.  Consequently, I'm going to have to reimplement that in our app to get going.  But it sure would be nice if I didn't have to do that.",Improvement,"Major - P3",Closed,"2013-10-03 01:54:08","2013-10-03 00:54:08",1
"The Mongo Java driver","Add independent setting for timeout when selecting a server to operate against","In 2.x, we use connectTimeout, which overloads the meaning (since we also use it for Socket connect timeout),  In 3.x, it's not configurable yet, but there's an easy place to hook a value in, in BaseCluster.getServer().  Possible name: serverSelectionTimeout    ",Improvement,"Major - P3",Closed,"2013-09-26 21:39:15","2013-09-26 20:39:15",1
"The Mongo Java driver","JSON.parse Efficiency improvement","json parsing, see attachment.",Improvement,"Major - P3",Closed,"2013-09-26 10:42:56","2013-09-26 09:42:56",1
"The Mongo Java driver","inconsistent behaviour between java drivers before and after version 2.7.3","We've found an iconsistent behaviour between java drivers before and after version 2.7.3 on a replica set (mongo version 2.2.3 - but doesn't matter for this issue). Before (incl.) v2.7.3 we could read from collection A and write into collection B within the same request even if ReadPreference.SECONDARY was set on the database.  After v2.7.3 a MongoException is thrown: not talking to master and retries used up. Our solution is either to set ReadPreference.PRIMARY (either on the database or on the find method) or to close the request and to open a new one for the write request.  It would be nice if the connection wouldn't be tied to a mongo node dependant on the *first* request, because it's very hard to find the cause of the above mentioned error (imagine a much more complex business logic, which we have nailed down to one well isolated test case). It would be smarter when Mongo could automatically decide to connect to the master when a write request comes in even when ReadPreference.SECONDARY is set and return to a secondary for further read requests.  Please see attached our test class to reproduce the issue.","New Feature","Major - P3",Closed,"2013-09-20 14:50:33","2013-09-20 13:50:33",1
"The Mongo Java driver","How to disable driver logging","The following code can be used to change the level at which events are logged by clients of the driver ...  import java.util.logging.Logger; Logger mongoLogger = Logger.getLogger( com.mongodb ); mongoLogger.setLevel(Level.SEVERE);  However, rather than have to write this code in many different places, I'd really like to have the ability to set the logging level in a single place. For example, the Amazon SDK, hc.apache and others allow me to do this in a log4j.properties file like so:  # Uncomment to turn off all logging from the SDK log4j.logger.com.amazonaws=OFF   # More examples -- adjust to see more / less logging #log4j.logger.com.amazonaws.ec2=WARN #log4j.logger.httpclient.wire=WARN  log4j.logger.org.apache.http=OFF #log4j.logger.org.apache.http=DEBUG #log4j.logger.org.apache.http.wire=ERROR #log4j.logger.org.apache.http.impl.conn=ERROR #log4j.logger.org.apache.http.impl.client=ERROR #log4j.logger.org.apache.http.client=ERROR  Is there any way to do something like this with the mongoDB java driver?  ",Task,"Minor - P4",Closed,"2013-08-22 00:29:32","2013-08-21 23:29:32",1
"The Mongo Java driver","deep copy for Document","Can you please enhance API to provide `deep copy` for a Document object.  It was possible in version 2.x of MongoDB Java driver via this simple hack (Scala code):  dbo.asInstanceOf[BasicDBObject].copy.asInstanceOf[DBObject]  I'm using it in Subset library: https://github.com/osinka/subset2/blob/master/src/main/scala/DBO.scala#L97","New Feature","Major - P3",Closed,"2013-08-14 13:02:35","2013-08-14 12:02:35",1
"The Mongo Java driver","Raise error if both exhaust flag and limit are specified on a query",,Improvement,"Major - P3",Closed,"2013-08-05 19:47:08","2013-08-05 18:47:08",1
"The Mongo Java driver","Raise error if exhaust flag is specified and the server is a mongos",,Improvement,"Major - P3",Closed,"2013-08-05 19:46:21","2013-08-05 18:46:21",1
"The Mongo Java driver","Support decoding of DBRef in Document class","DocumentCodec will encode an org.mongodb.DBRef, but it still decodes as a Document",Improvement,"Major - P3",Closed,"2013-08-02 20:14:54","2013-08-02 19:14:54",1
"The Mongo Java driver","Support write concern for findAndModify helpers ","The server is going to add support for write concern in findAndModify, so the driver needs to ensure that the write concern is added to the message.","New Feature","Major - P3",Closed,"2013-07-31 19:18:35","2013-07-31 18:18:35",1
"The Mongo Java driver","GridFS should be able to handle different Date types","Currently, its locked to `Date` however, if you have a custom decoder that converts Date to JodaDateTime it breaks gridfs",Improvement,"Major - P3",Closed,"2013-07-26 14:59:39","2013-07-26 13:59:39",1
"The Mongo Java driver","DefaultDBCallback.objectDone() creates invalid DBRef objects according to mongodb spec","In the official mongodb jstest dbref3.js (https://github.com/mongodb/mongo/blob/r2.4.5/jstests/dbref3.js) a DBRef must meet the following criteria: - contain $ref and $id fields - $ref must be the first field in the JavaScript object - $id must be the second field in the JavaScript object - the value associated with $ref must be a String  Currently the Java driver only checks if the two fields exist and performs a naive toString() on the $ref value.",Bug,"Minor - P4",Closed,"2013-07-21 04:01:18","2013-07-21 03:01:18",1
"The Mongo Java driver","Create a finer-grained exception hierarchy and use it consistently","It's currently rather difficult to handle exceptions thrown by the driver.  The driver throws MongoException subclasses like Network in some cases, but not in all cases that it seems like it should.  And it throws the generic MongoException in many cases, making it difficult to distinguish.  This is particularly important when clients are trying to use the exception type to determine a course of action, like whether to attempt a retry.  The goal of this ticket is to create a more fine-grained exception hierarchy and to use it consistently.",Improvement,"Major - P3",Closed,"2013-07-18 14:47:27","2013-07-18 13:47:27",1
"The Mongo Java driver","DBCollection.find doesn't use custom DBEncoder to encode query","{{DBApiLayer.__find}} ([DBApiLayer.java:273|https://github.com/mongodb/mongo-java-driver/blob/r2.11.2/src/main/com/mongodb/DBApiLayer.java#L273]) references the global {{DefaultDBEncoder.FACTORY}} instance, instead of using the collection's own custom encoder. This makes it impossible to pervasively use more than one encoder per process. We use a custom Scala wrapper, and this could cause us serious problems. Hopefully it's an easy fix. Thanks.",Bug,"Major - P3",Closed,"2013-06-28 04:07:18","2013-06-28 03:07:18",1
"The Mongo Java driver","DBRef should handle $db","DBRef should handle $db for reads and writes, as soon as the server itself will be able to handle it. ","New Feature","Major - P3",Closed,"2013-06-17 11:56:39","2013-06-17 10:56:39",1
"The Mongo Java driver","driver must authenticate before calling isMaster()",,Task,"Major - P3",Closed,"2013-06-04 21:33:57","2013-06-04 20:33:57",1
"The Mongo Java driver","update addUser to accomodate roles based syntax","http://docs.mongodb.org/manual/tutorial/add-user-to-database/  JAVA-691","New Feature","Major - P3",Closed,"2013-05-17 22:44:23","2013-05-17 21:44:23",1
"The Mongo Java driver","Better error message when ignoring user-specified read preference results in a failure","The driver has an internal list of commands that are allowed to be sent to a secondary. If the command is not on the list, we redirect it to primary, regardless of the requested read preference.  The driver tell the user this and error messages that result from not being able to talk to a primary are misleading. Error messages should indicate clearly that the read preference was switched to primary.",Improvement,"Major - P3",Closed,"2013-04-18 18:43:25","2013-04-18 17:43:25",1
"The Mongo Java driver","Jndi Object Factory","Would be nice to have a MongoJndiObjectFactoryBean which implements javax.naming.spi.ObjectFactory to help configuring MongoClient thru Jndi?  see:  https://github.com/juanlmelo/mongo-java-driver/blob/master/src/main/com/mongodb/jndi/MongoJndiObjectFactoryBean.java","New Feature","Major - P3",Closed,"2013-04-16 14:39:18","2013-04-16 13:39:18",1
"The Mongo Java driver","Support Monotonic Read Consistency when reading from secondaries","It would be great if the Java driver would support Monotonic Read Consistency when reading from secondaries.  The use case are read-heavy applications that don't need strong consistency but require updates to be visible in the same order as they happened on the primary. When using a secondary read preference, the driver currently distributes read load to different secondaries. This means the order of updates is not preserved and in the application it may look as if time is going backwards - sometimes.  One solution to achieve monotonic reads is to use the default primary read preference and distribute load via sharding only. However for read-heavy applications which do not need strong consistency (as ensured by read preference primary) it makes sense to scale read-load over secondaries to avoid the additional cost and complexity that comes with sharding. Even with sharding in place, the number of required shards can be lower if read load is not only handled by the primaries.  Note that the Java driver already supports monotonic reads per thread with requestStart()/requestEnd() methods. However there's no possibility to achieve this for the entire application. Using only one thread per application not an option as it would kill performance. ","New Feature","Major - P3",Closed,"2013-04-12 08:35:12","2013-04-12 07:35:12",1
"The Mongo Java driver","Document insert must call getLastError after each sub-batch even with WriteConcern.UNACKNOWLEDGED","To maintain the semantic that the batch stops as soon as there is a single error...if a user tries to insert 10k documents but only 2k documents fit with the maximum message size, the driver does five 2k inserts. In that case, after each 2k insert, it should call getLastError, and if there was an error, fail. This is whether or not the user wants GLEs called. This is the only way to guarantee the semantics.  So, even if WriteConcern.UNACKNOWLEDGED, you still must call getLastError after each batch, except for the last batch.  This is only if WriteConcern.continueOnError is false.  If it's true, the existing algorithm is ok.",Improvement,"Major - P3",Closed,"2013-04-12 02:01:27","2013-04-12 01:01:27",1
"The Mongo Java driver","Document Null pointer exception being thrown for BasicBSONObject.getLong() and .getInt()","Document that when the field is either not existent, or not a number, it throws an NullPointer exception    links: [http://api.mongodb.org/java/2.0/org/bson/BasicBSONObject.html#getLong(java.lang.String)|http://api.mongodb.org/java/2.0/org/bson/BasicBSONObject.html#getLong(java.lang.String)] [http://api.mongodb.org/java/2.0/org/bson/BasicBSONObject.html#getInt(java.lang.String)|http://api.mongodb.org/java/2.0/org/bson/BasicBSONObject.html#getInt(java.lang.String)]",Improvement,"Major - P3",Closed,"2013-03-08 17:55:26","2013-03-08 17:55:26",1
"The Mongo Java driver","add more constructors in BasicDBObject","Adding more constructors in BasicDBObject would allow a more indented type of writing that resembles the JSON notation. We use a DBO class like in https://bitbucket.org/<USER>mongo-java-aggregate/src/eb1f9884175254e13759ea61fdda40ded8a885a7/src/main/java/org/bitbucket/davidm24/mongodb/aggregate/DBO.java?at=default in all our projects.  (the same goes to DBList)",Improvement,"Minor - P4",Closed,"2013-03-05 12:43:03","2013-03-05 12:43:03",1
"The Mongo Java driver","Add support for asynchronous operations","It would be great to have the Java driver be non-blocking. As far as I know it's currently only possible to make non-blocking requests if I use the Scala ReactiveMongo driver",Epic,"Major - P3",Closed,"2013-02-28 07:41:45","2013-02-28 07:41:45",1
"The Mongo Java driver","Document that MongoClient doesn't throw an exception on connection failure","When creating a new MongoClient instance with valid host/port parameters, but no MongoDB running there, I would expect a MongoException to be thrown. This is not the case. I can even use the MongoClient instance to get a DB instance via getDB() and subsequently a collection instance via getCollection(), again without being actually connected and any exception. I also tried getConnector().isOpen() to verify whether the connection was established and it returned true, without a running server. The latter issue might be me misinterpreting the API though.  Others (see user litiales) are [observing|http://stackoverflow.com/questions/13994513/mongodb-and-java] this as well.",Improvement,"Major - P3",Closed,"2013-02-17 11:01:22","2013-02-17 11:01:22",1
"The Mongo Java driver","DBCollection.remove does not support justOne parameter","DBCollection.remove should optionally take a boolean parameter for the justOne flag.  This change needs to be tunneled down into OutMessage, as well.  Unfortunately, obvious workarounds (such as doing a find before the remove) introduce race conditions.",Bug,"Major - P3",Closed,"2013-02-15 23:10:41","2013-02-15 23:10:41",1
"The Mongo Java driver","GridFS should support WriteConcern option on writes","Namely for GridFSFile.save().  Arguably, might be useful for GridFS ctor as well (along with a ReadPref).  For now one can get the collections and set the WriteConcern/ReadPreference as a default before they use GridFS. The collections will be called <bucket> + .files/.chunks (where the default bucket name is fs) and one should set the options on both.","New Feature","Major - P3",Closed,"2013-01-21 13:28:07","2013-01-21 13:28:07",1
"The Mongo Java driver","Warn if gridfs bucket is missing indexes (and we don't create them)","In the case where there are too many docs, we should warn if the index doesn't exist.  https://github.com/mongodb/mongo-java-driver/blob/master/src/main/com/mongodb/gridfs/GridFS.java#L93",Improvement,"Minor - P4",Closed,"2013-01-10 18:31:38","2013-01-10 18:31:38",1
"The Mongo Java driver","Bulk insert method doesn't have clear failure contract","The JavaDoc for the bulk insert method does not describe the method's postconditions on failure: is the collection unchanged, or might the insertion have succeeded partially? In the latter case, the operation cannot be retried as-is, because there's no way to determine which documents were inserted and which were not.  Ideally, the method should succeed or fail atomically -- otherwise, bulk insert becomes a lot less useful. However, in the event that that behavior can't fit with Mongo's typical transactional behavior, then the operation should throw some subclass of MongoException that identifies which documents were inserted and which were not.  Without an ability to identify and respond to failures, this method is not useful in a production system.",Improvement,"Major - P3",Closed,"2012-12-18 21:43:02","2012-12-18 21:43:02",1
"The Mongo Java driver","Add QueryBuilder into the tutorial","Had a free asking if we could document the query builder in our Java driver tutorial.",Improvement,"Major - P3",Closed,"2012-12-17 05:02:08","2012-12-17 05:02:08",1
"The Mongo Java driver","com.mongodb.util.JSON should provide JSON#serialize( Object o, Writer writer) and JSON#serialize( Object o, OutputStream out) to have a better support for streaming","To serialize a BasicDBObject to a JSON content, it uses the https://github.com/mongodb/mongo-java-driver/blob/master/src/main/com/mongodb/util/JSON.java which provides the JSON#serialize( Object o, StringBuilder buf).  IMHO, I think it should be better to provides 2 methods with Writer and OutputStream:   * JSON#serialize( Object o, Writer writer) * JSON#serialize( Object o, OutputStream out)  Why? Because if you wish returns the JSON content from a Servlet (HttpServletResponse#getWriter(), HttpServletResponse#getOutputStream()), we cannot use streaming mode. We need to create a StringBuilder and put it to the http response writer or outpustream :  ------------------------------------------------------- protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {   BasicDBObject  o = ...;     StringBuilder buf = new StringBuilder ();   JSON.serialize(o, buf);   response.getWriter().write(buf.toString());   // same code :   //response.getWriter().write(o.toString()); } -------------------------------------------------------  I think it should be better to do with streaming mode like this:  ------------------------------------------------------- protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {   BasicDBObject  o = ...;     JSON.serialize(o, response.getWriter()); } ------------------------------------------------------- ",Improvement,"Minor - P4",Closed,"2012-12-08 16:16:29","2012-12-08 16:16:29",1
"The Mongo Java driver","Add round robin support for mongos connections","The driver will measure ping time to the available mongos routers and will randomly distribute queries across the mongos routers that have fast ping times.  ",Improvement,"Major - P3",Closed,"2012-11-22 13:39:24","2012-11-22 13:39:24",1
"The Mongo Java driver","Java driver concurrency documentation should state that db.requestStart() will not guarantee reading own writes with slaveOk if sharding","The [Java Driver Concurrency|http://www.mongodb.org/display/DOCS/Java+Driver+Concurrency] documentation documents the use of {{db.requestStart()}} and {{db.requestDone()}} to try to ensure that you read your own write when slaveOk is set. However, it should indicate that this technique is only effective if you are not sharding, and hence the driver is connecting directly to the mongod instances. When you are sharding, while requests will be processed sequentially by the mongos, there is no guarantee that writes and reads will be directed to the same underlying mongod. Hence you should never use slaveOk in a sharded environment if consistency across requests is important.  This documentation should also mention that slaveOk is deprecated, and instead (or also) refer to read preferences rather than slaveOk.  Perhaps some of the other driver documentation has the same issues. I did not check.",Improvement,"Major - P3",Closed,"2012-11-01 06:00:18","2012-11-01 06:00:18",1
"The Mongo Java driver","Improve driver robustness on node restart","a client using the java driver encounters exceptions when reading from a server that gets restarted gracefully, although another node is up and available for reads. I use a topology of 2 nodes, with one being set as a fixed primary for my tests. on the client side I use read mode primaryPreferred. during the test, I constantly query a collection while repeatdly restarting the primary using the windows service. the expectation is that a read never fails because: 1) the primary is gracefully restarted and 2) the secondary is always up and running.  during the course of running the test, I get exceptions on the client side from time to time. so far I identified these 2:    ",Improvement,"Major - P3",Closed,"2012-10-25 08:32:20","2012-10-25 07:32:20",1
"The Mongo Java driver","Add more fluent methods like appendIfNotNull() to BasicDBObject","{{BasicDBObject}} already has a fluent method {{append(String key, Object val)}} returning itself. It would be very convenient to have more {{append}} methods like * {{BasicDBObject appendIfNotBlank(String key, String val)}} * {{BasicDBObject appendIfNotEmpty(String key, Collection val)}} * {{BasicDBObject appendIfNotEmpty(String key, Object[] val)}} * {{BasicDBObject appendIfNotEmpty(String key, String val)}} * {{BasicDBObject appendIfNotNull(String key, Object val)}}  so that you could write code like this:   instead of ",Improvement,"Major - P3",Closed,"2012-10-23 09:59:38","2012-10-23 08:59:38",1
"The Mongo Java driver","JSON.parse should throw an exception if the input string is not fully consumed","This:    produces a DBObject equivalent to:    It should throw an exception instead",Improvement,"Major - P3",Closed,"2012-10-22 02:58:34","2012-10-22 01:58:34",1
"The Mongo Java driver","Caching Behavior Inconsistent when Dropping Indexes","Based on testing, it seems that the caching behavior of the Java driver in terms of the dropped indexes is different based on how the drop of indexes is called.  Does NOT remove cached indexes:    DOES remove cached indexes:    This behavior can cause subsequent re-addition of indexes to fail (as a no-op essentially, there is no indication of the fact that the index has not been created) and can cause significant issues when indexes fail to be added.",Bug,"Major - P3",Closed,"2012-10-15 16:11:27","2012-10-15 15:11:27",1
"The Mongo Java driver","Add information explaining MongoClientOptions to the java driver tutorial","There are no examples on how to use MongoOptions in the java tutorial. Specifically examples on how to set connectionsPerHost, and connect timeout. Also would be good to have some example exceptions when the connection pool is too small. ",Task,"Major - P3",Closed,"2012-08-14 15:01:44","2012-08-14 14:01:44",1
"The Mongo Java driver","Better support for lists in BasicDBObjectBuilder","The BasicDBObjectBuilder's javadoc says it's a utility to build complex objects but as far as I can tell it doesn't have a very good support for lists.  I wrote a few methods to help with that as well as some basic unit tests to go with it. It would be good if it gets added in your next version.",Improvement,"Minor - P4",Closed,"2012-09-07 18:21:33","2012-09-07 17:21:33",1
"The Mongo Java driver","Configurable throttling at the driver level","We find that we need to throttle our writes while ensuring that we don't negatively impact reads on our secondaries.  We are working on monitoring qr, lock% and replication lag as part of a replica set health check method.  Only once the health check returns a positive result will we process the updates (or deletes) to Mongo.  This will ensure that we don't overwhelm the oplog with a bunch of writes that then (via high lock contention) slow down reads so much that our jbosses start to run out of threads.  Instead of creating this method in our code and then calling it throughout our entire application, it would be ideal if we could make this a configuration of the driver that we set once and use throughout the entire application.","New Feature","Major - P3",Closed,"2012-09-05 02:25:44","2012-09-05 01:25:44",1
"The Mongo Java driver","Improve error code documentation","Currently there's no complete documentation of the error codes wrapped into MongoExceptions. There is http://www.mongodb.org/display/DOCS/Error+Codes but that is self-entitled as highly incomplete and I've repeatedly seen users seeing other error codes popping up. See https://jira.springsource.org/browse/DATAMONGO-512 for example (-5 seems to indicate an integrity violation).",Improvement,"Major - P3",Closed,"2012-08-16 13:50:03","2012-08-16 12:50:03",1
"The Mongo Java driver","Javadocs should include code samples, and links to docs/manual","The javadocs should include sample code for commonly used methods. The javadocs should link to the docs/manual to the conceptual pages like all findAndModify/Remove methods should link to the findAndModify docs page.",Improvement,"Minor - P4",Closed,"2012-08-14 17:31:27","2012-08-14 16:31:27",1
"The Mongo Java driver","Add nolock option to doEval()","No way to pass {nolock: true} to doEval(); user must construct a DBObject and pass to generic command interface.",Improvement,"Minor - P4",Closed,"2012-07-10 16:04:20","2012-07-10 15:04:20",1
"The Mongo Java driver","Improve the API documentation for tag-based write concern","The API documentation for the WriteConcern class does not contain a lot of detail of tag-based write concern.   # We should improve the class comment and the WriteConcern(String w) comment. # We need to correct the comments for the constructors that take additional parameters after the {{String w}}. The comments for them still refer to integer values of w, and this is confusing.",Improvement,"Major - P3",Closed,"2012-07-09 06:36:10","2012-07-09 05:36:10",1
"The Mongo Java driver","Long-running tailable cursors consume too much memory","I already posted it on the mailing list, it didn't receive any replies, but I still think it's a bug. Please correct me if I'm wrong.  A tailable cursor on a capped collection can live potentially for a long time and return multiple batches of data. However, it seems to me that the size of each batch that is read from Mongo is added to the list: DBApiLayer.Result._sizes, in the DBApiLayer.Result.init method, called from the _advance method.  The list is available via DBCursor.getSizes().  If the list grows and grows, especially if there's a lot of data added to the capped collection, eventually it will OOM.  Is that right?  UPDATE:  Added DBCursor.disableBatchSizeTracking() to work around this problem.",Bug,"Major - P3",Closed,"2012-06-29 11:43:39","2012-06-29 10:43:39",1
"The Mongo Java driver","Incorrect object type received in BSONObject encoding hook (Transformer) when multiple transformers found","When multiple encoding hooks are found for a class in BSONObject, they are applied linearly:    If the first transformer found transforms from C to T and the second from C to R (both were registered as encoding hooks for class C or one of its superclasses): 1. The first transformer transforms o (of type C) to o' of type T 2. The second transformer, programmed to transform things of type C receives an object of type T, which is potentially unrelated to C. So, we receive an object of an unexpected type.  Solution would be to apply the first transformer (getting o' of type T) and then looking for encoding hooks for class T and so on, until no encoding hooks are found.  Apart from that, if I have a class C1 that extends C2 and I have encoding hooks for both (EH1 and EH2), when given an object of type C2 to DBObject, EH2 should be applied, not EH1, since C2 is more specific than C1. Currently, it applies both (and this is how I found what I explained earlier).",Improvement,"Major - P3",Closed,"2012-04-20 10:16:07","2012-04-20 09:16:07",1
"The Mongo Java driver","Create a Java Driver test for ensuring an exception is thrown if a DBCursor in a capped collection gets wrapped over/overwritten by inserts.","Say we're tailing an oplog and have a DBCursor at May 9th, 2012 11:44AM. We keep this cursor alive (without incrementing it) long enough for the oplog to wrap around and overwrite the May 9th, 2012 11:44AM entry. If we call DBCursor.next() it should eventually throw an exception, if not immediately then when the cursor actually sends a getMore request to the server.",Task,"Minor - P4",Closed,"2012-03-15 16:50:02","2012-03-15 16:50:02",1
"The Mongo Java driver","DuplicateKey exception not properly thrown through findAndModify","Hi,  when I try to insert a duplicate through this code:  try {   coll.findAndModify( queryId( document ).and( p.revision ).is( document.getRevision() ).get(),                     new BasicDBObject( $set, mods ) );             } } catch ( MongoException e ) {  I'm not able to directly catch the DuplicateKey Exception, as findAndModify will not throw such exception. Rather it throws an CommandResult$CommandFailure, sadly this Exception isn't public so I can't catch it nor can I cast to it from MongoException. Catching a MongoException.DuplicateKey isn't working either as this exception isn't thrown at all. The only usage of DuplicateKey is within CommandResult.getException, but as CommandFailure isn't a subtype of CommandResult I neither can't cast to this, so the only way to currently find out what exception was thrown is to dig into the MongoException payload which looks like:  com.mongodb.CommandResult$CommandFailure: command failed [command failed [findandmodify] { serverUsed : <USER>0830:27017 , lastErrorObject : { err : E11000 duplicate key error index: test.address.$ns_1_t_1_p.doc_id_1  dup key: { : \__nn__\, : \N\, : \testPutPKTwice\ } , code : 11000 , n : 0 , connectionId : 44 , ok : 1.0} , errmsg : E11000 duplicate key error index: test.address.$ns_1_t_1_p.doc_id_1  dup key: { : \__nn__\, : \N\, : \testPutPKTwice\ } , ok : 0.0}  to check getLastError or to manually start another query to check if the key is already used. Neither of them feels to me that this is the way it's meant to be.",Bug,"Minor - P4",Closed,"2012-02-07 19:20:10","2012-02-07 19:20:10",1
"The Mongo Java driver","Ability to retrieve GridFSFile from oplog collection","When monitoring oplog for GridFS documents. It would be a good idea to have the ability to retrieve the object directly from this collection.  The current implementation does not support it GridFS document must be fetch for its collection.",Improvement,"Minor - P4",Closed,"2012-01-30 12:16:36","2012-01-30 12:16:36",1
"The Mongo Java driver","Add out.* support for MapReduceCommand","Currently you can only specify the db option for the out:{} option. We need support sharded as well.  https://github.com/mongodb/mongo-java-driver/blob/master/src/main/com/mongodb/MapReduceCommand.java#L253",Improvement,"Minor - P4",Closed,"2012-01-23 18:09:59","2012-01-23 18:09:59",1
"The Mongo Java driver","Add support for connection validation","The Java MongoDB driver has a built-in connection pool. One of the downsides of a connection pool is that pooled connections may become stale or broken over time (oftentimes due to timeouts, network issues, etc.).  As a result, many connection pooling implementations offer a mechanism by which pooled connections can be automatically validated (usually before use, after use, or periodically). Some examples include [DBCP's validationQuery |http://commons.apache.org/dbcp/configuration.html] and [c3p0's connection testing|http://www.mchange.com/projects/c3p0/index.html#configuring_connection_testing]. It would be great if the Java MongoDB driver could also support for some means of connection validation.  Such a feature need not be as complex or general as in the above implementations. For example, while arbitrary validation queries might be desirable in some cases, something as simple as a [ping command|http://www.mongodb.org/display/DOCS/List+of+Database+Commands] seems like it would be sufficient to validate a connection in many cases.  Test case: I've attached a rather contrived example which illustrates a case in which connection validation might be useful. To simulate connection failure, the test involves restarting mongod during a brief waiting period between two successive calls to MongoDB. The connection pool size is set to 1 to ensure that the pooled connection is used for the second call. ","New Feature","Major - P3",Closed,"2011-12-27 23:36:24","2011-12-27 23:36:24",1
"The Mongo Java driver","Update QueryBuilder to support all currently supported MongoDB Query Operators",,Improvement,"Major - P3",Closed,"2011-12-14 20:24:42","2011-12-14 20:24:42",1
"The Mongo Java driver","Gridfs remove should return WriteResult","Return the WriteResult from the files remove (not the chunks)",Improvement,"Minor - P4",Closed,"2011-12-05 19:13:53","2011-12-05 19:13:53",1
"The Mongo Java driver","DBCursor.count() <> DBCursor.toArray().size() using with $max","Code:   Result: {code:borderStyle=solid} cursor.count - 10 cursor.toArray.size - 0 {code}",Bug,"Minor - P4",Closed,"2011-11-30 14:58:12","2011-11-30 14:58:12",1
"The Mongo Java driver","DBObject.toString raises RuntimeException when the object contains Symbol","scala> com.mongodb.BasicDBObjectBuilder.start(key, new org.bson.types.Symbol(symbol)).get.toString java.lang.RuntimeException: json can't serialize type : class org.bson.types.Symbol  at com.mongodb.util.JSON.serialize(JSON.java:261)  at com.mongodb.util.JSON.serialize(JSON.java:141)  at com.mongodb.util.JSON.serialize(JSON.java:58)  at com.mongodb.BasicDBObject.toString(BasicDBObject.java:84) ",Bug,"Major - P3",Closed,"2011-11-29 18:26:58","2011-11-29 18:26:58",1
"The Mongo Java driver","GridFsFile should expose data keys as constants","{{GridFsFile}} currently uses hard-coded field names to store the metadata information (filename, content length etc.). To query documents in turn one has to know those field names and thus look into the implementation. It would be cool if the default metadata field names would be exposed via public constants so that one can build a query like this:  ",Improvement,"Major - P3",Closed,"2011-10-19 14:01:59","2011-10-19 13:01:59",1
"The Mongo Java driver","GridFSFile does not implement equals(...)/hashCode()","The {{GridFsFile}} class does not implement {{equals(...)}} and {{hashCode()}} so that comparing two instances of the class actually representing the very same file will not answer {{equals(...)}} with {{true}}. Also problematic when dealing with collections of {{GridFsFile}} instances.",Bug,"Major - P3",Closed,"2011-10-19 13:20:26","2011-10-19 12:20:26",1
"The Mongo Java driver","About closing of cursors","Given: a new cursor via a find on a collection If: the cursor is closed before any documents are fetched And: the sort order is changed Then: No exception is thrown  Is this a bug? I expect that an exception should be thrown if one changes the sort or limit etc on a closed cursor.",Task,"Minor - P4",Closed,"2011-10-18 13:29:15","2011-10-18 12:29:15",1
"The Mongo Java driver","DBObjects utility class - Working easier and more efficient with DBObjects","I have made a utility class which is useful for all users of the mongo-java-driver i suppose :)  See https://github.com/<USER>mongo-java-driver/blob/master/src/main/com/mongodb/DBObjects.java (there is even a junit test, but currently JUnit 4 based)  If you think it is good enough to include, i will create a pull request.  Regards,  Ronald","New Feature","Major - P3",Closed,"2011-10-07 09:03:53","2011-10-07 08:03:53",1
"The Mongo Java driver","In BasicBSONCallback treat a Symbol as such","The Ruby driver can save and retrieve a BSON Symbol if the saved document is modified via JVM langauges that wrap the Java driver, the Symbol would be replaced with a String.  To allow retrieving of Bson Symbols I would suggest this:  _put( name , new Symbol(v) );  I have patched and jar'ed my fork and my tests in JRuby do save and retrieve a Ruby Symbol.  I am not in a position to check the other JVM languages though, sorry.",Improvement,"Trivial - P5",Closed,"2011-10-03 08:33:42","2011-10-03 07:33:42",1
"The Mongo Java driver","implement the putAll( Map m ) method in GridFSFile.java","I use the method in my project, but found it's not implemented, it just throw new UnsupportedOperationException();  Why not implement it? it's easy. And the mothod is useful.",Improvement,"Major - P3",Closed,"2011-08-18 09:11:09","2011-08-18 08:11:09",1
"The Mongo Java driver","UUIDs are stored as little endian (should be big endian)","In python: >>> import pymongo >>> import uuid >>> from pymongo import Connection >>> c = Connection('localhost', 27017) >>> db = c.test >>> db.foo.insert({python: uuid.UUID(aaf4c61d-dcc5-e8a2-dabe-de0f3b482cd9)}) ObjectId('4e318fb98f1e81eac4000001')  In java:   UUID id = UUID.fromString(aaf4c61d-dcc5-e8a2-dabe-de0f3b482cd9);   Mongo mongo = new Mongo();   DB db = mongo.getDB(test);   DBObject dbo = new BasicDBObject();   dbo.put(java, id);   db.getCollection(foo).insert(dbo);  Result: > db.foo.find() { _id : ObjectId(4e318fb98f1e81eac4000001), python : UUID('aaf4c61ddcc5e8a2dabede0f3b482cd9') } { _id : ObjectId(4e318fc12746ac3aa375aee9), java : UUID('a2e8c5dc1dc6f4aad92c483b0fdebeda') }  (yes, with the patch from SERVER-1201 applied)  Java seems to serialize/deserialize the UUIDs as little endian according to BSONEncoder.java, line 354  Changing this would however break a lot of applications out there. However, the python/java incompatibility is really bad, so it should be fixed in my humble opinion...",Improvement,"Major - P3",Closed,"2011-07-28 17:41:01","2011-07-28 16:41:01",1
"The Mongo Java driver","Make GridFS index check/ensure less costly","Add a check for the index (doesn't require a server call if cached) before calling count (always requires a server call).",Improvement,"Trivial - P5",Closed,"2011-06-29 15:49:38","2011-06-29 14:49:38",1
"The Mongo Java driver","Secondary servers in a replication set that are out of date should not be read from","We are enabling slaveOK to read from secondary servers, but we have run into many problems with one of our servers that sometimes gets too far behind to be a useful reader.  Therefore, the client should not query from any secondary servers that are out of date.  Apparently the ability is there for the clients to query from the server, though I'm not sure if there's a way to tell the client to only read from one of the servers: http://www.mongodb.org/display/DOCS/Replica+Set+Internals#ReplicaSetInternals-ReadingfromSecondariesandStaleness",Improvement,"Major - P3",Closed,"2011-06-21 17:23:28","2011-06-21 16:23:28",1
"The Mongo Java driver","MongURI should allow configuring of the socketKeepAlive setting in the driver","MongoOptions.java supports the enabling of socket keep alive, yet the option is not accessible through MongoURI.java.  Is there a specific reason for this? or can the option be enabled by adding the following code to MongoURI.java in the parseOptions method?  (after line 171 in v2.6.3 of MongoURI.java)          else if ( key.equals( socketKeepAlive ) ) _options.autoConnectRetry = _parseBoolean( value ); ",Improvement,"Minor - P4",Closed,"2011-06-13 09:43:40","2011-06-13 08:43:40",1
"The Mongo Java driver","Add field selection in DBRef.fetch()","Something like that",Improvement,"Major - P3",Closed,"2011-05-10 21:29:16","2011-05-10 20:29:16",1
"The Mongo Java driver","when choosing slave to read from, should take into account slaveDelay and possibly oplog status","If a server has a slavedelay, it should probably not be used for reads if possible. Further, driver could take into account how uptodate a slave is, and avoid using one that is late.",Improvement,"Major - P3",Closed,"2011-03-27 19:45:38","2011-03-27 18:45:38",1
"The Mongo Java driver","Improve connection error message","Make sure to include both name and resolved address (ip) in the error message.",Improvement,"Minor - P4",Closed,"2011-02-25 19:42:03","2011-02-25 19:42:03",1
"The Mongo Java driver","DBCursor.toArray() should use exhaust flag","When it is clear all data is wanted then the exhaust query flag (6) should be used. ",Improvement,"Minor - P4",Closed,"2011-02-21 04:41:48","2011-02-21 04:41:48",1
"The Mongo Java driver","Add support for Java enums","Add support for encoding/decoding java enums","New Feature","Major - P3",Closed,"2011-02-08 21:30:37","2011-02-08 21:30:37",1
"The Mongo Java driver","Java driver throws a lot of undeclared/non MongoException runtime exceptions","The following runtime exceptions are thrown by the Java driver.   They make handling database failures incredibly difficult.   Essentially, the only thing the Java driver should throw are a) declared exceptions (e.g., IOException) or MongoException (or a subclass of).  Let me know if you are OK with these changes and I'll modify the driver.  RuntimeException IllegalArgumentException IllegalStateException UnsupportedOperationException",Improvement,"Major - P3",Closed,"2011-02-03 20:13:06","2011-02-03 20:13:06",1
"The Mongo Java driver","Support dot notation in DBObject.get and DBObject.put","While dot notation can be used in queries to look up objects, this is not supported in DBObject.get. Also DBObject.put should handle dots in specified keys accordingly.  I've written simple static methods that implement the desired behaviour, with related tests.  Tests first:    Implementation of get/put:     Could this behaviour be introduced to DBObject implementations or there any reason why this does not make sense? If it could be introduces I can implement it in current trunk and provide a patch.",Improvement,"Minor - P4",Closed,"2011-01-19 09:04:27","2011-01-19 09:04:27",1
"The Mongo Java driver","Support aliases on gridfs files, and in open methods","Add support for the aliases field on gridfs files in all related operations.","New Feature","Major - P3",Closed,"2011-01-06 21:18:27","2011-01-06 21:18:27",1
"The Mongo Java driver","DBCursor.toArray() should run decodes in thread-pool instead of serially","By using a thread-pool, or more than just a single thread, the decoding time can be greatly decreased. This means the results will be available to clients in a much shorter time.  This could also be done for each batch the cursor retrieves; that might be enough to take care of the issue. ","New Feature","Minor - P4",Closed,"2010-12-28 18:58:00","2010-12-28 18:58:00",1
"The Mongo Java driver","support duplicate operation in GridFS","add a duplicate() method in GridFS to support a duplication of a file.  When we duplicate a file ,we can increase the reference count of the metadata of the file, and when the reference count decrease to 0, we can delete the file indeed.","New Feature","Major - P3",Closed,"2010-12-24 02:32:57","2010-12-24 02:32:57",1
"The Mongo Java driver","Mongo constructors for single node should allow replicationset/slave/single option","Cleanup the constructors so they are clear about the behavior of finding other nodes in a replicaset/pair/etc. and offer control over that behavior. Maybe a MongoOptions option... or not.  See http://jira.mongodb.org/browse/JAVA-187 for some background.",Improvement,"Minor - P4",Closed,"2010-10-27 23:10:46","2010-10-27 22:10:46",1
"The Mongo Java driver","Add GridFS.drop(db, bucket)/(db)","Add way to remove bucket (basically .files.drop() + .chunks.drop())  This would be a static method and an instance method.",Improvement,"Major - P3",Closed,"2010-10-18 16:55:53","2010-10-18 15:55:53",1
"The Mongo Java driver","Portable cursor","Allow (notimeout) cursor to be used outside of the initial jvm.  This means we need a way to create a DBCursor from a cursorId (plus other options like start/getnum), disable automatic cleanup (killing) of the cursorId (on finalize)  and expose the cursor in a portable (serialized as a string?) way. Maybe it would be best to create a CursorBookmark object to encapsulate the information and so that a DBCursor can be constructed from it.  In some applications it may be more useful to do this rather than to construct a new query based on the current query parameters (copy + skip). The CursorBookmark could work in two ways potentially; one as a copy of the query (plus skip/position) or as a reference to the cursorId as described above. For this issue I will just address the second (cursorId) case.",Improvement,"Major - P3",Closed,"2010-10-09 16:29:34","2010-10-09 15:29:34",1
"The Mongo Java driver","Support Unix Domain Sockets","Add support for unix domain sockets (AF_UNIX) -- potentially with junixsocket (http://code.google.com/p/junixsocket/)  This dep should probably be done in way where it is optional. This can easily be done by checking for the existence of the classes used for unix domain sockets before using them. This would make it an optional runtime dep.; it will still be a compile time dep. ","New Feature","Major - P3",Closed,"2010-09-09 03:09:13","2010-09-09 02:09:13",1
"The Mongo Java driver","Verify server support for a feature before allowing using of that feature","Verify the server has a feature before issuing commands requiring that feature.  The driver can check (and warn/error-out) if the server limits the maxBsonSize and you send it something too big. Also, if you try to use $where or some other unsupported operation it could check that (like in the QueryBuilder, or other fluent interfaces).  This would be blocked until SERVER-1582 is done.",Improvement,"Major - P3",Closed,"2010-08-06 16:17:04","2010-08-06 15:17:04",1
"The Mongo Java driver","Support renaming of GridFS files","We don't have rename in any gridfs driver at the moment.","New Feature","Major - P3",Closed,"2010-05-11 10:09:07","2010-05-11 09:09:07",1
"The Mongo Java driver","Support for streams in com.mongodb.util.JSON","It would be nice if the JSON class supported streams in addition to strings as sources and targets for JSON during parsing and serialization. In particular, I am interested in providing JSON forms to the parser via an InputStream for situations where (a) the JSON objects are very large and more importantly (b) for situations where you want to parse JSON forms from a source one after the other (i.e. a file with a series of JSON forms one after the other). It would be nice if I could create a parser object, provide it with an Input Stream, and then call parse() repeatedly, with the parser parsing and consuming from the stream one JSON object per call.   Supporting serialization to OutputStreams would be for symmetry, but also useful for similar reasons.","New Feature","Minor - P4",Closed,"2010-04-14 23:00:30","2010-04-14 22:00:30",1
