Name,Title,Description,Type,Priority,Status,Creation_Date,Estimation_Date,Story_Point
"Apache MXNet","Add dType to Labels Data Desc in NDArrayDataIter","Based on a customer ask here :          [https://discuss.mxnet.io/t/why-does-ndarrayiter-use-float32-for-the-label-datadesc/3024] ",Improvement,Major,Done,"2019-01-31 17:48:56","2019-01-31 17:48:56",2
"Apache MXNet","Change Java API method signatures to take Iterables instead of List","If we can do that it'll also let people use the same method with Sets, queues, etc.",Improvement,Minor,Done,"2019-01-10 19:40:48","2019-01-10 19:40:48",2
"Apache MXNet","Add Bounding Boxes over the image in SSD Example","Similar to the Python SSD example : [https://gluon.mxnet.io/chapter08_computer-vision/object-detection.html], we should have a utility method to draw bounding boxes on the image rather than spit out numbers (co-ordinates and probabilities) for objects detected in the image.          Here's something similar on the above lines in Scala : [https://brunk.io/deep-learning-in-scala-part-3-object-detection.html]     ",Improvement,Minor,Done,"2018-12-27 23:01:12","2018-12-27 23:01:12",3
"Apache MXNet","Fix Performance issue with GPU of Object Detector",,"New Feature",Major,Done,"2018-12-04 05:42:07","2018-12-04 05:42:07",3
"Apache MXNet","Enable custom Shape input for inference","Currently, Predictor API only allows fixed-shape data as the input. Backend already capable for handling different shape input.",Improvement,Major,Done,"2018-11-20 05:17:37","2018-11-20 05:17:37",3
"Apache MXNet","Clean up the way to do test",,Sub-task,Major,Done,"2018-11-16 01:12:56","2018-11-16 01:12:56",1
"Apache MXNet","Clean up NDArray",,Sub-task,Major,Done,"2018-11-15 06:34:05","2018-11-15 06:34:05",1
"Apache MXNet","add CentOS build for Scala/Java package",,Bug,Major,Done,"2018-11-15 06:08:26","2018-11-15 06:08:26",3
"Apache MXNet","fix document issue","Fix the final merge document issue",Sub-task,Major,Done,"2018-11-14 19:14:16","2018-11-14 19:14:16",1
"Apache MXNet","Add Scala/Java Benchmark Memory Monitoring toolkit","User should be able to use toolkit that is available in Scala/Java to monitor the CPU/GPU RAM usage while running the benchmarks.",Improvement,Major,Done,"2018-11-09 04:37:13","2018-11-09 04:37:13",3
"Apache MXNet","cherry-pick resource scope PR",,Sub-task,Major,Done,"2018-11-07 21:47:02","2018-11-07 21:47:02",1
"Apache MXNet","NDArrayBuilder: Better api naming","In a recent discussion with Naveen and Andrew, the current API name was not right (such as setout should be setOut). In that case, there should be a minor fix apply on it",Sub-task,Major,Done,"2018-11-06 00:06:04","2018-11-06 00:06:04",5
"Apache MXNet","Resolve conflict: Refactor Macros with the up to date design","[https://github.com/apache/incubator-mxnet/pull/13038]     ",Sub-task,Major,Done,"2018-11-06 00:03:13","2018-11-06 00:03:13",1
"Apache MXNet","Set maven publishing CD into deployment","To put it into deployment, the pipeline script and binary build scripts should be merged into the ci_publish branch. Then, ci_publish must be merged into master. All final credentials must be stored for usage and the nightly jenkins job must be properly configured",Sub-task,Major,Done,"2018-11-01 21:16:01","2018-11-01 21:16:01",3
"Apache MXNet","Integrate binary build scripts into pipeline","The binary build scripts produced by the science team should be integrated into the pipeline. They must used to handle the build script, the testing should successfully utilize the result from the build, and the deployment should also run the binary build scripts.",Sub-task,Major,Done,"2018-11-01 21:14:06","2018-11-01 21:14:06",5
"Apache MXNet","OSX for maven publishing","The maven publishing should run on travis. This requires some additional support from the berlin team.  As only a single osx environment is available on travis, it does not require multi-environment testing.  But, the credential system must be setup separately from Jenkins.",Sub-task,Major,Done,"2018-11-01 21:09:46","2018-11-01 21:09:46",3
"Apache MXNet","GPU for Maven pipeline","GPU support should be added to the maven publish pipeline.  This requires restricted-gpu-nodes added to the ci for both testing during development and production. Then, it needs to work for build, all testing environments, and production.",Sub-task,Major,Done,"2018-11-01 21:05:30","2018-11-01 21:05:30",3
"Apache MXNet","Multi-Environment Testing","Build and implement the multi-environment testing for the CI as part of the publish pipeline. It should pull a dependency bundled jar (either from the build stage if available or nexus) and run it across various environments. Those environments must also include docker containers that install only the basic dependencies needed         The following environments are currently planned:   * 14.04   * 16.04   * 18.04   * centOS   * amazon linux",Sub-task,Major,Done,"2018-11-01 21:02:38","2018-11-01 21:02:38",5
"Apache MXNet","Create Java/Scala API example with using Predictor API","Make sure to include usage of both: predict(List<List<Float>>) and predictWithNDArray(NDArray)",Story,Major,Done,"2018-10-30 23:45:55","2018-10-30 23:45:55",3
"Apache MXNet","Enable users to choose deterministic algorithms in CNN layers","Currently some CUDNN algorithms are non-deterministic. Need a flag to enforce the choice of deterministic algorthms. ","New Feature",Major,"To Do","2018-10-27 00:49:36","2018-10-26 23:49:36",5
"Apache MXNet","Make ResourceScope.using() work in Java","Currently the ResourceScope is used using try-with-resources in Java which works, but is not intuitive and as easy to use as in Scala.    Make it's usage in Java similar to the one in Scala by using apply() method technique or any other suitable technique.      ",Improvement,Minor,Done,"2018-10-24 06:30:13","2018-10-24 05:30:13",8
"Apache MXNet","Review and improve code in Horovod final API design",,Task,Major,Done,"2018-10-22 23:33:36","2018-10-22 22:33:36",3
"Apache MXNet","Benchmark feedforward network with dense layers using Java API",,Story,Major,Done,"2018-10-22 20:45:21","2018-10-22 19:45:21",5
"Apache MXNet","Create Install and build instructions- per platform (Linux CPU/GPU, OSX CPU)",,Story,Major,Done,"2018-10-22 20:36:44","2018-10-22 19:36:44",1
"Apache MXNet","Make ResourceScope work in Java.",,Sub-task,Major,Done,"2018-10-22 20:04:50","2018-10-22 19:04:50",3
"Apache MXNet","Write an example to demonstrate the inference workflow using RNN",,Story,Major,Done,"2018-10-18 17:55:59","2018-10-18 16:55:59",8
"Apache MXNet","Fix issues in the prototype API design",,Task,Major,Done,"2018-10-17 18:19:31","2018-10-17 17:19:31",1
"Apache MXNet","Review and merge final API design to MXNet repository",,Task,Major,Done,"2018-10-17 18:00:43","2018-10-17 17:00:43",5
"Apache MXNet","Random segmentation fault during training",,Bug,Major,Done,"2018-10-17 17:59:09","2018-10-17 16:59:09",1
"Apache MXNet","Gather the customer usage details of training API in C++ package.","The current MXNet C++ package contains API for training and inference workflows.    In order to decide whether we should continue to support training APIs, we need to understand the customer use cases.    The goal of this story is to gather the internal and external customer usage details of training API available in C++ Package.    We will evaluate:    1.   Github issues.    2. Discussion forums, and    3. Website analytics    to establish these details. Based on this analysis we will recommend the future direction.    The output of this story will be a one-pager document.",Story,Major,Done,"2018-10-15 22:02:19","2018-10-15 21:02:19",3
"Apache MXNet","Write examples to demonstrate the inference workflow using C++ API.","We are planning to provide examples that demonstrate the inference workflow using C++ API.    These examples will demonstrate:   #   How to load pre-trained network and its parameters.   #   How to pre-process the data for validation/inference.   #  Measure the throughput and validation accuracy.         These examples will be available under separate folder. We will primarily focus on CiFar10 dataset. We will use models that are available at http://data.mxnet.io/models/    In order to be consistent with python examples, the cpp-package examples will demonstrate usage of   # 'imagenet1k-inception-bn'   # 'imagenet1k-resnet-18'   # 'imagenet1k-resnet-34'   # 'imagenet1k-resnet-50'   # 'imagenet1k-resnet-101'   # 'imagenet1k-resnet-152'   # 'imagenet1k-resnext-50'   # 'imagenet1k-resnext-101'   # 'imagenet1k-resnext-101-64x4d'   # 'imagenet11k-resnet-152'   # 'imagenet11k-place365ch-resnet-152'   # 'imagenet11k-place365ch-resnet-50'               ",Story,Major,Done,"2018-10-10 01:19:33","2018-10-10 00:19:33",3
"Apache MXNet","Update Scala/Java API bindings to log4j v2.11","Update log4j version used in Java/Scala bindings from v1.2.17 to 2.11 ([https://search.maven.org/artifact/org.apache.logging.log4j/log4j/2.11.1/pom).]",Story,Major,Done,"2018-10-10 00:13:37","2018-10-09 23:13:37",1
"Apache MXNet","Give a design review of the project to the team",,Task,Major,"To Do","2018-10-08 22:47:53","2018-10-08 21:47:53",1
"Apache MXNet","Write blogpost of MXNet multithreading",,Task,Major,"To Do","2018-10-08 22:47:24","2018-10-08 21:47:24",13
"Apache MXNet","Write the project plan and discuss with team",,Task,Major,"To Do","2018-10-08 22:46:20","2018-10-08 21:46:20",2
"Apache MXNet","Spike: Understand multithreading in CUDA GPU device",,Task,Major,"To Do","2018-10-08 22:43:40","2018-10-08 21:43:40",2
"Apache MXNet","Understand the Threaded engine and Naive engine",,Task,Major,"To Do","2018-10-08 22:41:41","2018-10-08 21:41:41",3
"Apache MXNet","Fix issues related to thread safety",,Task,Major,"To Do","2018-10-08 22:29:09","2018-10-08 21:29:09",99
"Apache MXNet","Implement an algorithm to select the optimal number of threads in OpenMP for each platform",,Task,Major,"To Do","2018-10-08 22:28:02","2018-10-08 21:28:02",5
"Apache MXNet","Performance benchmark with and without multithreaded training",,Task,Major,"To Do","2018-10-08 22:03:55","2018-10-08 21:03:55",5
"Apache MXNet","Performance benchmark with and without multithreaded inference","*Acceptance Criteria*  - Run inference with 1/2/4/8/16/32... threads on at least 4 different platforms  - Demonstrate the results in a spreadsheet  ",Task,Major,"To Do","2018-10-08 22:03:24","2018-10-08 21:03:24",5
"Apache MXNet","Fix the current issue in OpenMP",,Task,Major,"To Do","2018-10-08 22:01:12","2018-10-08 21:01:12",99
"Apache MXNet","Understand the multithreaded inference in Python API",,Task,Major,"To Do","2018-10-08 21:59:28","2018-10-08 20:59:28",2
"Apache MXNet","Understand the current workaround/difficulty in C++ API using multithreaded inference",,Task,Major,"To Do","2018-10-08 21:58:28","2018-10-08 20:58:28",2
"Apache MXNet","Document the thread safety and its limitation",,Task,Major,"To Do","2018-10-08 21:57:57","2018-10-08 20:57:57",5
"Apache MXNet","Implement a thread pool mechanism in engine",,Task,Major,"To Do","2018-10-08 21:57:36","2018-10-08 20:57:36",99
"Apache MXNet","Understand the current workaround in Java/Scala API using multithreaded inference","https://tt.amazon.com/0161011632  ",Task,Major,"To Do","2018-10-08 21:55:39","2018-10-08 20:55:39",3
"Apache MXNet","Reproduce the bug in multithreaded inference inside MXNet engine",,Task,Major,"To Do","2018-10-08 21:49:36","2018-10-08 20:49:36",2
"Apache MXNet","Enable Java API Benchmark runs","# Create Performance Test scripts to utilize SSD example   # Setup Inference dashboard and capture CloudWatch metrics on it   # Create Carnaval alarms for thresholds and absence of runs. Cut TT to MXNet-Java-API CTI.    Note: we should be able to compare Java and Scala Benchmarks for SSD example.",Story,Major,Done,"2018-10-08 21:24:23","2018-10-08 20:24:23",5
"Apache MXNet","Update binary build script for Ubuntu 14.04","GLIBC on 16.04 was too new to use within build script so that Amazon Linux image would be able to work with image built in that way.     Try to:    - update GLIBC to older version on 16.04 and test package built using older version of the lib    --OR--    - make current script for building package compatible with 14.    Reach out to Engine and Science teams to confirm the solution we chose works for them.",Sub-task,Major,Done,"2018-10-08 20:53:38","2018-10-08 19:53:38",8
"Apache MXNet","Scala Benchmark CLIParser",,Sub-task,Major,Done,"2018-10-08 20:42:18","2018-10-08 19:42:18",1
"Apache MXNet","Adding tests in operator to improve coverage","Currently some operators do not have sufficient tests.    *Acceptance Criteria*  Increase operator function coverage to 100%",Improvement,Major,"To Do","2018-10-08 17:31:03","2018-10-08 16:31:03",11
"Apache MXNet","Logging level for C++ library",,"New Feature",Major,"To Do","2018-10-08 07:29:56","2018-10-08 06:29:56",5
"Apache MXNet","BUG in MultiBoxTargetForward when there is single box label",,Bug,Major,Done,"2018-10-08 07:28:10","2018-10-08 06:28:10",3
"Apache MXNet","Support float16 in InstanceNorm operator",,"New Feature",Major,"To Do","2018-10-08 07:12:47","2018-10-08 06:12:47",8
"Apache MXNet","Dynamic shape inference",,"New Feature",Major,"To Do","2018-10-08 07:10:16","2018-10-08 06:10:16",21
"Apache MXNet","Get True value from MXNet Scala NDArray","[https://github.com/apache/incubator-mxnet/issues/12679]    With the conversation back and forth, the committers reach to a major vote that I should implement this feature in the right way. And that is what I am going to do.         Corresponding discussion and ongoing PR: https://github.com/apache/incubator-mxnet/pull/12690",Bug,Major,Done,"2018-09-28 00:25:01","2018-09-27 23:25:01",3
"Apache MXNet","Mode Operator","Should match behavior of https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html#scipy.stats.mode","New Feature",Major,"To Do","2018-09-27 20:49:11","2018-09-27 19:49:11",8
"Apache MXNet","CI for Java API","Scope of work:  - unit tests (per PR). These are Java unit tests, not Scala unit tests.  - integ tests of examples (nightly). There is at least one integ test needed - for SSD example",Story,Major,Done,"2018-09-24 20:19:06","2018-09-24 19:19:06",5
"Apache MXNet","Extend JNDArray class to proxy whole functionality of NDArray","Scope of work: reflect all the functionality from NDArray class (Scala) in JNDArray class (Java).",Story,Major,Done,"2018-09-24 20:11:39","2018-09-24 19:11:39",5
"Apache MXNet","Implement SSD example using Java API","Implement SSD example using Java API  + integ tests  + readme.md",Story,Major,Done,"2018-09-24 20:01:09","2018-09-24 19:01:09",8
"Apache MXNet","Support Higher Order Derivative",,Epic,Major,"To Do","2018-09-20 23:00:36","2018-09-20 22:00:36",99
"Apache MXNet","[Onnx] Hardmax operator",,"New Feature",Major,"To Do","2018-09-20 22:47:36","2018-09-20 21:47:36",8
"Apache MXNet","[Onnx] Support upsampling","Need support for a different scale per dimension.  Current WA: Use BiLinearResize2D – this means that Nearest Neighbour export/import is not supported","New Feature",Major,"To Do","2018-09-20 22:47:07","2018-09-20 21:47:07",5
"Apache MXNet","Investigate roi_align operator issue",,Bug,Major,"In Progress","2018-09-20 22:43:19","2018-09-20 21:43:19",3
"Apache MXNet","Median Operator",,"New Feature",Major,"To Do","2018-09-11 18:18:13","2018-09-11 17:18:13",8
"Apache MXNet","Standard Deviation Operator",,"New Feature",Major,"To Do","2018-09-11 18:17:56","2018-09-11 17:17:56",8
"Apache MXNet","Variance Operator",,"New Feature",Blocker,"To Do","2018-09-11 18:17:14","2018-09-11 17:17:14",8
"Apache MXNet","[Feature Request] Operator for Im2Col",,"New Feature",Major,"To Do","2018-09-10 23:45:05","2018-09-10 22:45:05",8
"Apache MXNet","Support new CuDNN features in RNN Op","[https://github.com/apache/incubator-mxnet/issues/12463]    [https://github.com/apache/incubator-mxnet/issues/9543]     ",Improvement,Major,"In Progress","2018-09-10 23:17:42","2018-09-10 22:17:42",5
"Apache MXNet","Feature Request: Improve ndarray.pad to be an numpy.pad equivalent",,"New Feature",Major,"To Do","2018-09-10 23:12:24","2018-09-10 22:12:24",8
"Apache MXNet","How to resize mx.NDArray 2d tensor with interp = 'Nearest'",,"New Feature",Major,"To Do","2018-09-10 23:11:00","2018-09-10 22:11:00",3
"Apache MXNet",mx.sym.constant,,"New Feature",Major,"To Do","2018-09-10 23:09:45","2018-09-10 22:09:45",8
"Apache MXNet","Potential indexing problem of SequenceMask, SequenceLast operators",,"New Feature",Major,"To Do","2018-09-10 23:08:09","2018-09-10 22:08:09",3
"Apache MXNet","trace operator",,"New Feature",Major,"In Progress","2018-09-10 23:06:39","2018-09-10 22:06:39",5
"Apache MXNet","Infer shape error with softmaxoutput",,"New Feature",Major,Done,"2018-09-10 23:04:44","2018-09-10 22:04:44",0
"Apache MXNet","mx.nd.topk does not work with ndarray of type float16",,"New Feature",Major,"In Progress","2018-09-10 23:03:20","2018-09-10 22:03:20",5
"Apache MXNet","2d fft operation with mxnet",,"New Feature",Major,"To Do","2018-09-10 23:02:13","2018-09-10 22:02:13",8
"Apache MXNet","As a MXNet user, I would like to have the common statistical operators supported",https://github.com/apache/incubator-mxnet/issues/12319,Story,Major,"In Progress","2018-09-10 22:57:19","2018-09-10 21:57:19",32
"Apache MXNet","reshape_like does not support different input types",,"New Feature",Major,"To Do","2018-09-10 22:56:53","2018-09-10 21:56:53",8
"Apache MXNet","Uniform sampling support for integers",,"New Feature",Major,"To Do","2018-09-10 22:48:46","2018-09-10 21:48:46",5
"Apache MXNet","Sparse support for SVD",,"New Feature",Major,"To Do","2018-09-10 22:47:28","2018-09-10 21:47:28",8
"Apache MXNet","Softmax Operator to take in Sparse Matrix",,"New Feature",Major,"To Do","2018-09-10 22:46:59","2018-09-10 21:46:59",8
"Apache MXNet","Fix the bug for matrices of multiple dimension, with one dimension much larger ",,Bug,Major,Done,"2018-09-10 21:50:07","2018-09-10 20:50:07",3
"Apache MXNet","Support Shape MXNet interface",,Story,Major,Done,"2018-09-10 19:49:17","2018-09-10 18:49:17",8
"Apache MXNet","Support Context MXNet interface",,Story,Major,Done,"2018-09-10 19:49:00","2018-09-10 18:49:00",8
"Apache MXNet","Support DataDesc MXNet interface",,Story,Major,Done,"2018-09-10 19:48:45","2018-09-10 18:48:45",8
"Apache MXNet","Support NDArray MXNet interface (no default values are supported)",,Story,Major,Done,"2018-09-10 19:48:21","2018-09-10 18:48:21",13
"Apache MXNet","Port the ctc_loss operator from contrib to operators and refactor the implementation","*Acceptance Criteria*  - Make ctc_loss operator a part of the regular operators  - Refactor the implementation so it uses the new operator interface  - Add unit test in test_operator.py to increase coverage.","New Feature",Major,Done,"2018-09-10 17:58:54","2018-09-10 16:58:54",2
"Apache MXNet","Test and Publish 1.3rc Scala package to Maven Staging",,Task,Major,Done,"2018-09-05 01:00:21","2018-09-05 00:00:21",3
"Apache MXNet","Maven publish - insert the keys and update the gnupg-agent cachettl",,Sub-task,Major,Done,"2018-08-28 23:19:46","2018-08-28 22:19:46",1
"Apache MXNet","Maven publish - Encrypt the keys and create .conf file","Key encryption",Sub-task,Major,Done,"2018-08-28 23:17:14","2018-08-28 22:17:14",1
"Apache MXNet","Build the LINUX GPU binary for Scala",,Sub-task,Major,Done,"2018-08-28 22:30:37","2018-08-28 21:30:37",13
"Apache MXNet","Pooling1D with same padding","Hi,     I need to implement an encoder for a speech recognition model in MXNet that uses a 1D temporal max pooling layer with 'same' padding between successive Bidirectional LSTM layers (as below). Currently, there is no support for 1D max pooling with same padding in MXNet - https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symb... .     Could you please implement the required max pooling with 'same' padding support and advise on how to implement the following encoder model in MXNet?     Thanks,   Sundeep     ===   # network   target = classes   EncKeyTotalDim = 1024   AttNumHeads = 1   EncKeyPerHeadDim = EncKeyTotalDim // AttNumHeads   EncValueTotalDim = 2048   EncValuePerHeadDim = EncValueTotalDim // AttNumHeads   LstmDim = EncValueTotalDim // 2     network = {   source: {class: eval, eval: tf.clip_by_value(source(0), -3.0, 3.0)},     lstm0_fw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: 1, from: [source] },   lstm0_bw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: -1, from: [source] },   lstm0_pool: {class: pool, mode: max, padding: same, pool_size: (2,), from: [lstm0_fw, lstm0_bw], trainable: False},     lstm1_fw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: 1, from: [lstm0_pool], dropout: 0.3 },   lstm1_bw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: -1, from: [lstm0_pool], dropout: 0.3 },   lstm1_pool: {class: pool, mode: max, padding: same, pool_size: (2,), from: [lstm1_fw, lstm1_bw], trainable: False},     lstm2_fw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: 1, from: [lstm1_pool], dropout: 0.3 },   lstm2_bw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: -1, from: [lstm1_pool], dropout: 0.3 },   lstm2_pool: {class: pool, mode: max, padding: same, pool_size: (2,), from: [lstm2_fw, lstm2_bw], trainable: False},     lstm3_fw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: 1, from: [lstm2_pool], dropout: 0.3 },   lstm3_bw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: -1, from: [lstm2_pool], dropout: 0.3 },   lstm3_pool: {class: pool, mode: max, padding: same, pool_size: (1,), from: [lstm3_fw, lstm3_bw], trainable: False},     lstm4_fw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: 1, from: [lstm3_pool], dropout: 0.3 },   lstm4_bw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: -1, from: [lstm3_pool], dropout: 0.3 },   lstm4_pool: {class: pool, mode: max, padding: same, pool_size: (1,), from: [lstm4_fw, lstm4_bw], trainable: False},     lstm5_fw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: 1, from: [lstm4_pool], dropout: 0.3 },   lstm5_bw : { class: rec, unit: nativelstm2, n_out : LstmDim, direction: -1, from: [lstm4_pool], dropout: 0.3 },     encoder: {class: copy, from: [lstm5_fw, lstm5_bw]}, # dim: EncValueTotalDim   ===   ","New Feature",Major,Done,"2018-08-28 19:14:49","2018-08-28 18:14:49",1
"Apache MXNet","Document MXNet engine architecture guide","*Acceptance Criteria*  ",Task,Major,Done,"2018-08-27 23:25:12","2018-08-27 22:25:12",2
"Apache MXNet","InferShape return false not caught in Symbolic mode",,Bug,Major,"To Do","2018-08-27 21:22:13","2018-08-27 20:22:13",3
"Apache MXNet","create basic build pipeline on CI","After the binary build, we need to automated this process and publish it to the CI.   * A general build process on Docker image   * Tested on CI",Sub-task,Major,Done,"2018-08-27 05:57:11","2018-08-27 04:57:11",5
"Apache MXNet","Build binary with Linux CPU","Build all dependencies and wrap them up into a single binary built in MXNet",Sub-task,Major,Done,"2018-08-27 05:55:05","2018-08-27 04:55:05",5
"Apache MXNet","Revamp existing Java API design",,Story,Major,Done,"2018-08-13 20:30:07","2018-08-13 19:30:07",13
"Apache MXNet","how to convert parameters dtype from float32 to float64 in gluon?",,Bug,Major,Done,"2018-08-13 17:55:29","2018-08-13 16:55:29",2
"Apache MXNet","different behaviour of customop in latest MXNet",,Bug,Major,Done,"2018-08-13 17:52:27","2018-08-13 16:52:27",5
"Apache MXNet","test_arange failure - Jetson TX2 (CPU)",,Bug,Major,Done,"2018-08-13 17:51:45","2018-08-13 16:51:45",1
"Apache MXNet","src/operator/./bilinear_sampler-inl.h:105: Have not implemented the data req combinations! gdata_req=0 ggrid_req=1",,Bug,Major,Done,"2018-08-13 17:51:13","2018-08-13 16:51:13",2
"Apache MXNet","Why does a tanh activation layer generates values greater than 1?",,Bug,Major,Done,"2018-08-13 17:50:43","2018-08-13 16:50:43",1
"Apache MXNet","Some mxnet ctc_loss bug",,Improvement,Major,Done,"2018-08-13 17:48:09","2018-08-13 16:48:09",5
"Apache MXNet","Undefined Behavior of mx.sym.where with shape-mismatched cond",,Bug,Major,Done,"2018-08-13 17:47:39","2018-08-13 16:47:39",2
"Apache MXNet","mx.nd.topk does not work with ndarray of type float16",,Bug,Major,Done,"2018-08-13 17:47:11","2018-08-13 16:47:11",5
"Apache MXNet","nd.softmax() doesn't support grad_req='add'",,Bug,Major,Done,"2018-08-13 17:45:54","2018-08-13 16:45:54",5
"Apache MXNet","Inconsistent / wrong output from sum",,Bug,Major,Done,"2018-08-13 17:45:16","2018-08-13 16:45:16",1
"Apache MXNet","Autograd fails when using `take` operator repeatedly",,Bug,Major,Done,"2018-08-13 17:44:16","2018-08-13 16:44:16",1
"Apache MXNet","non-deterministic backward of scatter_nd",,Bug,Major,Done,"2018-08-13 17:43:10","2018-08-13 16:43:10",2
"Apache MXNet","Gradient function not returning enough gradient",,Bug,Major,Done,"2018-08-10 00:34:55","2018-08-09 23:34:55",5
"Apache MXNet","Dangling outputs and dtype != float32: Gradient computation fails",,Bug,Major,Done,"2018-08-10 00:32:59","2018-08-09 23:32:59",1
"Apache MXNet","Prelu activation compution fault in expand_shape function",,Bug,Major,Done,"2018-08-10 00:24:03","2018-08-09 23:24:03",5
"Apache MXNet","Dropout may mask values even when ratio=0.0",,Bug,Major,Done,"2018-08-08 17:55:59","2018-08-08 16:55:59",1
"Apache MXNet","Create Plan for automated publish","Plan for the publish. Steps and man needed.         https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=89066928&draftShareId=c4faae05-41f6-4850-b923-53a396d8c921&src=shareui&src.shareui.timestamp=1534187046680",Sub-task,Major,Done,"2018-08-04 00:18:25","2018-08-03 23:18:25",3
"Apache MXNet","Summarize Scala issues and update CWiki",https://cwiki.apache.org/confluence/display/MXNET/Scala+Project+Status,Sub-task,Minor,Done,"2018-07-30 22:35:50","2018-07-30 21:35:50",2
"Apache MXNet",https://github.com/apache/incubator-mxnet/issues/11797,,Sub-task,Major,Done,"2018-07-26 21:48:02","2018-07-26 20:48:02",1
"Apache MXNet","MLP example failure -- update MLP example",https://github.com/apache/incubator-mxnet/issues/10753,Sub-task,Major,Done,"2018-07-26 21:47:04","2018-07-26 20:47:04",13
"Apache MXNet","Scala Module API resize is leaking memory on the native size. (#10867)",,Sub-task,Major,"To Do","2018-07-26 21:46:49","2018-07-26 20:46:49",13
"Apache MXNet","Run Scala API integration tests in nightly CI pipeline","As integration tests for Scala API were removed from per PR CI pipeline (MXNET-729), they need to be added into nightly CI pipeline to ensure they are still running, just less often than on every PR.",Sub-task,Major,"To Do","2018-07-26 02:15:38","2018-07-26 01:15:38",2
"Apache MXNet","Exclude Scala API integration tests from per PR CI runs","In order to reduce time necessary to complete CI runs - remove Scala API integration tests from per PR CI pipeline. Only unit tests should be run in per PR CI pipeline.",Sub-task,Major,"To Do","2018-07-26 02:10:36","2018-07-26 01:10:36",2
"Apache MXNet","Create scripts for generation of different flavors of MXNet static lib","Success criteria: have CI scripts ready that once run would generate static .a file with MXNet dependencies (OpenCV, OpenBLAS, MKLDNN, CUDA, CUDNN, etc.). Each script should produce its own flavor of lib.a: for MacOS X (CPU), Linux (CPU) and Linux (GPU).",Sub-task,Major,Done,"2018-07-26 01:47:54","2018-07-26 00:47:54",5
"Apache MXNet","Conduct discussion on dev@ list","Conduct discussion on dev@ list (subject: [DISCUSS]: Proposal to improve Scala API release process) about design. Success criteria for this story is to have agreed and approved design proposal for automating build and artifact publishing.",Sub-task,Major,Done,"2018-07-26 01:43:59","2018-07-26 00:43:59",5
"Apache MXNet","Create pipeline for running Scala API benchmark tests and publish results","Make sure to have a discussion on dev@ list as to where results should be published to. One suggestion is that it could be published into CloudWatch and then exposed on mxnet.io site via CloudWatch widget.",Sub-task,Major,Done,"2018-07-26 01:19:30","2018-07-26 00:19:30",2
"Apache MXNet","Benchmark for Inference API (single and batch)","This requires single and batch inference time for the image classification. Please provide   * script to download the required files   * script to run the file",Sub-task,Major,Done,"2018-07-26 01:10:49","2018-07-26 00:10:49",5
"Apache MXNet","Update the PyPI package of mxnet-to-coreml","See github issue: https://github.com/apache/incubator-mxnet/issues/10349",Task,Major,Done,"2018-07-24 17:57:47","2018-07-24 16:57:47",3
"Apache MXNet","Add mxnet-to-coreml converter tests to CI","*Acceptance Criteria*  Add coreml package in CI, just as the caffe2 package",Task,Major,Done,"2018-07-24 17:55:26","2018-07-24 16:55:26",3
"Apache MXNet","Fix all the unit tests under mxnet-to-coreml package","Currently all the 56 tests fail due to out-of-sync with MXNet release    *Acceptance Criteria*  Make all the tests pass    ",Task,Major,Done,"2018-07-24 17:54:08","2018-07-24 16:54:08",2
"Apache MXNet","Fix all the unit tests under mxnet-to-coreml package","Currently all the 56 tests fail due to out-of-sync with MXNet release    *Acceptance Criteria*  Make all the tests pass",Sub-task,Major,"In Review","2018-07-24 17:52:44","2018-07-24 16:52:44",2
"Apache MXNet","As a developer, I would like to keep mxnet-to-coreml package in sync with MXNet release",,Story,Major,Done,"2018-07-24 17:51:07","2018-07-24 16:51:07",5
"Apache MXNet","quantization Layers seg fault instead of displaying proper error message","[https://github.com/apache/incubator-mxnet/issues/11846]    quantized FC, pooling and convolution layers on native cpu w/o mkl throws segfault, instead of displaying error and graceful exit.",Bug,Major,"To Do","2018-07-24 00:56:46","2018-07-23 23:56:46",2
"Apache MXNet",CI-Integration,"Write a script to chain together stages that will be run on CI",Sub-task,Major,"In Review","2018-07-09 18:03:57","2018-07-09 17:03:57",3
"Apache MXNet","Implement Diff Collator",,Sub-task,Major,Done,"2018-07-09 17:59:56","2018-07-09 16:59:56",2
"Apache MXNet","Implement Test Selector","Write script to select which tests to check for flakiness given a list of changes to code",Sub-task,Major,Done,"2018-07-09 17:57:56","2018-07-09 16:57:56",2
"Apache MXNet","Implement Flakiness Checker","Write script to check a given test for flakiness",Sub-task,Major,Done,"2018-07-09 17:53:19","2018-07-09 16:53:19",1
"Apache MXNet","As a developer, I would like to set up the dev and debug environment for MXNet","*Acceptance Criteria*  A markdown doc in Apache MXNet cwiki    ",Story,Major,Done,"2018-06-28 22:25:10","2018-06-28 21:25:10",6
"Apache MXNet","As a user, I would like to use a simple fit() method to train my model in Gluon","*Acceptance Criteria*  Implementing a simple model.fit to remove the need to write the manual Gluon fit function which is mostly boilerplate",Story,Major,"To Do","2018-06-26 22:20:11","2018-06-26 21:20:11",13
"Apache MXNet","As a user, I would like to be able to use Numpy arrays in Gluon","*Acceptance Criteria*  Adding built-in support for numpy arrays in Gluon (converting to NDArray under the hood)",Story,Major,"To Do","2018-06-26 22:19:02","2018-06-26 21:19:02",13
"Apache MXNet","As a user, I would like to call data() on parameters initialized on multiple GPUs","*Acceptance Criteria*  .data() should return the NDArray even with multi-gpu context. ",Story,Major,"To Do","2018-06-26 22:15:54","2018-06-26 21:15:54",5
"Apache MXNet","Flaky Segfault test_autograd.test_unary_func",,Sub-task,Major,Done,"2018-06-26 00:39:36","2018-06-25 23:39:36",1
"Apache MXNet","Flaky test_loss.test_ctc_loss_train",,Sub-task,Major,Done,"2018-06-26 00:39:03","2018-06-25 23:39:03",1
"Apache MXNet","Flaky test_autograd.test_unary_func",,Sub-task,Major,Done,"2018-06-26 00:37:51","2018-06-25 23:37:51",1
"Apache MXNet","Flaky Test test_random.test_exponential_generator",,Sub-task,Major,Done,"2018-06-26 00:37:29","2018-06-25 23:37:29",1
"Apache MXNet","Improve Gluon Performance and memory conumption",,Improvement,Major,"To Do","2018-06-25 23:39:42","2018-06-25 22:39:42",13
"Apache MXNet","As a user, I would like to know how to do transfer learning using Gluon",,Story,Major,"To Do","2018-06-25 23:39:14","2018-06-25 22:39:14",21
"Apache MXNet","As a developer, I would like to survey other competitors on the usability and functinoality","*Acceptance Criteria*  Summarizing the missing functionalities of Gluon in the following categories:  - Operators list -> Driven by Models supported.  - Loss functions  - Optimizers  - Usability - End to End user experience  - Pre-processing and utils before training the model  - Post-processing and utils after training the model",Story,Major,"To Do","2018-06-25 23:37:31","2018-06-25 22:37:31",13
"Apache MXNet","As a user, I would like to have a cheatsheet to use Gluon quickly","*Acceptance Criteria*  Running document with issue/workaround. We will seed it initially with Q/A from discuss forum",Story,Major,Done,"2018-06-25 23:35:07","2018-06-25 22:35:07",5
"Apache MXNet","As a user, I would like to have a tutorial to use Custom Loss and Custom Callback in Gluon","*Acceptance Criteria*   - Tutorial to use Custom Loss and Custom Callback in Gluon.     - Identify any gaps for the development.",Story,Major,"To Do","2018-06-25 23:28:54","2018-06-25 22:28:54",5
"Apache MXNet","As a user, I would like to know how to implement SN-GAN in gluon",,Story,Major,Done,"2018-06-25 23:27:09","2018-06-25 22:27:09",8
"Apache MXNet","Gluon raises error if the user does not call nd.waitall()",,Bug,Major,Done,"2018-06-25 23:26:30","2018-06-25 22:26:30",5
"Apache MXNet","3D dilation support not working",,Bug,Major,"To Do","2018-06-25 23:26:15","2018-06-25 22:26:15",8
"Apache MXNet","Crash while running gluon image-classification.py example with float16",,Bug,Major,Done,"2018-06-25 23:23:00","2018-06-25 22:23:00",5
"Apache MXNet","Symbolic .json file not compatible with .params file generated since MXNet 1.2",,Bug,Major,Done,"2018-06-25 23:22:50","2018-06-25 22:22:50",5
"Apache MXNet","As a user, I would like to extend the DataIter to a video reader",,Story,Major,"To Do","2018-06-25 23:21:30","2018-06-25 22:21:30",5
"Apache MXNet","gradcheck for Gluon and NDArray",,"New Feature",Major,"To Do","2018-06-25 23:20:13","2018-06-25 22:20:13",8
"Apache MXNet","Add MaxUppooling in Gluon",,"New Feature",Major,"To Do","2018-06-25 23:18:52","2018-06-25 22:18:52",8
"Apache MXNet","As a user, I would like SoftmaxCrossEntropyLoss operator to have ignore labels option",,Story,Major,"To Do","2018-06-25 23:17:24","2018-06-25 22:17:24",8
"Apache MXNet","As a user, I would like to have an arg_scope function in tf.slim","what if I want to modify the batch normalization behavior in the vision model zoo.    an arg_scope function in tf.slim will help a lot.",Story,Major,"To Do","2018-06-25 23:15:44","2018-06-25 22:15:44",5
"Apache MXNet","As a user, I would like to be able to initialize parameter of SymbolBlock","It looks like SymbolBlock will initializing all the parameters using the initializer given in the block.initialize() function, including bias, which cannot be initialized by Xavier.    Alought that [post|https://discuss.mxnet.io/t/initializing-parameters-of-symbolblock/1213] suggests a solution, and in addition, I think we should try to initialize the parameters if it was given in the symbol/json file.    e.g.  {{bias = mx.sym.Variable(name='conv_bias', shape=(num_filter,), init=mx.init.Zero())}}  or    {{{ op: null, name: conv_bias, attrs: \{ __init__: [\zero\, {}], __shape__: (64,) }, inputs: [] }}}",Story,Major,"To Do","2018-06-25 23:12:03","2018-06-25 22:12:03",5
"Apache MXNet","Add README for tests",,Improvement,Minor,Done,"2018-06-25 19:54:24","2018-06-25 18:54:24",0.5
"Apache MXNet","Flaky test_operator_gpu.test_sparse_dot",,Sub-task,Major,Done,"2018-06-25 19:07:41","2018-06-25 18:07:41",1
"Apache MXNet","subgraph storage type inference for cachedOp",,Sub-task,Major,Done,"2018-06-17 22:46:40","2018-06-17 21:46:40",1
"Apache MXNet","Create unit test for CreateMKLDNNMem/CommitOutput",,Sub-task,Major,Done,"2018-06-14 21:38:30","2018-06-14 20:38:30",1
"Apache MXNet","Unit test for MKLDNNSum helper util",,Sub-task,Minor,Done,"2018-06-14 06:02:42","2018-06-14 05:02:42",1
"Apache MXNet","Fix MKLDNN performance regression",https://github.com/apache/incubator-mxnet/pull/11129,Sub-task,Minor,Done,"2018-06-13 19:05:48","2018-06-13 18:05:48",2
"Apache MXNet","add_n(dense, csr, dense) and add_n([dense, csr, rsp]*, dense, [dense, csr, rsp]*) on CPU & GPU",,Sub-task,Major,Done,"2018-06-12 17:46:33","2018-06-12 16:46:33",3
"Apache MXNet","Make cmake default build ",https://github.com/apache/incubator-mxnet/issues/8702,Bug,Minor,"To Do","2018-06-05 19:18:20","2018-06-05 18:18:20",4
"Apache MXNet","[Gluon-NLP] Vocabulary for GBW dataset",,Sub-task,Major,"In Review","2018-06-04 18:19:33","2018-06-04 17:19:33",1
"Apache MXNet","API documentation for distributed sparse gluon block",,Sub-task,Major,"To Do","2018-06-04 18:14:47","2018-06-04 17:14:47",3
"Apache MXNet",gluon.utils.clip_global_norm(row_sparse),,Sub-task,Major,Done,"2018-06-04 18:13:33","2018-06-04 17:13:33",1
"Apache MXNet","Plan the steps need to be taken to implement broadcast_to in MXNet similar to Pytorch",,Sub-task,Major,"To Do","2018-06-04 18:11:01","2018-06-04 17:11:01",2
"Apache MXNet","Understand the difference between broadcast_to of mxnet and broadcast_to of pytorch",,Sub-task,Major,"To Do","2018-06-04 18:09:57","2018-06-04 17:09:57",2
"Apache MXNet","Build aten library for Pytorch",,Sub-task,Major,Done,"2018-06-04 18:03:40","2018-06-04 17:03:40",1
"Apache MXNet","Look at Pytorch Implementation of broadcast",,Sub-task,Major,Done,"2018-06-04 18:03:17","2018-06-04 17:03:17",2
"Apache MXNet","Test backward pass operations",,Sub-task,Major,Done,"2018-05-29 20:21:07","2018-05-29 19:21:07",3
"Apache MXNet","Add kAddTo request type to C++ unit test framework","We currently only test the kWriteTo and kWriteInplace requests.",Sub-task,Minor,Done,"2018-05-29 20:20:20","2018-05-29 19:20:20",4
"Apache MXNet","[Gluon-NLP] Streaming data iterator for GBW dataset",,Sub-task,Major,Done,"2018-05-29 18:21:56","2018-05-29 17:21:56",0
"Apache MXNet","Add C++ unit tests for concat operator",,Sub-task,Minor,Done,"2018-05-27 07:15:14","2018-05-27 06:15:14",3
"Apache MXNet","Add C++ unit tests for convolutions operator",,Sub-task,Minor,"In Review","2018-05-27 07:14:56","2018-05-27 06:14:56",2
"Apache MXNet","Fix broken cpp engine unit test",,Bug,Major,Done,"2018-05-25 19:38:44","2018-05-25 18:38:44",3
"Apache MXNet","Test Story Ticket",,Story,Major,Done,"2018-05-25 01:05:02","2018-05-25 00:05:02",3
"Apache MXNet","test epic","epic success",Epic,Major,Done,"2018-05-25 00:51:00","2018-05-24 23:51:00",2
"Apache MXNet","test story issue",ignore,Story,Major,Done,"2018-05-25 00:27:04","2018-05-24 23:27:04",1
"Apache MXNet","Fix data type switch in elemwise_add between dense and rsp",,Bug,Major,Done,"2018-05-25 00:23:44","2018-05-24 23:23:44",1
"Apache MXNet","Flaky test_operator_gpu.test_sgd",https://github.com/apache/incubator-mxnet/issues/10945,Sub-task,Major,Done,"2018-05-21 22:20:12","2018-05-21 21:20:12",1
"Apache MXNet","Flaky test_operator_gpu.test_countsketch",https://github.com/apache/incubator-mxnet/issues/10988,Sub-task,Major,Done,"2018-05-21 22:17:27","2018-05-21 21:17:27",1
"Apache MXNet","Port GBW language model from symbol to Gluon for NLP toolkit",,Sub-task,Major,"To Do","2018-05-21 18:27:08","2018-05-21 17:27:08",5
"Apache MXNet","Sparse sampled logits block for NCE/Importance Sampling",,Sub-task,Major,"In Review","2018-05-21 18:15:56","2018-05-21 17:15:56",2
"Apache MXNet","Distributed sparse embedding block",,Sub-task,Major,Done,"2018-05-21 17:43:56","2018-05-21 16:43:56",0
"Apache MXNet"," broadcast_mul/div between csr and 1D dense vector on GPU",,Sub-task,Major,Done,"2018-05-14 19:40:59","2018-05-14 18:40:59",1
"Apache MXNet","distributed sparse block interface",,Sub-task,Major,Done,"2018-05-14 19:35:51","2018-05-14 18:35:51",0
"Apache MXNet","elemwise_add/sub between rsp and rsp on GPU",,Sub-task,Major,Done,"2018-05-07 19:38:28","2018-05-07 18:38:28",3
"Apache MXNet","elemwise_mul between dense and csr on CPU & GPU",,Sub-task,Major,Done,"2018-05-04 05:22:19","2018-05-04 04:22:19",3
"Apache MXNet","elemwise_add/sub between csr and csr on GPU",,"New Feature",Major,"To Do","2018-05-04 05:21:38","2018-05-04 04:21:38",3
"Apache MXNet","Reshape Operator design change","As per this - [https://github.com/apache/incubator-mxnet/issues/10789] ",Improvement,Minor,"To Do","2018-05-03 06:28:29","2018-05-03 05:28:29",5
"Apache MXNet","unary for csr",,Sub-task,Major,Done,"2018-05-03 05:56:43","2018-05-03 04:56:43",2
"Apache MXNet","sum/mean of CSR matrices on GPU",,"New Feature",Major,"To Do","2018-05-01 18:37:14","2018-05-01 17:37:14",2
"Apache MXNet","Size and Shape Operator",,Sub-task,Major,Done,"2018-05-01 18:31:50","2018-05-01 17:31:50",1
"Apache MXNet","Take Operator Enhancement",,Sub-task,Major,Done,"2018-05-01 18:30:12","2018-05-01 17:30:12",3
"Apache MXNet","Make use of pooling_convention parameter in AvgPool Operator",,Sub-task,Major,Done,"2018-05-01 18:29:57","2018-05-01 17:29:57",2
"Apache MXNet","L1 Normalization / Reduce L1 Operator",,Sub-task,Major,Done,"2018-05-01 18:27:45","2018-05-01 17:27:45",2
"Apache MXNet","SpaceToDepth and DepthToSpace Operator",,Sub-task,Major,Done,"2018-05-01 18:27:26","2018-05-01 17:27:26",3
"Apache MXNet",Hardmax,,Sub-task,Major,"To Do","2018-05-01 18:24:14","2018-05-01 17:24:14",1
"Apache MXNet","[Memory] investigate inplace BatchNorm",[https://discuss.gluon.ai/t/topic/1377] ,Improvement,Major,Done,"2018-04-30 19:30:08","2018-04-30 18:30:08",3
"Apache MXNet","Histogram Operator",,"New Feature",Major,Done,"2018-04-21 04:29:34","2018-04-21 03:29:34",5
"Apache MXNet","Benchmark integration design",,Sub-task,Major,Done,"2018-03-29 18:56:27","2018-03-29 17:56:27",3
"Apache MXNet","Benchmark Script for Large Scale inference","Do large Scale inference for scala api which means run a long time and monitor the memory usage.     - Script to run",Sub-task,Major,Done,"2018-03-29 18:55:55","2018-03-29 17:55:55",2
"Apache MXNet","update on setting up Scala with MXNet and the IntelliJ IDE",https://github.com/apache/incubator-mxnet/pull/10013,Improvement,Major,Done,"2018-03-07 18:03:12","2018-03-07 18:03:12",1
