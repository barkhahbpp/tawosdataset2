Name,Title,Description,Type,Priority,Status,Creation_Date,Estimation_Date,Story_Point
"Hyperledger Indy Node","Migrate indy-node CD pipeline from Jenkins",,Task,Medium,New,"2020-03-27 12:49:37","2020-03-27 12:49:37",5
"Hyperledger Indy Node","Migrate indy-plenum CD pipeline from Jenkins",,Task,Medium,New,"2020-03-27 12:48:33","2020-03-27 12:48:33",5
"Hyperledger Indy Node","Move indy-node CI pipeline to GitHub Actions",,Task,Medium,New,"2020-03-27 08:31:25","2020-03-27 08:31:25",3
"Hyperledger Indy Node","Move indy-plenum CI pipeline to GitHub Actions",,Task,Medium,New,"2020-03-27 08:31:10","2020-03-27 08:31:10",8
"Hyperledger Indy Node","Release 1.13.0",,Task,Medium,New,"2020-03-13 12:30:04","2020-03-13 12:30:04",2
"Hyperledger Indy Node","Rich Schema Design: process feedback","Process feedback for HIPEs and RFCs created in the scope of INDY-2337 and INDY-2347.",Task,Medium,Complete,"2020-03-13 12:28:08","2020-03-13 12:28:08",2
"Hyperledger Indy Node","Future Ledger options: discuss and continue",,Task,Medium,Complete,"2020-03-13 12:26:27","2020-03-13 12:26:27",5
"Hyperledger Indy Node","Research Stellar: Part 2","*Acceptance criteria:*   * whether it's possible to implement Indy txns in Stellar   * whether it's possible to support light clients in Stellar",Task,Medium,Complete,"2020-02-28 15:28:29","2020-02-28 15:28:29",5
"Hyperledger Indy Node","Improvements in Rich Schema common code","*Acceptance criteria:*   * {color:#000000}Make sure that @id == id{color}   * {color:#000000}json_ld validation for objects that must be json_lds{color} (presence of @id, @type)   ** Schema   ** Mapping   ** Presentation Definition     ",Task,Medium,Complete,"2020-02-28 02:06:01","2020-02-28 02:06:01",1
"Hyperledger Indy Node","Blog Post March","Draft at least one technical blog post. Possible options:  * Troubleshooting Indy  * Considering consensus protocols  * Why Rich Schemas will be awesome.",Task,Medium,Complete,"2020-02-27 22:54:44","2020-02-27 22:54:44",2
"Hyperledger Indy Node","Design W3C Presentation","*Acceptance criteria*   * Write Indy HIPE and Aries RFC for W3C presentation format applying to Indy/Aries",Task,Medium,New,"2020-02-26 22:58:22","2020-02-26 22:58:22",2
"Hyperledger Indy Node","Design W3C Credential","*Acceptance criteria*   * Write Indy HIPE and Aries RFC for W3C credential format applying to Indy/Aries",Task,Medium,Complete,"2020-02-26 22:56:06","2020-02-26 22:56:06",2
"Hyperledger Indy Node","Design Presention Definition","*Acceptance criteria*   - Create Indy HIPE and Aries RFC with examples and description of   Presentation Definition object",Task,Medium,New,"2020-02-26 22:47:31","2020-02-26 22:47:31",2
"Hyperledger Indy Node","Simplify use of ZMQ connection client","*Acceptance criteria:*  - The client should be a fat binary    - consider writing in Rust with dependencies statically linked    - provide manually built packages for Ubuntu18.04 and Windows 10  - The client binary needs to output whether there is a ZMQ connection   - Client should work against a given Validator Node (through the client stack)    *Notes*  * Automated CI / CD is not required. Manual builds from a developer machine is sufficient.  ** Builds for Centos 8 would be nice, but are not required.  ** We do not expect to need Mac builds.  * We do not need a fat binary for the server-side tool.  ** If we need additional work on the server-side tool, it will be to have the docker file setup the service.  * It is optional to work against the server-side script in addition to the validator node.  ",Task,Medium,Complete,"2020-02-26 22:44:35","2020-02-26 22:44:35",3
"Hyperledger Indy Node","Write a proposal of options","Acceptance criteria:  * List of all options  * Assumptions  * Cons and pros  * Rough WBS  * Rough estimate",Task,Medium,Complete,"2020-02-17 07:52:39","2020-02-17 07:52:39",5
"Hyperledger Indy Node","Investigate IBC and Cosmos","Acceptance criteria:  * Research IBC protocol and implementation  * Check whether there are any issues there  * Check if implementation exists and how stable/mature is it  * Check if it can be applied to our use cases  * Google doc with report should be the output of the ticket",Task,Medium,Complete,"2020-02-17 07:50:17","2020-02-17 07:50:17",3
"Hyperledger Indy Node","Implement Credential Definition specific","Implement Encoding-specific validation logic which is not covered by INDY-2338.   - Static Validation: Make sure that all required fields are present (see [CredDef HIPE|https://github.com/hyperledger/indy-hipe/blob/30d1155090170f9d17844d8fbdb1b8df7b871687/text/0156-rich-schema-cred-def/README.md]  - Dynamic validation: CredDef points to an existing Schema on the current Ledger  - Dynamic validation: CredDef points to an existing Mapping on the current Ledger    Note: we may disable or modify the dynamic validation rules if we are going to expect pointing to Schema or Mapping from other Indy ledgers or from other types of ledgers (compatible to Aries). ",Task,Medium,Complete,"2020-01-29 16:00:00","2020-01-29 16:00:00",1
"Hyperledger Indy Node","Implement Mapping specific","Implement Mapping-specific validation logic which is not covered by INDY-2338.    - Static Validation: Make sure that all required fields are present (see [Mapping HIPE|https://github.com/hyperledger/indy-hipe/blob/30d1155090170f9d17844d8fbdb1b8df7b871687/text/0155-rich-schema-mapping/README.md]  - Dynamic validation: points to an existing Encoding    Note: we may disable or modify the dynamic validation rules if we are going to expect pointing to Encodings from other Indy ledgers or from other types of ledgers (compatible to Aries). ",Task,Medium,Complete,"2020-01-29 15:58:43","2020-01-29 15:58:43",1
"Hyperledger Indy Node","Implement Encoding specific","Implement Encoding-specific validation logic which is not covered by INDY-2338.    - Static Validation: Make sure that all required fields are present (see [Encoding HIPE|https://github.com/hyperledger/indy-hipe/blob/30d1155090170f9d17844d8fbdb1b8df7b871687/text/0154-rich-schema-encoding/README.md]  ",Task,Medium,Complete,"2020-01-29 15:58:25","2020-01-29 15:58:25",1
"Hyperledger Indy Node","Common implementation of all Rich Schema objects","According to https://github.com/hyperledger/indy-hipe/pull/153, it should be possible to implement all Rich Schema objects in a generic way.  It may exclude some specific validation dependent on object type, but should be enough to continue efficient implementation on the client side.     *Acceptance criteria:*  - create common code for dealing with all rich schema transactions as described in https://github.com/hyperledger/indy-hipe/pull/153  - specific object-related validation may not be implemented on this phase  - re-factor existing Rich Schema code to match the generic approach",Task,Medium,Complete,"2020-01-29 15:57:53","2020-01-29 15:57:53",5
"Hyperledger Indy Node","Design Schema, Context, Encoding, Mapping, CredDef","Make sure there are examples of all Rich Schema objects.   *Acceptance criteria*   - Update existing Indy HIPEs and Aries RFCs   ** Mapping and CredDefs use a single Schema only   ** Update Schema, Context and Encoding HIPEs/RFCs to match the Common format of rich schema objects     - Create Indy HIPEs and Aries RFCs for the following objects:   -- Mapping   -- Credential Definition",Task,Medium,Complete,"2020-01-29 15:45:44","2020-01-29 15:45:44",3
"Hyperledger Indy Node","Design identification and relationship between Rich Schema objects","Write a HIPE and RFC answering the questions from https://github.com/hyperledger/aries-rfcs/issues/398",Task,Medium,Complete,"2020-01-29 15:35:35","2020-01-29 15:35:35",5
"Hyperledger Indy Node","Node troubleshooting checklist","We need documentation to help other people to maintain an Indy network.    *Acceptance Criteria*  Best practices for troubleshooting a node are documented.  * What logs are useful  * What tools exist for analyzing a node",Task,Medium,Complete,"2020-01-17 15:08:44","2020-01-17 15:08:44",3
"Hyperledger Indy Node","Create tool to test ZMQ connectivity to Validators","Create a simple binary tool that we can put on an Ubuntu VM that will tell us if zmq traffic is being allowed to pass to a destination. More specifically, this would be for an administrator who is making a new agent or diagnosing an existing one, to be able to tell if a firewall is preventing contact with Validators due to deep-packet-inspection (DPI) blocking ZMQ traffic. By having control of both sides of the connection, we can pair the tool with other network diagnostic techniques to identify the source of connection errors.    *Acceptance Criteria*  * Command line tool exists that can be run to test that ZMQ traffic can be passed.  * The tool is run on both sides of a connection (a machine on the client network, and the support representative's machine).  * Network traffic generated by the tool mimics Indy Node traffic in order to diagnose behavior of DPI firewalls.  * Command line tool runs in Ubuntu 18.04.  * Installation and execution should be as simple as possible, ideally just copying the binary over and executing it from the command line. (No dependencies would be ideal.)    *Notes*  * It would be undesirable to have this baked into the indy-node library, since the diagnostic will be of use to agents as well as to new validators.",Task,Medium,Complete,"2020-01-14 21:48:29","2020-01-14 21:48:29",3
"Hyperledger Indy Node","Allow view change triggering on node count changed in sim tests","Acceptance criteria:  - Move logic of view change triggering from node.py to service- and bus-based places so that it can be triggered from sim tests  - Make sure https://github.com/hyperledger/indy-plenum/pull/1455 is merged and passes  - [Optionally] Re-factor PoolManager to get rid of multiple places for node reg  ",Task,Medium,Complete,"2020-01-17 13:32:31","2020-01-17 13:32:31",3
"Hyperledger Indy Node","Formalize the architectural plan for Rich Schemas","We need to formalize the architecture plan for Rich Schemas so that we understand how to partition the work so that additional people can accelerate the delivery.    Acceptance Criteria  * Investigate the W3C VC standard  * Review the existing the work in progress  * Create a Plan of Attack (POA) for the remaining work to bring our implementation inline with the standard.  ** Define the relationships between the Rich Schema objects  ** Should include the work necessary to implement in Indy SDK / LibIndy  * Create the relevant issues.    *Notes*  * The work necessary to support Rich Schemas in LibVCX will be tracked separately.  * The work necessary to support Rich Schemas in Aries Shared Libraries will be tracked separately.",Task,Medium,Complete,"2020-01-14 14:25:40","2020-01-14 14:25:40",3
"Hyperledger Indy Node","Debug View Change when nodes added/demoted/promoted","Follow-up of debug and stabilization for INDY-2308, INDY-2322, INDY-2320.    *Acceptance criteria*  - Make sure all PRs are merged  -- https://github.com/hyperledger/indy-plenum/pull/1456 DONE  -- PR for follow-up fixes in INDY-2308 DONE  -- PR with simulation tests https://github.com/hyperledger/indy-plenum/pull/1455 [~<USER>  - Make sure simulation tests pass (probably after INDY-2329 is done) [~<USER>  - Write more tests related to demotion, promotion and adding of nodes [~<USER>  - Make sure all system tests pass [~<USER>  - Make sure load tests with random promotions, demotion and restarts pass [~<USER>  ",Task,Medium,Complete,"2020-01-14 13:11:30","2020-01-14 13:11:30",5
"Hyperledger Indy Node","Release 1.12.2",,Task,Medium,Complete,"2020-01-14 13:09:38","2020-01-14 13:09:38",3
"Hyperledger Indy Node","Improve node promotion/demotion simulation tests","*Rationale*  We discovered a bunch of edge cases related to promotion/demotion combined with view change (for example INDY-2308, INDY-2320, INDY-2322). Despite that we have a plan of attack for fixing these problems it is still quite alarming that these cases were caught by intermittent failures of system-level tests, and were completely uncaught by simulation tests. In order to be sure that these problems are really fixed (and improve general confidence in current implementation) we need to make sure these edge cases are reliably caught in simulation tests.    *Acceptance criteria*  Make sure these edge cases are captured in simulation tests. If improved simulation tests uncover more edge cases that cannot be fixed fast - create follow-up tickets.",Task,Medium,Complete,"2020-01-08 13:45:08","2020-01-08 13:45:08",8
"Hyperledger Indy Node","Improve load testing with demotions and promotions","We should extend load script with demotion and promotion operations.",Task,Medium,Complete,"2019-12-28 13:27:55","2019-12-28 13:27:55",3
"Hyperledger Indy Node","A lagging node may be the only one who started view change in case of F Nodes added/promoted in 1 batch","*Issue:*   * Pool of 4 Nodes   * Adding Node5   * Node5 is committed on Nodes1-3 (so their N=5 now), but not committed yet on Node4 (so N is still 4 there)   * Nodes 1-3 sending InstanceChange as they added a new Node   ** No quorum yet for Nodes1-3 (they expect N-F=4 Instance Changes)   ** Quorum for Node4 (N-F=3 for Node4)   * So, Node4 only started the view change   * Other nodes will not start it since Node4 hasn't ordered NODE txn for Node5 yet so it hasn't sent Instance Change    *Possible Options for Fixes:*   * O1: Change the logic how and when we calculate N and F   ** For example, update N and F not on every committed Node txn, but at the beginning of new View only.   * O2: Enhance Instance Change triggering logic to include N value into InstanceChanges, and start view change only if Nodes' N is equal to the one in the quorum of Instance Changes.   * O3: Do not allow 3PC batches to Pool Ledger with more than 1 txn   * O4: Allow only one NODE txn per view with adding/demoting   * O5: Start View Change immediately (without waiting for a quorum of Instance Changes) once a node is added or promoted.   ** It may make sense to send Instance Change as well to propagate view change to lagged nodes.",Task,Medium,Complete,"2019-12-27 16:20:26","2019-12-27 16:20:26",5
"Hyperledger Indy Node","A lagging node may use wrong N and F quorum values and never finish view change if there are NODE txns being processed","*Issue*   * Pool of 4 Nodes, viewNo=0   * Adding Node5 and demoting Node4   * Node 5 is committed on all nodes, Instance Change to view=1 is sent   ** => N=5, F=1 on all nodes   * Before view change is started, Node4 demotion is also committed, but on Nodes2, 3 and 5 only   ** Node1: N=5, F=1   ** Nodes2, 3 ,5: N=4, F=1   * View Change to view=1 is started on all nodes   * Nodes 2, 3, 5 finish View change using N-f=3 View Change messages in a NewView   * Node1 receives NewView from the  Node2 and tries to verify it.   As Node1's N=5, it expects N-F=4 ViewChange messages, while only 3 ViewChanges are there   * So, Node1 can not finish View Change    *Possible Options for Fixes:*   * Enhance changes done in the scope of INDY-2308 to finish View Change if a Node sees a quorum of equal NewView messages (not necessary verifiable).   * Change the logic how and when we calculate N and F   ** For example, update N and F not on every committed Node txn, but at the beginning of new View only.   * Enhance Instance Change triggering logic to include N value into InstanceChanges, and start view change only if Nodes' N is equal to the one in the quorum of Instance Changes.   * Do not allow 3PC batches to Pool Ledger with more than 1 txn   * Allow only one NODE txn per view with adding/demoting   * Start View Change immediately (without waiting for a quorum of Instance Changes) once a node is added or promoted.   ** It may make sense to send Instance Change as well to propagate view change to lagged nodes.     ",Task,Medium,Complete,"2019-12-27 12:49:20","2019-12-27 12:49:20",2
"Hyperledger Indy Node","Allow multiple active TAAs: Debug","*Acceptance criteria*   * Make sure there is enough test coverage by unit and integration tests   * Make sure that SDK is used in all integration tests   * Make sure all unit and integration tests pass reliably   * Make sure there is enough test coverage by system tests   * Make sure all system tests pass   * Make sure all changes in code are implemented:   ** Need to validate TAA acceptance against uncommitted TAA   ** Fix retirement time for disabling (set to the current pp_time instead of 0)   ** Do not allow empty text for TAA   * Make sure all comments in [https://github.com/hyperledger/indy-plenum/pull/1424] are processed",Task,Medium,Complete,"2019-12-20 07:23:02","2019-12-20 07:23:02",3
"Hyperledger Indy Node","Debug move to Aardvark: Phase 2",,Task,Medium,New,"2019-12-17 15:48:27","2019-12-17 15:48:27",5
"Hyperledger Indy Node","Improve TAA acceptance date validation","*Story*  As a developer building solutions for a network that enforces a Transaction Author Agreement, I want my organization's legal team to be able to review and accept the TAA in advance of it being written to the ledger. Thus when we submit a transaction we can report the real date of meaningful acceptance, instead of an arbitrary date engineered to be newer than when the TAA is added. This will also allow us to stage the configuration change in advance of the TAA being added, which reduces our risk of disruption when the TAA is written.    The TAA could be legally accepted at any point after the TAA is approved by network governance. After approval, it takes some time for the TAA to be encoded in MarkDown and anchored on the ledger.    Validating that the date of TAA acceptance is after the TAA was written to the ledger causes organizations that accepted the TAA in advance to risk a disruption to the ability to write to the ledger because they can't finalize their configuration until after the date the TAA is added to the ledger. They should be able to deploy the configuration change earlier so that it is ready when the ledger is updated, and indicate a more accurate date of TAA acceptance.    *Acceptance Criteria*  * When a TAA is anchored to the ledger, it must include a property representing the date when the TAA was approved for use on the network. Let's call it taa_ratified_date.  * The taa_ratified_date must be in the past.  * The taa_ratified_date cannot be edited by the administrator.  * If a TAA is already on the ledger, then the date the transaction was written to the ledger should be used as the taa_ratified_date.  * When a TAA is enforced on a ledger, accept any date for the TAA acceptance that is equal to or earlier than the present time and newer than or equal to the ratified_date (preserving the current logic to account for a few minutes of clock skew).  * Update the documentation.    *Notes*  * The taa_ratified_date must be in the past to avoid the challenges that come with the latest TAA being invalid, or dynamically updating the value with the proper BLS signatures when it becomes valid.  * The taa_ratified_date cannot be modified in order to simplify development and testing (changing requirements are threatening to delay the release). If we need to be able to modify it, we can scope that as a separate story.  * Reason for the change:  ** When we originally designed the TAA, we thought it would be important to validate that the date of TAA acceptance was realistic. We considered this important for the TAA acceptance to fulfill its role as evidence of meaningful acceptance.  ** However, we erroneously had a mental model of individuals interacting with the ledger in real-time, whereas individual data should not be written to the public ledger so the majority of writes are made by organizations where the software operator is not authorized to bind the organizations to an agreement like the TAA. That must be done by the legal team and management.  ** Further, we did not consider that there will be a lag between the TAA being published, and when it will actually be put on the ledger. In reality, the date of acceptance can predate the enforcement on the ledger.  ** Finally, we did not appreciate the coordination difficulty caused by having to coordinate the date of TAA acceptance. We don't want to require every client to have to code around the problem.",Task,Medium,Complete,"2019-12-12 21:49:53","2019-12-12 21:49:53",3
"Hyperledger Indy Node","Replace Indy Crypto with Ursa","     [https://github.com/hyperledger/indy-plenum/pull/1369]",Task,Medium,"In Progress","2019-12-06 15:48:14","2019-12-06 15:48:14",3
"Hyperledger Indy Node","Release 1.12.1",,Task,Medium,Complete,"2019-12-05 14:50:24","2019-12-05 14:50:24",3
"Hyperledger Indy Node","A node lagging behind may not be able to finish view change if nodes have been added/demoted","*Issue*  * A node is lagging behind and doing a view change to viewNo=X. This may happen if a node joins a pool under heavy load.  * Other nodes in the pool are already on viewNo=X+Y, and *their node registry is different* (that is new Nodes have been demoted/added).  * The node finishes the view change, and stashes all 3PC messages for future view coming from other nodes  * The node doesn't unstash the messages until it performs re-ordering till last prepared certificate  * The node gets a quorum of instance changes for viewNo=X+1  * The same situation happens for viewNo=X+2, X+3, etc.  * At some view lagging Node expects different Primaries than other nodes did since it doesn't have up-to-date node registry  * As we don't process any checkopoints and don't start catchup by checkpoints during the view change, the node will be in the view change forever    *Workaround*  * Node restart    *Possible fix*  * Option 1: Process future checkpoints and allow to start catchup during view change by checkpoints  * Option 2: request NEW_VIEW from a quorum of Node and trust who is the Primary based on this NEW_VIEW",Task,Medium,Complete,"2019-11-27 19:48:38","2019-11-27 19:48:38",5
"Hyperledger Indy Node","Support historical req handlers for non-config ledgers","We supported pool-version-based pluggable req handlers for old version of txns in the scope of INDY-2292, but the fix there was working for config ledger only.  We need to support it for all ledgers.    One of the options how it can be implemented is to use timestamps",Task,Medium,Complete,"2019-11-27 13:20:16","2019-11-27 13:20:16",2
"Hyperledger Indy Node","Auth_Rules documentation should explain how endorsers work","This page should have a brief description of when endorsers are required to authorize a transaction:    [https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md]    Specifically:    * Unpriviledged users cannot submit any transaction (including administrative transactions like pool upgrade or restart) without a signature from a DID with the endorser role that is specified in the endorser field.",Task,Medium,Complete,"2019-11-22 16:07:21","2019-11-22 16:07:21",1
"Hyperledger Indy Node","Allow multiple active TAAs","*Story*    As an organization using an Indy Network, I need to adopt the Transaction Author Agreement at the time my legal team approves it, rather than coordinating closely with the network administrators for when to cut-over to a new agreement.    Current TAA flow   * Network governance adopts a TAA, and then publishes it on a web site.   * Users of the network can review the TAA.   * After the advisory period has expired, it is published to the ledger and immediately enforced and the old agreement is immediately retired.    Problems   * Users of the network have to wait to adopt the new TAA until the advisory period has expired and it is on the ledger even if they have already accepted it.   * Users of the network have to immediately switch versions of the TAA in their software at the time the new TAA is on the ledger.   * Users of the network can't store their acceptance in their software configuration until the TAA is on the ledger and the official hash is available.    Revised TAA flow   * Network governance adopts a TAA, and it enters the public advisory period.   * It is immediately put on the ledger.   * When the new agreement's public advisory period has ended and the old agreement is no longer acceptable, network governance marks the old agreement as retired.    *Acceptance Criteria*   * When a new TAA is put on the ledger, old agreements are not automatically rejected. Marking a TAA as retired is done in a separate transaction from adding a new TAA.   * A transaction that includes acceptance of any non-retired TAA should be accepted by the ledger.   * A query to the ledger for the TAA which does not specify a specific version should always return the newest TAA.   * Documentation is updated.    *Notes*   * The number of acceptable TAAs is determined by the network admin based on when agreements are marked as retired. It is expected that only one TAA will be active most of the time, but during a transition period two TAAs may be active. It is expected that there will never be more than two TAAs active at a time. However, we don't need to enforce that on the ledger.   * The current logic that a TAA can be accepted the day before it gets on the ledger (due to clock skew) should be preserved.",Story,Medium,Complete,"2019-11-21 15:59:24","2019-11-21 15:59:24",8
"Hyperledger Indy Node","Backups should start ordering in new view only after master instance ordered till prepared cert from NewView","As INDY-2295 and other load tests showed, it's quite common that multiple view changes happen in a row because master needs to re-order old requests (with potentially non-zero latency getting bigger because of view change), but backups start ordering new requests with pretty low latency.  So, the difference can easily get more than our limit (20 secs), and a new view change will be triggered.     *Acceptance criteria:*  * Integration tests reproducing the issue  * Do not start ordering on backups until ReOrdered is sent on master",Task,Medium,Complete,"2019-11-20 10:17:25","2019-11-20 10:17:25",5
"Hyperledger Indy Node","Do not restore Primaries from the audit ledger","We made Primary selection pretty deterministic in the scope of INDY-2262.  Because of the situation reproduced in SN-10 on MainNet after upgrade that some lagging nodes restored primaries from the audit that aren't equal to the current primaries on most of the Nodes, we will need to do the following:    *Changes:*  * Do not restore Primaries from the audit ledger after initial catchup, but rather calculate them using the common primary selection logic    *Integration Test*  * Mock writing primaries to audit so that it always writes Node4 as a Primary  * Restart Nodes 3 and 4  * Make sure that these Nodes selected the same Primary as the other Nodes have (Node1) and the the pool can ordering    *System Test*  * Install version 1.11.0, pool of 4 Nodes  * Demote Node1 => view change to viewNo=1, Primary - Node3  * Promote Node1 => view change to viewNo=3 (skip viewNo=2 since the same Primary is chosen), Primary - Node4  * Update 3 Nodes to the latest master  * Make sure that they order anything (NODE_UPGRADE txns for example).   * Update the 4th Node  * Make sure that the 4th node selected the same Primary as other 3 nodes and all nodes are in consensus  ",Task,High,Complete,"2019-11-20 08:27:26","2019-11-20 08:27:26",2
"Hyperledger Indy Node","get TAA should return the hash","The API call to get the transaction author agreement should return the hash along with the text, so that applications can confirm that they have correctly computed the hash.    [https://github.com/hyperledger/indy-node/blob/master/docs/source/requests.md#get_transaction_author_agreement]",Task,Medium,Complete,"2019-11-19 22:50:35","2019-11-19 22:50:35",2
"Hyperledger Indy Node","Investigate reasons of hundreds VCs during 15 txns per sec production load","Investigate reasons of hundreds VCs during 15 txns per sec production load against the latest stable packages.    Logs and metrics:  ev@evernymr33:logs/performance_results_12_11_2019.tar.gz  ev@evernymr33:logs/stable_15_writes_per_sec_12_11_2019.tar.gz  ",Task,Medium,Complete,"2019-11-18 12:54:44","2019-11-18 12:54:44",2
"Hyperledger Indy Node","Get rid of transport batches","*Acceptance criteria:*  - Do same load tests with transport batches on and off (TRANSPORT_BATCH_ENABLED = False)  - Compare metrics  - Turn off batches in production if we see better performance  - Remove batches-related code (optionally, can be done in a followup task).  ",Task,Medium,Complete,"2019-11-11 13:28:20","2019-11-11 13:28:20",2
"Hyperledger Indy Node","Recreate config state on all Staging Net nodes as part of migration script","The problem discovered in INDY-2283 could be addressed by recreating the config state, but that won't fix the historical audit ledger entries that might be needed to perform a ledger audit.",Task,Medium,Complete,"2019-11-11 13:20:49","2019-11-11 13:20:49",1
"Hyperledger Indy Node","Take into account txn history when recovering state from the ledger for new nodes","Due to undetected bugs in the way properties are added to the node state, there are live Indy ledgers with incorrectly calculated hashes. This prevents new nodes in those pools from entering into consensus.    We should special case the merkle hash calculation so that entries in the audit ledger are created using the logic that was used by historical versions of Indy this allows new nodes to arrive at the same merkle root as all other nodes in the pool.    *Acceptance criteria:*   * Take into account the txn version as it was at the time of applying txn when recovering Config state from the ledger (as part of catchup or as part of recover state from the ledger procedure)   * Implement it in a generic way using a txn version concept   * Get the virtual txn version using the POOL_UPGRADE txns from the Config Ledger   * The transaction history must explicitly account for auth_rules transactions in version 1.9.1 (see INDY-2283).   ** A comment exists in the code linking to this INDY-2283 so that future developers can easily find the context for the change.   * _Evaluate if needed:_ There needs to also be a way to allow ledger plugins to specify a historical calculation (see SN-8 for a historical problem with Sovrin Fees in Indy 1.7.1)",Task,High,Complete,"2019-11-11 13:15:39","2019-11-11 13:15:39",5
"Hyperledger Indy Node","Write a script to upgrade audit ledger for the new state recovered from ledger","As a result of bugs (INDY-2283), we have live Indy networks that have incorrect merkle hash calculations in the audit history. This prevents new nodes from coming into consensus in these pools.    We should migrate the history to be correct.",Task,Medium,Complete,"2019-11-11 13:14:33","2019-11-11 13:14:33",5
"Hyperledger Indy Node","Enable zeroMQ auto-reconnection","*Issue Description*    Today we use monitor sockets for:   # detecting and getting a list of disconnected/unreachable nodes (in Validator Info for example)   # triggering a View Change when a Primary is disconnected    This logic seems to cause some unexpected behavior in zmq (like zmq not being able to reconnect by itself - INDY-2261), so we need to see why monitor sockets are preventing this, and fix it.    If there is no conclusion the logic for which we are now using monitor sockets should be implemented with heartbeats.    *Note*    An issue for the monitor behavior is opened with PyZMQ community:   * [https://github.com/zeromq/pyzmq/issues/1340]    Check the status of this issue before making changes to the code because it can potentially provide us with   * an easy fix if we are not using monitor sockets correctly, or   * a new version without this problem    if this turns out to be the case, this ticket should be ignored and a follow up should be created.    *Acceptance Criteria*   * zeroMQ auto-reconnection works   * All existing tests pass   * New logic covered with unit and integration tests     ",Task,Medium,Complete,"2019-11-08 17:02:33","2019-11-08 17:02:33",5
"Hyperledger Indy Node","Improve simulation tests to include NODE txns","* Extend view change during ordering cases with NODE txs  ** Start with a simple sending of different combination of NODE txns interleaved with view changes  ** Consider adding new Replicas in sim tests once a new NODE is added/promoted as well as removing Replicas when  there is NODE txn for demotion  It may be easier to do this after INDY-2262 is done.",Task,Medium,Complete,"2019-11-07 09:39:20","2019-11-07 09:39:20",5
"Hyperledger Indy Node","PrePrepare's Digest need to take into account all PrePrepare's fields","As of now, PrePrepare's digest is created over a list of requests proposed by this PrePrepare. It doesn't take into account other fields such as txn roots, timestamp, etc.   So, a malicious Primary may send PrePrepare with the same requests (that is the same digest in the current implementation) but with different other fields (audit roots, timestamp, etc.) to every Node.   This is not so big problem for common ordering since ViewChange will be triggered.   But this is a problem for View Change when we do re-ordering, since nodes may have different PrePrepares for the same digest and will not be able to finish re-ordering (and hence finish view change).    *Accepted criteria:*  * Write an integration test:  ** Test 1:  *** Malicious Primary sends correct and equal PrePrepares to F+1 Npdes, and sends PrePrepares with the same list of requests but with different audit txn roots to other nodes   *** Make sure that view change is started and successfully finished in this case. The requests sent in the PrePrepare above are expected to be ordered.    ** Test 2:  *** Malicious Primary sends PrePrepares with the same list of requests but with different audit txn roots to all nodes  *** Make sure that view change is started and successfully finished in this case. The requests sent in the PrePrepare above are expected to NOT be ordered.    * Do a fix:  ** Calculate PrePrepares' digest against all PrePrepare's fields     ",Task,Medium,Complete,"2019-11-06 13:18:45","2019-11-06 13:18:45",3
"Hyperledger Indy Node","Implement system tests to check all states' consistency","Implement integration and system tests to check all states' consistency.",Task,Medium,Complete,"2019-11-06 09:14:37","2019-11-06 09:14:37",1
"Hyperledger Indy Node","Optimize Message Request logic","If a Node experiences network issues, it may start requesting 3PC messages. If the node is lagging behind, then it may start lagging behind even more since because of these message requests.   *Acceptance criteria*  * Check if we need message requests at all (we use TCP-based ZMQ, so we should not really loose messages).  * Optimize message request logic to not request too much. ",Task,Medium,New,"2019-11-05 10:07:29","2019-11-05 10:07:29",5
"Hyperledger Indy Node","Improve BLS signature performance","Currently about 15% of all time is spent on validation of BLS signatures in Commits (see metrics below).  One of the reason, is that we have to sign both Domain and Token states if FEEs are enabled.    *Acceptance Criteria*  - Investigate how we can improve performance (INDY-2252 is one of the ways)  - Do necessary fixes.   - Create follow-up tickets for further improvements if needed.      *Metrics*  Start time: 2019-10-22 00:04:00  End time: 2019-10-22 00:14:00  Duration: 0:10:00  Number of messages processed in one looper run:     Node:   236 samples, 94906.00 None, 26.00/402.14/1000.00 min/avg/max, 356.38 stddev     Client: 236 samples, 5917.00 None, 4.00/25.07/51.00 min/avg/max, 10.70 stddev  Seconds passed between looper runs:     236 samples, 591.61 None, 0.33/2.51/5.37 min/avg/max, 1.10 stddev  Number of messages in one transport batch:     5808 samples, 341460.00 None, 2.00/58.79/134.00 min/avg/max, 28.76 stddev  Node message size, bytes:     Outgoing: 5808 samples, 240228528.00 None, 7028.00/41361.66/130387.00 min/avg/max, 25096.80 stddev     Incoming: 94906 samples, 240158125.00 None, 53.00/2530.48/90366.00 min/avg/max, 5589.89 stddev  Client message size, bytes:     Outgoing: 11399 samples, 14249723.00 None, 81.00/1250.09/11572.00 min/avg/max, 1851.20 stddev     Incoming: 5917 samples, 7277834.00 None, 644.00/1229.99/9478.00 min/avg/max, 1947.02 stddev  Number of requests in one 3PC batch:     Created: 521 samples, 5954.00 None, 0.00/11.43/28.00 min/avg/max, 3.48 stddev     Ordered: 482 samples, 5477.00 None, 0.00/11.36/27.00 min/avg/max, 3.33 stddev  Time spent on write request processing, ms:     10.59/16.06/264.80 ms min/avg/max, 4.32 stddev  Monitor throughput, TPS     Master: 8.22/9.53/10.31 min/avg/max, 0.45 stddev     Backup: 8.26/9.51/10.17 min/avg/max, 0.44 stddev  Monitor latency, seconds     Master: 4.30/13.92/39.08 min/avg/max, 7.93 stddev     Backup: 4.21/13.79/38.48 min/avg/max, 8.09 stddev  RAM info:     Available, Mb: 6504.41/6536.23/6540.99 min/avg/max, 5.75 stddev     Node RSS, Mb: 1099.84/1103.71/1113.77 min/avg/max, 3.82 stddev     Node VMS, Mb: 1542.84/1547.76/1550.84 min/avg/max, 3.93 stddev  Additional statistics:     Client incoming/outgoing: 0.52 messages, 0.51 traffic     Node incoming/outgoing traffic: 1.00     Node/client traffic: 22.31     Node traffic per batch: 996652.81     Node traffic per request: 87709.81  Profiling info:     BACKUP_REQUEST_PROCESSING_TIME : 45020 samples, 0.07 seconds, 0.00/0.00/0.05 ms min/avg/max, 0.00 stddev     REQUEST_PROCESSING_TIME : 5954 samples, 95.62 seconds, 10.59/16.06/264.80 ms min/avg/max, 4.32 stddev     GC_GEN0_TIME : 3898 samples, 1.28 seconds, 0.07/0.33/4.13 ms min/avg/max, 0.54 stddev     GC_GEN1_TIME : 354 samples, 1.39 seconds, 1.78/3.94/6.34 ms min/avg/max, 0.91 stddev     GC_GEN2_TIME : 32 samples, 8.28 seconds, 218.85/258.84/310.44 ms min/avg/max, 21.70 stddev     NODE_PROD_TIME : 236 samples, 591.60 seconds, 328.18/2506.76/5371.07 ms min/avg/max, 1103.59 stddev     SERVICE_REPLICAS_TIME : 236 samples, 331.08 seconds, 28.46/1402.89/3807.80 ms min/avg/max, 740.81 stddev     SERVICE_NODE_MSGS_TIME : 236 samples, 199.49 seconds, 264.44/845.30/1594.13 ms min/avg/max, 311.43 stddev     SERVICE_CLIENT_MSGS_TIME : 236 samples, 9.70 seconds, 9.56/41.09/120.63 ms min/avg/max, 18.61 stddev     SERVICE_NODE_ACTIONS_TIME : 236 samples, 2.68 seconds, 0.01/11.35/70.10 ms min/avg/max, 20.99 stddev     SERVICE_VIEW_CHANGER_TIME : 236 samples, 0.01 seconds, 0.03/0.03/0.07 ms min/avg/max, 0.01 stddev     SERVICE_OBSERVABLE_TIME : 236 samples, 0.02 seconds, 0.01/0.10/0.26 ms min/avg/max, 0.05 stddev     SERVICE_OBSERVER_TIME : 236 samples, 0.00 seconds, 0.01/0.01/0.03 ms min/avg/max, 0.00 stddev     FLUSH_OUTBOXES_TIME : 236 samples, 48.27 seconds, 22.03/204.52/523.24 ms min/avg/max, 108.24 stddev     SERVICE_NODE_LIFECYCLE_TIME : 236 samples, 1.25 seconds, 2.29/5.30/16.60 ms min/avg/max, 1.52 stddev     SERVICE_CLIENT_STACK_TIME : 236 samples, 0.29 seconds, 0.21/1.21/3.04 ms min/avg/max, 0.51 stddev     SERVICE_MONITOR_ACTIONS_TIME : 236 samples, 0.01 seconds, 0.00/0.03/1.09 ms min/avg/max, 0.12 stddev     SERVICE_TIMERS_TIME : 236 samples, 0.01 seconds, 0.01/0.05/0.46 ms min/avg/max, 0.10 stddev     SERVICE_NODE_STACK_TIME : 236 samples, 149.57 seconds, 187.95/633.75/1286.02 ms min/avg/max, 247.05 stddev     PROCESS_NODE_INBOX_TIME : 236 samples, 49.91 seconds, 73.57/211.49/323.07 ms min/avg/max, 68.53 stddev     SEND_TO_REPLICA_TIME : 233458 samples, 2.69 seconds, 0.01/0.01/0.15 ms min/avg/max, 0.00 stddev     NODE_CHECK_PERFORMANCE_TIME : 52 samples, 0.04 seconds, 0.59/0.74/1.08 ms min/avg/max, 0.14 stddev     NODE_CHECK_NODE_REQUEST_SPIKE : 10 samples, 0.00 seconds, 0.02/0.03/0.04 ms min/avg/max, 0.01 stddev     UNPACK_BATCH_TIME : 59490 samples, 122.06 seconds, 0.19/2.05/313.04 ms min/avg/max, 6.12 stddev     VERIFY_SIGNATURE_TIME : 142496 samples, 15.47 seconds, 0.01/0.11/236.43 ms min/avg/max, 0.66 stddev     SERVICE_REPLICAS_OUTBOX_TIME : 236 samples, 13.09 seconds, 0.52/55.48/174.41 ms min/avg/max, 36.29 stddev     NODE_SEND_TIME : 15602 samples, 1.36 seconds, 0.04/0.09/1.27 ms min/avg/max, 0.04 stddev     NODE_SEND_REJECT_TIME : no samples     VALIDATE_NODE_MSG_TIME : 429527 samples, 116.12 seconds, 0.05/0.27/311.31 ms min/avg/max, 2.20 stddev     INT_VALIDATE_NODE_MSG_TIME : 429527 samples, 51.20 seconds, 0.03/0.12/310.87 ms min/avg/max, 2.15 stddev     PROCESS_ORDERED_TIME : 4605 samples, 11.98 seconds, 0.04/2.60/64.74 ms min/avg/max, 7.78 stddev     MONITOR_REQUEST_ORDERED_TIME : 4605 samples, 0.56 seconds, 0.02/0.12/0.73 ms min/avg/max, 0.09 stddev     EXECUTE_BATCH_TIME : 482 samples, 11.22 seconds, 0.99/23.28/64.10 ms min/avg/max, 9.13 stddev     SERVICE_REPLICA_QUEUES_TIME : 236 samples, 281.30 seconds, 2.63/1191.94/3411.61 ms min/avg/max, 668.92 stddev     SERVICE_BACKUP_REPLICAS_QUEUES_TIME : 1888 samples, 36.64 seconds, 1.35/19.41/303.59 ms min/avg/max, 14.13 stddev     PROCESS_PREPREPARE_TIME : 521 samples, 162.39 seconds, 2.01/311.69/799.74 ms min/avg/max, 146.81 stddev     PROCESS_PREPARE_TIME : 14365 samples, 55.33 seconds, 0.01/3.85/415.72 ms min/avg/max, 27.95 stddev     PROCESS_COMMIT_TIME : 14526 samples, 167.67 seconds, 0.01/11.54/89.20 ms min/avg/max, 10.89 stddev     PROCESS_CHECKPOINT_TIME : 114 samples, 0.33 seconds, 0.10/2.92/125.58 ms min/avg/max, 14.42 stddev     SEND_PREPREPARE_TIME : no samples     SEND_PREPARE_TIME : 521 samples, 0.18 seconds, 0.26/0.35/3.76 ms min/avg/max, 0.16 stddev     SEND_COMMIT_TIME : 462 samples, 1.50 seconds, 2.12/3.24/12.12 ms min/avg/max, 0.97 stddev     SEND_CHECKPOINT_TIME : 5 samples, 0.00 seconds, 0.25/0.25/0.26 ms min/avg/max, 0.00 stddev     CREATE_3PC_BATCH_TIME : no samples     ORDER_3PC_BATCH_TIME : 482 samples, 5.49 seconds, 7.84/11.38/38.69 ms min/avg/max, 2.76 stddev     BACKUP_PROCESS_PREPREPARE_TIME : 4256 samples, 14.71 seconds, 0.69/3.46/274.13 ms min/avg/max, 5.33 stddev     BACKUP_PROCESS_PREPARE_TIME : 145205 samples, 17.57 seconds, 0.01/0.12/271.80 ms min/avg/max, 0.93 stddev     BACKUP_PROCESS_COMMIT_TIME : 145781 samples, 12.65 seconds, 0.01/0.09/137.15 ms min/avg/max, 0.49 stddev     BACKUP_PROCESS_CHECKPOINT_TIME : 1058 samples, 1.02 seconds, 0.09/0.96/79.24 ms min/avg/max, 5.37 stddev     BACKUP_SEND_PREPREPARE_TIME : no samples     BACKUP_SEND_PREPARE_TIME : 4256 samples, 0.59 seconds, 0.11/0.14/4.57 ms min/avg/max, 0.07 stddev     BACKUP_SEND_COMMIT_TIME : 2994 samples, 0.51 seconds, 0.09/0.17/55.01 ms min/avg/max, 1.05 stddev     BACKUP_SEND_CHECKPOINT_TIME : 40 samples, 0.01 seconds, 0.17/0.19/0.25 ms min/avg/max, 0.02 stddev     BACKUP_CREATE_3PC_BATCH_TIME : no samples     BACKUP_ORDER_3PC_BATCH_TIME : 4123 samples, 2.92 seconds, 0.39/0.71/137.03 ms min/avg/max, 2.95 stddev     PROCESS_PROPAGATE_TIME : 136579 samples, 33.41 seconds, 0.14/0.24/9.34 ms min/avg/max, 0.13 stddev     PROCESS_MESSAGE_REQ_TIME : no samples     PROCESS_MESSAGE_REP_TIME : 1115 samples, 0.25 seconds, 0.13/0.22/0.93 ms min/avg/max, 0.09 stddev     PROCESS_LEDGER_STATUS_TIME : no samples     PROCESS_CONSISTENCY_PROOF_TIME : no samples     PROCESS_CATCHUP_REQ_TIME : no samples     PROCESS_CATCHUP_REP_TIME : no samples     PROCESS_REQUEST_TIME : 5917 samples, 2.50 seconds, 0.20/0.42/7.38 ms min/avg/max, 0.23 stddev     SEND_PROPAGATE_TIME : 5917 samples, 2.23 seconds, 0.24/0.38/1.15 ms min/avg/max, 0.11 stddev     SEND_MESSAGE_REQ_TIME : 5386 samples, 0.38 seconds, 0.02/0.07/8.47 ms min/avg/max, 0.14 stddev     SEND_MESSAGE_REP_TIME : no samples     BLS_VALIDATE_PREPREPARE_TIME : 4777 samples, 11.64 seconds, 0.00/2.44/67.94 ms min/avg/max, 8.04 stddev     BLS_VALIDATE_COMMIT_TIME : 77201 samples, 160.54 seconds, 0.00/2.08/69.12 ms min/avg/max, 6.20 stddev     BLS_UPDATE_PREPREPARE_TIME : no samples     BLS_UPDATE_COMMIT_TIME : 462 samples, 1.39 seconds, 1.95/3.00/6.25 ms min/avg/max, 0.86 stddev     DESERIALIZE_DURING_UNPACK_TIME : no samples",Task,Medium,Complete,"2019-11-05 09:14:27","2019-11-05 09:14:27",3
"Hyperledger Indy Node","Research Libra HotStuff implementation","     Project documentation:    [https://developers.libra.org/docs/crates/consensus]    Summary:    [https://medium.com/ontologynetwork/hotstuff-the-consensus-protocol-behind-facebooks-librabft-a5503680b151]    Protocol paper:    [https://arxiv.org/pdf/1803.05069.pdf]         We need to find out how stable Libra HotStuff implementation is .   Timebox the effort to 4 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.   Discussion should include:   ** Pros and Cons   ** Characteristics of current deployments in production usage   ** Current project roadmap   ** Health of the open source community / level of investment   ** Rough estimate of work required to bring it to the same level as Plenum   ** Rough understanding of performance   ** Rough understanding of the difficulty of migrating from the existing Plenum to this implementation based   * A recommendation on whether to research more or kill the proposal   * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]         Things to consider:   * Research how clients are authorized by ledger   * How does this relate to work with observers   * How much better is performance   * How many nodes would be practical (50? 500? 5000?)   * Get feedback from community   * Notice: One of PoC goals should be smooth upgrade path   * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.",Task,Medium,Complete,"2019-11-04 19:33:48","2019-11-04 19:33:48",3
"Hyperledger Indy Node","A new node joining the pool during the view change may not be able to start ordering immediately","*Issue*  If a new NODE txn is sent and a new Node is started immediately after this, then the Node may node join the consensus immediately, but only after 200 batches.     *Workaround*  Wait for some time before starting the newly added node. Make sure that view change happened after NODE txn is sent, and the pool started ordering in new View, and only then start a new Node.     *Issue Reason*  If a new Node joins the Pool when view change is in progress and don't get enough Instance Changes to start the View Change as well, it will not caught up the new view and new primaries, so it will not be able to order until the quorum of checkpoints (200 batches) from other nodes is received  The situation gets worse by the fact that we write new viewNo and update primaries only when we finish re-ordering which may be done not immediately.         *Possible fix*  * If a Node didn't start a view change to view X but sees a quorum of COMMIT messages for originalViewNo = X, then it needs to do catcup.  * There are chances that the COMMITs are not ordered yet on nodes, so catchup will not move the Node to view X. But then the Node can repeat the procedure until it moves to view X.",Task,Medium,New,"2019-10-28 16:49:31","2019-10-28 16:49:31",3
"Hyperledger Indy Node","A Node missing a View Change may not be able to finish it if NODE txns have been sent","*Issue*  * There are NODE txns affecting primary selection (promotion/demotion/new nodes + specific viewNo)  * A node didn't finish view change from X to X+1 while the pool finished, and the node tries to do a view change from X to X+2  * A node doesn't know who was the Primary on view X+1 during re-ordering, so it gets wrong audit ledger (since Primary is part of audit txn)  * The Node will have to wait for a quorum of checkpoints from other nodes (~200 batches) to be able to continue ordering    *Possible fix*  * Keep a map of primaries for every view (it needs to be cleaned up on GC)  * Get primaries from Audit ledger's uncommitted txns if a primary for view X is unknown  * Add Primaries to PrePrepare  * Get Primaries from PrePrepare if it can not be get audit ledger as well (it means re-ordering after the view change, so that we can trust such primaries as they have prepared certificate). ",Task,Medium,Complete,"2019-10-28 16:29:18","2019-10-28 16:29:18",2
"Hyperledger Indy Node","PBFT View change: cleanup and debug Part 3","* Fix the issue with primary selection when Node txns are in flight IN PROGRESS [~<USER>** (INDY-2275)   * Finish cleanup   ** Do we need `primaryChanged` in replica - TBD   ** Cleanup methods we call in services after catchup   * More load tests",Task,Medium,Complete,"2019-10-25 08:48:12","2019-10-25 08:48:12",2
"Hyperledger Indy Node","Release 1.12.0",,Task,Medium,Complete,"2019-10-24 08:53:52","2019-10-24 08:53:52",3
"Hyperledger Indy Node","Cleanup Removing all backup Replicas","*Acceptance Criteria*  * Remove all code related to backup instances  * Make sure all tests pass",Task,Medium,New,"2019-10-24 08:44:11","2019-10-24 08:44:11",13
"Hyperledger Indy Node","Improve simulation tests to include processing of InstanceChanges","* Allow starting new view changes by a quorum of Instance Changes  ",Task,Medium,Complete,"2019-10-24 08:27:48","2019-10-24 08:27:48",5
"Hyperledger Indy Node","All nodes need to select the same primary during view change","*Issue*  If there are NODE txns for adding/removing nodes interleaved with View Changes (not any view changes, but a specific subset), then either  1) Up to F Nodes may not be able to finish view change   OR  2) All nodes will not be able to finish view change     *Workaround*  Restart should help    *When issue is reproduced during demotion*  Let X be a demoted Node seqno (historical order of nodes how they have been added), N - number of nodes, viewNo - current viewNo.  * If  *X >= (viewNo mod N) + 3*, then no issue is possible.  * If  *X < (viewNo mod N) + 3*, then issue *may* be reproduced depending on network conditions.     *When issue is reproduced during new Node adding*  Let N - number of nodes, viewNo - current viewNo.  * If  *N != (viewNo mod N) + 1*, then no issue is possible.  * If  *N == (viewNo mod N) + 1*, then issue *may* be reproduced depending on network conditions.       *Example 1: F nodes out of consensus till next View Change after new Node is added*  1) The pool has 24 Nodes, and the current viewNo is 23, so that master Primary is Node24 (the last one)  2) Node25 is added (NODE txn is sent)  3) N-F Nodes applied NODE txn, and F Nodes haven't applied it yet  4) View Change is started  5) N-F Nodes selected Node25 as a Primary and F Nodes selected Node1 as a Primary  6) N-F Nodes finished view change, but F Nodes didn't   7) If the next View Change is started, the F Nodes will be able to finish it and join consensus    *Example 2: F nodes out of consensus till next View Change after a Node is demoted*  1) The pool has 24 Nodes, and the current viewNo is 10  2) Node10 is demoted  3) N-F Nodes applied NODE txn, and F Nodes haven't applied it yet  4) View Change is started  5) N-F Nodes selected Node12 as a Primary and F Nodes selected Node11 as a Primary  6) N-F Nodes finished view change, but F Nodes didn't   7) If the next View Change is started, the F Nodes will be able to finish it and join consensus    *Example 3: All nodes can not finish view change when adding a Node*  In Example 1, if more than F nodes have a different state than others (didn't apply NODE txn for example), then all nodes will not be able to finish the view change    *Example 4: All nodes can not finish view change when demoting a Node*  In Example 2, if more than F nodes have a different state than others (didn't apply NODE txn for example), then all nodes will not be able to finish the view change    *Example 5: No issues when a Node is demoted*  If current viewNo is 22 in Example 1, then there is no issue    *Example 6: No issues when a Node is demoted*  If Node 12 is demoted in Example 2, then there is no issue      *Problem Reason*  * As of now, nodes select a Primary at the beginning of View Change (once receive a quorum of Instance Changes).  * Primary selection depends on the current node registry state which depends on the Node's uncommitted state (Node txns to add/remove nodes in flight)  * Node may come to view change in different states  * => The may select different primaries and view change may never finish on some nods since they expect NewView from the Primary they selected    *Acceptance criteria*  * Write integration tests reproducing the issue  ** Test 1   *** Initial state: Pool with 4 Nodes, viewNo=3, currentPrimary - Node4  *** Delay X msg on Y Nodes  **** params: X = PrePrepare/Commit; Y=1 Node/2 Nodes/3 Nodes/4 Nodes  *** Add a NODE txn to add a new Node (Node5)  *** Start a View Change     *** Make sure that it's finished on all nodes, all nodes have the same state and can order  ** Test 2  *** Initial state: Pool with 5 Nodes, viewNo=3, currentPrimary - Node4  *** Delay X msg on Y Nodes  **** params: X = PrePrepare/Commit; Y=1 Node/2 Nodes/3 Nodes/4 Nodes  *** Add a NODE txn to remove Node5  *** Start a View Change     *** Make sure that it's finished on all nodes, all nodes have the same state and can order  ** Test 3  *** Initial state: Pool with 5 Nodes, viewNo=2, currentPrimary - Node3  *** Delay X msg on Y Nodes  **** params: X = PrePrepare/Commit; Y=1 Node/2 Nodes/3 Nodes/4 Nodes  *** Add a NODE txn to demote Node3  *** Make sure that view change is started and finished on all nodes, all nodes have the same state and can order  * Do a fix    *Possible fix*  * Do not commit nodeRegistry (current list of nodes used for Primary selection) till the next View Change, so that all nodes can select the same Primaries regardless of their state.  * Do view change on every change of nodeRegistry",Task,High,Complete,"2019-10-24 08:10:57","2019-10-24 08:10:57",5
"Hyperledger Indy Node","Audit manual reconnection logic in ZStack ","*Issue (see INDY-2274)*  * System tests doing restart fail intermittently  * Some nodes can not join the pool if they restart during the load     Zstack tries to do handle reconnections manually by using ping/pongs and reconnecting sockets. It looks like that may lead to issues. Moreover, it looks like ZMQ is able to handle reconnection by it own.    *Acceptance Criteria*  * Prove that the current logic is really needed, or prove that we can live without it  * If latter then  ** Get rid of ping/pongs  ** Assume that we connected to a Node once we receive any message from it  ** Still use monitor socket to detect disconnections (it's needed to initiate view change when Primary is disconnected) ",Task,Medium,Complete,"2019-10-23 14:16:21","2019-10-23 14:16:21",8
"Hyperledger Indy Node","Debug move to Aardvark: Phase 1",,Task,Medium,New,"2019-10-22 09:42:10","2019-10-22 09:42:10",8
"Hyperledger Indy Node","Update Documentation with a new Consensus Protocol",,Task,Medium,New,"2019-10-22 09:37:07","2019-10-22 09:37:07",1
"Hyperledger Indy Node","Get rid of Propagates","We may get rid of Propagates at all once move to Aardvark:  Primary can send the whole req instead of just reqId    Propagate phase is one of the bottlenecks, so we may see performance improvements if remove it.  ",Task,Medium,New,"2019-10-21 13:11:44","2019-10-21 13:11:44",8
"Hyperledger Indy Node","Optimize Propagate logic","Do not send as many propagates as of now.  * Primary still uses finalized requests to be included into PrePrepare  * Non-primaries process requests from PrePrepare if they are aware of it (either received a request or at least one Propagate). This is a step towards Aardvark protocol. Also we have digital signatures, so are sure requests came from the clients. In order to check that Primary doesn't skip any requests, other means and checks are needed in any case (regardless if this is RBFT or Aardvark), and this has nothing to do with finalized requests.  * Return even non-finalized requests when processing Propagate MessageReq    ",Task,Medium,Complete,"2019-10-21 11:10:02","2019-10-21 11:10:02",2
"Hyperledger Indy Node","Do regular view changes","* Do regular view changes as required by Aardvark protocol (https://www.usenix.org/legacy/events/nsdi09/tech/full_papers/clement/clement.pdf)  * Implement this in a way to turn off or on (off by default)",Task,Medium,New,"2019-10-21 10:17:24","2019-10-21 10:17:24",8
"Hyperledger Indy Node","Remove all backup Replicas","Only master Replica is needed in Aardvark protocol (https://www.usenix.org/legacy/events/nsdi09/tech/full_papers/clement/clement.pdf)    Acceptance Criteria:  * A flag to turn off all Backup Replicas  * A build for testing  * Tests adaption and code cleanup will be continued in INDY-2260  ",Task,Medium,New,"2019-10-21 10:16:45","2019-10-21 10:16:45",5
"Hyperledger Indy Node","Make all catchup system tests pass","Please take into account findings from INDY-2222",Task,Medium,Complete,"2019-10-16 09:56:40","2019-10-16 09:56:40",3
"Hyperledger Indy Node","Release 1.11.0","*Release Goal*  Release everything that is ready. Especially   * Improved view change algorithm    *Acceptance Criteria*  Regular instructions:   * Code is tested   * Review upstream releases to decide what should be included in this build  Build is produced with automated CI / CD  Official build is tagged as released in Git  Official builds should be the only builds that are not marked in GitHub as pre-release  Prepare basic documentation on new features  Prepare the Release Notes  Link to the documentation on new features  Stored as a CHANGELOG.md in the root of the repo  Latest release at the top, above the release notes for all previous versions  The release should be on GitHub “Releases” tab.  Release notes should be in the “Description field of the GitHub release artifact  The release is marked in JIRA  Add the correct fixVersion to included issues  Enable others to use the release:  Email stakeholders with links to the artifacts and release notes",Task,Medium,Complete,"2019-10-10 14:34:56","2019-10-10 14:34:56",3
"Hyperledger Indy Node","PBFT View Change Debug: Part 2","* Make sure that all the tests are unskipped - {color:#0747a6}DONE{color}   * Finish cleanup   ** Do we need `self.schedule_view_change_completion_check(self._view_change_timeout)` in `on_view_change_start` - [DONE|https://github.com/hyperledger/indy-plenum/pull/1371]   ** Do we need `replica.on_view_change_start()` - [DONE|https://github.com/hyperledger/indy-plenum/pull/1371]   ** Remove `notify_view_change_complete` - [DONE|https://github.com/hyperledger/indy-plenum/pull/1371]   ** Do we need `primaryChanged` in replica - TBD   * Do more load testing   * Improve simulation tests if needed   * Fix the issue with primary selection when Node txns are in flight IN PROGRESS [~<USER>",Task,Medium,Complete,"2019-10-10 08:13:11","2019-10-10 08:13:11",8
"Hyperledger Indy Node","Remove primaries field from audit ledger","* Do View Change on every change of N   * Simplify the formula for primary calculation",Task,Medium,Complete,"2019-10-07 18:42:42","2019-10-07 18:42:42",3
"Hyperledger Indy Node","Make sure that only valid OldViewPrePrepares are processed","*Problem*   * When a replica doesn't have a PrePrepare associated with a batch from NewView (that is a PrePrepare to be re-ordered), it requests it from other nodes.   * We don't have digital signatures for messages, so we can not trust that a PrePrepare for view X returned by a non-Primary for view X is valid.   * We can not ask only a primary for view X (previous view) in this case since it can be malicious or offline (that's why we did view change).   * So, we are asking all nodes   * As of now, we get an old view PrePrepare from any Node, and try to apply it, which may lead to some problems (raising Suspicious on a new primary which can be honest) if this is not the original PrePrepare we are looking for    *Acceptance criteria*   * Write an integration test reproducing the issue:     ** Delay PrePrepares on 1 Node (without processing)   ** Delay receiving of OldViewPrePrepareRequest on all nodes but 1   ** Patch OldViewPrePrepareRequest  processing on the node it isn't delayed by returning an invalid PrePrepare   ** Start a view change   ** Make sure it's finished (NewView is sent)   ** Make sure that the lagging node received OldViewPrePrepareReply from the malicious node   ** Reset delay for OldViewPrePrepareRequest  on other nodes   ** make sure that the lagging node was able to re-order everything   ** make sure the pool is functional   * Analyze the issue (if it exists and how severe it is)   * Do a fix if needed   ** Wait for F+1 equal OldViewPrePrepareReply for example",Task,Medium,Complete,"2019-10-03 13:07:38","2019-10-03 13:07:38",2
"Hyperledger Indy Node","Persist 3PC messages during Ordering","*Problem*   * Let's consider the following example:     ** All 4 nodes prepared a batch with ppSeqNo=1   ** All nodes sent commits, but only the 1st Node got then (and possibly committed/ordered the batch)   ** Other 3 nodes stopped before receiving Commits   ** Only 2 nodes are restarted   ** After restart the 1st Node has a Batch with ppSeqNo=1 in its preprepared and prepared queues.   ** Other 2 nodes don't have it in preprepared and prepared queues (and INDY-2235 will not help to restore it since it hasn't been ordered yet).   ** The 3d Node is still stopped.   ** View Change is started   ** View Change will not be finished, since according to Fig4 from [https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf,] decision procedure can not be complete.   ** The problem that we don't have quorums to complete the decision procedure:   *** F=1 so we can not trust the 1st Node that committed the batch   *** But if we finish the View Change without taking into account the fact that 1st Node may potentially commit the batch, the 1st Node's ledger will be different from others forever    *Acceptance criteria*   * Persist all 3PC messages before sending them to others (or before storing them to preprepared and prepared)   * Restored stored 3PC messages (put to preprepared and prepared) when node starts up (after catch-up is finsihed).   * Write tests (or unskip existing tests showing the problem)",Task,Medium,"In Progress","2019-10-03 10:24:41","2019-10-03 10:24:41",5
"Hyperledger Indy Node","Start catch-up before processing NewView if the node is behind the NewView's checkpoint","*Problem*   * According to Fig 4 in [https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf,] we select the highest Checkpoint that has a weak certificate (F+1 nodes) for NewView.   * The requests will be re-ordered only after this checkpoint   * So, there can be N-F-1 nodes that can not re-order requests (and since view change will not finish correctly) because they haven't ordered yet all requests before the chosen in NewView checkpoint   * As a workaround, we now require a strong certificate (N-F nodes) when calculating the checkpoint (see INDY-2231). This may increase the number of requests to be re-ordered.   * If a new Primary doesn't have the selected checkpoint, than according to INDY-2230 we just don't send a NewView and will wait till timeout to select the next Primary    *References*   * [https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf]   * INDY-2231   * INDY-2230    *The fix*    Quote from `New-View Message Processing` in [https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf]  {quote}It obtains any requests in X that it is missing and if it does not have thecheckpoint with sequence numberh, it also initiates the protocol to fetch themissing state  {quote}  So, we need to start a catch-up till the checkpoint in NewView before going forward.    *Acceptance criteria*   * Start a catchup if the node receives a NewView and doesn't have a checkpoint selected in NewView     *    ** F+1 nodes are enough to finish the catch-up   * Do not finish View Change (wait_for_new_view should still be True) until this catch-up is finished   * Catch-up needs to be done till checkpoint from NewView   * Go back to weak certificate in `NewViewBuilder`'s `calc_checkpoint`     ",Task,Medium,New,"2019-10-03 09:57:16","2019-10-03 09:57:16",5
"Hyperledger Indy Node","Start View change on receiving a quorum of ViewChange messages","*Problem*   * A node may not receive a quorum of InstanceChanges for some reasons, but the rest of the pool may start a View Change.   * Currently the node will have to wait until the quorum of checkpoints from other nodes is received, and do a catchup. Until this is will not be able to order.   * Old view change logic contained to start a view change on a quorum of ViewChangeDone messages (`_next_view_indications `). We need to so something similar with a new view change protocol.    *Acceptance Criteria*   * Start a view change on a quorum (F+1?) of ViewChange messages from other nodes   * Add tests",Task,Medium,Complete,"2019-10-03 09:55:18","2019-10-03 09:55:18",2
"Hyperledger Indy Node","Save PrePrepare's BatchID in audit ledger and restore list of preprepares and prepares on node startup","*Problem*   * In order to successfully finish ViewChange (not go to infinite loop), we need a quorum of `preprepared` and `prepared` messages (P and Q) in consensus shared data.   * We guarantee to get the quorum if all nodes are up and no new nodes are added   * But if nodes are restarted or just added, they don't have `preprepared` and `prepared` filled up.   * Please note, that this can help with requests alкeady ordered, as well as with the new nodes joining the pool. If there are requests in flight (not ordered on all nodes) like in INDY-2238, then this will not help and can still lead to infinite view change. Persisiting of 3PC messages (INDY-2238) looks like the only solution here.    *Acceptance criteria*   * Add PrePrepare's digest to audit ledger   ** See [https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/batch_handlers/audit_batch_handler.py]   ** Write unit tests:   *** [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/audit_ledger/helper.py#L17]   *** [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/audit_ledger/test_audit_ledger_handler.py]   * Restore `preprepared` and `prepared` in consensus shared data when a node starts up (after initial catch-up) from the audit Ledger   ** See [https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/consensus/consensus_shared_data.py]   ** See [https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/consensus/ordering_service.py#L2163]   ** Write unit tests:   *** [https://github.com/hyperledger/indy-plenum/tree/master/plenum/test/consensus/order_service]     * Unskip `test_view_change_after_back_to_quorum_with_disconnected_primary`, make sure it passes   * Consider writing more integration tests, for example:   ** Test1:   *** Delay commits on Node1   *** Send a request and make sure it's ordered on Nodes2-4   *** Restart Nodes2-4   *** Start view change   *** Make sure that view change is finished, and all nodes have equal data   ** Test2:   *** Delay commits on Nodes1 and 2   *** Send a request and make sure it's ordered on Node3 and 4   *** Restart Nodes 3 and 4   *** Start view change   *** Make sure that view change is finished, and all nodes have equal data",Task,Medium,Complete,"2019-10-02 13:52:26","2019-10-02 13:52:26",2
"Hyperledger Indy Node","Do not stabilize checkpoint after the view change if a Replica doesn't have this checkpoint","*Acceptance criteria:*  * Modify `plenum/test/view_change_service/test_lag_by_checkpoint.py` to send just 1 Checkpoint after the View Change  * Do a change in a Checkpointer Service to not stabilize a checkpoint from a NewView if replica doesn't have this checkpoint",Task,Medium,Complete,"2019-09-25 15:27:08","2019-09-25 15:27:08",1
"Hyperledger Indy Node","A Primary lagging behind a stable chedkpoints should not send NewView","*Acceptance criteria:*   1. Write an integration test   * Write CHK_FREQ number of batches on all nodes except the next Primary   * Do a View Change   * Make sure that View Change is finished and viewNo == originalViewNo + 2   * Make sure that pool can order    2. Do changes in code:    * Make sure that a Primary doesn't send NewView if it doesn't have a checkpoint calculated for a NewView",Task,Medium,Complete,"2019-09-25 15:21:08","2019-09-25 15:21:08",1
"Hyperledger Indy Node","A lagging Node need to be able to start catchup by checkpoints when view change is in progress","Let's consider the following situations:  * A node started view change  * A node didn't receive NewView for some reasons while other nodes received it and finished view change  * So, other nodes finished view change and started ordering while a lagging node is still in `waiting_for_new_view` (view change in progress) state.     Expected behavior here is that the lagging node should receive a quorum of 2 stable checkpoints from other nodes and do a catchup which should finish its view change.     *Acceptance criteria:*  1. Write integration tests:  * Delay (without processing) receiving of  NewView by Gamma  * Start a view change and wait until it finished on all nodes except Gamma  * Order by all nodes except Gamma. Order 2 stable checkpoints  * Make sure that Gamma eventually caught up all transactions, finished view change, and can participate in ordering    2. Do the following changes in code:  * Process checkpoints from other nodes during view change  * Make sure that we can start catchup during view change if receive a quorum of 2 stable checkpoints from other nodes  * Make sure that finish of catchup finishes view change as well. ",Task,Medium,Complete,"2019-09-25 14:28:57","2019-09-25 14:28:57",3
"Hyperledger Indy Node","All ledgers in a batch need to be BLS multi-signed","If a batch writes to multiple ledgers (it can be when plugins are installed; see for example https://sovrin.atlassian.net/browse/ST-623), then only the main ledger is BLS multi-signed.  It means that  - GET_TXN for the second ledger will not support BLS multi-signed audit proofs until the second ledger is multi-signed again.   - Get requests for the second ledger will not support BLS multi-signed state proofs until the second ledger is multi-signed again. ",Task,Medium,Complete,"2019-09-24 11:07:37","2019-09-24 11:07:37",1
"Hyperledger Indy Node","Improve automated load test analysis","PoA:    Option 1:  Split pool_get_logs playbook into 2 playbooks for summary and for the whole data and perform jctl and log `grep` remotely, add default metric plotter to setup.py, plot figures remotely, save all results (grepped files and figures) and pull it using the first script. Maybe it will be reasonable to run perf_res_processor script in the first playbook too to avoid 2 separate runs.    Option 2:  Use ansible python3 package instead of yaml playbooks (to keep all actions and logic in python files) and implement 2 scripts described in Option 1 in python not in yaml.",Task,Medium,Complete,"2019-09-16 16:50:51","2019-09-16 16:50:51",3
"Hyperledger Indy Node","Drop ppSeqNo on Backups after View Change","As was discovered in INDY-1336, not dropping ppSeqNo after view change on backup instances may stop instances from ordering if they have been removed by some nodes. ",Task,Medium,Complete,"2019-09-13 17:13:51","2019-09-13 17:13:51",3
"Hyperledger Indy Node","Run production load test to validate moving 3PC Message Request logic into a separate service","Run production load test to validate moving 3PC Message Request logic into a separate service against:  indy-node 1.9.2~dev1082 master  indy-plenum 1.9.2~dev893 master  plugins 1.0.3~dev92 master",Task,Medium,Complete,"2019-09-13 13:27:04","2019-09-13 13:27:04",1
"Hyperledger Indy Node","Recover from a situation when View Change is finished on >= N-F of other nodes","Request `NewView` from a Primary if a Node sees that other nodes finished view change",Task,Medium,Complete,"2019-09-12 10:17:12","2019-09-12 10:17:12",2
"Hyperledger Indy Node","Basic integration tests with a new View Change protocol need to pass","*Acceptance criteria:*   * A new View Change protocol should work   * Basic integration tests for View Change should pass   * Some of existing integration tests can be skipped and processed in INDY-2140   * Some of existing integration tests may be removed or rewritten if they are specific to the old view change protocol",Task,Medium,Complete,"2019-09-12 10:12:01","2019-09-12 10:12:01",8
"Hyperledger Indy Node","Move 3PC Message Request logic into a separate service ","We need to have Node and Replica independent service to process 3PC message requests and responses.    Other services will communicate with it via internal messages and buses.",Task,Medium,Complete,"2019-08-27 16:37:32","2019-08-27 16:37:32",3
"Hyperledger Indy Node","Repeat: Prove production stability of an Indy network","We have made a lot of changes to Indy Node, Indy SDK, and Plugins. We need to repeat the tests from last year and measure improvements.    *Acceptance Criteria*   Perform a test of an Indy network that has the following attributes:   * The ledger is pre-loaded with 1 million transactions   * Pool size at least matches the number of network nodes initially expected in the Sovrin network (currently 25 nodes).   * 1K concurrent clients   * Over a 3 hour period induce a sustained throughput of *15* write transactions per second and 100 read transactions per second on average.   * Write load is a mixture of:   ** writing credentials schema (5%),   ** writing credential definition (5%)   ** revoke registry definition (5%)   ** revoke registry update (5%)   ** write DID to ledger (20%)   ** write payment to ledger (45%)   ** write attrib to ledger (15%)   * Read load is a mixture of:   ** read DID from ledger (45%)   ** read credential schema (10%)   ** read credential definition (10%)   ** read revoke registry definition (10%)   ** read revoke registry delta (10%)   ** read attrib from ledger (10%)   ** read payment balance from ledger (5%)   * Write response time should be less that 5 seconds (would also like a report of the average).   * Read response time should be less than 1 second (would also like a report of the average).  * Fees are enabled using the Sovrin Token plugins.    Near the end of the test, evaluate whether the ledger is keeping pace such that it could likely sustain the load indefinitely, or whether it is falling behind and will need a period to complete processing the queued load.    Any problems found will be logged in JIRA as separate issues for independent prioritization.",Task,High,New,"2019-08-23 01:59:41","2019-08-23 01:59:41",8
"Hyperledger Indy Node","Bump pyzmq to the latest version","pyzmq needs to be updated to the latest version (we use 17.0.0 but the latest version is 18.1.0)    In particular, ZMQ needs to be updated because of the following reasons:   * [https://github.com/zeromq/libzmq/releases/tag/v4.2.4:] fix race condition with ZMQ_LINGER socket option (looks similar to the issues we see)   * [https://github.com/zeromq/libzmq/releases/tag/v4.3.2:] a remote, unauthenticated client connecting to a   libzmq application, running with a socket listening with CURVE   encryption/authentication enabled, may cause a stack overflow and   overwrite the stack with arbitrary data, due to a buffer overflow in   the library. Users running public servers with the above configuration   are highly encouraged to upgrade as soon as possible, as there are no   known mitigations. All versions from 4.0.0 and upwards are affected.   Thank you Fang-Pen Lin for finding the issue and reporting it!",Task,Medium,Complete,"2019-08-22 10:04:33","2019-08-22 10:04:33",3
"Hyperledger Indy Node","1.10.0 Release","*Release Goal*  Release everything that is ready. Especially:  * Improved view change algorithm    *Acceptance Criteria*  Regular instructions:  * Code is tested  * Review upstream releases to decide what should be included in this build  * Build is produced with automated CI / CD  ** Official build is tagged as released in Git  ** Official builds should be the only builds that are not marked in GitHub as pre-release  * Prepare basic documentation on new features  * Prepare the Release Notes  ** Link to the documentation on new features  ** Stored as a CHANGELOG.md in the root of the repo  ** Latest release at the top, above the release notes for all previous versions  * The release should be on GitHub “Releases” tab.  ** Release notes should be in the “Description field of the GitHub release artifact  * The release is marked in JIRA  * Add the correct fixVersion to included issues  * Enable others to use the release:  ** Email stakeholders with links to the artifacts and release notes",Task,Medium,Complete,"2019-08-19 14:32:49","2019-08-19 14:32:49",3
"Hyperledger Indy Node","Integration of Services: Cleanup","The following tech debt is left after integration of Ordering and Checkpointer services into existing code base:   * There are similar checks if a message is for the given instance (`inst_id` checks). It would be great to have a common place with these checks.   * `replica_unstash` needs to be either removed from stashing router, or renamed to something replica-agnostic (something like `unstash_handler`)   * remove `l` prefix from Ordering service methods   * `l_update_watermark_from_3pc` needs to be removed from Ordering Service   * simplify `primaries_batch_needed` logic   * Cleanup internal messages (remove unused ones)   * `requestQueues` needs to be removed from Shared Data   * Cleanup what's stored in Shared Data (make sure that only the data used by more than 1 service is there)   * Remove hooks   * remove process_requested_prepare from Replica   * Fixes in `_bootstrap_consensus_data` (get rid of doing actions twice)   * Remove `to_nodes` from Replica's `send`   * `_init_internal_bus` subscribed twice   * Answer/apply fixes for the comments in test from https://github.com/hyperledger/indy-plenum/pull/1280",Task,Medium,Complete,"2019-08-16 09:48:57","2019-08-16 09:48:57",3
"Hyperledger Indy Node","Implement helper script for AWS pool operations with snapshots, volumes and instances","Implement helper script for AWS pool operations with snapshots, volumes and instances to reduce switching time between upgrade and load AWS setups.",Task,Medium,Complete,"2019-08-15 10:26:10","2019-08-15 10:26:10",2
"Hyperledger Indy Node","Design and implement production-like system test with complex pool creation","Design and implement production-like system test that contains minimal 4 nodes pool installation, 7 or more nodes adding (to increase f several times and provoke VCs), multiple nodes demotion and promotion (to change f several times and provoke VCs) and txns sending during all this actions.",Task,Medium,Complete,"2019-08-15 10:24:35","2019-08-15 10:24:35",2
"Hyperledger Indy Node","Automated load test analysis","*Story*   As a developer or QA analyzing load test results, I need to have all common errors and exception already grepped and reported, so that I can understand faster what's going on.    It would save a lot of time if load script automation could grep all common errors and exception in logs (each Node logs and journalctl).         *Acceptance criteria*    The result of load tests need to have the following information (as a report):   * How many transactions have been written on every node   * How many transaction expected to be written (from the load script point of view)   * How many view changes happened   * Graphs from each nodes   * Metrics summary from each node (`get_metrics` output)   * Get validator info result from each node   * Journalctk analysis:   ** output all exceptions (if any)   * Logs analysis:   ** All ERROR log messages   ** The following patterns:   *** `blacklisting`   *** `invalid BLS signature`   *** `request digest is incorrect`   *** `has incorrect digest`   *** `time not acceptable`   *** `incorrect state trie root`   *** `incorrect transaction tree root`   *** `has incorrect reject`   *** `error in plugin field`   *** `incorrect audit ledger`   *** <to be continued......>",Story,Medium,Complete,"2019-08-15 10:15:31","2019-08-15 10:15:31",3
"Hyperledger Indy Node","1.9.2 Release","*Release Goal*  Release everything that is ready. Mostly bug fixes.    *Acceptance Criteria*  Regular instructions:  * Code is tested  * Review upstream releases to decide what should be included in this build  * Build is produced with automated CI / CD  ** Official build is tagged as released in Git  ** Official builds should be the only builds that are not marked in GitHub as pre-release  * Prepare basic documentation on new features  * Prepare the Release Notes  ** Link to the documentation on new features  ** Stored as a CHANGELOG.md in the root of the repo  ** Latest release at the top, above the release notes for all previous versions  * The release should be on GitHub “Releases” tab.  ** Release notes should be in the “Description field of the GitHub release artifact  * The release is marked in JIRA  * Add the correct fixVersion to included issues  * Enable others to use the release:  ** Email stakeholders with links to the artifacts and release notes",Task,Medium,Complete,"2019-08-15 08:57:51","2019-08-15 08:57:51",3
"Hyperledger Indy Node","Simplify Primary Selection logic",,Task,Medium,Complete,"2019-08-08 10:53:02","2019-08-08 10:53:02",3
"Hyperledger Indy Node","Use production-like environment in simulation tests","Currently simulation tests run in environment which is agnostic of ledgers. However in order to make sure our implementation is really correct these tests need to interact with ledgers (especially audit ledger) in the same way production code interacts with them. In other words - simulation tests should create ledgers and register request handlers with the same code which runs in production.    The problem here is that production code that performs initialization is still tightly coupled with node, which is slow and hard to initialize (due to network, databases and lots of other cruft), so simulation tests would take forever to run. So, we need to decouple node initialization code and make it possible to run with in-memory storages instead of RocksDB    *Acceptance criteria*  * Make ledgers and request handlers initialization code agnostic of node  * Make it possible to use in-memory ledger storage in tests  * Use new initialization code in simulation tests and make sure they are still deterministic and run in sensible time",Task,Medium,Complete,"2019-08-08 10:49:15","2019-08-08 10:49:15",3
"Hyperledger Indy Node","Endorser field can contian a DID with a known role only","If Endorser points to a DID without a specific role (Node role; common user), then it will be unknown who should be charged.    So, Endorser can point to DIDs with known (not-None) roles only such as TRUSTEE. ENDORSER, STEWARD, etc.    Question:  * Can the endorser field contain any defined role (transaction author is an undefined role), or should it only contain DIDs with the role of endorser?",Task,Medium,Complete,"2019-08-02 14:13:51","2019-08-02 14:13:51",1
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.9.1","*Version Information*  indy-node 1.9.1  indy-plenum 1.9.1  sovrin 1.1.52    *Notices for Stewards*  There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.    *Major Changes*  - New DIDs can be created without endorsers  - Transaction authors don't need to be endorsers  - TAA acceptance should use date, not time  - Bug fixes    *Detailed Changelog*    +Major Fixes+  INDY-2164 - Incorrect request validation  INDY-2112 - Need to make reask_ledger_status repeatable  INDY-2143 - When view change takes too long instance change should be sent periodically    +Changes and Additions+  INDY-2171 - New DIDs can be created without endorsers  INDY-2173 - Transaction authors don't need to be endorsers  INDY-2141 - Grab pool data for failed system tests  INDY-2182 - Memory profiling needs to be removed from GET_VALIDATOR_INFO output  INDY-2147 - Implement PBFT viewchanger service with most basic functionality  INDY-2136 - Extract Orderer service from Replica  INDY-2139 - Extract and integrate ConsensusDataProvider from Replica  INDY-2157 - TAA acceptance should use date, not time  INDY-2154 - Clean-up Pluggable Request Handlers    +Known Issues+  -  ",Task,Medium,Complete,"2019-07-31 12:54:15","2019-07-31 12:54:15",1
"Hyperledger Indy Node","Explore python dependencies in ubuntu 20.04 canonical debian repositories","Currently we have set of pinned top level dependencies in both indy-plenum and indy-node. Transitional (dependencies of the dependencies)  dependencies' versions are not managed and might use fuzzy constraints.    Seems canonical repositories for LTS releases are rarely updated for python debian packages it doesn't lead to any observable issues (at least we haven't encountered them yet).    But in ubuntu 20.04 we likely would have new set of versions of that dependencies and that might shift the env.    The general idea is to fix testing env and recommend it for production usage as only one tested. Please refer to tasks INDY-1701 for more details. For python there was no standard way for that in the past but there were a set of options (INDY-1706). It might be different now.    The goal of the task is to make decision:   # is it possible to keep the same dependency list (tree) on both platforms   # dependencies versions for each (both)          ",Task,Medium,New,"2019-07-30 15:12:07","2019-07-30 15:12:07",5
"Hyperledger Indy Node","System testing on CI server in ubuntu 20.04 env","Options:   # nightly build pipeline for ubuntu 20.04   # both nightly and CD pipelines check system tests in ubuntu 20.04 env   # nightly for master, CD only for releases    Acceptance criteria:   # system tests in 20.04 env should be run by CI server automatically   ** We may reduce the frequency of system tests run. For example, one nigh run tests on 16.04 only, and the other night on 20.04 only.   # CD for master shouldn't require much more time than it is now   # CD during release process must check system tests in both env    Notes:   * We need to continue building and testing Ubuntu 16.04 for some period of time, but Ubuntu 20.04 should be our focus. Most automated tests should be with Ubunut 20.04, and Ubuntu 16.04 only needs sanity checks and release tests.",Task,Medium,New,"2019-07-30 14:09:46","2019-07-30 14:09:46",3
"Hyperledger Indy Node","Endorser field must be present in GET requests with state proofs","As of now the only way to check what Endorser was used to submit a transaction is to look at the transaction itself (GET_TXN request). This looks fine from most of the current use cases point of view (for example, Sovrin scripts).    But it would be great to know who was transaction Endorser when querying data via other GET requests with state proofs such as GET_NYM, GET_SCHEMA, GET_CLAIM_DEF, GET_REVOC_REG_DEF, GET_REVOC_REG_ENTRY.",Task,Medium,New,"2019-07-30 14:08:15","2019-07-30 14:08:15",3
"Hyperledger Indy Node","Support ubuntu 20.04 in system tests","Need to update system tests to support ubuntu 20.04 testing env.    Acceptance criteria:   # test pool can be run in ubuntu 20.04 env (all nodes the same env for now)   # client might be run in any env   # no tests regression",Task,Medium,New,"2019-07-30 14:07:44","2019-07-30 14:07:44",3
"Hyperledger Indy Node","Publish indy-node debian artifacts to ubuntu 20.04 distro","Need to update indy-node CD pipeline to publish to both ubuntu 16.04 and 20.04 distros.    Acceptance criteria:   # CD should publish bionic artifacts to the same debian repository as for xenial   # indy-node package should be published along with its 3rd parties dependencies that not presented in canonical repositories   # artifacts should be installable",Task,Medium,New,"2019-07-30 14:04:16","2019-07-30 14:04:16",2
"Hyperledger Indy Node","Publish indy-plenum debian artifacts to ubuntu 20.04 distro","Need to update indy-plenum CD pipeline to publish to both ubuntu 16.04 and 20.04 distros.    Acceptance criteria:   # CD should publish bionic artifacts to the same debian repository as for xenial   # indy-plenum package should be published along with its 3rd parties dependencies that not presented in canonical repositories   # artifacts should be installable",Task,Medium,New,"2019-07-30 14:03:58","2019-07-30 14:03:58",2
"Hyperledger Indy Node","Set up indy-node CI pipeline for ubuntu 20.04 env","Options:   # run 20.04 env along with current 16.04 one   # run 20.04 env nightly for master, CI for PRs checks only 16.04    Acceptance criteria:   # all tests should be run by CI server   # CI process (PRs verification) shouldn't be impacted drastically (much longer)",Task,Medium,New,"2019-07-30 14:01:54","2019-07-30 14:01:54",3
"Hyperledger Indy Node","Set up indy-plenum CI pipeline for ubuntu 20.04 env","Options:   # run 20.04 env along with current 16.04 one   # run 20.04 env nightly for master, CI for PRs checks only 16.04    Acceptance criteria:   # all tests should be run by CI server   # CI process (PRs verification) shouldn't be impacted drastically (much longer)",Task,Medium,New,"2019-07-30 14:01:36","2019-07-30 14:01:36",3
"Hyperledger Indy Node","Support python3.8 by indy-node","Acceptance criteria:   # All indy-node tests should pass in python3.8 env (default in ubuntu 20.04)",Task,Medium,New,"2019-07-30 13:58:02","2019-07-30 13:58:02",3
"Hyperledger Indy Node","Support python3.8 by indy-plenum","Acceptance criteria:   # All indy-plenum tests should pass in python3.8 env (default in ubuntu 20.04)",Task,Medium,New,"2019-07-30 13:57:23","2019-07-30 13:57:23",3
"Hyperledger Indy Node","Move signature verification into Pluggable Req Handlers","Move logic from `ReqAuthenticator` and `ClientAuthNr` etc. into pluggable req handlers.   * A method `verify_signatures(request)` needs to be present in `WriteRequestManager` and `WriteRequestHandler`.   * It can use a common verification logic from one of the WriteRequestHandler's parent classes.   * NymReqHandler needs to override to support new NYM use case (see INDY-2171)     ",Task,Medium,New,"2019-07-24 12:20:05","2019-07-24 12:20:05",3
"Hyperledger Indy Node","Memory profiling needs to be removed from GET_VALIDATOR_INFO output","* GET_VALIDATOR_INFO execution may be quite long (6 sec) and it's done synchronously blocking main execution.   * It affects connections, so Nodes started to disconnect from each other after such long delays.   * The main reason of long execution GET_VALIDATOR_INFO  is memory profiling.   * This can lead to system tests failing where GET_VALIDATOR_INFO is used a lot (see INDY-2171)    *Suggested fix:*   * Remove memory profiling from GET_VALIDATOR_INFO execution   * Do not change output JSON structure, just put N/A into memory-related fields.",Task,Medium,Complete,"2019-07-24 10:35:10","2019-07-24 10:35:10",1
"Hyperledger Indy Node","Integrate Checkpointer Service into Replica",,Task,Medium,Complete,"2019-07-23 10:46:24","2019-07-23 10:46:24",3
"Hyperledger Indy Node","Request missing ViewChange messages when receiving NewView ","*Acceptance criteria:*  * Implement requesting of `ViewChange` messages we don't have a Node for the given `NewView`.    Have a look at https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf, `New-View Message Processing`",Task,Medium,Complete,"2019-07-23 10:40:31","2019-07-23 10:40:31",3
"Hyperledger Indy Node","Use audit ledger in Checkpoints","As of now, Checkpoints calculate digests based on the digests of PrePrepares forming the Checkpoints. It can lead to a problem when the Checkpoint is started not from the beginning. For example we caught-up till ppSeqNo=53, so the current code will start the next Checkpoint from 54 instead of 0. So, this Checkpoint will have different digest comparing to other nodes participating in consensus (these nodes have all 100 PrePrepares forming the Checkpoint, but caught-up node has only 46).   This can be critical in a new view change protocol relying on checkpoints a lot.    With a presence of audit ledger we can use audit ledger's root hash as a digest to solve the issue mentioned above.    *Acceptance criteria*   * Use audit ledger's root hash instead of digests   * Checkpoint's startSeqNo can be equal to always 0. In general, it's not needed at all, but we have to keep it for backward compatibility.   * Simplify checkpoint logic in general",Task,Medium,Complete,"2019-07-23 10:32:11","2019-07-23 10:32:11",5
"Hyperledger Indy Node","Transaction authors don't need to be endorsers","*Story*  As a transaction author, I need my transactions to be written to the ledger preserving me as the author without my needing to accept the responsibilities of an endorser so that I can focus on my business. Instead, I will have a business relationship with an endorser who will endorse my transactions.    *Goals*  * It is easy to tell from the ledger who is the endorser and who is the transaction author for each transaction.  * A transaction author can use a different transaction endorser for future transactions, including updates to attribs and key rotations.  * The transaction must use the author key to sign the transaction author agreement regardless of whether an endorser signature is also needed.    *Acceptance Criteria*  * Analyze the work required.  * Raise issues for completing the work.    *Notes*  * The transaction author field is intended to be used for all future permissions.  * The endorser field only needs to be recorded in order to:  ** Allow auditability, that the transaction was written with correct permissions  ** Allow the network administrator to bill the endorser for the write  * Sub-endorsers or chains of endorsers would be desirable for a global network, but are not required at this time. We recognize that in the future adding multiple levels of endorsers or delegated endorsers could be a breaking change.  ** Endorsers could be a list, but it would still require the transaction author to know all endorsers at the time the transaction is created. That list could be built off-ledger by sharing a draft transaction.  * Suggested user flow:  ** Transaction author identifies the need for creating a transaction  ** Transaction author identifies which endorser they want to use  ** Transaction author includes the DID of the endorser in the transaction, and signs the transaction.  ** Transaction author passes the signed transaction to the endorser  ** Endorser adds their signature  ** Endorser submits the multi-signed transaction to the ledger  ** If the endorser will not accept the transaction, and the transaction author wants to try a different endorser, the transaction author must recreate the transaction in order to include the new endorser DID.  * Token holders can also choose to endorse third party transactions so that they get submitted to the ledger. See [ST-508|https://sovrin.atlassian.net/browse/ST-608]",Story,Medium,Complete,"2019-07-19 07:53:20","2019-07-19 07:53:20",2
"Hyperledger Indy Node","New DIDs can be created without endorsers","*Story*  As an administrator of an Indy network, I want to be able to configure auth_rules such that new network users can write their own nyms to the public ledger so that the ledger can be managed in a completely decentralized way.    *Acceptance Criteria*  * The ledger can be configured so that new DIDs can be anchored without a signature from an entity already on the ledger.  ** The configuration happens in auth_rules.  * The transaction that anchors the DID is signed with the DID that will be added to the ledger.  ** This ensures that any transaction author agreements are signed.  ** This ensures that someone cannot anchor a DID that they do not control with their private keys.    *Notes*  * It is expected that decentralized public write access allows unknown users to anchor their own nym to the ledger if they provide payment through a payment plugin without going through an endorser for approval.  * Allowing anonymous users to anchor nyms will avoid certain ledger checks for validity that could reduce the system's resilience to denial of service attacks, but we don't consider that to be a significant risk. Other levels of DOS protection needs to be instituted by the network administrators regardless of the ledger checks, such as firewall rules to blacklist hostile connections.  * We considered whether we should allow transactions besides nym creation to be allowed without on-ledger signatures because use cases with peer DIDs that could benefit. We decided against doing this work because we don't have customer demand or clear requirements.  * We also considered whether transactions should be allowed without any signatures at all for some development use cases, as previously considered in INDY-2170. But we similarly decided not to do this work without clear demand.  ",Story,Medium,Complete,"2019-07-18 15:28:47","2019-07-18 15:28:47",5
"Hyperledger Indy Node","Integrate OrderingService into Replica","* Most of Replica code will be removed in favor of OrderingService  * Replica will be a bridge between new approach (OrderingService) and legacy Node  * All tests must pass",Task,Medium,Complete,"2019-07-17 13:58:19","2019-07-17 13:58:19",5
"Hyperledger Indy Node","GitLab CI Research","*Acceptance Criteria*   * Learn about GitLab CI   * Evaluate efforts to move Indy SDK from Jenkins to GitLab   * Estimate effort of moving Indy Node from Jenkins to GitLab CI    *Current version of GitLab CI/CD*: 12.1    *The things to check* (requirements for the CI server, filling is in progress):   +CI+  ||Requirement||Supported||Notes||  |build PRs (from forks / branches) on a merged result|{color:#de350b}no{color}|see [comment|https://jira.hyperledger.org/browse/INDY-2168?focusedCommentId=62504&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-62504]|  |pass build statuses back to GitHub|{color:#de350b}no {color}|see [comment|https://jira.hyperledger.org/browse/INDY-2168?focusedCommentId=62504&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-62504]|  |do not use pipeline changes in PRs from untrusted authors (non-maintainers)|{color:#de350b}no {color}|see [link|https://docs.gitlab.com/12.1/ce/ci/merge_request_pipelines/index.html#important-notes-about-merge-requests-from-forked-projects] where they mention that it's not possible (in 12.1)|  |ability to run jobs in parallel and sequentially|yes|[link|https://docs.gitlab.com/12.1/ce/ci/yaml/README.html#stages]|  |(SDK) Ability to share artifacts between jobs|yes|[link1|https://docs.gitlab.com/12.1/ce/ci/caching/index.html#cache-vs-artifacts], [link2|https://docs.gitlab.com/ee/ci/yaml/README.html#dependencies] |  |support dockers|yes|[link|https://docs.gitlab.com/12.1/runner/executors/docker.html] |  |(SDK) support linked jobs running on multiple runners in scope of one pipeline (services)|{color:#de350b}seems no{color} |as far as i understand [services|https://docs.gitlab.com/ee/ci/yaml/README.html#services] are run on the same runner only, but [the kubernetes executor|https://docs.gitlab.com/12.1/runner/executors/kubernetes.html] should be checked as an option|  |(nice to have) support (parse and display in UI) junit-xml style test reports|yes |[link1|https://docs.gitlab.com/12.1/ce/ci/yaml/README.html#artifactsreportsjunit], [link2|https://docs.gitlab.com/12.1/ce/ci/junit_test_reports.html] |    +CD+  ||Requirement|| ||Supported||Notes||  |Keep secrets safe| | | |  | |run CD pipelines only on trusted runners|{color:#de350b}seems no{color}|[supported|https://docs.gitlab.com/12.1/ce/ci/pipelines.html#security-on-protected-branches] only for GitLab repositories protected branches, no support for GitHub |  | |do not run any non-CD pipelines where CD ones are run OR provide strong env isolation|{color:#de350b}no {color}|even for GitLab repositories there is no way to protect pipeline against changes from untrusted PRs (mentioned above)|  | |ability to mask secret values in logs|yes |[link|https://docs.gitlab.com/12.1/ce/ci/variables/README.html#masked-variables] |  |support the following secret types:| | | |  | |secret text (token)|yes|[link|https://docs.gitlab.com/12.1/ce/ci/variables/README.html#variable-types] |  | |username / password pars|yes|as two variables|  | |secret files|yes |[link|https://docs.gitlab.com/12.1/ce/ci/variables/README.html#variable-types] |  | |ssh keys|yes|as one secret file [link|https://docs.gitlab.com/12.1/ce/ci/ssh_keys/README.html] |  | |(nice to have) ssh keys with passphrases|no |only variables and files are [supported|https://docs.gitlab.com/12.1/ce/ci/variables/README.html#variable-types]| |",Task,Medium,Complete,"2019-07-10 14:27:31","2019-07-10 14:27:31",5
"Hyperledger Indy Node","Integrate PrimarySelector into View Change Service","ViewChangerService needs to correctly select next primary taking into account pool state (demoted/promoted nodes, etc.).    So, it needs to do it the same way as PrimarySelector does.",Task,Medium,Complete,"2019-07-10 09:14:48","2019-07-10 09:14:48",2
"Hyperledger Indy Node","Sovrin GPG keys should be updated in all scripts and dockerfiles","Due to recent rotation of sovrin keys all mentions of old key 68DB5E88 needs to be replaced by new one: CE7709D068DB5E88",Task,Highest,Complete,"2019-07-04 11:44:56","2019-07-04 11:44:56",2
"Hyperledger Indy Node","Indy-Node 1.9.1 Release",,Task,Medium,Complete,"2019-07-03 14:09:22","2019-07-03 14:09:22",3
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.9.0","*Version Information*  indy-node 1.9.0  indy-plenum 1.9.0  sovrin 1.1.50      *Notices for Stewards*  There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.  Some nodes can fail to send a REJECT or REPLY to client under specific network conditions. See Known Issues for more details.    *Major Changes*  - Pluggable Request Handlers have been implemented    *Detailed Changelog*        +Major Fixes+      INDY-2144 - Propagates with invalid requests can lead to node crashes      INDY-2142 - There is no validation of the ISSUANCE_TYPE field for the transaction REVOC_REG_DEF      INDY-2083 - Reduce CONS_PROOF timeout to speed up catchup under the load        +Changes and Additions+      INDY-2087 - As a Trustee(s), I need to have a way to set multiple AUTH_RULES by one command      INDY-2127 - Make more system tests to be ready for Indy Node CD pipeline      INDY-1861 - Integrate new handlers into the codebase      INDY-1338 - Define Interfaces needed for View Change Service      INDY-1950 - Rename TRUST_ANCHOR to ENDORSER      INDY-2134 - Update PBFT view change plan of attack      INDY-2131 - Apply a new Docker-in-docker approach for system tests      INDY-2108 - More tests for pluggable request handlers      INDY-1956 - Remove ANYONE_CAN_WRITE      INDY-1290 - [Design] ViewChange protocol must be as defined in PBFT      INDY-1405 - Batch containing some already executed requests should be applied correctly      INDY-2097 - Update Pluggable Req Handlers      INDY-2077 - As a Network Admin, I need to be able to forbid an action in AUTH_RULE, so that no changes in code are needed      INDY-1860 - Create Builders for handlers        +Known Issues+      INDY-2164 - Incorrect request validation",Task,Medium,Complete,"2019-06-27 15:39:56","2019-06-27 15:39:56",1
"Hyperledger Indy Node","TAA acceptance should use date, not time","*Story*  As a user of an Indy network that enforces a transaction author agreement, I do not want my agreement acceptance time to be recorded with my transactions as it is specific enough that it could allow correlation across my various DIDs and payment addresses.    *Acceptance Criteria*  * When an agreement is anchored to the ledger, only the date needs to be recorded.  * When a transaction is written to the ledger, only the date of the agreement acceptance should be recorded on the ledger as part of the transaction.  * If the signed transaction includes too much precision in the timestamp, the transaction should be rejected with the error: TAA timestamp is too precise and is a privacy risk.  * Validation of agreement acceptance should accept any transaction with the same acceptance date as the current agreement was posted to the ledger, or newer.  * If the timestamp when the author agreement was anchored to the ledger is within two minutes of zero UTC, then future transactions with a user agreement acceptance date of the previous day will also be accepted to account for potential clock skew.    *Note*  * The format of the transaction on the ledger does not need to change from a datetime, we will just ignore the timestamp portion of the datatype.",Story,Medium,Complete,"2019-06-24 17:59:42","2019-06-24 17:59:42",3
"Hyperledger Indy Node","Update metadata for PyPI packages","Some of packages in Evernym PyPI account miss such important metadata as license, description... Need to review the following packages data and update:   * indynotifieremail: A tool for stewards of an Indy network to be notified by email when a traffic spike is detected.   * sovrinnotifierawssns: A tool for Sovrin Stewards to receive notifications.   * sovrin-client-rest-dev: A library that can be used by a Sovrin client application to check the status of the validation pool.   * sovrinnotifieremail: A tool for Sovrin Stewards to be notified by email when a traffic spike is detected. Also see the package indynotifieremail.   * sovringui: A helper to build GUIs for checking the Sovrin pool status.    Acceptance criteria:   * ensure that each PyPi listing specifies the Apache license,  thelink to the source code and use the description provided above    Options:    Seems there is only one option:   * reach out the source code (I believe all of them shoould ave some GitHub rpeository)   * update their metadata   * use `twine` to upload changes to PyPI as new releases    Additionally here a quote from PyPI:  {quote}Project description and sidebar  To set the 'indynotifieremail' description, author, links, classifiers, and other details for your next release, use the setup() arguments in your setup.py file. Updating these fields will not change the metadata for past releases. Additionally, you must use Twine to upload your files in order to get full support for these fields. See the Python Packaging User Guide for more help.  {quote}",Task,Medium,New,"2019-06-24 11:26:17","2019-06-24 11:26:17",2
"Hyperledger Indy Node","Review and organise test_misc.py","Review and organise into new test suites test_misc.py test cases.",Task,Medium,Complete,"2019-06-21 13:00:18","2019-06-21 13:00:18",2
"Hyperledger Indy Node","Clean-up Pluggable Request Handlers","Please do the following:   * Remove all old ReqHandlers   * Remove hooks   * Fix updating of ts_store for config ledger after catchup   * Move seq_no_db to DatabaseManager   * Move node_status_db to DatabaseManager   * Go through all the interfaces once more and check for consistency. In particular:   ** remove `dynamic_validation` from ActionReqHandler**   * Move `commitAndSendReplies` into `WriteRequestManager` as much as possible   * Update docs and class diagrams   * Do we need `is_valid_ledger_id` in so many places?   * Move taa validation into `WriteRequestManager`   * Use TaaReqHandler for taa validation (use static methods?)manager   * remove `reqProcessors `   * remove `opVerifiers`   * Incude trackers into request_manager system   * make all TODO items in main's of plugins   * Unify format of chain of responsibility in req_handlers   * Rewrite old unit tests for new handlers (including [https://github.com/sovrin-foundation/token-plugin/pull/261/commits/396190fcbbea8f80f54fe1f184dffa372ab36e9e)|https://github.com/sovrin-foundation/token-plugin/pull/261/commits/396190fcbbea8f80f54fe1f184dffa372ab36e9e]",Task,Medium,Complete,"2019-06-20 16:48:29","2019-06-20 16:48:29",3
"Hyperledger Indy Node","Debug and validation: Pluggable Request Handlers","We need to run more tests to make sure that nothing is broken:   * load tests (with plugins and not) checking all types of read and write requests   * actions (GET_VALIDATOR_INFO, POOL_RESTART)   * check edge cases   ** sending unknown request;   ** sending requests with invalid inputs   * catchup   * pool upgrade",Task,Medium,Complete,"2019-06-20 12:32:27","2019-06-20 12:32:27",5
"Hyperledger Indy Node","Add more system tests to Indy Node CD pipeline","New system tests that are coming would be quite slow and seems their duration unlikely can be changed drastically. Thus, they should be added to CD pipeline conditionally:   * It is not desirable to increase duration of the master CD pipeline since it would increase the load of the CI server   * it must have for release process    Acceptance criteria:   * system tests for master are run partly (only some base and selective checks that are valuable)   * system tests for rc and stable are run completely",Task,Medium,Complete,"2019-06-20 12:09:53","2019-06-20 12:09:53",1
"Hyperledger Indy Node","Nightly builds for Indy Node master with system tests","Current number of available system tests in https://github.com/hyperledger/indy-test-automation repo and their running time is not acceptable for Indy Node master CD pipeline. which is triggered for each commit to master branch.    But we want to be sure that all these tests are passed. Thus it makes sense to run them periodically (nightly). Unit and integration tests might be run as well.    Acceptance criteria:  * build is auto triggered once a day (nightly) on the HEAD of the master branch  * (optional) build can be triggered manually at any time  * all unit/integration/system tests should be run",Task,Medium,Complete,"2019-06-20 11:57:48","2019-06-20 11:57:48",2
"Hyperledger Indy Node","Integrate view change property-based tests into CI",,Task,Medium,Complete,"2019-06-17 10:42:24","2019-06-17 10:42:24",3
"Hyperledger Indy Node","Integrate and run PBFT View Changer simulation  tests with a real implementation","*Acceptance criteria:*   * Property-based tests need to be integrated with a real (no mocks) implementations of      ** View Changer Service   ** 3PC State   ** Orderer service   ** Checkpointer service   ** WriteReqManager   * Database manager needs to use real instances of Ledgers and states (especially audit ledger one). It can use in-memory storages to speed-up tests.   * Network needs to be mocked up.",Task,Medium,Complete,"2019-06-17 10:38:54","2019-06-17 10:38:54",3
"Hyperledger Indy Node","Create simulation tests for catchup services","* Use service and bus oriented approach for catchup  * Cleanup catchup logic  * Write sim tests",Task,Medium,"To Develop","2019-06-17 10:35:50","2019-06-17 10:35:50",8
"Hyperledger Indy Node","Implement PBFT viewchanger service with most basic functionality ","Finalize PBFT View Change Service Implementation. Use interfaces for 3PC State and Network.   * 3PC State <-> Orderer   * 3PC State <->Checkpointer   * 3PC State <-> View Changer   * Networker <-> Orderer   * Networker <->Checkpointer   * Networker <-> View Changer    This better be done using TDD, implementing mocks for network, executor (WriteReqManager), 3PCState, Orderer and Checkpointer as needed.    *Acceptance Criteria:*   * View Changer Implementation",Task,Medium,Complete,"2019-06-17 10:32:09","2019-06-17 10:32:09",5
"Hyperledger Indy Node","Load testing: PBFT View Change",,Task,Medium,Complete,"2019-06-17 08:38:28","2019-06-17 08:38:28",5
"Hyperledger Indy Node","When view change takes too long instance change should be sent periodically","*Summary*   Currently when view change takes too long instance change is sent only once. However due to discarding old instance change messages (>2 hours old) this can sometimes lead to failure to start a view change. This already has happened on BuilderNet (see  INDY-2133)    *Acceptance criteria*   * implement periodic sending of instance changes   * cover by tests",Task,Medium,Complete,"2019-06-14 09:16:16","2019-06-14 09:16:16",3
"Hyperledger Indy Node","Grab pool data for failed system tests","As of now failed system tests are not supplied with any pool data. It makes hard to understand the reasons of failures. It would be useful to grab such data as node logs, journal logs and metrics from the tests pools.    Concerns:   # some failures are not related to pool functionality and client-pool communication but rather to python code (e.g. syntax errors, key/index errors)   # in case of many failures there would be too many data, options:   ** archive data   ** filter data (e.g. by time, by node, by request id)   ** omit some data (e.g. there is a fixture to check failures in journal logs, thus these logs might be omitted if no such errors detected)    Acceptance criteria:   # pool data should be enough to understand system tests failures   # CD pipeline should be updated to attach that data to pipeline results",Task,Medium,Complete,"2019-06-13 14:07:15","2019-06-13 14:07:15",2
"Hyperledger Indy Node","Debug: Integrate PBFT viewchanger service into current codebase","Acceptance criteria:   * Makse sure there are no critical/major issues with View Change   * Unskip tests we added long time ago that failed with the old view change implementation.   * Make sure all existing tests are unskipped/rewritten/removed and pass   ** Remove tests related to old view change if necessary   * Remove code related to old View Change logic         Tasks:   # Unskip all tests (create a ticket if a test can not be unskipped by a reason) - [~<USER>   # Code cleanup [~<USER>   ** Make sure that `primaries_batch_needed ` is used correctly - DONE   ** `future_primary_handler.set_node_state()` is called from at least 4 places - DONE   ** Do we need `self.schedule_view_change_completion_check(self._view_change_timeout)` in `on_view_change_start` - IN PROGRESS   ** Do we need `replica.on_view_change_start()` - IN PROGRESS   ** Do we need `primaryChanged` in replica - TBD   ** Remove `notify_view_change_complete` - TBD   ** VCDone_queue in validator info output - _DONE_   ** Should we still call self.instance_changes.remove_view(self.view_no) - _DONE_   # Refactor Future Primaries batch needed [~<USER> - DONE   # Remove old view change logic [~<USER> - IN PROGRESS   # Unskip view change written long time ago that failed with old protocol [~<USER> - DONE   # Analyze load tests results [~<USER> - DONE",Task,Medium,Complete,"2019-06-13 10:01:28","2019-06-13 10:01:28",8
"Hyperledger Indy Node","Extract and integrate ConsensusDataProvider from Replica",,Task,Medium,Complete,"2019-06-13 09:56:55","2019-06-13 09:56:55",5
"Hyperledger Indy Node","Document PBFT view change protocol","* Add docs for thew new view change protocol (docs, sequence diagrams)  * Update catchup docs (to get rid of dependency between view change and catchup)  * Update audit ledger docs to include info about `digest` and `node_reg`",Task,Medium,Complete,"2019-06-13 09:46:35","2019-06-13 09:46:35",2
"Hyperledger Indy Node","Extract Checkpointer service from Replica","* Checkpointer Service needs to deal with all Checkpoints-related logic (creation, processing and stabilization of checkpoints as well as updating of watermarks)   * Checkpointer Service code needs to be taken from Replica into a separate class   * ConsensusSharedData needs to be used for a common data needed for multiple services (such as Ordering, Checkpointer, View Change)   ** In particular, watermarks need to be there   * No references to Replica or Node classes can be there   * All network communication must be done via ExternalBus   ** It needs to subscribe to Checkpoint messages and process them   * Stashing must be implemented via StashingRouter   * All internal (between services) communication must be done via InternalBus   * Tests must be adopted      Docs  * https://github.com/hyperledger/indy-plenum/blob/master/design/plenum_2_0_architecture_class.png  * https://github.com/hyperledger/indy-plenum/blob/master/design/plenum_2_0_architecture_object.png  * https://github.com/hyperledger/indy-plenum/blob/master/design/plenum_2_0_communication.png",Task,Medium,Complete,"2019-06-13 09:16:11","2019-06-13 09:16:11",3
"Hyperledger Indy Node","Extract Orderer service from Replica",,Task,Medium,Complete,"2019-06-13 09:15:43","2019-06-13 09:15:43",5
"Hyperledger Indy Node","Simulation  tests for View Changer (no integration)","*Summary*    We need to implement property-based tests for the new View Changer.       Original PBFT paper assumes that same batches can be ordered on different nodes with different views. However since we write batch parameters to audit ledger it can diverge on different nodes after view change, which is a big problem.    There could be different solutions to this problem, the most promising idea is not to increase view no of batches that came from old view, and probably add some flag to them. However this is a serious deviation from original protocol and should be analyzed thoroughly.    *Acceptance criteria*   * protocol update should be designed and proved to be safe   * proof should be confirmed by randomized simulation tests  ",Task,Medium,Complete,"2019-06-11 16:11:10","2019-06-11 16:11:10",3
"Hyperledger Indy Node","Update PBFT view change plan of attack","*Acceptance criteria*  High level plan of attack for PBFT view change should be created. It should be based on plans created a year ago and take into account changes that happened since then.",Task,Medium,Complete,"2019-06-11 13:54:25","2019-06-11 13:54:25",2
"Hyperledger Indy Node","Apply a new Docker-in-docker approach for system tests",,Task,Medium,Complete,"2019-06-07 14:24:47","2019-06-07 14:24:47",2
"Hyperledger Indy Node","Hotfix 1.8.1",,Task,Medium,Complete,"2019-06-05 14:22:10","2019-06-05 14:22:10",2
"Hyperledger Indy Node","Make more system tests to be ready for Indy Node CD pipeline","Need to:   * improve system tests reduce running time (especially for *TestAuditSuite.py* and *TestAuthMapSuite.py*), options:   ** replace unnecessary delays with eventually based waiters (currently there a lot of delays based on sleep with quite big sleeping periods)   ** split test modules into parts   * makes tokens related tests optional, options   ** move all tokes tests to separate modules   ** use some conditional flag to skip those tests   * review and organize *test_misc.py*    Acceptance criteria:   # each test module should need not more that a half an hour   # there should be a possibility to run mixed tests modules without tokens related tests ",Task,Medium,Complete,"2019-06-03 12:58:56","2019-06-03 12:58:56",3
"Hyperledger Indy Node","Improve system tests according to review","System tests should be improved according to https://github.com/hyperledger/indy-test-automation/pull/23 review.",Task,Medium,Complete,"2019-05-30 19:34:45","2019-05-30 19:34:45",2
"Hyperledger Indy Node","Publish RC to pypi",,Task,Medium,Complete,"2019-05-24 15:32:18","2019-05-24 15:32:18",1
"Hyperledger Indy Node","More tests for pluggable request handlers",,Task,Medium,Complete,"2019-05-24 10:36:16","2019-05-24 10:36:16",5
"Hyperledger Indy Node","DevOps CI / CD training","The Sovrin Foundation is accepting responsibility for Indy CI / CD. To support this effort, the Evernym team needs to :  * Ensure that a new member of the team understands CI / CD sufficient to replace Andrey.  * Ensure that new members of the team have sufficient permissions to assist with CI / CD in emergency situations.  * Learn GitLab sufficient to support the work of the Sovrin Foundation on CI / CD items.",Task,Medium,Complete,"2019-05-21 15:19:42","2019-05-21 15:19:42",3
"Hyperledger Indy Node","As a Network Admin, I would like to use GET_AUTH_RULE output as an input for AUTH_RULE","* As a Network Admin, I would like to use GET_AUTH_RULE output as an input for AUTH_RULE, so that I don't need to do additional re-formatting.   * I need to use it in 2 cases:   ** GET_AUTH_RULE(specific rule) -> AUTH_RULE (set specific rule)   ** GET_AUTH_RULE(all rules) -> AUTH_RULES (set all rules)\    *Acceptance criteria*   * Modify GET_AUTH_RULE output to match AUTH_RULE input   * Fix tests   * Add tests to check that GET_AUTH_RULE outputs can be used as AUTH_RULE input for 2 cases above",Story,Medium,Complete,"2019-05-21 10:01:38","2019-05-21 10:01:38",2
"Hyperledger Indy Node","Add missing tests for state proof","* Add tests to check State Proof for Revocation txns   * Add tests to check State Proof for Auth Rule txns",Task,Medium,Complete,"2019-05-17 10:07:30","2019-05-17 10:07:30",1
"Hyperledger Indy Node","Update Pluggable Req Handlers ","* Integrate all recent commands into pluggable request handlers   ** TAA   ** AML   ** AUTH_RULES",Task,Medium,Complete,"2019-05-17 09:49:27","2019-05-17 09:49:27",5
"Hyperledger Indy Node","libindy needs to be used for AUTH_RULE and GET_AUTH_RULE in Indy-Node's tests",,Task,Medium,Complete,"2019-05-17 09:40:23","2019-05-17 09:40:23",1
"Hyperledger Indy Node","Indy-Node 1.9.0 Release",,Task,Medium,Complete,"2019-05-17 08:55:54","2019-05-17 08:55:54",3
"Hyperledger Indy Node","Extend load script to have all Auth Rules in config ledger","*Acceptance criteria*   * All Auth Rules needs to be overridden (by default constraints) so that they are put into config ledger. It means that an AUTH_RULE txn needs to be sent for every action during start-up.",Task,Medium,Complete,"2019-05-17 08:21:44","2019-05-17 08:21:44",2
"Hyperledger Indy Node","Extend load script with GET_TAA, GET_AML, GET_AUTH_RULES","*Acceptance criteria*   * Load script needs to be able to to send GET_AUTH_RULE   * Load script needs to be able to to send GET_TAA   * Load script needs to be able to to send GET_AML",Task,Medium,Complete,"2019-05-17 08:18:49","2019-05-17 08:18:49",1
"Hyperledger Indy Node","Extend load script with TAA","*Acceptance criteria*   * There needs to be am AML and TAA written at the beginning of the load   * All requests need to be signed against TAA.   * Make sure that TAA/AML is sent only once",Task,Medium,Complete,"2019-05-17 08:13:54","2019-05-17 08:13:54",1
"Hyperledger Indy Node","As a Trustee(s), I need to have a way to set multiple AUTH_RULES by one command","As a Trustee(s), I need to have a way to set multiple AUTH_RULES by one command, so that it's easier for adding/changing auth rules and requires less communication with the Ledger.    *Acceptance criteria*   * Define the command format to add/edit multiple auth rules (from 1 to all)   * The auth rules override previous ones   * Cover by tests",Story,Medium,Complete,"2019-05-15 10:59:34","2019-05-15 10:59:34",2
"Hyperledger Indy Node","As a Network Admin, I need to have a validation against invalid settings of AUTH_RULES, so that the Network can not be moved to a state when no txns are accepted","Example of validation that can be added to AUTH_RULE txn:   * Check that the required number of TRUSTEES set is not more than the number of Trustees signed the AUTH_RULE txn.     ",Task,Medium,New,"2019-05-13 18:02:24","2019-05-13 18:02:24",2
"Hyperledger Indy Node","The owner for RevocRegDef must be the author of the corresponding CredDef","* As of now, changing the `owner` in REVOC_REG_DEF's AuthRule doesn't have any affect.   * However, it may make sense to consider the REVOC_REG_DEF's owner as the owner of the corresponding CredDef to fulfill the following use cases:   ** As a Network Admin, I need to be able to set an auth constraint for RevocRegDef, so that its creation/editing can be done only by the author of the corresponding CredDef",Task,Medium,New,"2019-05-08 15:59:28","2019-05-08 15:59:28",2
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.7.1","*Version Information:*  indy-node 1.7.1  indy-plenum 1.7.1  sovrin 1.1.41    *Notices for Stewards:*  (!) *There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*  (!) *Pool upgrade to sovrin 1.1.32 and above should be performed simultaneously for all nodes due to txn format changes.*  (!) *Pool upgrade to indy-node 1.7.1 should be performed simultaneously for all nodes due to audit ledger.*  (!) *There should be no fees set up.*    *Major Changes*  - Audit Ledger   -- helps keeping all other ledgers in sync   -- helps recovering of pool state by new or restarted nodes   -- can be used for external audit  - Correct support of multi-signatures  - Configurable Auth Rules in config state  - Stability fixes    *Detailed Changelog*    +Fixes:+  INDY-2008 - Validator-info doesn't show view change information and sometimes shows node info as unknown  INDY-2020 - Schema can't be written with error 'Version' object has no attribute 'dev'  INDY-2018 - Node fails to start after the load  INDY-2022 - POA: Sovrin TestNet lost consensus  INDY-2047 - Nodes can fail on first start after upgrading from version without audit ledger to version with audit ledger  INDY-2035 - Pool is getting out of consensus after a forced view change and writes to all the ledgers  INDY-1720 - View Change processing - replica ends up with incorrect primaries  INDY-2031 - Validator node shows False for consensus  INDY-2060 - Watermarks may not be updated correctly after view change by a lagging node  INDY-2061 - ATTRIB doesn't have auth rules in auth map  INDY-2050 - Some nodes are stalled and throw an error under load  INDY-2055 - Some nodes failed to join consensus after upgrade    +Changes and Additions:+  INDY-1946 - Implementation: Restore current 3PC state from audit ledger  INDY-1992 - Implementation (not active): As a user/steward I want to have better understanding of release version and changelog  INDY-2001 - Implement auth rule maps in config ledger  INDY-1944 - Add audit ledger  INDY-1984 - INSTANCE_CHANGE messages should be persisted between restarts  INDY-2006 - Add updateState method for ConfigReqHandler  INDY-2002 - Use auth constraints from config ledger for validation  INDY-1945 - Implementation: Improve catch-up to use audit ledger for consistency  INDY-2003 - Implement a command to set auth constraints  INDY-1995 - Debug and validation: Move the auth_map structure to the config ledger  INDY-1554 - Need to enhance write permissions for Revocation transactions  INDY-2010 - Implement a command to get auth constraint  INDY-2016 - Integrate testinfra-based system tests to Indy CD  INDY-2019 - Debug and Validation: As a user/steward I want to have better understanding of release version and changelog  INDY-2028 - As a QA I want system tests to be run in parallel in CD pipeline  INDY-1993 - Debug and Validation: Audit Ledger and improving catch-up to use audit ledger for consistency  INDY-1757 - Need to track same transactions with different multi-signatures  INDY-2025 - Debug and Validation: Restore current 3PC state from audit ledger - Phase 1  INDY-1983 - A Node need to be able to order stashed requests after long catch-ups  INDY-1674 - Need to account fields from PLUGIN_CLIENT_REQUEST_FIELDS when calculating digest  INDY-2046 - Debug and validation: Multi-signature support  INDY-2051 - Debug and Validation: Restore current 3PC state from audit ledger - Phase 2",Task,Medium,Complete,"2019-04-25 11:04:50","2019-04-25 11:04:50",1
"Hyperledger Indy Node","As a Network Admin, I need to be able to forbid an action in AUTH_RULE, so that no changes in code are needed","*Acceptance Criteria*   * Support an Auth Constraint that forbids an action   * Apply this to Schema editing   * Cover by tests",Story,Medium,Complete,"2019-04-24 15:53:53","2019-04-24 15:53:53",2
"Hyperledger Indy Node","Transaction Author Agreement: debug and validation",,Task,Medium,Complete,"2019-04-24 09:07:29","2019-04-24 09:07:29",5
"Hyperledger Indy Node","Update docs for Transaction Author agreement","* Update `requests.md` and `transactions.md`   ** Add info about new TAA fields   ** Add info about the new txns and requests for TAA and AML   * Update `auth_map.md` with the information about the `owner` field",Task,Medium,Complete,"2019-04-24 09:05:55","2019-04-24 09:05:55",1
"Hyperledger Indy Node","Validate transaction author agreement as part of consensus","*Acceptance criteria*   * The author agreement is must have for all Domain transactions   * Plugins must be able to specify for what ledgers the author agreement is also must have   * Enhance dynamic validation as follows:   ** If this is DOMAIN txn, or a Plugin txn from a ledger for which TAA is required - process. Otherwise - OK.   ** Get the latest TAA (using 'last_taa' key in state)   ** If there is no TAA - OK   ** If the TAA's text is empty - OK   ** Get the latest AML (using 'last_aml' key in state)   ** If there is no AML - REJECT   ** Get the TAA's hash and compare with the one in the request. If they are not equal - REJECT   ** Get the request's timestamp. Make sure that the ts is in the interval [TAA's ts - 2 mins; current PP time + 2 mins]. If not - REJECT   ** Get the requests' acceptance mechanism string. Make sure that it's present in the latest AML. If not - REJECT     * Cover by tests",Task,Medium,Complete,"2019-04-24 09:05:17","2019-04-24 09:05:17",3
"Hyperledger Indy Node","Support Transaction Author Agreement in Write Requests","*Acceptance criteria*   * Support agreement setting in the Request and transaction   ** Define new request format with an optional values in every Request:   *** hash of the transaction author agreement   *** acceptance timestamp   *** agreement mechanism   ** Define new transaction format with new fields   ** Modify signature verification to take into account new fields (they all are part of signature)   * Cover by tests",Task,Medium,Complete,"2019-04-24 09:04:05","2019-04-24 09:04:05",1
"Hyperledger Indy Node","Get Transaction Author Agreement Acceptance Mechanisms from the Config Ledger","*Acceptance criteria*   * GET_TAA_AML request   * Input parameters:   ** ts - optional timestamp string   ** version- optional string   * Static validation:   ** only 0 or 1 parameters are supported   * How it works   ** If no parameters - get the latest TAA_AML from state   ** If `ts` is present - get the config state_root fro the given `ts` from the ts_store, and then get the latest TAA_AML for this state_root   ** If `version` is present - get the TAA_AML by `tag` from the state   * Cover by tests",Task,Medium,Complete,"2019-04-24 08:57:01","2019-04-24 08:57:01",2
"Hyperledger Indy Node","Cover all AUTH_RULE changes by integration tests","*Acceptance criteria*   * Write a helper method (to be re-used in plugins) to change multiple AUTH_RULEs     *    ** list of auth rules as an input   * Write a test for every Action that does the following:   ** checks that the default rule works   ** changes Auth rule for the action   *** consider more complex auth rules with multi-sigs   ** checks that the new Auth rule works and the old one doesn't work   * There need to be a map 'action -> test' as an output        # Adding new TRUSTEE                  *Done*   # Adding new STEWARD                *Done*   # Adding new TRUST_ANCHOR     *Done*   # Adding new NETWORK_MONITOR      *Done*   # Adding new Identity Owner        *Done*   # Change Trustee to Steward        *Done*   # Change Trustee to Trust Anchor        *Done*   # Change Trustee to Network Monitor        *Done*   # Demote Trustee        *Done*   # Change Steward to Trustee        *Done*   # Change Steward to Trust Anchor        *Done*   # Change Steward to Network Monitor        *Done*   # Demote Steward        *Done*   # Change Trust Anchor to Trustee        *Done*   # Change Trust Anchor to Steward        *Done*   # Change Trust Anchor to Network Monitor        *Done*   # Demote Trust Anchor        *Done*   # Change Network Monitor to Trustee        *Done*   # Change Network Monitor to Steward        *Done*   # Change Network Monitor to Trust Anchor        *Done*   # Demote Network Monitor        *Done*   # Promote roleless user to Trustee        *Done*   # Promote roleless user to Steward        *Done*   # Promote roleless user to Trust Anchor        *Done*   # Promote roleless user to Network Monitor        *Done*   # Assign Key to new DID   # Key Rotation - Artem O *Done*   # Adding new Schema -  Artem O *Done*   # Editing Schema -  Artem O not handled by auth_map for now   # Adding new CLAIM_DEF transaction -  Artem O *Done*   # Editing CLAIM_DEF transaction -  Artem O *Done*   # Adding new node to pool - Renata *Done*   # Adding new node to pool with empty services - Renata *Done*   # Demotion of node - Renata *Done*   # Promotion of node - Renata *Done*   # Changing Node's ip address - Renata *Done*   # Changing Node's port - Renata *Done*   # Changing Client's ip address - Renata *Done*   # Changing Client's port - Renata *Done*   # Changing Node's blskey - Renata *Done*   # Starting upgrade procedure - Artem *Done*   # Canceling upgrade procedure - Artem *Done*   # Restarting pool command - Artem *Done*   # Pool config command (like a <code>read only</code> option) - Artem *Done*   # Change authentification rules - Artem *Done*   # Getting validator_info from pool - Artem *Done*   # Adding new REVOC_REG_DEF   # Editing REVOC_REG_DEF   # Adding new REVOC_REG_ENTRY   # Editing REVOC_REG_ENTRY",Task,Medium,Complete,"2019-04-24 08:52:37","2019-04-24 08:52:37",3
"Hyperledger Indy Node","Write Transaction Author Agreement Acceptance Mechanisms to the Config Ledger","*Acceptance criteria*   * Create a new TAA_AML transaction in the config ledger for the Transaction Author Agreement Acceptance Mechanism   ** version   ** list of   *** label: 64 chars string   *** description: 256 chars string   ** url     * Static Validation:   ** the list can not be empty   * Dynamic validation:   ** version needs to be unique   * The mechanisms need to be stored in State as well   ** Store the latest AML   ** Store the AML by version   * Extend the validation of TAA txn:   ** It can be added to the ledger only if there is already a TAA_AML txn present   * Cover by tests",Task,Medium,Complete,"2019-04-24 08:14:13","2019-04-24 08:14:13",3
"Hyperledger Indy Node","Get Transaction Author Agreement from the config ledger","*Acceptance criteria*   * Support a read request to get the Transaction Author Agreement from the config ledger   * Request format   ** Name: GET_TAA   ** input parameters:   *** version - optional string (max 256)   *** ts - optional timestamp string   *** hash - optional hash string   * Static validation:   ** there can be either 0 or 1 input parameter   * State proof with BLS multi-sig must be returned   * How it works:   ** no parameters - get the latest TA   *** get hash by last_key in config state   *** get value by last_taa   ** hash -get by TAA's hash   *** get the value from state   ** version - get the TAA for the given version     *** get a hash from state, and then the value by hash   ** ts - get the latest TAA for the given timestamp   *** get the config state_root for the given ts   *** get last_taa value   * Cover by tests",Task,Medium,Complete,"2019-04-24 08:00:50","2019-04-24 08:00:50",2
"Hyperledger Indy Node","Write Transaction Author Agreement to Config Ledger","*Acceptance Criteria*   * Create a new Transaction Author agreement transaction in the Config Ledger   ** Txn payload:   *** 'tag': <any string 256 chars max>   *** 'text': <any string>   ** Hash is sha256 calculated against concatentation of 'tag' and 'text':   `version || text`     * Store it in the Config State:   ** {{taa:h:<hash>}} -> <TAA payload>   ** {{:taa:v:<version>}} -> {{<hash>}}   ** {{:taa:latest}} -> {{<hash>}}   * Static validation:   ** 'hash' must match the content   * Dynamic validation:   ** 'tag' must be unique   * Add config txns into ts_store   ** Update ts_store on every write into config ledger   ** key: <CONFIG_PREFIX>:<ts>   ** value: state_root_hash   * Add TAA to the auth map   ** default rule: 3 (TBD) Trustees   * Cover by tests          ",Task,Medium,Complete,"2019-04-24 07:53:34","2019-04-24 07:53:34",5
"Hyperledger Indy Node","Remove out-of-date PyPi packages","Evernym PyPI's account holds dozens outdated packages: plenum, plenum-dev, indy-plenum-dev, indy-node-dev, anoncreds ones, some other tmp/dev variations of plenum and node. It makes sense to review and clean.    Questions:  * Should we review the packages first?  * Is there additional clean-up that is needed, such as out-of-date build scripts?",Task,Medium,Complete,"2019-04-23 16:45:53","2019-04-23 16:45:53",1
"Hyperledger Indy Node","As a Netwrok admin, I need to have a command to set multiple auth rules","As of now, we have AUTH_RULE txn that can set/modify one AUTH_RULE.    However, there are cases (such as initial bootstrap), when we need to set multiple (or even all) auth rules.    In order to simplify and improve usability, especially in the case of multi-signature, we need to have a command to set multiple auth rules.    Option:   # Modify existing AUTH_RULE command so that it supports a list of rules to be set   ** Pros:   *** Easier and more consistent   # Creates a new AUTH_RULES command to set multiple rules   ** Pros   *** More granular in terms of auth rule permissions for AUTH_RULE/AUTH_RULES, so that we may potentially have different policies on who can change each rule.",Story,Medium,Complete,"2019-04-22 08:08:40","2019-04-22 08:08:40",3
"Hyperledger Indy Node","Improve dockerfiles for system tests","Requirements:   * dockerfiles should accept parameters   * system tests CD pipeline should configure docker builds instead of patching",Task,Medium,Complete,"2019-04-18 13:47:43","2019-04-18 13:47:43",1
"Hyperledger Indy Node","Support docker-in-docker for docker client in CD","We should support docker-in-docker for docker client in CD to set up and tear down test docker pools from system tests and get rid of using jenkinsfile and ~/indy-node/environment/docker/pool scripts for this actions.",Task,Medium,Complete,"2019-04-18 13:47:38","2019-04-18 13:47:38",3
"Hyperledger Indy Node","Catch-up should take into account state of other nodes when sending requests","*Problem description*  Current catch-up code doesn't take into account ledger states of other nodes when sending catch-up requests. This can lead to some nodes receiving requests for transactions that they don't have yet. In this case they just reject request, which leads to much longer catch-up since leecher node waits for reply until timeout and then resends request to different nodes.     *Proposed solution*  Use information from LEDGER_STATUS and CONSISTENCY_PROOF messages gathered by ConsProofService as well as from audit ledger to make sure catch-up requests are sent to nodes that can really fulfill them.  ",Task,Medium,Complete,"2019-04-11 13:23:11","2019-04-11 13:23:11",3
"Hyperledger Indy Node","Debug and Validation: Restore current 3PC state from audit ledger - Phase 2","Continue debug and validation started in the scope of INDY-2025   * Load tests:   ** acceptance load (1943) with forced view change (/)   ** acceptance load with fees (1943) (!)   ** Restart n-f-1 non-primary nodes (!) INDY-2060   *** Start acceptance load as in 1983   *** Wait for 1 min   *** Provoke view change (stop 1st Node; make sure view has changed; start it back)   *** Restart n-f-1 nodes at the same time (Nodes 10-25)   *** Make sure that every node participates in consensus   *** Stop the load   *** Make sure that all nodes have equal data   ** Restart n-f-1 primary nodes (/)   *** Start acceptance load as in 1983   *** Wait for 1 min   *** Provoke view change (stop 1st Node; make sure view has changed; start it back)   *** Restart n-f-1 nodes at the same time (Nodes 1-16)   *** Make sure that every node participates in consensus   *** Stop the load   *** Make sure that all nodes have equal data   ** Restart more than n-f nodes with disabled watchdog (/)   *** Set ENABLE_INCONSISTENCY_WATCHER_NETWORK to False   *** Start acceptance load as in 1983   *** Wait for 1 min   *** Provoke view change (stop 1st Node; make sure view has changed; start it back)   *** Restart more than n-f nodes at the same time (Nodes 1-20)   *** Make sure that every node participates in consensus   *** Stop the load   *** Make sure that all nodes have equal data",Task,Medium,Complete,"2019-04-11 10:56:44","2019-04-11 10:56:44",3
"Hyperledger Indy Node","Do not send 3PC batches for all the ledgers at the same time","When we write to multiple ledgers all the time, we create a 3PC batch for all the ledgers at the same time, which increases the probability of out-of-order messages, stashes, lagging nodes, and different time of view change start on nodes.    *Acceptance criteria*   * Do not send 3PC batches for all the ledgers at the same time   * Send every next 3PC batch in 1 sec, as for the case of writing to 1 ledger.   * Freshness updates should also follow this logic and be sent for every ledger not earlier than in 1 sec.",Task,Medium,New,"2019-04-10 10:02:22","2019-04-10 10:02:22",3
"Hyperledger Indy Node","Debug and validation: Multi-signature support",,Task,Medium,Complete,"2019-04-09 14:45:59","2019-04-09 14:45:59",3
"Hyperledger Indy Node","Sanity check on requests, which payload_digests already written","As far as we consider prePrepares with repeated payload_digests as malicious, we may have situation, when some double signed requests get stuck in requests queue and sender would not ever get reply.    We need to do some kind of sanity check to clear such a requests (after ordering or using schedule)",Task,Medium,Complete,"2019-04-09 12:01:40","2019-04-09 12:01:40",3
"Hyperledger Indy Node","A node needs to start a view change for the maximum viewNo it has a quorum of InstanceChanges for","Test: test_catchup_to_next_view_during_view_change_0_to_2",Task,Medium,New,"2019-04-05 16:19:04","2019-04-05 16:19:04",2
"Hyperledger Indy Node","Modify dynamic validation to satisfy multisignature rules","Modify signature validation to satisfy multisignature rules",Task,Medium,Complete,"2019-04-05 14:47:09","2019-04-05 14:47:09",3
"Hyperledger Indy Node","As a QA I want system tests to be run in parallel in CD pipeline","As a QA I want system tests to be run in parallel in CD pipeline to reduce test run time.",Task,Medium,Complete,"2019-03-18 09:51:36","2019-03-18 09:51:36",1
"Hyperledger Indy Node","Debug and Validation: Restore current 3PC state from audit ledger - Phase 1","PR: [https://github.com/hyperledger/indy-plenum/pull/1096]    *The following needs to be tested*   # Regression testing   ** Load tests:   *** acceptance load (1943) no fees   *** NYM txns (10 writes per sec) with forced view change   *** acceptance load (1943) with forced view change   *** load to all ledgers with forced view change   # Non-primary Node(s) join the pool after restart   ** Docker / system tests (pool with 7 nodes):   *** Restart 1 Node no load:   **** send 5 txns; viewNo=0; restart the 6th node  => node set correct viewNo=0 and ppSeqNo=5;   send more txns => node participates in ordering   **** do view change so that viewNo=1; send 5 txns; restart the 6th node => node set correct viewNo=1 and ppSeqNo=5;   send more txns => node participates in ordering   **** do view change so that viewNo=1; stop the 4th node; send5 txns; start 6th Node => node set correct viewNo=1 and ppSeqNo=5;    send more txns => node participates in ordering   *** Restart 1 Node with load:   **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart the 6th node=> node set correct viewNo=1 and ppSeqNo;   node participates in ordering with the current load   **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; stop 6th node; wait for 1 minute; start the 6th Node=> node set correct viewNo=1 and ppSeqNo;   node participates in ordering with the current load   *** Restart master primary no load   **** do view change so that viewNo=1; send 5 txns; restart the 2d node (primary) => node set correct viewNo=1 and ppSeqNo=5;   send more txns => node participates in ordering   *** Restart master primary with load   **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart the 2d node (primary) => node set correct viewNo=1 and ppSeqNo;   node participates in ordering with the current load   *** Restart all nodes at the same time no load:   **** do view change so that viewNo=1; send 5 txns; restart all nodes (simultaneously)  => all nodes set correct viewNo=1 and ppSeqNo=5;   send more txns => nodes participate in ordering   *** Restart n-f-1 nodes no load:   **** do view change so that viewNo=1; send 5 txns; restart Nodes4-7 (simultaneously)  => all nodes set correct viewNo=1 and ppSeqNo=5;   send more txns => nodes participate in ordering   *** Restart f nodes with load   **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart Nodes 6 and 7 (simultaneously) => nodes set correct viewNo=1 and ppSeqNo;   nodes participates in ordering with the current load   *** Restart n-f-1 nodes with load:   **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart Nodes4-7  (simultaneously) => all nodes set correct viewNo=1 and ppSeqNo;   nodes participates in ordering with the current load   *** Restart all nodes at the same time with load:   **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart all nodes  (simultaneously) => all nodes set correct viewNo=1 and ppSeqNo;   nodes participates in ordering with the current load   *** Restart all nodes one by one no load:   **** do view change so that viewNo=1; send 5 txns; restart all nodes 1 by 1 (from Node1 till Node 7) => all nodes set correct viewNo=1 and ppSeqNo;   nodes participates in ordering with the current load   *** Restart all nodes one by one with load:   **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec;; restart all nodes 1 by 1 (from Node1 till Node 7) => all nodes set correct viewNo=1 and ppSeqNo;   nodes participates in ordering with the current load   ** Load testing on 25 Nodes AWS pool   *** Restart all nodes (/)   **** Start acceptance load as in 1983   **** Wait for 1 min   **** Provoke view change (stop 1st Node; make sure view has changed; start it back)   **** Restart all nodes at the same time   **** Make sure that every node participates in consensus   **** Stop the load   **** Make sure that all nodes have equal data   *** Restart 1 by 1 (/)   **** Start acceptance load as in 1983   **** Wait for 1 min   **** Provoke view change (stop 1st Node; make sure view has changed; start it back)   **** Restart nodes 1 by 1 (Node1 - Node25)   **** Make sure that every node participates in consensus   **** Stop the load   **** Make sure that all nodes have equal data   *** Restart f non-primary nodes (/)   **** Start acceptance load as in 1983   **** Wait for 1 min   **** Provoke view change (stop 1st Node; make sure view has changed; start it back)   **** Restart f nodes at the same time (Nodes 18-25)   **** Make sure that every node participates in consensus   **** Stop the load   **** Make sure that all nodes have equal data   *** Restart f primary nodes (/)   **** Start acceptance load as in 1983   **** Wait for 1 min   **** Provoke view change (stop 1st Node; make sure view has changed; start it back)   **** Restart f nodes at the same time (Nodes 1-8)   **** Make sure that every node participates in consensus   **** Stop the load   **** Make sure that all nodes have equal data   *** Restart n-f-1 non-primary nodes   **** Start acceptance load as in 1983   **** Wait for 1 min   **** Provoke view change (stop 1st Node; make sure view has changed; start it back)   **** Restart n-f-1 nodes at the same time (Nodes 10-25)   **** Make sure that every node participates in consensus   **** Stop the load   **** Make sure that all nodes have equal data   *** Restart n-f-1 primary nodes   **** Start acceptance load as in 1983   **** Wait for 1 min   **** Provoke view change (stop 1st Node; make sure view has changed; start it back)   **** Restart n-f-1 nodes at the same time (Nodes 1-16)   **** Make sure that every node participates in consensus   **** Stop the load   **** Make sure that all nodes have equal data   *** Restart more than n-f nodes with disabled watchdog   **** Set ENABLE_INCONSISTENCY_WATCHER_NETWORK to False   **** Start acceptance load as in 1983   **** Wait for 1 min   **** Provoke view change (stop 1st Node; make sure view has changed; start it back)   **** Restart more than n-f nodes at the same time (Nodes 1-20)   **** Make sure that every node participates in consensus   **** Stop the load   **** Make sure that all nodes have equal data   # Backup primary continues ordering after restart   ** Docker / system tests (pool with 7 nodes):   *** do view change so that viewNo=1; send 5 txns; restart the 3d node (primary on instance 1)   send more txns => instance 1 orders transactions   ** Load testing on 25 Nodes AWS pool   *** Start acceptance load as in 1983   *** Wait for 1 min   *** Provoke view change (stop 1st Node; make sure view has changed; start it back)   *** Restart the 3d node (primary on instance 1)   *** Make sure that instance 1 orders transaction and not removed   # Backup non-primary continues ordering after restart   ** Docker / system tests (pool with 7 nodes):   *** do view change so that viewNo=1; send 5 txns; restart the 6th node (non-primary on instance 1)   send more txns => Node 6 orders on instance 1   ** Load testing on 25 Nodes AWS pool   *** Start acceptance load as in 1983   *** Wait for 1 min   *** Provoke view change (stop 1st Node; make sure view has changed; start it back)   *** Restart the 6th node (non-primary on instance 1)   *** Make sure that Node 6 orders on instance 1   # Correct primaries after demotion and promotion leading to changes of F (6 -> 7; 7 ->6; etc.)   ** See INDY-1720   ** Docker / system tests (pool with 7 nodes):   *** Demote non-primary   **** do view change so that viewNo=1; send 5 txns;   **** Demote Node 7   **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order   *** Demote master primary   **** do view change so that viewNo=1; send 5 txns;   **** Demote Node 2 (master primary)   **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order   *** Demote backup primary   **** do view change so that viewNo=1; send 5 txns;   **** Demote Node 3 (primary on instance 1)   **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order   ** Docker / system tests (pool with 6 nodes):   *** Add a node   **** do view change so that viewNo=1; send 5 txns;   **** Add 7th node   **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order   *** Promote a non-primary Node   **** do view change so that viewNo=1; send 5 txns;   **** Demote Node 7   **** Promote Node 7   **** send more txns => make sure that view has changed twice, all nodes have the same primaries on all instances, and the pool can order   *** Promote master primary   **** do view change so that viewNo=1; send 5 txns;   **** Demote Node 3 (master primary for viewNo 2)   **** Wait till view changed to 2   **** Promote Node 3   **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order   *** Promote backup primary   **** do view change so that viewNo=1; send 5 txns;   **** Demote Node 4 (backup primary for viewNo 2)   **** Wait till view changed to 2   **** Promote Node 4   **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order",Task,Medium,Complete,"2019-03-15 12:26:07","2019-03-15 12:26:07",5
"Hyperledger Indy Node","Debug and Validation: As a user/steward I want to have better understanding of release version and changelog ",,Task,Medium,Complete,"2019-03-12 13:02:43","2019-03-12 13:02:43",5
"Hyperledger Indy Node","Integrate testinfra-based system tests to Indy CD","Options:  1. Switch all tests to ssh testinfra backend to connect pool nodes (need to configure ssh on jenkins machine).  2. Use docker-in-docker for indyclient contatiner and save docker testinfra backend.",Task,High,Complete,"2019-03-01 13:51:18","2019-03-01 13:51:18",3
"Hyperledger Indy Node","Debug and validation: Apply new request handlers approach to Token Plugins",,Task,Medium,Complete,"2019-02-27 09:30:31","2019-02-27 09:30:31",5
"Hyperledger Indy Node","Stable release 1.8.0",,Task,Medium,Complete,"2019-02-27 09:07:47","2019-02-27 09:07:47",3
"Hyperledger Indy Node","Debug and validation: Apply new multi-signature approach to Token Plugins",,Task,Medium,Complete,"2019-02-27 09:06:10","2019-02-27 09:06:10",3
"Hyperledger Indy Node","Stable release 1.7.1",,Task,Medium,Complete,"2019-02-27 09:03:50","2019-02-27 09:03:50",3
"Hyperledger Indy Node","Implement a command to get auth constraint","As an administrator (Trustee?) changing auth rule permissions on the network,  I need to be able to know what is the current permission (constraint) for every action (rule), so that I can use this information to update the permission.      *Acceptance criteria*   * Implement a command to get the current auth constraint for a particular auth rule     *    ** Command: GET_AUTH_RULE   ** Input: auth rule fields (the same as in AUTH_RULE cmd)   ** Output: auth constraint   ** Who can send: anyone   * (Optional) Implement a command to get all auth constraints for all auth rules, that is get the whole auth map   ** Command: GET_ALL_AUTH_RULES   ** Input: no   ** Output: auth map (auth_rule_id -> auth constraint)   ** Who can send: anyone",Task,Medium,Complete,"2019-02-27 08:43:36","2019-02-27 08:43:36",3
"Hyperledger Indy Node","Make simulation tests of view change use actual ViewChanger code","*Acceptance criteria*  * ViewChanger should be testable in full isolation  ** It shouldn't directly depend on node  ** It shouldn't directly depend on system time  * Simulation tests written for view change model in scope of INDY-1897 should use actual ViewChanger",Task,Medium,Complete,"2019-02-24 14:11:08","2019-02-24 14:11:08",5
"Hyperledger Indy Node","Add updateState method for ConfigReqHandler ","As of now, we don't have updateState method into ConfigReqHandler. In this case, after node's starting up we don't have actual auth constraints into config state.  h4. Acceptance criteria:   * Add updateState method into ConfigReqHandler (indy-node side)   * Create test, like:  ** Send command to change auth constraint  ** Restart node  ** Check, that state include already changed constraint",Task,Medium,Complete,"2019-02-21 13:34:01","2019-02-21 13:34:01",2
"Hyperledger Indy Node","Stub: Issue with tracker and fees","Stub for ST-511",Task,Medium,Complete,"2019-02-21 11:19:05","2019-02-21 11:19:05",3
"Hyperledger Indy Node","Implement a command to set auth constraints","See INDY-1732 for details    *Acceptance criteria*   * The command should change the constraint for the given rule_id   * There should be a rule for sending this command. Initially it's defined locally, but the command can be used to override it in config ledger.   * Default rule is that the command can be sent by 1 TRUSTEE",Task,Medium,Complete,"2019-02-20 11:31:25","2019-02-20 11:31:25",3
"Hyperledger Indy Node","Use auth constraints from config ledger for validation","See INDY-1732 for details.    *Acceptance criteria*   * Use constraints from config ledger for validation   * If there is no constraint for the rule in the config ledger, then default (local) value should be used. Initially all constraints are local.   * Consider using a cache instead of asking state trie each time   * cover by tests     ",Task,Medium,Complete,"2019-02-20 11:30:56","2019-02-20 11:30:56",3
"Hyperledger Indy Node","Implement auth rule maps in config ledger","See INDY-1732 for details    *Acceptance criteria*   * It should be possible to have constraints for every rule in config ledger   * Rules (keys in auth map) are static and can not be changed by the user; only the constraints are configurable. So, rule_id can be a key in the state, and constraint be the value   * If there is no constraint for the rule in the config ledger, then default (local) value should be used. Initially all constraints are local.   * Correctly serialize/deserialize constraints (json?) in a proper format   * Cover by tests",Task,Medium,Complete,"2019-02-20 11:28:34","2019-02-20 11:28:34",2
"Hyperledger Indy Node","POA: Transaction authors don't need to be endorsers","*Story*  As a transaction author, I need my transactions to be written to the ledger preserving me as the author without my needing to accept the responsibilities of an endorser so that I can focus on my business. Instead, I will have a business relationship with an endorser who will endorse my transactions.    *Goals*  * It is easy to tell from the ledger who is the endorser and who is the transaction author for each transaction.  * A transaction author can use a different transaction endorser for future transactions, including updates to attribs and key rotations.  * The transaction must use the author key to sign the transaction author agreement regardless of whether an endorser signature is also needed.  * If the endorser field is included in a transaction, then the ledger will reject the transaction if it is not signed by the endorser.    *Acceptance Criteria*  * Analyze the work required.  * Raise issues for completing the work.    *Notes*  * The transaction author field is intended to be used for all future permissions.  * The endorser field only needs to be recorded in order to:  ** Allow auditability, that the transaction was written with correct permissions  ** Allow the network administrator to bill the endorser for the write  * Sub-endorsers or chains of endorsers would be desirable for a global network, but are not required at this time. We recognize that in the future adding multiple levels of endorsers or delegated endorsers could be a breaking change.  ** Endorsers could be a list, but it would still require the transaction author to know all endorsers at the time the transaction is created. That list could be built off-ledger by sharing a draft transaction.  * Suggested user flow:  ** Transaction author identifies the need for creating a transaction  ** Transaction author identifies which endorser they want to use  ** Transaction author includes the DID of the endorser in the transaction, and signs the transaction.  ** Transaction author passes the signed transaction to the endorser  ** Endorser adds their signature  ** Endorser submits the multi-signed transaction to the ledger  ** If the endorser will not accept the transaction, and the transaction author wants to try a different endorser, the transaction author must recreate the transaction in order to include the new endorser DID.  * This process is separate from a potential process by which payment could be attached to a third party transaction so as to allow it to be written to the ledger. That potential feature of the Sovrin Network is being tracked in [ST-608|https://sovrin.atlassian.net/browse/ST-608]. In other words, transaction endorsers and authors don't need to be tracked separately for Sovrin XFER transactions.",Story,Medium,Complete,"2019-02-18 14:49:28","2019-02-18 14:49:28",2
"Hyperledger Indy Node","Add all system tests to Indy CD (preparation)","Things to do:    1. Add all existing system tests that can be run on each build in pipeline to Indy CD pipeline.  2. Run system tests against 7 nodes docker pool instead of 4 nodes docker pool (if possible).  3. Use docker_setup_and_teardown fixture to run separate pool for VC and consensus tests (if possible). ",Task,High,Complete,"2019-02-18 14:01:20","2019-02-18 14:01:20",3
"Hyperledger Indy Node","Stable release 1.7.0","The next Pool upgrade (to 1.7) can be forced to avoid some issues with audit ledger.",Task,Medium,Complete,"2019-02-14 14:18:56","2019-02-14 14:18:56",3
"Hyperledger Indy Node","Debug and validation: Move the auth_map structure to the config ledger","See INDY-1732 for details",Task,Medium,Complete,"2019-02-14 12:12:14","2019-02-14 12:12:14",2
"Hyperledger Indy Node","Stub: Load tests with FEEs","This is a stub for https://sovrin.atlassian.net/browse/ST-507",Task,Medium,Complete,"2019-02-14 09:42:55","2019-02-14 09:42:55",5
"Hyperledger Indy Node","Debug and Validation: Audit Ledger and improving catch-up to use audit ledger for consistency","See details in INDY-1945",Task,Medium,Complete,"2019-02-13 12:23:28","2019-02-13 12:23:28",5
"Hyperledger Indy Node","Implementation (not active): As a user/steward I want to have better understanding of release version and changelog ","Currently release versions of indy-node looks like semver but actually doesn't follow it having build numbers as patch part of the version.     Requirements:   * 3rd number of the version should have a real patch meaning (follow semver)   * versions of different branches of the code should be clearly related   * indy-node should be easily linked to indy-plenum version for both dev and stable code   * there should be clear relation between artifacts and code",Story,Medium,Complete,"2019-02-08 13:43:18","2019-02-08 13:43:18",5
"Hyperledger Indy Node","INSTANCE_CHANGE messages should be persisted between restarts","As a result of investigation done in scope of INDY-1897 it was found that one way to avoid situations like in INDY-1903 is to actually persist INSTANCE_CHANGE messages between nodes restarts. However this can potentially bring problems if viewNo is not persisted as well, so this can be potentially blocked by INDY-1946.    *Acceptance criteria*  * Write tests checking for edge cases which can arise when INSTANCE_CHANGE messages are persisted and viewNo is not  * If it turns out that in order to pass these tests INDY-1946 is required then wait until this issue is done  * Implement and test persisting of received INSTANCE_CHANGE messages to some local database  * (Optional) Implement and test garbage collection of old INSTANCE_CHANGE messages so that database doesn't grow indefinitely",Task,Medium,Complete,"2019-02-01 15:01:54","2019-02-01 15:01:54",5
"Hyperledger Indy Node","A Node need to be able to order stashed requests after long catch-ups","*Problem*   * A node is doing a long catchup (>10mins) whike the pool continues ordering   * The node successfully stashes all 3PC messages received during catch-up and tries to re-apply them once catch-up is finished   * However, since catch-up took more than 10 mins, the current time on the node will be at least 10 mins greater than the one in stashed PrePrepares.   * So, the PrePrepare will be discarded as having incorrect time.    *Acceptance criteria*   * Write tests simulating the issue   * The PrePrepares processed (unstashed) after the catch-up should be discarded or applied because of incorrect time the same way as if there were no catch-up.    *Possible fix*   * Create a map PrePrepare -> receiving_time   * Every PrePrepare stashed during the catch-up will be put there   * Once a PrePrepare is being processed, we need to have a look if there is received time for it in the map, and, if so, pop it and use for the ppTime validation   * Make sure that the map is cleared properly in GC. so there is no OOM",Task,Medium,Complete,"2019-02-01 10:09:55","2019-02-01 10:09:55",3
"Hyperledger Indy Node","Integration tests to verify auth rules for ATTRIB","Add tests to ATTRIB's Auth Rules (the same way as in Done in https://jira.hyperledger.org/browse/INDY-2070)",Task,Medium,Complete,"2019-02-01 08:02:59","2019-02-01 08:02:59",1
"Hyperledger Indy Node","Integration tests to verify auth rules for NYM","In scope of INDY-1957 integrations tests were added but they don't cover all possible cases.    Should check all cobinations of the following:   * signer's role:  any of indy_common.roles.Roles   * signer is: the same as dest, creator, other   * dest's role in ledger: any of indy_common.roles.Roles   * dest's role in NYM: omitted, any of indy_common.roles.Roles   * dest's verkey in ledger: None, val   * dest's verkey in NYM: omitted, same as in ledger, new",Task,Medium,Complete,"2019-02-01 07:58:29","2019-02-01 07:58:29",2
"Hyperledger Indy Node","Stub: Stability fixes for plugins catchup and UTXO cache","A stub for ST-499    https://sovrin.atlassian.net/browse/ST-499",Task,Medium,Complete,"2019-01-31 13:08:22","2019-01-31 13:08:22",3
"Hyperledger Indy Node","Problem with VCStartMsgStrategy","*Steps to Reproduce:*    1) A node is started.    2) The node receive InstanceChange for viewNo = 3 and collect Quorum to start View Change 2 -> 3. The node calls _on_verified_instance_change_msg() and starts prepare_view_change process.    3) The node begins to process FutureViewChangeDone (CurentState early) and starts propagate primary for view no 3.  View_change_in progress = True, catchup is started.    4) The node process ViewChangeContinueMessage in the method on_view_change_continued() but proposed_view_no = 3 and  replica.node.viewNo = 3. It means that message will not be processed and prepare_view_change  will not be finished. As a result a new InstanceChanges will be received but condition for a new view change will not be called because is_preparing is False still.         *Acceptance Criteria:*    To solve this problem we need to collect InstanceChanges but don't process them before propagate primary. And process InstanceChanges after.   * Add test to reproduce this case   * Fix a bug with processing InstanceChanges before propagate primary. If a node starts with a big lag and pool has 2 view changes one by one then the started node should does catch up, propagates primary, does view change for the last view and start ordering.    *Logs:*    ev@evernymr33:logs/1949_25_01_2019_metrics.tar.gz    ev@evernymr33:logs/1949_25_01_2019.tar.gz",Task,Medium,Complete,"2019-01-28 13:55:34","2019-01-28 13:55:34",3
"Hyperledger Indy Node","Sample configuration for multi-sig permissions","The default deployment of Indy should have liberal permissions so that developers do not have unnecessary hurdles to learning the system. No multi-signature should be required.    But Indy should include a sample transaction that configures the permissions in INDY-1729 in order demonstrate how multi-signature ledger permissions can be configured.    *TODO: Need to define*   * A single trustee can setup a permission scheme   * Once the permission scheme is established, it will define how many trustees are required to change the scheme (3 as the default in the sample transaction)         Examples of rules requiring a multi-signature (that can be used by default in Indy deployments such as Sovrin):   * Three trustees are required to   ** create a new trustee   ** remove a trustee   * Two trustees are required to:   ** create a steward   ** revoke the role of a steward   ** demote a steward's nodes   ** can write an upgrade transaction to the ledger",Task,Medium,Complete,"2019-01-21 14:51:22","2019-01-21 14:51:22",3
"Hyperledger Indy Node","Update technical documentation with Audit Ledger concept",,Task,Medium,Complete,"2019-01-21 09:16:33","2019-01-21 09:16:33",1
"Hyperledger Indy Node","Signature validation must be in-sync on all nodes","h2. *Problem description*   * Signatures are validated by nodes as part of static validation, that is at the time when either a request or a first Propagate is received.   * Signature validation depends on the current uncommitted state of the domain ledger, since it's validated against the keys associated with DIDs in domain ledger.   * Different nodes may have different domain uncommitted state because of the lags in the network so that some nodes are behind.   * So, some nodes may reject (NACK) the request because of invalid signature, and some nodes may accept it (ACK).   * Also if request's signature is valid, it will not be re-checked anymore when corresponding Propagates are received.    h2. *Decision matrix*       | |Not propagated to Primary|Propagated to Primary|  |Propagated to  non-primary|Case 1, 2|OK|  |Not propagated to  non-primary|Case 3|Case 4, 5|    **       h2. *Consequences*   * *Case 1: A request stays in Request queue and not ordered for some time*   **  Description:   *** A request is accepted (ACK) by a (non-master-primary) Node which is behind the primary (hasn't yet applied the recent uncommitted state where the request is rejected).   *** Other nodes reject the request   *** So, there is no quorum of PROPAGATEs   *** The request on the node which accepted it will not be removed from the requests queue until it's cleared by timeout   *** => potential risk of OOM   **  Severity   *** Low   *** There are not so high chances for such situations, and the number of such requests is not significant, so the chances of OOM are pretty low   *** Also we have a sanity check to clean the requests queue from time to time     * *Case 2: Unequal number of requests ordered by Master and non-master Instances*   **  Description:   *** A request is rejected (NACK) by a Node which is behind the master primary (hasn't yet applied the recent uncommitted state where the request is rejected).   *** This node is a primary on a backup instance   *** Other nodes accept the request   *** So, there is a quorum of PROPAGATEs   *** The node that rejected the request rejects the PROPAGATEs as well since it's still behind the master primary   *** The backup instance where this node is a primary will not order the request, while master will   *** => the backup instance is faster   **  Severity   *** Low   *** There are not so high chances for such situations, and the number of such requests is not significant   *** However, this is a source of potential false-positive view changes     * *Case 3: A client rotates the key and then sends the txn using the new key, and this txn is rejected with invalid signature*   ** Descriptions:   *** A request is accepted (ACK) by the Primary (BTW the Primary's uncommitted state is always up-to-date and can be used as a reference when validation signatures)   *** The request is rejected (NACK) by n-f non-primaries which are behind the primary (haven't yet applied the recent uncommitted state where the request can be accepted).   *** => Client receives a quorum of NACKs and assumes that request is not ordered.   ** Severity:   *** Medium   *** If the client re-sends the same request, the request will be ordered successfully     * *Case 4: Message Requests for Propagates*   ** Description:   *** A request is accepted (ACK) by the Primary (BTW the Primary's uncommitted state is always up-to-date and can be used as a reference when validation signatures)   *** A request is accepted by n-f nodes in total and propagated to n-f nodes including the Primary   *** A request is rejected and not propagated to f nodes which are behind the primary (haven't yet applied the recent uncommitted state where the request can be accepted).   *** Primary starts ordering the request   *** Non-primaries who haven't propagated the request send MessageRequest for the corresponding Propagates   *** If the Propagates are received at the time when signature validation passes (the node applied the same state as the Primary), then the node can continue ordering.   *** If the Propagates are received at the time when signature is still considered to be invalid (the node hasn't yet applied the same state as the Primary), then the Propagates will not be requested anymore, and the Node stops ordering until the quorum of stashed stable checkpoints and catch-up.   ** Severity:   *** Low   *** The situation is quire rare and the message request saves the node     * *Case 5: Node stops ordering until catch-up*   ** Description:   *** See Case 4   **  Severity:   *** High    h2. *How to fix*   * Option1: move signature verification to dynamic validation, so that the same (proposed by the primary) uncommitted state is used   ** Cons:   *** A source of attacks, so that anyone can send a lot of invalid requests   ** Pros:   *** Easy to implement   *** Fixes all the issues   * Option 2: TBD",Task,Medium,New,"2019-01-21 08:54:28","2019-01-21 08:54:28",5
"Hyperledger Indy Node","Add docker node to persistent pool for upgrade tests","The last two upgrades have had the IBM node spamming the other nodes config ledger. They have their node running in a docker. We need to add it to the pool.",Task,High,Complete,"2019-01-18 11:53:32","2019-01-18 11:53:32",2
"Hyperledger Indy Node","Confirm that Trust Anchor role works as intended","Our requirements around the Trust Anchor role has evolved a lot over the last year, and we need to confirm that the current implementation will meet the needs of the organizations who are now ready to write to the Sovrin Ledger.    *Acceptance Criteria*   Confirm that the Trust Anchor role has the following behaviors, and raise tickets for any functionality that deviates from these requirements:   * Any steward or trustee can create the role.   * Any trustee can revoke the role.   * Holders of this role can demote themselves (remove the role).   * Anyone with the role should be able to write to the ledger without proof of payment.   * If a ANYONE_CAN_WRITE (INDY-1528) is false, this role is required to write to the ledger.   * If ANYONE_CAN_WRITE is true, then anyone can write to the ledger with or without this role.   * The plugin interface supports plugins such that if ANYONE_CAN_WRITE is true, writes can be allowed without this role only if payment is attached, and can be allowed without a payment if the author has this role. (Check the Sovrin payment plugins to ensure they work this way. Issues with the plugin implementation should be raised in the Sovrin jira.)   * This document reflects these rules:   [https://github.com/hyperledger/indy-node/blob/master/docs/auth_rules.md]   * Make sure that all necessary integration and system tests exist    *Notes*   * The current Trust Anchor role will soon be called Endorser (INDY-1950)   * ANYONE_CAN_WRITE does not need to be tracked on the config ledger for now (INDY-1956)   * This role does not need to be able to authorize 3rd party transaction authors; for now all authors must have this role. (INDY-1563)",Task,Medium,Complete,"2019-01-18 00:35:57","2019-01-18 00:35:57",3
"Hyperledger Indy Node","Remove ANYONE_CAN_WRITE","The flag for ANYONE_CAN_WRITE is currently tracked in the configuration file deployed to each node. If an admin were to change this flag on a single node, bad things would happen.    It is preferable to have this option set on a network-wide basis using the config ledger.    UPD: The new Multi-signature approach doesn't require this flag at all.    *Acceptance criteria:*   * Either remove this flag from code (most likely) or move it to config ledger if it's really needed          ",Task,Medium,Complete,"2019-01-18 00:24:41","2019-01-18 00:24:41",2
"Hyperledger Indy Node","As a user, I need to be able to know what was the last update time of the ledger when querying a txn via GET_TXN request","Build Info:   indy-node 1.6.760    Steps to Reproduce:   1. Send GET_TXN to any ledger (including domain) for any txn.   2. Check reply.    Actual Results:   There is no 'state_proof' field in the reply so state proof and freshness functionality doesn't affect pool and config ledgers since there are no special get txns for this ledgers.  Actually since GET_TXN is based on Ledger's Merkle Tree, not Patricia State Trie, the more correct term here is `audit_proof`.    Expected Result:  - There should be 'audit_proof' field in the reply for GET_TXN (see `{color:#333333}handle_get_txn_req{color}` in `node.py`).  - The field content needs to be the same as for `state_proof` in other get requests (see [https://github.com/hyperledger/indy-node/blob/master/docs/source/requests.md#reply-structure-for-read-requests).]   - {{`proof_nodes}}` need to be the actual audit proof which can be obtained by `{color:#333333}merkleInfo{color}` method in `ledger.py`    Notes:  * Ensure that the pluggable request handler for GET_TXN is properly called.",Task,Medium,Complete,"2019-01-17 14:01:18","2019-01-17 14:01:18",3
"Hyperledger Indy Node","Rename TRUST_ANCHOR to ENDORSER","The term TRUST_ANCHOR is no longer being used to refer to permission to write to the ledger. Instead it is used to refer to the organization sponsoring a governance framework around credentials that are trusted by a specific industry. (See the Sovrin Foundation glossary for more details.)    The role required to write to the ledger will now be called ENDORSER.    *Acceptance Criteria*  * TRUST_ANCHOR role is called ENDORSER    Not included:  * Changes to the ENDORSER role to allow write delegation to transaction authors. The expectation is that all transaction authors must have the endorser role in order to write.",Task,Medium,Complete,"2019-01-16 14:52:25","2019-01-16 14:52:25",1
"Hyperledger Indy Node","Move logic from ActionReqHandler","Move logic from ActionReqHandler(both in node and plenum) to separate implementations of QueryHandler, TransactionHandler, BatchHandler and ActionHandler (according to step 2 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#])",Task,Medium,Complete,"2019-01-16 08:31:31","2019-01-16 08:31:31",1
"Hyperledger Indy Node","Implementation: Restore current 3PC state from audit ledger","*Acceptance criteria*  * viewNo and ppSeqNo should be included into transactions in audit ledger  * current viewNo and ppSeqNo should be restored from audit ledger    *Additional notes*  * This is based on design described in https://github.com/hyperledger/indy-plenum/blob/master/design/catchup-and-audit-proposal.md  * It might be safer to implement this after INDY-1945, but probably they can be done in parallel",Task,Medium,Complete,"2019-01-16 07:09:00","2019-01-16 07:09:00",5
"Hyperledger Indy Node","Implementation: Improve catch-up to use audit ledger for consistency","*Acceptance criteria*  * Catch-up should be improved roughly as described in https://github.com/hyperledger/indy-plenum/blob/master/design/catchup-and-audit-proposal.md#catch-up:  ** LEDGER_STATUS is asked for Audit ledger only, returning its last seq_no (let’s call it audit_seq_no)  ** after figuring out that catch-up is needed node sends REQUEST_CONSISTENCY_PROOF consisting of:  *** ledger_id  *** minimum/maximum audit_seq_no  ** node that receives REQUEST_CONSISTENCY_PROOF:  *** given min/max audit_seq_no finds corresponding min/max seq_no inside ledger defined by ledger_id (thanks to Audit ledger it’s O(1) operation)  *** sends CONSISTENCY_PROOF message as usual  ** then catch-up proceeds as usual  * It might make sense to try to be closer to current logic by sending CONSISTENCY_PROOFs in response to LEDGER_STATUS (thus not requiring explicit REQUEST_CONSISTENCY_PROOF message), however this will require sending multiple CONSISTENCY_PROOFs per one LEDGER_STATUS, which feels like a questionable design decision and can potentially lead to some unexpected edge cases. This requires some additional analysis as part of PoA  ",Task,Medium,Complete,"2019-01-16 07:07:11","2019-01-16 07:07:11",5
"Hyperledger Indy Node","Add audit ledger","*Acceptance criteria*  * New ledger should be created with new AUDIT_LEDGER_ID  * Each transaction should include dictionary ledger_id : last_seq_no  * For each 3PC batch ordered on master instance transaction should be written to audit ledger  * Audit ledger should participate in catch-up  * Relevant tests should be implemented    *Additional notes*  * This is based on design described in https://github.com/hyperledger/indy-plenum/blob/master/design/catchup-and-audit-proposal.md  ",Task,Medium,Complete,"2019-01-16 07:06:17","2019-01-16 07:06:17",5
"Hyperledger Indy Node","Transaction Author Agreement","*Story*  As a steward of an Indy network, I want transaction authors to agree that the information they submit to the ledger meets the requirements outlined by ledger governance for the current transaction to minimize the liability that stewards accept when storing the data on an immutable ledger.    *Acceptance Criteria*  * An author agreement which covers all transaction types is anchored to the domain ledger or plugin ledgers. (Might also include the hash to save on future computation.)  * The text is available to be looked up from the ledger for audit purposes.  * When the author agreement is written to the ledger, it must be signed by a configurable number of TRUSTEEs.  * New versions of the agreement can be added to the ledger, but previous versions are retained.  * The transaction format on the domain ledger has a field that contains a hash indicating the acceptance of the author agreement. This hash is signed when the transaction is signed with the private key of the DID that will make the future writes to the ledger.  * The digital signature and hash is created by the transaction author who is submitting the transaction and explicitly ties the transaction to the specific agreement that was accepted in order to submit the transaction.  * The transaction will only be accepted with the current version of the agreement.  * The transaction format should record the timestamp when the agreement was accepted by a user in a UI.  * The timestamp must be in a UTC ISO format. It should pass basic validation: that it is not more than two minutes in the future (allowing for minor clock skew), or more than two minutes before the time the current agreement was submitted to the ledger. If it has an invalid time, the transaction should be rejected with a clear error: Transaction rejected because the timestamp for accepting the author agreement is invalid.  * The transaction format should include a field that contains the label of one of the agreement acceptance mechanisms enumerated on the list of valid values.  * The list of valid agreement acceptance mechanisms must be anchored on the config ledger. Each mechanism includes a label (max 64 characters) and a description (max 256 characters).  Initial mechanisms will include:  ** Label: Product EULA, Description: Included in the EULA for the product being used.  ** Label: Service Agreement, Description: Included in the agreement with the service provider managing the transaction.  ** Label: Click Agreement, Description: Agreed through the UI at the time of submission.  ** Label: Session Agreement, Description: Agreed at wallet instantiation or login.  * The list of valid agreement acceptance mechanisms includes a link to an off-ledger document containing additional information about the acceptance mechanisms such as how the agreement must be displayed in order to be legally meaningful.  * Documentation exists for how to put the transaction author agreement and agreement acceptance mechanism list onto the config ledger.  * If there is no Transaction Author Agreement registered on the ledger, then transactions should be accepted with an empty hash, empty acceptance timestamp, and empty agreement mechanism. (This is the new developer testing use case.)  * If there is a Transaction Author Agreement registered on the ledger, but no list of valid agreement mechanisms, then all transactions should be rejected.    *Questions*  Q. Is it sufficient to accept the agreement at the time a new DID is written to the ledger rather than for each transaction?  A. No it is not sufficient. We need confidence that users remember their acceptance for each transaction they right. We also want to avoid having to create a mechanism for having users re-accept the agreement when it changes.    Q. Is the signature of acceptance of the transaction author agreement an additional signature, or is the current transaction signature sufficient? Is it a new set of keys, or is it the same set of keys necessary for submitting the transaction?  A. The transaction author agreement is signed with the same set of keys that signs the transaction for submission to the ledger. As a result, the current transaction signature is sufficient. When we separate transaction authors from endorsers (INDY-1999), we will have to ensure that the author key is still used to sign the author agreement.    Q. Do we need acceptance for the Transaction Author Agreement for every transaction type, or just transactions that allow writing arbitrary data to the ledger?  A. We need acceptance of the agreement for all transaction types, as encryption keys are considered personal data and require acceptance of the agreement.    Q. Do plugin ledgers require the same agreement as writes to the domain ledger?  A. Yes. We expect to have a single agreement for all transactions for the foreseeable future.    Architecture question:  * What transaction type is best?  * Which ledger is best to store this transaction?",Epic,Medium,Complete,"2019-01-15 22:44:19","2019-01-15 22:44:19",8
"Hyperledger Indy Node","Stable release 1.6.83",,Task,Medium,Complete,"2019-01-14 11:34:29","2019-01-14 11:34:29",3
"Hyperledger Indy Node","Implementation: Apply new multi-signature approach to Token Plugins","Stub for ST-509",Task,Medium,Complete,"2018-12-21 08:37:32","2018-12-21 08:37:32",8
"Hyperledger Indy Node","Implementation: Apply new request handlers approach to Token Plugins","Stub for ST-510",Task,Medium,Complete,"2018-12-21 08:36:41","2018-12-21 08:36:41",5
"Hyperledger Indy Node","As a user of Valdiator Info script, I need to know whether the pool has write consensus and when the state was updated the last time","*Acceptance Criteria:*   * Add the time when state was updated for every ledger (assuming that INDY-933 is done) into validator info output   * This needs to be present in the verbose and json output, but not in the default output.",Task,Medium,Complete,"2018-12-21 08:29:43","2018-12-21 08:29:43",2
"Hyperledger Indy Node","Fix documentation related to old client","*Acceptance criteria*   # Remove any mentioning of old client from [https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/README.md]   # Update [https://github.com/hyperledger/indy-node/blob/master/README.md#how-to-install-a-test-network]   # Mention at the beginning of [https://github.com/hyperledger/indy-node/blob/master/docs/start-nodes.md] that the recommended way of starting a Pool is to use Docker (with the corresponding reference to Docker doc)   # Mention at the beginning of [https://github.com/hyperledger/indy-node/blob/master/docs/start-nodes.md] that the recommended way of starting a Pool is to use Docker (with the corresponding reference to Docker doc)   # Mention at the beginning of [https://github.com/hyperledger/indy-node/blob/master/docs/start-nodes.md|https://github.com/hyperledger/indy-plenum/blob/master/docs/start_nodes.md] that the recommended way of starting a Pool is to use Docker (with the corresponding reference to Docker doc)   # Remove  indy-running-locally.md from indy node docs   # Remove cluster-simulation.md from indy node docs",Task,Medium,Complete,"2018-12-19 12:25:51","2018-12-19 12:25:51",1
"Hyperledger Indy Node","PoA: ledger synchronization","*Acceptance criteria:*    Create a PoA for the following requirements:    Requirement 1: No synchronization issues between any ledgers    Requirement 2: Auditability/history. Must have for domain/config. Nice to have for all combinations.",Task,Medium,Complete,"2018-12-19 09:06:11","2018-12-19 09:06:11",3
"Hyperledger Indy Node","Hotfix 1.6.82",,Task,Medium,Complete,"2018-12-19 07:46:31","2018-12-19 07:46:31",2
"Hyperledger Indy Node","Network maintenance role","*Story*  As a systems administrator responsible for solutions built on top of an Indy network, I want to be able to monitor the health of the network to ensure that my solutions will be reliable.    As a steward, I want to be able to setup network maintenance without having to use my steward keys in a dangerous manner.    Notes:  * This user is not a steward on the network.  * It is reasonable to require that this user has a trusted position on the network.  * This user is very technical and motivated to build a monitoring system.  * The monitoring system requires:  ** The information returned by validator-info across the various stewards, but today this information is only available to users with a steward role.  ** Other information that can be provided by querying the ledger. This does not require any changes.    *Acceptance Criteria*  * A non-steward role exists by which a trusted administrator can access validator-info across all stewards in a consensus pool.  * This role is called Network Monitor.  * The only permissions this role has beyond an unprivileged user is to access validator-info.  * A user can be assigned this role by a steward (no multi-signature necessary).",Story,Medium,Complete,"2018-12-17 19:42:12","2018-12-17 19:42:12",2
"Hyperledger Indy Node","POA: track same transactions with different multi-signatures","As of now transaction's digest is used to uniquely identify the transaction.    Signature (as well as multi-signature) is not part of the digest.  This is not a problem for a common (single) signature, but nay cause issues in case of multi-signature:   * There is a transaction signed by Trustees 1,2,3, and by Trustees 3,4,5.   * Both Trustee 1 and Trustee 4 send this txn   * Since multi-signature is not part of the digest, the pool will not be able to distinguish if this is txn signed by 1,2,3, or by 3,4,5.   * Some nodes in the pool may select for ordering a txn signed by 1,2,3, and some signed by 3,4,5. Since multi-sig is part of the txn in the Ledger, it will lead to different txn root hashes and different ledgers, so the pool may lost consensus.      *Acceptance Criteria*  * Design a solution to the above problem.  * Get feedback from the architects.  * Create relevant issues in this project.",Task,Medium,Complete,"2018-12-17 14:57:23","2018-12-17 14:57:23",3
"Hyperledger Indy Node","Limit the number of attributes in schema","This story is an outcome of https://jira.hyperledger.org/browse/IS-1102  We should limit our number of attributes in schema to 125 to make cred def pass the request limit check.    *Acceptance Criteria*  * Clearly document the limit  * Implement a check for the limit  * Test that a transaction with 125 attributes with names of the maximum length will succeed  * Test that a transaction with 126 attributes will fail    Excluded:  * The performance of a transaction with 125 attributes does not need to be tested with this story",Story,Medium,Complete,"2018-12-14 12:13:41","2018-12-14 12:13:41",2
"Hyperledger Indy Node","Hotfix 1.6.80",,Task,Medium,Complete,"2018-12-13 06:34:59","2018-12-13 06:34:59",2
"Hyperledger Indy Node","Send INSTANCE_CHANGE if state doesn't get updated long enough","*Acceptance criteria*  * Tests should be implemented which show that if some request doesn't get ordered for long enough for some reason then view change starts and successfully completes (probably can be shared with INDY-1911)  * Tests should be implemented which show that if more than _f_ and less than _n-f_ nodes entered view change then remaining honest will also enter and successfully complete a view change, regardless of presence or absence of incoming requests  * Implement broadcasting of INSTANCE_CHANGE message when state doesn't get updated long enough  * Threshold timeout should be configurable, and it should be possible to turn off this functionality    *Additional notes*  * This will make sure that if more than _f_ and less than _n-f_ nodes enter view change then if (and as soon as) there are any incoming write requests then remaining nodes that are not yet in view change will also enter view change allowing it to finish  * In other words it should reliably fix problem occured in INDY-1903  * Unlike INDY-1910 this doesn't increase risk of perpetual view change under high load  * There should be functionality to periodically update state even if there are no incoming requests  * In other words this is blocked by INDY-933  ",Task,Medium,Complete,"2018-12-12 14:18:44","2018-12-12 14:18:44",5
"Hyperledger Indy Node","Send INSTANCE_CHANGE if some request doesn't get ordered by master Primary for long enough","*Acceptance criteria*  * Tests should be implemented which show that if some request doesn't get ordered for long enough for some reason then view change starts and successfully completes  * Broadcasting INSTANCE_CHANGE when some request is stuck for long enough should be implemented  * Threshold timeout should be configurable, and it should be possible to turn off this functionality  * Extensive testing should be performed to make sure that this won't provoke perpetual view change under high enough load    *Additional notes*  * This will make sure that if more than _f_ and less than _n-f_ nodes enter view change then if (and as soon as) there are any incoming write requests then remaining nodes that are not yet in view change will also enter view change allowing it to finish  * In other words it should reliably fix problem occured in INDY-1903  * Also this is basically what original PBFT (not RBFT) paper requires  * This will also provide pretty good resistance to censorship attacks  * However there is a high risk that this change can provoke perpetual view change under high enough load, so extensive testing is required, as well as probably some more research. Implementation of INDY-1863 _might_ also help here  ",Task,Medium,New,"2018-12-12 14:17:01","2018-12-12 14:17:01",8
"Hyperledger Indy Node","Discard old INSTANCE_CHANGE messages","*Acceptance criteria*  * Tests should be written that confirm that old enough INSTANCE_CHANGE messages are discarded  * Accumulated INSTANCE_CHANGE messages should be discarded based on some configurable timeout  * Also it should be possible to turn of this functionality from config file    *Additional notes*  * This won't totally prevent incidents like INDY-1903, just reduce probability of such situation  * This can potentially introduce other failure modes  * Most probably this should be turned off when proper fix is delivered  * Main reason why this task exists is because it's one of the simplest and fastest ways to significantly reduce chance of incidents like INDY-1903",Task,Medium,Complete,"2018-12-12 14:14:52","2018-12-12 14:14:52",2
"Hyperledger Indy Node","Unable to choose volume type using pool automation scripts","Actual Results:  Now we unable to choose volume type using pool automation scripts and all volumes have standart (magnetic) type but when we create instances manually all volumes have gp2 (ssd) type.    Expected Results:  It would be nice to configure volume type the same as its size.",Task,Medium,Complete,"2018-12-11 07:31:19","2018-12-11 07:31:19",1
"Hyperledger Indy Node","Prepare a doc with useful info about ZeroMQ","ZeroMQ has several unclear features that affects consumer application. In scope of this ticket I'm going to describe known features/issues/limitation of ZeroMQ that were found during our testing as a part of knowledge sharing.",Task,Medium,Complete,"2018-12-07 16:04:20","2018-12-07 16:04:20",1
"Hyperledger Indy Node","Stub: PoA for combining Domain and Payment ledgers","This is a stub for ST-501.    https://sovrin.atlassian.net/browse/ST-501",Task,Medium,Complete,"2018-12-07 12:21:25","2018-12-07 12:21:25",3
"Hyperledger Indy Node","Statistics for outdated requests","In scope of INDY-1836 we enabled a strategy for dropping outdated requests. Now we need to have a statistics regarding requests dropped on propagates and ordering phases to diagnose pool's health.    *Acceptance criteria:*  * Separate counters for requests dropped on propagates phase and ordering phase  * Show these counters:  ** as a part of extended validator info  ** as a part of diagnostic metrics",Task,High,Complete,"2018-12-06 12:13:31","2018-12-06 12:13:31",1
"Hyperledger Indy Node","PoA: View Change needs to be triggered in BFT way","The issue caused problems from INDY-1893.   * As of now, INSTANCE_CHANGE message doesn't have instance change ID as required by RBFT paper (see IV.D).   * So, a node can process ALL instance change messages, including very old ones (from the nodes which are already not available).   * So, a node may start a view change receiving INSTANCE_CHANGE from less than n-f nodes, if it's stashed some old INSTANCE_CHANGEs from nodes that didn't send INSTANCE_CHANGE now.   * This breaks BFT principles, that either all nodes should start the view change, or all nodes should not start the view change.         *Acceptance criteria*   * Create a test reproducing a problem   ** Send INSTANCE_CHANGE by f+1 nodes only, so that other nodes don't accept it, and don't start the view change.   One of this nodes must be a master primary.   ** Stop these f+q nodes   ** Wait until other nodes send INSTANCE_CHANGE because they lost primary   ** *Make sure they don't start view change (this will fail with the current code)*   ** Restart f+1 nodes   ** make sure the pool is functional.   * Explore options of how we can fix it   * Create necessary tickets for implementation    *One of the possible options*   * Add instance change ID into INSTANCE_CHANGE message   ** See [https://pakupaku.me/plaublin/rbft/5000a297.pdf,] IV.D. - *looks like there is a bug there*   ** *Every new INSTANCE_CHANGE needs to be sent with a new (incremented) ID - this is not explicitly said in RBFT paper (is it a bug there?)*   ** Think about whether we need a new protocol version for nodes (most probably no, since we can add it as an optional field at the end).   * Change INSTANCE_CHANGE processing logic to start view change only if there is a quorum of INSTANCE_CHANGEs with the same ID and viewNo.   ** Has an INSTANCE_CHANGE ID parameter for the node.   ** Discard (stash?) INSTANCE_CHANGE  messages with ID less than the current node's one   ** Check and send INSTANCE_CHANGE to others if INSTANCE_CHANGE ID is greater than the current one     * Think about restoring INSTANCE_CHANGE ID   ** Either clear it after the view change, or make it persistent and increment during the whole life of the pool   ** If clear it after a view change / restart, then a node needs to start a view change if it gets a quorum of INSTANCE_CHANGE with ids less than this node's one.   This is needed to handle the situation when all nodes re-started except the current one.",Task,High,Complete,"2018-12-06 11:30:06","2018-12-06 11:30:06",5
"Hyperledger Indy Node","Parametrize Docker file to work better with the System Tests",,Task,Medium,Complete,"2018-12-05 12:58:30","2018-12-05 12:58:30",3
"Hyperledger Indy Node","Fix throughput class creation bug","Parameter {{start_ts}} in function {{create_throughput_measurement}} is defined by default as {{start_ts=time.perf_counter()}}. In python this definition will lead to the start initialization of the parameter. If we wil call this method in further without the parameter, {{start_ts}} will be equal to the first value.      *Acceptance Criteria*   * Move the definition of the parameter start_ts to the function body.   * Add test",Task,Medium,Complete,"2018-12-03 13:14:26","2018-12-03 13:14:26",1
"Hyperledger Indy Node","Documentation about Message Requests",,Task,Medium,Complete,"2018-11-29 08:00:38","2018-11-29 08:00:38",1
"Hyperledger Indy Node","Diagram for Checkpoints and Watermarks",,Task,Medium,Complete,"2018-11-29 08:00:07","2018-11-29 08:00:07",2
"Hyperledger Indy Node","Change catchup logic for correctly clearing requests queues","For now after we finish catchup we do not correctly clear our request queues (node.requests, replica.requestQueues, etc..). In scope of this ticket, we need PoA and fix for this problem.",Task,Medium,Complete,"2018-11-29 06:48:43","2018-11-29 06:48:43",3
"Hyperledger Indy Node","Don't use all pool ledger data in NODE transactions ordering","Method processPoolTxn() is called in each ordering of transactions for pool ledger. The method calls ledger.getAllTxn() which returns all transactions in the ledger. This takes a very long time in proportion to the size of the ledger.    *Acceptance Criteria:*   * Change call getNodeInfoFromLedger() in processPoolTxn() to faster way.   * Change call getNodeInfoFromLedger() in nodeServiceChange() to faster way.   * Add tests",Task,Medium,Complete,"2018-11-28 13:25:02","2018-11-28 13:25:02",3
"Hyperledger Indy Node","Support volume size customization in pool automation scripts","We should support volume size customization in pool automation scripts because now all volumes are created with default size according to instance type.",Task,Medium,Complete,"2018-11-27 14:51:26","2018-11-27 14:51:26",1
"Hyperledger Indy Node","A Node needs to be able to order requests received during catch-up","As of now, Replicas discard PrePrepares received during catch-up.    So, if catch-up is long enough, it may lead to infinite catch-ups and a lot of requests for missing PrePrepares, since during a high load the pool may order a lot.    Also we should not process Checkpoints during catch-up    *Acceptance criteria*   * Add tests to make sure that a Node can order and apply all requests and 3PC messages received during catch-up   * Simplify processing of 3PC messages during catch-up:   ** Stash all 3PC messages and Checpoints during catch-up   ** Once catch-up is finished - process them",Task,Medium,Complete,"2018-11-27 11:21:36","2018-11-27 11:21:36",5
"Hyperledger Indy Node","Stub: Stability with fees plugins","Work on the issue tracked in the Sovrin JIRA    https://sovrin.atlassian.net/browse/ST-497    POA: Pool ran into an incosistent state after the view change",Task,High,Complete,"2018-11-26 14:48:55","2018-11-26 14:48:55",5
"Hyperledger Indy Node","Create a Diagram for Components","Create Components Diagram for all Ledgers, States and other storages.",Task,Medium,Complete,"2018-11-22 08:04:23","2018-11-22 08:04:23",1
"Hyperledger Indy Node","Create Catch-up Sequence Diagram","Create diagram(s) for Catch-up logic including   * catch-up from client point of view   * catch-up from node-to-node point of view",Task,Medium,Complete,"2018-11-22 08:02:03","2018-11-22 08:02:03",2
"Hyperledger Indy Node","Write and Read request flow","Create Activity diagrams for write and read request flows including optimistic and negative scenarios.",Task,Medium,Complete,"2018-11-22 08:00:59","2018-11-22 08:00:59",1
"Hyperledger Indy Node","Possible memory leak during catchup","During long sustained load test one of nodes was restarted, successfully completed short catchup and continued ordering. However after that all other nodes increased memory consumption and number of live objects, but number of requests in their queues (which is major contributor to number of live objects) was not increased. This increase in number of live objects seems constant. There is probability that node has memory leak related to catchup and/or other activity related to return of some node to consensus.    *PoA*  * run load test in docker with on of nodes continuosly restarted  * check memory consumption and number of live objects on all nodes except one being restarted  * if there is increase in memory consumption try to identify its source (logging top 50 types of live objects can help here)  * if there is an identified leak either fix it or create issue dedicated to fix (if fix would take more than 1 day)",Task,Medium,Complete,"2018-11-21 13:17:47","2018-11-21 13:17:47",3
"Hyperledger Indy Node","Limit number of 3PC batches in flight","Currently batches are created as soon as there are _Max3PCBatchSize_ requests in queue OR _Max3PCBatchWait_ seconds passed since last batch was created. When under high load it can lead to creation of batches faster than they are ordered leading to uncontrollable growth of node traffic and consensus related data structures. At the same time batch size is not maxed out, which means that nodes are not using resources optimally (actually, the bigger batch the lower overhead of consensus).    *Proposed solution*  * Add _Max3PCBatchesInFlight_ parameter  * Create batches as soon as there are _Max3PCBatchSize_ requests in queue OR (_Max3PCBatchWait_ seconds passed AND there are less than _Max3PCBatchesInFlight_ batches in flight)    *Expected improvements*  Under high load bigger batches will be created, reducing number of consensus messages per request, so throughput and probably latency are improved",Task,Medium,Complete,"2018-11-19 13:25:21","2018-11-19 13:25:21",2
"Hyperledger Indy Node","Integrate new handlers into the codebase","Make united places for all request handlers to be created (according to step 3 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#])",Task,Medium,Complete,"2018-11-19 10:23:46","2018-11-19 10:23:46",8
"Hyperledger Indy Node","Create Builders for handlers","Incorporate request handler creation as said in step 4 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#]",Task,Medium,Complete,"2018-11-19 10:00:27","2018-11-19 10:00:27",2
"Hyperledger Indy Node","Move logic from ConfigReqHandler","Move logic from ConfigReqHandler (both in node and plenum) to separate implementations of QueryHandler, TransactionHandler and BatchHandler (according to step 2 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#])",Task,Medium,Complete,"2018-11-19 09:53:34","2018-11-19 09:53:34",2
"Hyperledger Indy Node","Move logic from PoolReqHandler","Move logic from PoolReqHandler (both in node and plenum) to separate implementations of QueryHandler, TransactionHandler, BatchHandler and ActionHandler (according to step 2 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#])",Task,Medium,Complete,"2018-11-19 09:49:03","2018-11-19 09:49:03",2
"Hyperledger Indy Node","Move logic from ReqHandler in DomainReqHandler","Move logic from DomainReqHandler (both in node and plenum) to separate implementations of QueryHandler, TransactionHandler and BatchHandler (according to step 2 of solution 2 by the link: https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#)",Task,Medium,Complete,"2018-11-19 09:45:28","2018-11-19 09:45:28",3
"Hyperledger Indy Node","Define interfaces for handlers for transactions, queries, actions and batches","See the step 1 in solution 2 in this google document:  https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#",Task,Medium,Complete,"2018-11-19 09:39:44","2018-11-19 09:39:44",2
"Hyperledger Indy Node","Verbose response from validator-info should have a predictable order","Alphabetize by key",Task,Medium,Complete,"2018-11-15 21:58:15","2018-11-15 21:58:15",1
"Hyperledger Indy Node","Detailed plan for resolving plugin upgrades","*Acceptance Criteria*  * Create the issues needed in INDY-1852",Task,Medium,Complete,"2018-11-15 14:11:04","2018-11-15 14:11:04",1
"Hyperledger Indy Node","Plenum Consensus Protocol Diagram","Create a diagram for request ordering by a Node and replicas (validation, 3PC batch creation, 3PC protocol, BLS multi-sigs), see [https://github.com/hyperledger/indy-plenum/blob/master/docs/request_handling.md]",Task,Medium,Complete,"2018-11-15 12:45:32","2018-11-15 12:45:32",5
"Hyperledger Indy Node","Add dates to the release notes","The release notes for Indy Node and Sovrin should mention the date of each release.",Task,Medium,Complete,"2018-11-14 22:40:04","2018-11-14 22:40:04",1
"Hyperledger Indy Node","Change pool state root hash for BLS-signature in Commit messages","Change pool_state_root_hash in Commit MultiSignatureValue to pre_prepare.pool_state_root_hash.  *Acceptance criteria*   * Add field POOL_STATE_ROOT_HASH to PrePrepare message (Put it before PLUGIN_FIELDS)   * Add validation of new PrePrepare format. Its POOL_STATE_ROOT_HASH should be included in the uncommitted state root hash   * Change commit BLS signature creating. Now it should use pool state root hash from PrePrepare message instead committed state root hash.   * Change validation of a commit BLS signature. Now it should use pool state root hash from PrePrepare message instead committed state root hash.   * Change logging message in case with incorrect BLS signature for commit message. Add information about an incorrect Commit message   * Add tests",Task,Medium,Complete,"2018-11-13 14:25:30","2018-11-13 14:25:30",3
"Hyperledger Indy Node","Investigate reasons of OOM crashes","We should investigate logs and metrics gathered in scope of INDY-1795 to find reasins of OOM crashes.    Steps to Reproduce:  1. Run 20 nyms/sec for 15 hours.  2. Check metrics and logs for OOM reasons at any node.    Actual Results:  Pool works normally. Only 8th node breaks due to OOM.  All logs and journals are in ev@evernymr33:logs/1795-1683.tar.gz",Task,Medium,Complete,"2018-11-12 14:06:01","2018-11-12 14:06:01",5
"Hyperledger Indy Node","Additional data needed from validator-info","The following should be displayed in all outputs of validator-info (regular, -v and --json):    The BLS key    The date (including timezone or UTC offset) that the info was last refreshed.    The software package versions of indy-node and sovrin (this was there previously)",Task,Medium,Complete,"2018-11-09 22:27:36","2018-11-09 22:27:36",2
"Hyperledger Indy Node","Need to have ec2 volumes tagged as well","As of now EC2 volumes are created automatically during instances creation and are not tagged Thus, it would be hard to track costs for them.    Acceptance criteria:   # volumes should have tags   # tags should be added at creation time to make creation and tagging atomic operation",Task,Medium,Complete,"2018-11-09 16:16:59","2018-11-09 16:16:59",1
"Hyperledger Indy Node","3rd party open source manifest","*Hints*   * A spreadsheet with dependencies about 1.5 year old: [https://docs.google.com/spreadsheets/d/1vz3kVt281EF-qp_tKB5z7zJTviI9YPV0NO4x0DsLSuk/edit#gid=0]   * We should have a look at setup.py in indy-plenum and indy-node (in particular, install-requires section)   * We may also install indy-node deb package, and get all dependencies for Node and Plenum using `apt-cache show`   ** we already have utilities to get dependency tree that we use during Upgrade   ** have a look at https://github.com/hyperledger/indy-node/blob/master/indy_node/utils/node_control_utils.py#L111    *Acceptance criteria:*   * A spreadsheet with all direct dependencies for Indy-Node containing the following information about each dependency:   ** Name   ** License   ** Version  ** Reason for usage    Destination spreadsheet:  https://drive.google.com/drive/folders/1crXe2FSzaojc_QWfsl0jYfYCliVly4jj",Task,Medium,Complete,"2018-11-09 13:21:11","2018-11-09 13:21:11",1
"Hyperledger Indy Node","Enable Clear Request Queue strategy","A sanity check to remove old requests from queues was implemented in the scope of INDY-1780. It's disabled by default.    *Acceptance criteria*   * Make sure that it doesn't have any side-affects   * Do load testing with enabled strategy   * Enable it by default",Task,Medium,Complete,"2018-11-09 08:03:39","2018-11-09 08:03:39",3
"Hyperledger Indy Node","Enable PreViewChange Strategy ","A PreViewChange strategy was implemented in the scope of INDY-1687 to speed-up and improve stability of the view change. It's currently disabled by default.    *Acceptance criteria*   * Make sure that it doesn't have any side-affects   * Do more load testing (with regular view changes) with strategy enabled:   ** do 10 NYMs per sec    ** check that most of view changes are quite fast   ** check that view changes don't lead to broken ledgers (all ledger are in sync eventually)   ** enable the strategy by default",Task,Medium,Complete,"2018-11-09 07:59:42","2018-11-09 07:59:42",3
"Hyperledger Indy Node","Research Honeybadger implementation","[https://github.com/poanetwork/hbbft]    We need to find out how stable Honeybadger Implementation is.   Timebox the effort to 4 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.   Discussion should include:   ** Pros and Cons   ** Characteristics of current deployments in production usage   ** Current project roadmap   ** Health of the open source community / level of investment   ** Rough estimate of work required to bring Honeybadger to the same level as Plenum   ** Rough understanding of performance   ** Rough understanding of the difficulty of migrating from the existing Plenum to an implementation based on Honeybadger   * A recommendation on whether to research more or kill the proposal   * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]    Things to consider:   * Research how clients are authorized by ledger   * How does this relate to work with observers   * How much better is performance   * How many nodes would be practical (50? 500? 5000?)   * Get feedback from community   * Notice: One of PoC goals should be smooth upgrade path   * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.     ",Task,Medium,New,"2018-11-08 11:21:03","2018-11-08 11:21:03",3
"Hyperledger Indy Node","Research Stellar Implementation","[https://github.com/stellar/stellar-core]    We need to find out how stable Stellar Implementation is.   Timebox the effort to 4 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.   Discussion should include:   ** Pros and Cons   ** Characteristics of current deployments in production usage   ** Current project roadmap   ** Health of the open source community / level of investment   ** Rough estimate of work required to bring Stellar to the same level as Plenum   ** Rough understanding of performance   ** Rough understanding of the difficulty of migrating from the existing Plenum to an implementation based on Stellar   * A recommendation on whether to research more or kill the proposal   * [|https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]    Things to consider:   * Research how clients are authorized by ledger   * How does this relate to work with observers   * How much better is performance   * How many nodes would be practical (50? 500? 5000?)   * Get feedback from community   * Notice: One of PoC goals should be smooth upgrade path   * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.     ",Task,Medium,Complete,"2018-11-08 11:19:58","2018-11-08 11:19:58",3
"Hyperledger Indy Node","Research Honeybadger protocol","[https://eprint.iacr.org/2016/199.pdf]    Find out if Honeybadger protocol looks good for Indy and is better than RBFT.   Timebox the effort to 2 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community. Discussion should include:   ** Pros and Cons   ** Characteristics of current production implementations of the protocol   ** Rough estimate of work required to bring implement Stellar in Plenum   * A recommendation on whether to research more or kill the proposal   * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]",Task,Medium,New,"2018-11-08 11:17:19","2018-11-08 11:17:19",3
"Hyperledger Indy Node","Research Stellar Protocol","[https://www.stellar.org/papers/stellar-consensus-protocol.pdf]    Find out if Stellar protocol looks good for Indy and is better than RBFT.   Timebox the effort to 2 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community. Discussion should include:   ** Pros and Cons   ** Characteristics of current production implementations of the protocol   * A recommendation on whether to research more or kill the proposal  ",Task,Medium,Complete,"2018-11-08 11:16:43","2018-11-08 11:16:43",3
"Hyperledger Indy Node","Load and performance testing of GET_TXN","As of now, GET_TXN doesn't support State (Audit) Proofs, and requires to be sent to all nodes and waiting for f+1 equal replies.    GET_TXN may be used a lot in some cases, so we need to evaluate max performance the pool can handle with GET_TXN.    *Acceptance criteria*   * Run load tests sending GET_TXN, find out max performance and evaluate stability   ** Start with sending 10 GET_TX per sec   ** Increment the load sequentially",Task,Medium,Complete,"2018-11-08 11:09:16","2018-11-08 11:09:16",3
"Hyperledger Indy Node","Need to add Names to AWS ec2 instances and security groups","Currently AWS ec2 instances and security groups are not provided with Name (which is a tag).         *PoA*:   # add Name tag to instances and groups basing on other tags: Project, Namespace, Role etc.     ",Task,Medium,Complete,"2018-11-08 09:13:21","2018-11-08 09:13:21",1
"Hyperledger Indy Node","Need to securely automate SSH authenticity checking","Currently pool_create playbook provisions hosts and provides ssh keys to reach them.  Initially these hosts are unknown, thus ssh does host checks which requires user interaction and usually would happen during pool_install playbook. It is not the desired behavior and should be automated in a secure way.      Useful links:   * [https://stackoverflow.com/questions/32297456/how-to-ignore-ansible-ssh-authenticity-checking]   * [https://stackoverflow.com/questions/23074412/how-to-set-host-key-checking-false-in-ansible-inventory-file]   * [https://docs.ansible.com/ansible/latest/user_guide/intro_getting_started.html#host-key-checking]   * [https://man.openbsd.org/ssh-keyscan]   * [https://blog.0xbadc0de.be/archives/300]     ",Task,Medium,Complete,"2018-11-08 08:54:34","2018-11-08 08:54:34",1
"Hyperledger Indy Node","Limit RocksDB memory consumption","In the scope of INDY-1821 it was found that RockDB's readers-mem is increasing.    This looks like the main cause of slow memory increasing during long load tests.    *Acceptance criteria*   * Find out RocksDB config to limit readers-mem   * Find out if RocksDB wrapper keeps Python objects not-GCed   * [Optional] Get metrics about BlockCache (it may require changes in wrapper)   * [Optional] Explicitly set limits for BlockCache",Task,Medium,Complete,"2018-11-07 12:33:09","2018-11-07 12:33:09",5
"Hyperledger Indy Node","Run very long load test on a small local pool","*Acceptance Criteria:*  * Create 4-node docker pool  * Run load script with 10 NYMs per second for as long as possible  * Check graphs for memory consumption    ",Task,Medium,Complete,"2018-11-07 10:13:07","2018-11-07 10:13:07",2
"Hyperledger Indy Node","Init Indy Node should output Base58-encrypted verkey already","[In the Steward Getting Started Guide|https://docs.google.com/document/d/1AH618bj4q9U8FS1uyoIgbcvwNzaghBCQ1v44tNpZ2OU/edit#heading=h.votw8a9lan5j]    Shows that the verkey will come out looking something like this:  {quote}{{bfede8c4581f03d16eb053450d103477c6e840e5682adc67dc948a177ab8bc9b}}  {quote}  Which to the best of my knowledge is a hexlified version of the verkey.  Later on, [in the guide|https://docs.google.com/document/d/1AH618bj4q9U8FS1uyoIgbcvwNzaghBCQ1v44tNpZ2OU/edit#heading=h.eg8r1qxkvix7], to send the the client to add the Steward's node to the ledger, they must execute:  {quote}python3 -c from plenum.common.test_network_setup import TestNetworkSetup; print(TestNetworkSetup.getNymFromVerkey(str.encode('bfede8c4581f03d16eb053450d103477c6e840e5682adc67dc948a177ab8bc9b')))  {quote}  If they send the hex version of their verkey, the transaction would be rejected.    I've been able to produce the Base 58 version of the verkey in Node.js with just a simple Base 58 library, decoding the hex verkey into a string and then encoding it with the library.    This way when the user gets to this step:  {quote}indy> ledger node target=<validator_verkey_in_Base58> node_ip=<validator_node_ip_address> node_port=<node_port> client_ip=<validator_client_ip_address> client_port=<client_port> alias=<validator_alias> services=VALIDATOR blskey=<validator_bls_key> blskey_pop=<validator_bls_key_pop>  {quote}  They then use this base58 key they obtained above.    Can we make it so that the base58-encrypted key comes out when you run init_indy_node?          ",Story,Medium,Complete,"2018-11-06 22:42:32","2018-11-06 22:42:32",1
"Hyperledger Indy Node","validator-info default response missing key fields","The default response from validator-info should include the following additional fields:  * Transaction Count on all ledgers  * Current time the tool was run, including timezone  * BLS key  * IP addresses of the node and client port  * The listing of node names that are reachable and unreachable  * Software versions of indy-node and sovrin    Goals:  * The critical information necessary to understand the health of a steward  * Brief enough that it can be easily understood in one page of text  * The default information should be useful enough that it is unusual to need to run the tool in verbose mode.",Task,High,Complete,"2018-11-01 21:21:07","2018-11-01 21:21:07",1
"Hyperledger Indy Node","AWS tags for pool automation AWS resources","We need to add a set of tags.    To track charges:   * Project (might be discussed) to track AWS charges using AWS Cost Explorer    Other:      * Department   * Expiry   * Name   * Owner   * Purpose",Task,Medium,Complete,"2018-11-01 13:07:12","2018-11-01 13:07:12",2
"Hyperledger Indy Node","Adjust last_ordered_3pc and perform GC when detecting lag in checkpoints on backup","Currently, when a lag in checkpoints is detected on a backup replica, only the watermarks are shifted. This is insufficient to resume ordering of 3PC-batches. To make it possible to resume ordering starting from the 3PC-batch next to the last quorumed checkpoint, {{last_ordered_3pc}} must be also adjusted and the messages below its new value should be removed (i.e. garbage collection should be performed). In scope of this task add these actions to the logic of handling a lag in checkpoints on a backup replica.",Task,Medium,Complete,"2018-10-30 13:57:12","2018-10-30 13:57:12",3
"Hyperledger Indy Node","Improve usability of current pool automation PoC","Current PoC pool automation require changing root playbooks in order to alter configuration parameters, which is inconvenient and error prone.  It would be much better to have following workflow:  * create experiment directory using some tool (probably bash/python script or yet another playbook)  ** invocation of tool should be as simple as something like _init_pool <experiment-name>_  ** directory should be populated with some sensible default configuration, including number and type of instances created and versions of software installed  ** ideally experiment directory should be usable as ansible inventory (so later ansible invocations could look as simple as _ansible-playbook -i <experiment-name> <playbook-name>_)  ** probably it would be also good to have some software configuration files (like indy_config.py) in experiment directory as well  ** subsequent invocation of tool with same experiment name should NOT overwrite existing directory  * modify parameters/configuration files in experiment directory  * run pool_create/pool_install/whatever playbook passing experiment directory (or some subdirectory) as inventory  * playbooks are allowed to modify experiment directory (for example to update information about hosts or retrieve logs)  * when done run pool_destroy (which should shut down AWS instances), attach retrieved data to JIRA issue if needed and delete experiment directory    Probably this better be done in conjunction with INDY-1788",Task,Medium,Complete,"2018-10-26 12:17:38","2018-10-26 12:17:38",3
"Hyperledger Indy Node","As a dev/QA I need know how much test pools cost during given period","This can be done using AWS cost explorer [API|https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ce.html#CostExplorer.Client.get_cost_and_usage]    The main goal is to automate checks for monthly charges performed before/after ansible roles running to warn if we are going out of the budget.",Story,Medium,New,"2018-10-26 12:05:00","2018-10-26 12:05:00",3
"Hyperledger Indy Node","Update documentation regarding aws_manage cross region feature","In scope of INDY-1622 it was implemented logic of cross region nodes provisioning. Need to update docs regarding that.",Task,Medium,New,"2018-10-26 11:48:54","2018-10-26 11:48:54",1
"Hyperledger Indy Node","As a dev/QA I need to be able to refer different groups in the same namespace using one inventory","Currently creation of different groups in the same tagged namespace leads to creation of different inventory dirs. Having one inventory dir per namespace would be highly preferable.",Story,Medium,Complete,"2018-10-26 11:32:13","2018-10-26 11:32:13",5
"Hyperledger Indy Node","As a dev/QA I need to have a possibility to use spot instances for testing","Currently pool automation scripts can provision only on-demand AWS EC2 instances. It might be much cheaper to utilize spot instances as well (instead).",Story,Medium,Complete,"2018-10-26 11:03:47","2018-10-26 11:03:47",5
"Hyperledger Indy Node","Remove security groups at tear-down phase for both tests and playbooks","Currently integration tests don't remove dynamically created AWS security groups.    'aws_manage' playbook might fail during security groups removal as well.    Need to fix that since it pollutes the list of AWS account's used resources.",Task,Medium,Complete,"2018-10-26 10:58:55","2018-10-26 10:58:55",2
"Hyperledger Indy Node","Clear Requests queue periodically","As of now, there is a shared Requests queue for all Replica. A request is removed from the queue only when it's ordered on all Replicas.    So, if at least one Replica doesn't order a request, it will stay there forever.    There are a couple of strategies implemented that fixed it in most of the cases (see for example INDY-1684 and INDY-1759). So, if  one of the Backup Instances doesn't order, or order too slow, we can detect it and remove the replica (allowing requests to be cleared).    However, there are cases where some requests may still be not cleared (until view change happens):    1) Request queue is different on nodes, so a Node has requests which are not present (by some reason) on one of the primaries, so it's never ordered on this instance and hence never removed from the queue.    2) A malicious primary on one of the backup instances doesn't order some of the request (in a way that backup instance performance is not changed significantly, otherwise it will be detected by others because of the strategy from INDY-1684). So, some requests will not be cleared on all nodes.         *Acceptance criteria*   * Define a strategy of removing outdated requests from the queue periodically.   Possible options:   ** Assign a timestamp for each request when it goes to the queue. Schedule a timer (let's say every X minutes) which will clear all requests staying in the queue for more than X minutes   ** Clear all requests for a stable checkpoint on master (once it's ordered by master some time ago, it's unlikely that it's still not ordered by backups; so this rather means that backups doesn't order or order maliciously)   * It may make sense to implement it as a strategy disabled by default in config, and enabled once it's validated by QA     ",Task,Medium,Complete,"2018-10-25 15:35:20","2018-10-25 15:35:20",5
"Hyperledger Indy Node","Research R3 Corda","Evaluate if R3 Corda with BFT consensus (BFT Smart) could be made into a viable replacement for Indy.   Timebox the effort to 3 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.   Discussion should include:   ** Pros and Cons   ** The state of the project's community   ** The current roadmap   ** Rough estimate of work required to bring R3 Corda-based solution to the same level as Plenum   ** A recommendation on whether to research more or kill the proposal   ** Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]    Things to consider:   * Research how clients are authorized by ledger   * How does this relate to work with observers   * How much better is performance   * How many nodes would be practical (50? 500? 5000?)   * Get feedback from community   * Notice: One of PoC goals should be smooth upgrade path    There is already a couple of PoC of integration Indy SDk with Corda-based pool (see, for example, [https://github.com/Luxoft/cordentity])   * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.",Task,Medium,New,"2018-10-25 10:28:25","2018-10-25 10:28:25",3
"Hyperledger Indy Node","Test ZMQ Memory Consumption with restarting of listener on every X connections","As was understood in INDY-1686, ZMQ may consume a lot of memory if there are a lot of simultaneous client requests, since it uses round-robin to read client queues.    Plenum does (be default) 100 reads in one looper run.    We need to test if situation with memory consumption improves when we restrict the number of client connections.    *Acceptance criteria*   * Run test with restarting of listener on every 1000 connections   ** Run Python tests (created in the scope of INDY-1686) restarting listener    ** Run C tests (created in the scope of INDY-1686) restarting listener   ** Run Indy load tests   *** set MAX_CONNECTED_CLIENTS_NUM = 100   *** Run two tests: 10 NYMs per sec and 100 NYMs per sec   ** Compare results",Task,Medium,Complete,"2018-10-25 10:07:49","2018-10-25 10:07:49",2
"Hyperledger Indy Node","Get information about how many client connections is usually in progress","As was understood in INDY-1686, ZMQ may consume a lot of memory if there are a lot of simultaneous client requests, since it uses round-robin to read client queues.    Plenum does (be default) 100 reads in one looper run.    We need to test if situation with memory consumption improves when we restrict the number of client connections.    *Acceptance criteria*   * Get information about how many client connections is usually in progress:   ** Include metrics to track current number of open client connections (a PR)   ** Run two tests with metrics enabled: 10 NYMs per sec and 100 NYMs per sec. Compare results and how many connections are used.",Task,Medium,Complete,"2018-10-25 10:06:04","2018-10-25 10:06:04",2
"Hyperledger Indy Node","Do a long test with a load pool can handle","We can see that memory is slightly increasing during a load that pool can handle.   We suspect that there can be a limit where it's cut off, but in order to prove it we need to run a long test.    Increasing memory consumption may be caused by RocksDB (see comments in INDY-1724).    *Acceptance Criteria:*   * Run load script with 1-10 NYMs per second for 2-3 days.   * Check graphs for memory consumption     ",Task,Medium,Complete,"2018-10-25 07:48:39","2018-10-25 07:48:39",2
"Hyperledger Indy Node","Find out why validation of PrePrepares with Fees takes so long","During the recent tests it was found, that dynamic validation of PrePrepares with fees take a lot of time (100 ms vs 2.5 ms), which led to high latency on master and hence a View Change.    We need to check what is the cause of such behaviour.    Logs can be found in s3://qanodelogs/indy-1717/live-23-10-18-production-mix",Task,Medium,Complete,"2018-10-24 14:41:42","2018-10-24 14:41:42",2
"Hyperledger Indy Node","Check why backup instances stop ordering so often","A lot of load tests showed that backup instances stop ordering. This, in particular, can lead to out-of-memory issues (since requests queue can not be cleared).    *Acceptance criteria*   * Find out the reason   * Create tickets for fixes if needed    *Additional information*:  Logs for the tests mentioned above: ~/logs/indy-1711/success.tar.gz",Task,Medium,Complete,"2018-10-24 14:29:23","2018-10-24 14:29:23",5
"Hyperledger Indy Node","As a dev I need to be able to perform tests on docker","Currently tests are performed using molecule and its vagrant driver only. It makes sense to have docker based way as a default to speed up development since docker containers are much lighter than vagrant VMs. Also it might be useful for future integration into CI pipelines.    Acceptance criteria:   * tests can be run on docker containers as a default scenario   * tests are passed",Story,High,Complete,"2018-10-24 12:31:15","2018-10-24 12:31:15",3
"Hyperledger Indy Node","Test ZMQ Memory Consumption with restricted number of client connections","As was understood in INDY-1686, ZMQ may consume a lot of memory if there are a lot of simultaneous client requests, since it uses round-robin to read client queues.    Plenum does (be default) 100 reads in one looper run.    We need to test if situation with memory consumption improves when we restrict the number of client connections.    *Acceptance criteria*   * Run load test with restricting maximum number of connections   ** Configure a pool with maximum number of incoming client connections set to 200   ** set MAX_CONNECTED_CLIENTS_NUM = 100   ** Run two write only tests: 10 NYMs per sec and 100 NYMs per sec.   ** Run a test with write and read: 10 write NYMs per sec and 100 read NYMs per sec from 1000 clients   ** Run a test with write and read: 100 write NYMs per sec and 1000 read NYMs per sec from 1000 clients   ** Run more experiments if needed   * Monitor and analyze memory consumption in every case   * Check how many client requests were processed, and how many rejected because the pool was busy (due to timeout). Do it for the both read and write reqs   * Decide if we should reduce MAX_CONNECTED_CLIENTS_NUM  in production     ",Task,Medium,Complete,"2018-10-24 12:30:34","2018-10-24 12:30:34",2
"Hyperledger Indy Node","Run load tests with file storages","Acceptance Criteria:   * Adapt code to support file storages for all DBs   * Run load tests using file storage   * Analyse memory consumption",Task,Medium,Complete,"2018-10-24 10:08:27","2018-10-24 10:08:27",5
"Hyperledger Indy Node","Verify Upgrade of the MainNet to the latest IndyNode","We need to check that Upgrade of the MainNet works correctly to the latest version of IndyNode.    *Acceptance citeria*   * Check that POOL_UPGRADE of MainNet works from 1.3.62 to 1.6.78 (force=True)",Task,Medium,Complete,"2018-10-17 07:28:13","2018-10-17 07:28:13",2
"Hyperledger Indy Node","Change dependency building for upgrade procedure","As of now, we have the next dependency building procedure for sovrin and indy-node packages:   * DEPS list in node_control_tool source code file   * PACKAGES_TO_HOLD list also located in node_control_tool source file   * Calculating deps list for current installed package (upgrade_entry)   * Using calculating deps and deps from source's code list as a filter for recursive dep list building.    *There is 2 main problems for this approach*:   * Manually hardcoded list of DEPS in source code. Upgrade procedure is performed by previous version of upgraded package. In this case there is no any way to add new deps into source code list (DEPS list)   * Deps tree, performed by using calculated dep list as a filter do not taking into account holded packages. Holded packages must be included into deps list    *Possible changes:*   * Use new version of upgrade_entry's package and build all dependency tree for it.    * Get all holded package for current environment   * Get versions from calculated whole dep list for packages which in holded lists.",Task,Medium,Complete,"2018-10-16 09:08:18","2018-10-16 09:08:18",2
"Hyperledger Indy Node","Use persisted last_pp_seq_no for recovery of backup primaries","As of now, if master primary crashed (stopped) and re-started, it will get last_pp_seq_no from other Nodes and continue correct ordering.    For backup primaries this information is not propagated, so backup primary will start sending PRE-PREPARES from 1, and other nodes will reject it. It may eventually cause a memory leak, since ordering on this backup is stopped for some time.    *Acceptance criteria*   * Create a new key-value DB for persisting current states   * Persist last pp_seq_no for each Instance in the pool   * Use persisted last_pp_seq_no as a new pp_seq_no for a Replica if it's a primary in one of the Instances (including Master?)",Task,Medium,Complete,"2018-10-12 12:27:57","2018-10-12 12:27:57",5
"Hyperledger Indy Node","Need to track same transactions with different multi-signatures","As of now transaction's digest is used to uniquely identify the transaction.    Signature (as well as multi-signature) is not part of the digest.  This is not a problem for a common (single) signature, but nay cause issues in case of multi-signature:   * There is a transaction signed by Trustees 1,2,3, and by Trustees 3,4,5.   * Both Trustee 1 and Trustee 4 send this txn   * Since multi-signature is not part of the digest, the pool will not be able to distinguish if this is txn signed by 1,2,3, or by 3,4,5.   * Some nodes in the pool may select for ordering a txn signed by 1,2,3, and some signed by 3,4,5. Since multi-sig is part of the txn in the Ledger, it will lead to different txn root hashes and different ledgers, so the pool may lost consensus.",Task,Medium,Complete,"2018-10-12 10:05:30","2018-10-12 10:05:30",2
"Hyperledger Indy Node","Extend Load Script with GET_TXN","As of now, GET_TXN doesn't support State (Audit) Proofs, and requires to be sent to all nodes and waiting for f+1 equal replies.    GET_TXN may be used a lot in some cases, so we need to evaluate max performance the pool can handle with GET_TXN.    *Acceptance criteria*   * Extend load script to be able to send GET_TXN   * It needs to be possible to use GET_TXN to get UTXO (the most common case)",Task,Medium,Complete,"2018-10-12 09:22:56","2018-10-12 09:22:56",2
"Hyperledger Indy Node","Research Exonum implementation","[https://github.com/exonum/exonum]    We need to find out how stable Exonum Implementation is.   Timebox the effort to 4 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.   Discussion should include:   ** Pros and Cons   ** Characteristics of current deployments in production usage   ** Current project roadmap   ** Health of the open source community / level of investment   ** Rough estimate of work required to bring Exonum to the same level as Plenum   ** Rough understanding of performance   ** Rough understanding of the difficulty of migrating from the existing Plenum to an implementation based on Exonum   * A recommendation on whether to research more or kill the proposal   * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]    Things to consider:   * Research how clients are authorized by ledger   * How does this relate to work with observers   * How much better is performance   * How many nodes would be practical (50? 500? 5000?)   * Get feedback from community   * Notice: One of PoC goals should be smooth upgrade path   * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.     ",Task,Medium,New,"2018-10-12 08:54:14","2018-10-12 08:54:14",3
"Hyperledger Indy Node","Avoid redundant static validation during signature verification","As of now we don't re-validate signatures in Requests came in Propagates.   However, it lead to a lot of creation of Request (SafeRequest class) instances, which implies static (schema) validation. This process is quite expensive and affects performance a lot.  We need to avoid static (schema) validation for a Request if it has been already perfromed.     ",Task,Medium,Complete,"2018-10-11 13:12:07","2018-10-11 13:12:07",3
"Hyperledger Indy Node","Find out why max node prod time increases during long load test","Some observations:  * growth is same for loads of 1 NYM/sec and 10 NYMs/sec (so it doesn't depend on number of transactions written, although it's likely to depend on number of batches ordered)  * growth is same when using RocksDB and LevelDB as storage  * maximum node-to-node message size grows as well, but not as fast  * _average_ node prod time doesn't seem to be affected, which means spikes are in fact rare    In my opinion prime suspect here is cyclic garbage collector, but more research is needed.    *Acceptance criteria*  - find out why max prod time grows  - find out if this growth has upper bound  - create issues for actual fix  ",Task,Medium,Complete,"2018-10-10 09:49:20","2018-10-10 09:49:20",3
"Hyperledger Indy Node","Review old issues","The Evernym Indy Node team will be closing old issues and bug reports that are no longer needed or are unlikely to be reproducible on the current release.    *Acceptance Criteria*   Overall:   * Each engineer will spend one day of the sprint in this activity.   * The goal is to review at least 8 issues (less than 1 hour per issue).    For each issue:   * Read the issue and the comments.   * Briefly investigate the issue using the current code base (not the code base where the issue was found).   * Close the issue as Done if:   ** It appears to be something that we tested as part of our most recent release, and we didn't see the issue.   ** You try to reproduce the issue, and didn't see the issue.   ** A corresponding task/story exists (or created by the engineer) that addresses the same issue. Make sure that the task/story is linked to the bug.   * Keep the issue open if you are able to reproduce it or suspect that it is a real issue after your brief investigation.   * Include a comment documenting:   ** why you closed the issue,   ** or what you found during your investigation, including your current environment, symptoms observed, and steps to remediate.   * Create any related tickets that your investigation shows are necessary.   * If you think any of these issues should be immediately addressed, please escalate to [~<USER> and [~<USER>    *Notes*   * Close the issue as Done if you are at least 80% confident that the issue does not exist in the latest code base. The goal is to separate important issues from unimportant issues, and real issues will surface again at some point in the future.   * If during your investigation you see that you can fix the issue quickly (only a couple of hours), proceed to fix the issue.",Task,Medium,New,"2018-09-29 00:28:53","2018-09-29 00:28:53",0
"Hyperledger Indy Node","Triage old bug reports 2","The Evernym Indy Node team will be closing old bug reports that are unlikely to be reproducible on the current release.    *Acceptance Criteria*   Overall:   * Each engineer will spend one day of the sprint in this activity.   * The goal is to review at least 8 issues (less than 1 hour per issue).    For each issue:   * Read the issue and the comments.   * Briefly investigate the issue using the current code base (not the code base where the issue was found).   * Close the issue as Done if:   ** It appears to be something that we tested as part of our most recent release, and we didn't see the issue.   ** You try to reproduce the issue, and didn't see the issue.   ** A corresponding task/story exists (or created by the engineer) that addresses the same issue. Make sure that the task/story is linked to the bug.   * Keep the issue open if you are able to reproduce it or suspect that it is a real issue after your brief investigation.   * Include a comment documenting:   ** why you closed the issue,   ** or what you found during your investigation, including your current environment, symptoms observed, and steps to remediate.   * Create any related tickets that your investigation shows are necessary.   * If you think any of these issues should be immediately addressed, please escalate to [~<USER> and [~<USER>    *Notes*   * Close the issue as Done if you are at least 80% confident that the issue does not exist in the latest code base. The goal is to separate important issues from unimportant issues, and real issues will surface again at some point in the future.   * If during your investigation you see that you can fix the issue quickly (only a couple of hours), proceed to fix the issue.",Task,Medium,Complete,"2018-09-29 00:28:39","2018-09-29 00:28:39",0
"Hyperledger Indy Node","GitHub prerelease builds should be clearly marked","*Problem*  In the Indy Node repository in GitHub, development releases appear alongside the stable releases.    *Acceptance Criteria*  Builds that have not been approved by QA are marked in GitHub as pre-release.",Task,Medium,Complete,"2018-09-28 20:25:55","2018-09-28 20:25:55",2
"Hyperledger Indy Node","PoA: Move the auth_map structure to the config ledger","In order to make the _auth_map_ structure configurable we need to move it to the config ledger. Here we should:   - define a data format for storing in the config ledger   - define a new transactions for adding/modifying of authorization rules   - create required set of genesis transactions to be able to bootstrap   - modify applying of the rules stored in the config ledger according to new format    There is the question of atomicity of adding and modification of the rules for all administrative actions. Possible solutions:   - each rule can be added/modified by separate transaction;   - all rules are added/modified by single transaction, i.e. this transaction contains a list of all rules for administrative actions for atomic update.    I think, the first point is better.",Task,Medium,Complete,"2018-09-28 13:28:13","2018-09-28 13:28:13",1
"Hyperledger Indy Node","Support a strategy to specify per-cent of signers instead of absolute number","As a result of INDY-1730 we have a rules that define absolute quorums for administrative actions. In scope of this ticket we have to implement a strategy of combination of absolute numbers and percentage to implement a rules like 33% of trustees but not less than 2, but using of percentage requires INDY-1594 complete.    *Notes*  * It is expected that requiring a percentage of users will be implemented after the capability of requiring a specific threshold of signatures and recording the rules on the config ledger is already implemented.  * We do not yet have an efficient mechanism for getting from the ledger the total number of potential signers with a specific role. That will have to be done as part of this work.",Task,Medium,New,"2018-09-28 13:11:44","2018-09-28 13:11:44",5
"Hyperledger Indy Node","Modify applying of the rules stored in the auth_map according to changes made by INDY-1728 and INDY-1729","Since we have modified/extended _auth_map_ we need to implement applying of  authorization rules that define quorums for important actions (implemented in scope of INDY-1729) and modify applying of other rules stored in the _auth_map_ as they may be changed in scope of INDY-1728.",Task,Medium,Complete,"2018-09-28 12:55:22","2018-09-28 12:55:22",3
"Hyperledger Indy Node","Implement the authorization rules for administrative actions with quorums as constants (hard-coded constants for absolute number of signers)","Implement authorization rule with constant quorums according to _auth_map_ modification/extension made in scope of INDY-1728.        The current rules can be the same as before (see `auth_map.md`).     ",Task,Medium,Complete,"2018-09-28 12:27:01","2018-09-28 12:27:01",3
"Hyperledger Indy Node","PoA: Modify and extend the auth_map structure to store the authorization rules for administrative actions","Current implementation of the _auth_map_ structure does not support multi-signature quorums for actions. In order to support such functionality we need to modify and extend it to implement the rules like:    _<action>: [<who can do this>: <required number of signatures>]_  i.e.  _<action1>: [<role1>: <quorum1>, <role2>: <quorum2>]_    Of course the format may be different, it's just a concept.",Task,Medium,Complete,"2018-09-28 10:59:42","2018-09-28 10:59:42",5
"Hyperledger Indy Node","1.6.79 Release","Task Overview  Perform all necessary tasks in order to release    Acceptance Criteria  * Code is tested  * Release is tagged  * Release notes are drafted  * Release notes are reviewed by Docs and Product Management  * A new release of Indy-Plenum is part of this release",Task,Medium,Complete,"2018-09-27 21:31:44","2018-09-27 21:31:44",3
"Hyperledger Indy Node","Check how RocksDB affects memory omsumption","*Acceptance criteria*   * Run a test with NYMs   * run the same test limiting RocksDB memory usage in config   * run the same test with leveldb instead of RocksDB for all storages   * add metrics for RocksDB and analyze them   * run the same test with file storages instead of RocksDB for all storages (may be a separate task)",Task,Medium,Complete,"2018-09-27 17:56:27","2018-09-27 17:56:27",2
"Hyperledger Indy Node","Find out if there are memory leaks in interanl Plenum structures and queues","*Acceptance criteria:*   * Investigate the code base and find out if there are some queues or data structure that can grow and not GCed     ** have a look at the case when the load is more than node can handle, and a lot of node-to-node messages may be stashed   ** take into account results obtained in INDY-1688   * Run python profiler to check if there are any leaks in our main code",Task,High,Complete,"2018-09-27 16:40:55","2018-09-27 16:40:55",5
"Hyperledger Indy Node","1.6.74 Release","Task Overview  Perform all necessary tasks in order to release    Acceptance Criteria  * Code is tested  * Release is tagged  * Release notes are drafted  * Release notes are reviewed by Docs and Product Management  * A new release of Indy-Plenum is part of this release",Task,Medium,Complete,"2018-09-27 16:38:10","2018-09-27 16:38:10",3
"Hyperledger Indy Node","Measure performance improvements","In recent sprints, we made a number of changes that should improve performance. We want to measure and report on the results of these changes.    *Acceptance Criteria*  Perform a test of an Indy network that has the following attributes:    * The ledger is pre-loaded with 1 million transactions  * Pool size is 25 nodes.  * 1K concurrent clients  * Over a 3 hour period induce a sustained throughput of 10 write transactions per second and 100 read transactions per second on average (from clients)  Write load is a mixture of:  ** writing credentials schema (5%),  ** writing credential definition (5%)  ** revoke registry definition (5%)  ** revoke registry update (5%)  ** write DID to ledger (20%)  ** write payment to ledger (45%)  ** write attrib to ledger (15%)  Read load is a mixture of:  ** read DID from ledger (45%)  ** read credential schema (10%)  ** read credential definition (10%)  ** read revoke registry definition (10%)  ** read revoke registry delta (10%)  ** read attrib from ledger (10%)  ** read payment balance from ledger (5%)  * Write response time should be less that 5 seconds (would also like a report of the average).  * Read response time should be less than 1 second (would also like a report of the average).  * Unless view change in progress, pool should write 10 txns/sec and read 100 txns/sec  * Fees should be defined for all write transaction types.",Task,Medium,Complete,"2018-09-26 15:15:49","2018-09-26 15:15:49",5
"Hyperledger Indy Node","Triage old bug reports","The Evernym Indy Node team will be closing old bug reports that are unlikely to be reproducible on the current release.    *Acceptance Criteria*   Overall:   * Each engineer will spend one day of the sprint in this activity.   * The goal is to review at least 8 issues (less than 1 hour per issue).    For each issue:   * Read the issue and the comments.   * Briefly investigate the issue using the current code base (not the code base where the issue was found).   * Close the issue as Done if:   ** It appears to be something that we tested as part of our most recent release, and we didn't see the issue.   ** You try to reproduce the issue, and didn't see the issue.   ** A corresponding task/story exists (or created by the engineer) that addresses the same issue. Make sure that the task/story is linked to the bug.   * Keep the issue open if you are able to reproduce it or suspect that it is a real issue after your brief investigation.   * Include a comment documenting:   ** why you closed the issue,   ** or what you found during your investigation, including your current environment, symptoms observed, and steps to remediate.   * Create any related tickets that your investigation shows are necessary.   * If you think any of these issues should be immediately addressed, please escalate to [~<USER> and [~<USER>    *Notes*   * Close the issue as Done if you are at least 80% confident that the issue does not exist in the latest code base. The goal is to separate important issues from unimportant issues, and real issues will surface again at some point in the future.   * If during your investigation you see that you can fix the issue quickly (only a couple of hours), proceed to fix the issue.",Task,Medium,Complete,"2018-09-24 23:54:49","2018-09-24 23:54:49",0
"Hyperledger Indy Node","Assist EV QA","The Evernym Indy Node team will be assisting other Evernym teams with QA tasks.    *Acceptance Criteria*  * Each engineer will spend one day running QA scripts provided by the Evernym QA team.  * Instructions on what to test, and how to report the results will be provided by [~<USER>.  * Part of the day should be spent doing structured QA, and part should be spent doing related exploratory testing.   * Also provide feedback on how to improve the testing instructions and activities.",Task,Medium,Complete,"2018-09-24 23:36:20","2018-09-24 23:36:20",0
"Hyperledger Indy Node","Explore how fees affect node stability and performance","As we can see in test results in INDY-1661, our stability and performance in case of load with fees significantly degrade in comparison with the same transactions types without fees. Examples from test results:  || ||Txns written before OOM   (10 txns/sec load)||Pool throughput||  |NYMs|at least 419389|at least 20 nyms/sec|  |NYMs+fees|273739|14 nyms+fees/sec (with input load 15 txns/sec)|    *Logs and metrics:*   Test case ID: Ext-25-09-18-nym-schema-fees-nym   Load: 10 writes/sec (5 nyms with fees + 5 schemas with fees), ~1000 mixed reads/sec.   Metrics visualization:   !Ext-25-09-18-nym-schema-fees-nym.png|thumbnail!   AWS s3 link: s3://qanodelogs/INDY-1713-nym-schem   To get logs, run following command on log processor machine:    aws s3 cp --recursive s3://qanodelogs/INDY-1713-nym-schem/ /home/ev/logs/INDY-1713-nym-schem/         *Acceptance criteria*   * Find out how much time request validation takes for every txn type with FEEs and without",Task,High,Complete,"2018-09-24 20:29:02","2018-09-24 20:29:02",5
"Hyperledger Indy Node","POA: Delegate issuance to a processor","*Story*  As an organization issuing claims on an Indy network, I want to be able to delegate the issuing of credentials to a third party so that I benefit from them simplifying my business processes. Verifiers of those credentials should see that they came from me as the controller, but should also be able to identify the processor for audit purposes.    *Requirements*  * The controller should be able to change processors by updating the credential definition, but the history of processors should be retained.    *Acceptance Criteria*  * Create a plan  * Get architectural review of the plan from stakeholders including Evernym and the Sovrin Foundation  * Create the necessary issues    *Possible Approaches*    _Add issuer field to cred def transactions_  When using delegation, where one company (the processor) issues credentials for another (the controller), to the verifier the claims appear to have come from the controller.  This is fine, but we also need to be able to tell that the processor was involved with issuing the claim.     To do this, please add an additional field to cred def transactions called issuer.  The value of the field will be a DID that will be set by the controller before it is written to the ledger.    In addition it will be necessary for the verifier to be able to determine what cred def was used to make the claim in the proof. In the current proof format, the schema sequence number is provided for each claim, but the cred def sequence number is not. Other than parsing the ledger to find the cred def with a particular ref and identifier, which would be very inefficient, there is not an apparent way for the verifier to find the correct cred def. If indeed there currently is no better way, then please also add a cred def sequence number to the claims in a proof.    It will then be the responsibility of the verifier to retrieve the cred def from the ledger to determine the processor, when needed. Two very key customers currently need this capability, with more to follow.    The additional field in the cred def allows the controller to show that he has delegated proof issuance to the processor who actually holds the private keys to the cred def.    *Notes*  This story introduces a new participant in credential verification. The 3-party story (issuer, prover, and verifier) is modified into a 4-party story, where the issuer is split into 2, the controller and the processor. Both will need DIDs on the network, so that the verifier is able to see that the credential is from the controller, by means of the processor.",Story,High,Complete,"2018-09-20 21:12:35","2018-09-20 21:12:35",3
"Hyperledger Indy Node","POA: Require multiple signatures for important transactions","*Story*  As a trustee of an Indy Network, the network should require other trustees to sign administrative transactions I want to sponsor so that I am confident that no single trustee can abuse the network.    *Acceptance Criteria*  * All transactions that require a trustee role should require multiple trustee signatures. Specifically:  ** Add a trustee  ** Remove a trustee  ** Add a steward  ** Revoke the role of steward  ** Demote a consensus node  ** Send an upgrade transaction to the ledger  * The number of signatures required should be specified on the configuration ledger  * Create a Plan of Attack for how we would address this problem.  * Create a HIPE describing how we want to handle trustee permissions.  * Create an Epic and issues for doing the work.    *Notes*  * In the future, it would be useful to specify the number of signatures as a ratio of total trustees, but this is not required for this story. (Depends on INDY-1594)",Story,Medium,Complete,"2018-09-13 23:29:40","2018-09-13 23:29:40",5
"Hyperledger Indy Node","Review and  fix not pinned dependencies in indy-anoncreds","The task reflects recent discussion with Daniel about the list of things that should be presented in projects.  Need to use strict dependencies instead of omitted ('latest') ones.    * Acceptance Criteria*   * List the dependencies than need pinning  * For each dependency, briefly evaluate whether the project has traditionally broken during minor upgrades  ** Review previous release notes  ** Review our previous experience depending on the package  * Make a proposal for how to pin  ** Weigh the QA risk of floating packages  ** Compare with the risk of us missing a service pack security fix or breaking when ported to other platforms (Fedora)  ** Get approval from Daniel  * Implement the change  * If a dependency currently has an upgrade available or that will be available soon, create an issue for testing the upgrade.    *Notes*  * We are pinning to the currently known working versions. Upgrading the packages is not required.  * We don't want to pin packages used exclusively for testing. They don't impact QA, and we can pin them later if they cause a build breakage and fixing them is too big of a burden.  * We are guessing that Indy-Anoncreds has less than 5 dependencies.",Task,Medium,Complete,"2018-09-13 16:52:50","2018-09-13 16:52:50",5
"Hyperledger Indy Node","Review and  fix not pinned dependencies in indy-node","The task reflects recent discussion with Daniel about the list of things that should be presented in projects.  Need to use strict dependencies instead of omitted ('latest') ones.    * Acceptance Criteria*   * List the dependencies than need pinning  * For each dependency, briefly evaluate whether the project has traditionally broken during minor upgrades  ** Review previous release notes  ** Review our previous experience depending on the package  * Make a proposal for how to pin  ** Weigh the QA risk of floating packages  ** Compare with the risk of us missing a service pack security fix or breaking when ported to other platforms (Fedora)  ** Get approval from Daniel  * Implement the change  * If a dependency currently has an upgrade available or that will be available soon, create an issue for testing the upgrade.    *Notes*  * We are pinning to the currently known working versions. Upgrading the packages is not required.  * We don't want to pin packages used exclusively for testing. They don't impact QA, and we can pin them later if they cause a build breakage and fixing them is too big of a burden.  * We are guessing that Node has less than 3 dependencies.",Task,Medium,Complete,"2018-09-13 16:52:06","2018-09-13 16:52:06",3
"Hyperledger Indy Node","Review and  fix not pinned dependencies in indy-plenum","The task reflects recent discussion with Daniel about the list of things that should be presented in projects.   Need to use strict dependencies instead of omitted ('latest') ones.    *Acceptance Criteria*   * List the dependencies than need pinning   * For each dependency, briefly evaluate whether the project has traditionally broken during minor upgrades   ** Review previous release notes   ** Review our previous experience depending on the package   * Make a proposal for how to pin   ** Weigh the QA risk of floating packages   ** Compare with the risk of us missing a service pack security fix or breaking when ported to other platforms (Fedora)   * Get approval from Daniel   * Implement the change   * If a dependency currently has an upgrade available or that will be available soon, create an issue for testing the upgrade.    *Notes*   * We are pinning to the currently known working versions. Upgrading the packages is not required.   * We don't want to pin packages used exclusively for testing. They don't impact QA, and we can pin them later if they cause a build breakage and fixing them is too big of a burden.   * We are guessing that Plenum has less than 3 not pinned dependencies.    *Questions*   * What is our plan for upgrading package dependencies in the future?   ** Can we have development packages that always use the latest dependencies so that we identify problems quickly?   ** How often should we schedule work to upgrade the dependencies?",Task,Medium,Complete,"2018-09-13 16:51:29","2018-09-13 16:51:29",3
"Hyperledger Indy Node","Define how node deals with exceptions thrown by plugins","Observed:    An AttributeError was thrown during the execution of PRE_DYNAMIC_VALIDATION hook. This exception was not caught and surfaced all the way out to start_indy_node. Effectively crashing the node.    processReqDuringBatch in indy-plenum/plenum/server/replica.py catches InvalidClientMessageException and UnknownIdentifier but don't deal with more generic exceptions         Expected:  A pooly written hook should not be able to crash the node. At least not so easily.         This issue was obsured in work done at Evernym. If this ticket is worked on by an Evernym employee, [https://evernym.atlassian.net/browse/TOK-425 |https://evernym.atlassian.net/browse/TOK-425,] could provide additional context.    *Acceptance Criteria:*  * Produce a document that defines:  ** the contract between Indy Node and Plugins  ** description of how Indy Node will respond when a plugin does not fulfill the contract  * Write tests that ensure that Indy Node fulfills this contract.  * Ensure the contract is properly implemented in Plenum's code base",Task,High,Complete,"2018-09-12 22:57:09","2018-09-12 22:57:09",5
"Hyperledger Indy Node","Create CHANGELOG.md","There should be a CHANGELOG.md file in the root of the Indy Node git repo.    It should contain brief release notes for the previous releases, back to 1.1.37.    Release notes should contain:  * A summary of major changes  * Brief notes about upgrades and behavior  * Any known issues that are important to communicate    The top of the changelog should include this explanation (with links):  Details about what is included in this release can be found by browsing the fix version in the Hyperledger Issue Tracker and the tag in GitHub.    Format should be similar to the CHANGELOG.md in the indy-skd.    Contents can come from the Sovrin release notes that accompanied each Indy Node release:  https://github.com/sovrin-foundation/sovrin/blob/master/release-notes.md",Task,Medium,Complete,"2018-09-06 14:38:09","2018-09-06 14:38:09",1
"Hyperledger Indy Node","Investigate Out of memory issues with the current load testing","* Check results of the latest load testing where we fail with out of memory   * Check if it depends on txn type and load         *Acceptance criteria*   # Run load tests with different txn types and different load   # Find out if txn type affects memory issues   # Find out if load affects memory issues and what load we can handle without issues   # Find out if the number of unprocessed requests affects memory issues   # Create necessary bugs or do simple fixes if needed",Task,High,Complete,"2018-09-06 14:15:19","2018-09-06 14:15:19",8
"Hyperledger Indy Node","We need to take as much as possible Node-to-Node messages from queues before starting View Change","The current View Change protocol is based on catch-up and last prepared certificate calculated by each node individually before starting the View Change.    The idea is the following:   * Each nodes calculates the maximum 3PC seqNo for a quorum of received prepares.   * Once we have a quorum (n-f) Prepares, other nodes should also have this (in an ideal perfect world). So, we should have n-f nodes at the same state, while other f nodes can catchup to this state.    However, in reality this approach doesn't work in 100% cases and may lead to some nodes go beyond others because   * The network is async and unpredictable, so all nodes may send Prepares, some nodes may receive them, but some not   * *There is a message queue with node-to-node messages, and messages are grabbed from there by small portions to avoid looper iterations taking too much time*    *The problem:*   * It's possible that a Node has enough Prepares to set a higher prepared certificate, but they are still queued when View Change  is started, and prepared certificate is older than can be.   * So, if we have fast and slow nodes, for fast nodes we may have higher prepared certificate than for slow ones because fast nodes process message queue faster. This may lead to a situation where fat nodes go beyond others, and we lost consistency.    *Acceptance criteria:*   * Provide a configurable strategy that can process as much as possible messages form Node-to-Node message queue before starting the View Change and setting last prepared certificate.     ** We would like to grab all messages   ** However, there is a risk of infinite grabbing if there is intensive load   ** So, probably we still need to have some limits, or say ZMQ to not process any incoming messages while we are flushing the queue.   * Test the strategy and enable it by default if everything is fine",Task,Medium,Complete,"2018-09-06 14:07:25","2018-09-06 14:07:25",5
"Hyperledger Indy Node","Check if ZMQ connections and queues affect memory consumption","As of now we limit ZMQ message queues, but this limit is per-connection (there is a queue for every connection)    *Acceptance criteria:*   - create tests (can be without Plenum, just isolated one based on ZMQ) emulating SDK client load more than server can process   ** for example 40 requests per sec from client, and 20 reads per sec by listener   ** client need to create a new connection for every 5 requests   ** create tests for Python and C implementation of ZMQ   - compare memory consumption   ** compare Python and C   ** analyse how ZMQ deals with a lot of connection (how it reads requests, and releases queues for each connections)   ** check if and when memory is released once load is stopped.   - check how Monitor affect memory consumption    Compare memory usage in each case",Task,High,Complete,"2018-09-06 13:52:36","2018-09-06 13:52:36",8
"Hyperledger Indy Node","Improve the switch off replica logic to do it after InstanceChange quorum","A backup replica should be switch off after quorum of INSTANCE_CHANGE messages on backup instance.    *Acceptance criteria:*   - A strategy done in INDY-1682 should result in sending a special INSTANCE_CHANGE about instance change on backup instance X   - Once node got a quorum (n-f) of instance changes on backup instance X, it switches off the replica (INDY-1680)   - add tests to check correct replica switch off     - testing perfomance changes (shouldn't be worse then in task INDY-1682) after disconnect or degraded a backup primary node.     - testing memory consumption (should be better then in task INDY-1682)",Task,Medium,Complete,"2018-09-05 11:04:24","2018-09-05 11:04:24",5
"Hyperledger Indy Node","Improve the switch off replica logic to collecting all reasons (not only disconnected primary)","A backup replica should be switch off not only with disconnected backup primary (task: INDY-1681) but with backup instance degradation. It shouldn't send BACKUP_INSTANCE_FAULTY but should collect its own messages with this type.    *Acceptance criteria:*   - Implement a new strategy (see INDY-1681) which will check for performance degradation (the same way as master does) in addition to disconnections   - switch off a backup replica when it has more then M its BACKUP_INSTANCE_FAULTY messages in K time.   - add tests to check correct replica switch off   - testing performance changes (shouldn't be worse) after disconnect or degraded a backup primary node.   - testing memory consumption (should be better in case a backup primary node degraded)",Task,Medium,Complete,"2018-09-05 10:31:34","2018-09-05 10:31:34",5
"Hyperledger Indy Node","Switch off a replica that stopped because disconnected from a backup primary","If a backup primary node is disconnected, all replicas on this backup instance store all new requests and other replicas can't remove already ordered messages.   For solve this problem we should detect that a backup primary node was disconnected for a long constant time and switch off the replica with this primary. (task: INDY-1680)    *Acceptance criteria:*   - Implement an abstract strategy to detect malicious backup primaries   - Implement a strategy which detects malicious backup primaries by disconnection   ** We need to have a tolerance time we wait before reporting disconnection (like being disconnected for 10 secs in a row)   - switch off a replica (a code in INDY-1680) once strategy detects malicious.   - make sure that all replicas are switch on after a View Change   - add tests     - testing performance changes (shouldn't be worse) after disconnect a backup primary node   - testing memory consumption (should be better)",Task,Medium,Complete,"2018-09-05 10:08:51","2018-09-05 10:08:51",5
"Hyperledger Indy Node","Ability to switch off (remove) replicas with no changes of F value ","If a backup primary node is malicious (for example, disconnected), all replicas on this backup instance store all new requests and other replicas can't remove already ordered messages.   For solve this problem we should have ability to switch off (remove) replicas with no changes value F:         *Acceptance criteria*   - add function to remove a replica (and cleaning client requests for this replica)   - It should not lead to change of F value   - re-evaluate requests queue once replica is removed to check if we can clear some requests   - correctly process messages for switched off replicas   ** discarding them may be fine for this version   - make sure monitor works as expected   - add tests for check this function and check the correct work of other systems (like a monitor, requests removing)",Task,Medium,Complete,"2018-09-05 09:40:21","2018-09-05 09:40:21",3
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.6.73","*Version Information*  indy-node 1.6.73    indy-plenum 1.6.51    indy-anoncreds 1.0.11    sovrin 1.1.17     *Major Fixes*  INDY-1583 - Pool stopped writing after F change  INDY-1645 - read_ledger passes incorrectly formatted stdout and breaks convention  INDY-1595 - Node can't catch up large ledger  INDY-1603 - Validator Info may hang for a couple of minutes  INDY-1548 - read_ledger tool is not able to read the sovrin plugin ledger    *Changes and Additions*  INDY-1642 - 3PC Batch should preserve the order of requests when applying PrePrepare on non-primary  INDY-1643 - Monitor needs to take into account requests not passing dynamic validation when trigerring view change  INDY-1565 - Improve throughput calculation to reduce a chance of false positive View Changes  INDY-1660 - Performance of monitor should be improved  INDY-1588 - As a Steward, I need to have a script that can generate Proof of possession for my BLS key, so that I can use this value in a NODE txn  INDY-1389 - Support Proof of Possession for BLS keys  INDY-1582 - Do not use average when calculating total throughput/latency of backups  INDY-1564 - Discard any client requests during view change  INDY-1568 - As a developer I need a simple tool to show graphical representation of some common metrics  INDY-1549 - Change default configs for better performance and stability    *Known Issues*  INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66  (!) Note that INDY-1447 was fixed in indy-node 1.5.68, but it still presents in indy-node 1.3.62 and 1.4.66 code. So, *some of the nodes may not to be upgraded during simultaneous pool-upgrade*. If this problem will appear, stewards should perform manual upgrade of indy-node in accordance with this instruction: [https://docs.google.com/document/d/1vUvbioL5OsmZMSkwRcu0p0jdttJO5VS8K3GhDLdNaoI]  (!) To reduce the risk of reproducing INDY-1447, it is *recommended to use old CLI for pool upgrade*.    (!) *Pool upgrade from indy-node 1.3.62 to indy-node 1.6.73 should be performed simultaneously for all nodes due to txn format changes.*  (!) *There must be sovrin package upgrade to 1.1.17 version after indy-node package upgrade. You need to specify package=sovrin in pool-upgrade command to do this.*   (!) *All indy-cli pools should be recreated with actual genesis files.*  (i) *For more details about txn format changes see INDY-1421.*  (i) *There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*",Task,Medium,Complete,"2018-09-04 16:31:18","2018-09-04 16:31:18",1
"Hyperledger Indy Node","Need to account fields from PLUGIN_CLIENT_REQUEST_FIELDS when calculating digest","We need to account PLUGIN_CLIENT_REQUEST_FIELDS in digest calculation.    For fields from PLUGIN_CLIENT_REQUEST_FIELDS we might call some special method that will provide structure that won't change if we will shuffle elements in the request field. It will be implemented in a place where these fields are defined.",Task,Medium,Complete,"2018-09-03 12:51:02","2018-09-03 12:51:02",3
"Hyperledger Indy Node","Need to call a PRE_SEND_REPLY hook when request is already successfully written to ledger","If some request writes transaction for example to domain ledger and to some plugged one, it will be returned as two transactions one inside another in some special field. However, when we send the same request the second time, ledger will send the response from transaction log only from domain ledger, disregarding the plugged one. It can be solved with the call to the PRE_SEND_REPLY hook.",Task,Medium,Complete,"2018-09-03 12:45:41","2018-09-03 12:45:41",2
"Hyperledger Indy Node","Create Service Pack Release 1.6.73",,Task,Medium,Complete,"2018-08-30 16:25:18","2018-08-30 16:25:18",3
"Hyperledger Indy Node","Extend load scripts emulating non-smooth load according to the changes in the core script","The core load script functionality will be changed in the scope of INDY-1605.    We need to support it in the scripts created in INDY-1567 and INDY-1666.",Task,Medium,Complete,"2018-08-30 14:26:09","2018-08-30 14:26:09",2
"Hyperledger Indy Node","Write load scripts emulating non-smooth incrementing load ","We created an extension to load scripts emulating non-smooth load in the scope of INDY-1567.    *Acceptance criteria*    We need to improve the script to support the following patterns:   * Permanently incrementing load   * Permanently incrementing load with spikes",Task,Medium,Complete,"2018-08-30 14:24:09","2018-08-30 14:24:09",2
"Hyperledger Indy Node","Support all FEEs txns in the load script","As of now, we support FEEs for NYM and SCHEMA only. We need to support it for all Domain txns.    This needs to be dome after improvements in INDY-1605",Task,Medium,Complete,"2018-08-30 14:16:53","2018-08-30 14:16:53",3
"Hyperledger Indy Node","Find out maximal pool performance numbers for mixed mode (read and write)","Find out maximal pool performance for a mix of of writs and reads:   * Mixture mode as in INDY-1343   * Mixture mode with FEEs (for the txns we support FEEs in the load script)    Run strategy:   * Consider have metrics enabled and increase the load starting from a small number. If according to metrics everything is fine, increase the load further.",Task,Medium,Complete,"2018-08-30 14:09:33","2018-08-30 14:09:33",5
"Hyperledger Indy Node","Find out maximal pool performance numbers for read only","Find out maximal pool performance for reads only:   * Only NYMs   * Mixture mode as in INDY-1343   * Payments only",Task,Medium,Complete,"2018-08-30 14:08:25","2018-08-30 14:08:25",5
"Hyperledger Indy Node","Test domain transactions with FEEs","We need to test stability and performance of the Ledger with FEEs enabled.    The load script was extended in the scope of INDY-1636 to support FEEs (FEEs support there will be continued in the scope of INDY-1605, but this is enough to start testing).    *Acceptance criteria*   * Run load tests with FEEs enabled     ** NYM write only   *** 5 writes per sec   *** 10 writes per sec   *** 15 writes per sec   ** NYM read and write   *** 10 writes and 100 reads per sec   ** NYM and SCHEMA   *** 10 writes and 100 reads per sec   * Check for Stability issues   * Check for the number of view changes   * Metrics output   * Performance values",Task,Medium,Complete,"2018-08-30 08:41:23","2018-08-30 08:41:23",8
"Hyperledger Indy Node","Performance of monitor should be improved","Under high load when requests are received faster than they are ordered _monitor.requestOrdered_ starts taking significant portion of time, which leads to even slower ordering. This should be fixed.      There is a place in the monitor where we have quadratic asymptotic for 3PC batch size because we iterate twice.    We need to fix this, which can give us a benefit in performance, especially in case of DDoS where we may have huge 3PC batches.",Task,Medium,Complete,"2018-08-30 08:33:00","2018-08-30 08:33:00",2
"Hyperledger Indy Node","[Design] Fix node release numbers","*Story*   As an administrator of a node for an Indy network, I want to clearly understand the scope of changes in each release and map that release to version numbers in JIRA and the release notes.    *Goals:*   * Version numbers use 4 values, generally following semver: major release, minor release, service pack, and build.   * The build number increases with each build, starting with zero on each service pack.   * The major, minor, and service pack numbers should be the same for all artifacts in the release: debian, android, etc. The build number can float.   * Next release needs to be one higher than the current stable build.    *Acceptance criteria:*   * Design/PoA    * WBS and technical tasks   * Review of Design",Task,Medium,Complete,"2018-08-28 14:12:12","2018-08-28 14:12:12",5
"Hyperledger Indy Node","Support Incremental creation of 3PC batches","As of now each PrePrepare defines 1 3PC batch. Ordering each batch requires sending and processing lots of 3PC messages, so the more requests we put into batch, the better throughput we achieve (theoretically). Unfortunatelly now dynamic validation is performed when PrePrepare is created or received, which means that we need to perform dynamic validation of all requests in a batch at once, and for batches consisting of 1000s requests it means pause of tens seconds during which node is not responsive. Furthermore, this validation takes place on primary replica, and only after it is finished here it can start on non-primary.    Proposed solution is to allow multiple PrePrepares define 1 3PC batch,  so that validation of each PrePrepare is relatively fast and can overlap between primary and non-primary nodes reducing total latency. This will allow much bigger batches which in turn can significantly reduce workload per request, leading to higher throughput.",Task,Medium,New,"2018-08-28 13:48:26","2018-08-28 13:48:26",8
"Hyperledger Indy Node","Research performance of static validation against message schema","Each message (node and client one) is validated against Message Schema in the scope of static validation.    According to metrics, it takes quite a lot of time.    *Acceptance criteria*   * Come with a plan (PoA) on how to improve performance.   One of the following options is possible:     *    ** Everything is fine, and validation is slow just because of a huge number of messages   ** There is an obvious issue in the code   ** A fundamental change to the approach is proposed   * Create a ticket for implementation of the proposed solution",Task,Medium,New,"2018-08-28 13:45:16","2018-08-28 13:45:16",3
"Hyperledger Indy Node","Do not re-verify signature for Propagates with already verified requests ","In order to improve performance, we can avoid redundant verification of request signatures in Propagates for which we already verified signature",Task,High,Complete,"2018-08-28 13:41:31","2018-08-28 13:41:31",3
"Hyperledger Indy Node","Find out maximal pool performance numbers for writes only","Find out maximal pool performance for writes only:   * Only NYMs   * Payments only   * Mixture mode as in INDY-1343   * NYMs with FEEs   * Mixture mode with FEEs (for the txns we support FEEs in the load script)    Run strategy:   * Consider have metrics enabled and increase the load starting from a small number. If according to metrics everything is fine, increase the load further.    Results:  |NO FEES|Initial load|Version| Result|  |nym|10|1.6.738| 33|  |attrib|10|1.6.738| 30|  |schema|8|1.6.747| 23.5|  |cred_def|6|1.6.747| 9|  |revoc_reg_def|5|1.6.753| 20|  |revoc_reg_entry|5| | |  |payment|5|1.6.738 | 20|  |production load|10|1.6.747| 15|  |FEES|Initial load|Version| Result|  |nym|10|1.6.738 | 20|  |attrib|8|1.6.738| 17.7|  |schema|8|1.6.747| 17.5|  |cred_def|3|1.6.752| 8|  |revoc_reg_def|3|1.6.761| 14|  |revoc_reg_entry|3| | |  |payment|3|1.6.738| 20|  |production load|3|1.6.747| 11|",Task,Medium,Complete,"2018-08-27 20:06:22","2018-08-27 20:06:22",5
"Hyperledger Indy Node","Monitor needs to take into account requests not passing dynamic validation when trigerring view change","When Monitor calculates throughput and latency for master, it takes into account only valid requests, that is requests passing dynamic validation. But non-master instances don't have dynamic validation, so all requests are taken into account there.    It may lead to false-positive View Changes since backup instances will think that master degraded, while it just had a lot of invalid requests.    *Acceptance criteria:*   * Include information about all requests into Ordered message   * Pass all requests into Monitor   * Take only valid requests when processing Ordered message on Node.    Can be done together with INDY-1642 to take into account a new PrePrepare format.",Task,High,Complete,"2018-08-24 10:16:39","2018-08-24 10:16:39",2
"Hyperledger Indy Node","3PC Batch should preserve the order of requests when applying PrePrepare on non-primary","Primary doesn't preserve the order of requests it applied them, so it may lead to different results of dynamic validation and Suspicious Exception on non-primaries.    It may lead to a lot of View Changes.    *Example:*   * When master primary creates a PrePrepare it iterates through requests in some order, let's say [R1, R2, R3, R4, R5]   * Let's assume that request R1 and R2 can be created only after R3, so they fail dynamic validation.   * As a result, a list of valid requests is [R3, R4, R5], and a list of invalid is [R1, R2].   * When PrePrepare is created, it get a list of reqs as valid + invalid, that is [R3, R4, R5, R1, R2], which is not equal to the initial list the master primary iterated through.   * Non-primary replicas will apply PrePrepares in the order as specified in PrePrepare, that is [R3, R4, R5, R1, R2], and for it R1 and R2 will pass dynamic validation since R3 is already created.   * So, non-primaries will raise Suspicious code and trigger view change.         *Acceptance criteria:*   * Preserve the order of requests for master primary in PrePrepare   * Have a bitmask to mark invalid requests   * Extend dynamic validation to eack the bitmask   * Enhance creation of Ordered requests to get only the valid ones",Task,High,Complete,"2018-08-24 09:23:06","2018-08-24 09:23:06",3
"Hyperledger Indy Node","A new strategy to reduce a risk of false positive view changes","We need to implement a pluggable strategy to trigger the view change as follows:   * Measure how many requests are ordered (on each instance) on a given window MEASUREMENT_INTERVAL   * If the number of ordered requests on master is less than the average (or median) on backup instances to some Delta during DEGRADADED_INTERVALS_COUNT windows in a row, then trigger View Change",Task,Medium,Complete,"2018-08-22 16:19:28","2018-08-22 16:19:28",3
"Hyperledger Indy Node","Make validator info as a historical data","For now validator-info dumps state of node structures into single file, so that we can just check node state at the time of the last dump.    It is very useful to have ability to get historical data of validator-info to track node's state transition in time.    Proposed solution:   * Add ability to save validator-info data in RocksDB storage in form   ** key = timestamp   ** value = json_data   * Add ability to switch between file dump (current implementation, just last state) and RocksDB (historical data)   * Change validator-info console app so that it checks config and finds out which data source to use. Several additional parameters may be added in case of RocksDB used, i.e. number of last records to return, time range etc.    Of course RocksDB with historical data is not for production usage.    Also for now there is no any synchronisation between validator-info writer and reader of file, it should be fixed too. Fortunately, RocksDB supports read-only mode, so there is no need to care about synchronisation here.",Task,Medium,Complete,"2018-08-22 14:51:25","2018-08-22 14:51:25",3
"Hyperledger Indy Node","Load script needs to be able to set transactions with FEEs","*Acceptance criteria:*   * Need to be able to apply FEEs to all Domain transactions supported by load script.",Task,Medium,Complete,"2018-08-22 13:59:19","2018-08-22 13:59:19",5
"Hyperledger Indy Node","As a dev/QA I need a way to create repeatable and fully automated load test (or other experiment)",,Story,Medium,New,"2018-08-22 10:38:13","2018-08-22 10:38:13",5
"Hyperledger Indy Node","As a dev/QA I need an easy way to clean pool data without destroying AWS instance",,Story,Medium,New,"2018-08-22 10:35:37","2018-08-22 10:35:37",1
"Hyperledger Indy Node","As a dev/QA I need an easy way to give access to testing pool to other team members","This is closely coupled with INDY-1622.",Story,Medium,Complete,"2018-08-22 10:34:34","2018-08-22 10:34:34",3
"Hyperledger Indy Node","As a dev/QA I need an easy way to download from S3 bucket all or some parts of data from specific experiment",,Story,Medium,New,"2018-08-22 10:31:33","2018-08-22 10:31:33",2
"Hyperledger Indy Node","As a dev/QA I need an easy way to upload all data from group of nodes to S3 bucket",,Story,Medium,New,"2018-08-22 10:29:06","2018-08-22 10:29:06",2
"Hyperledger Indy Node","As a dev/QA I need an easy way to add/remove load test agents from group",,Story,Medium,New,"2018-08-22 10:25:43","2018-08-22 10:25:43",2
"Hyperledger Indy Node","As a dev/QA I need an easy way to add/remove nodes from already configured pool",,Story,Medium,New,"2018-08-22 10:24:47","2018-08-22 10:24:47",3
"Hyperledger Indy Node","As a dev/QA I need an easy way to run load test from several agent instances",,Story,Medium,"In Progress","2018-08-22 10:23:32","2018-08-22 10:23:32",5
"Hyperledger Indy Node","As a dev/QA I need an easy way to install CLI and load scripts pointing to previously configured pool","*Acceptance criteria*  * _inventory_init.py_ script should prepare group vars file where number of node and client instances are separately configurable  * _pool_create.yml_ playbook should provision both node and client instance according to inventory group vars  * _pool_install.yml_ playbook should configure both node and client instances according to inventory group vars  * client instances should have load script installed as well as pool genesis transaction pointing at nodes  * _pool_destroy.yml_ playbook should destroy both node and client instances  * _pool_*.yml_ playbooks probably should be renamed to better reflect that they manage not only nodes, but clients also (probably just _create.yml_, _configure.yml_, _destroy.yml_)",Story,Medium,Complete,"2018-08-22 10:22:38","2018-08-22 10:22:38",3
"Hyperledger Indy Node","As a dev/QA I need an easy way to preload indy node pool with a large number of transactions","Probably this should be backup/restore workflow",Story,Medium,New,"2018-08-22 10:21:17","2018-08-22 10:21:17",3
"Hyperledger Indy Node","As a dev/QA I need an easy way to quickly adjust configuration of Indy Node on a number of instances",,Story,Medium,New,"2018-08-22 10:17:21","2018-08-22 10:17:21",1
"Hyperledger Indy Node","As a dev/QA I need an easy way to install and configure Indy Node (possibly with plugins) on a number of instances",,Story,High,Complete,"2018-08-22 10:16:12","2018-08-22 10:16:12",3
"Hyperledger Indy Node","As a dev/QA I need an easy way to create and manage a number of AWS instances","Additional requirements:  - groups should be named  - instances of group should be created in different AWS regions  - there should be easy way to generate ssh config to access instances in pool  - there should be easy way to access all instances in group as a whole using Ansible    There are two major options how this can be implemented:  1. Tool that creates pool should outputs ssh config and ansible inventory file that could be used to access group instances without requiring access to AWS API.  2. Tool that creates pool just tags instances somehow, so Ansible dynamic or in-memory inventory could be used, which can be easier for further automation.  ",Story,High,Complete,"2018-08-22 10:13:46","2018-08-22 10:13:46",5
"Hyperledger Indy Node","Research Sawtooth","Evaluate if Sawtooth could be made into a viable replacement for Indy.   Timebox the effort to 3 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.   Discussion should include:   ** Pros and Cons   ** The state of the project's community   ** The current roadmap   ** Rough estimate of work required to bring Sawtooth to the same level as Plenum   ** A recommendation on whether to research more or kill the proposal   ** Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]    Things to consider:   * Research how clients are authorized by ledger   * How does this relate to work with observers   * How much better is performance   * How many nodes would be practical (50? 500? 5000?)   * Get feedback from community   * Notice: One of PoC goals should be smooth upgrade path   * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.",Task,Medium,New,"2018-08-21 12:41:45","2018-08-21 12:41:45",3
"Hyperledger Indy Node","Run load tests just with one Replica","Patch the code to have one (master) replica only and compare performance with the normal case. Needs to be done after INDY-1649 is fixed.    *Acceptance criteria:*   * Patch the code (should not go to production)   * Run load tests with patched code to write NYMs only   * Run load tests with patched code in a mixed mode     ",Task,Medium,New,"2018-08-21 11:39:15","2018-08-21 11:39:15",3
"Hyperledger Indy Node","Evaluate alternatives to RBFT","Consider other BFT-based algorithms which can be better than RBFT.  Timebox the effort to 3 engineering days.    *Acceptance Criteria*  * Brief summary of findings evaluating the top alternatives to RBFT for review by Richard, Nathan, Daniel, Jason, and the broader community.  * Each alternative should cover  ** Characteristics of current implementations of the protocol in production usage  ** Pros and Cons  ** Rough estimate of work required  * Current recommendations (given the limited research done to date)",Task,Medium,Complete,"2018-08-21 11:35:06","2018-08-21 11:35:06",3
"Hyperledger Indy Node","Research Tendermint implementation","[https://github.com/tendermint/tendermint]    We need to find out how stable Tendermint implementation is .   Timebox the effort to 4 engineering days.    Acceptance Criteria   * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.   Discussion should include:   ** Pros and Cons   ** Characteristics of current deployments in production usage   ** Current project roadmap   ** Health of the open source community / level of investment   ** Rough estimate of work required to bring Tendermint to the same level as Plenum   ** Rough understanding of performance   ** Rough understanding of the difficulty of migrating from the existing Plenum to an implementation based on Tendermint   * A recommendation on whether to research more or kill the proposal   * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]         Things to consider:   * Research how clients are authorized by ledger   * How does this relate to work with observers   * How much better is performance   * How many nodes would be practical (50? 500? 5000?)   * Get feedback from community   * Notice: One of PoC goals should be smooth upgrade path   * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.",Task,Medium,New,"2018-08-21 11:33:44","2018-08-21 11:33:44",3
"Hyperledger Indy Node","Research Tendermint protocol","Find out if Tendermint protocol looks good for Indy and is better than RBFT  Timebox the effort to 2 engineering days.    Acceptance Criteria  * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community. Discussion should include:  ** Pros and Cons  ** Characteristics of current production implementations of the protocol  ** Rough estimate of work required to bring implement Tendermint in Plenum  * A recommendation on whether to research more or kill the proposal  ",Task,Medium,Complete,"2018-08-21 10:31:23","2018-08-21 10:31:23",3
"Hyperledger Indy Node","Proof of stability under load","We need to prove that it will be stable under conditions similar to production use.    *Acceptance Criteria*  Perform a test of an Indy network that has the following attributes:    * The ledger is pre-loaded with 1 million transactions  * Pool size is 25 nodes.  * 1K concurrent clients  * Over a 3 hour period induce a sustained throughput of 10 write transactions per second and 100 read transactions per second on average (from clients)  Write load is a mixture of:  ** writing credentials schema (5%),  ** writing credential definition (5%)  ** revoke registry definition (5%)  ** revoke registry update (5%)  ** write DID to ledger (20%)  ** write payment to ledger (45%)  ** write attrib to ledger (15%)  Read load is a mixture of:  ** read DID from ledger (45%)  ** read credential schema (10%)  ** read credential definition (10%)  ** read revoke registry definition (10%)  ** read revoke registry delta (10%)  ** read attrib from ledger (10%)  ** read payment balance from ledger (5%)  * Write response time should be less that 5 seconds (would also like a report of the average).  * Read response time should be less than 1 second (would also like a report of the average).  * Unless view change in progress, pool should write 10 txns/sec and read 100 txns/sec    *Main cases for research:*  1) mixed set (imitation of active production work) - 1000 clients / 10 writes / 100 reads  2) payments only (understanding of pluggable ledgers productivity) - 1000 clients / as much as possible writes (1 - 4 - 10) / as much as possible reads (up to 10 - 40 - 100)  3) not payment transactions (comparision with historical productivity data) - 1000 clients / 10 writes / 100 reads",Task,Medium,Complete,"2018-08-17 13:08:20","2018-08-17 13:08:20",5
"Hyperledger Indy Node","Add tests for load script","Load script became huge and requires unit tests",Task,Medium,Complete,"2018-08-17 09:46:31","2018-08-17 09:46:31",3
"Hyperledger Indy Node","Load script became huge and and should be split into several files","Old load scripts are not supported now and should be deleted.    New load script should be split into several files.    *Acceptance criteria:*   * Split the script into multiple parts   * Cover with tests   * Create a pypi package   * Remove old (deprecated) load scripts",Task,Medium,Complete,"2018-08-17 09:44:24","2018-08-17 09:44:24",5
"Hyperledger Indy Node","Find out optimal message quotas ","*Acceptance criteria:*   * Do more load testing and find out optimal message quotas after analysing metrics:   ** NODE_TO_NODE_STACK_QUOTA   ** CLIENT_TO_NODE_STACK_QUOTA   ** NODE_TO_NODE_STACK_SIZE   ** CLIENT_TO_NODE_STACK_SIZE",Task,Medium,"In Progress","2018-08-16 13:04:05","2018-08-16 13:04:05",5
"Hyperledger Indy Node","Use average latency for all clients when check the latency for View Change triggering","As of now, Monitor tracks latency for every client independently. This may make it more sensitive for triggering View Change, but on the other hand it means that   * It means that we have a to have a huge map (if we have 1,000,000 clients, then it will take about 1GB of memory).   * We clean the map only after view change   * We are too sensitive to spikes for 1 client only.    Let's keep track of the average (for all clients) latency only.    *Acceptance criteria:*   * Move monitoring of latency into a strategy   * Move existing calculation of latency per client into a strategy   * Create a new strategy for calculation of average latency for all clients   ** As we use moving average, we can just take into account latencies for all reqs from all clients within the window   * Use the new strategy by default   * Write unit tests          ",Task,Medium,Complete,"2018-08-16 11:58:59","2018-08-16 11:58:59",3
"Hyperledger Indy Node","Run load with a lot of requests not passing dynamic validation (Rejects)",,Task,Medium,Complete,"2018-08-15 07:42:56","2018-08-15 07:42:56",3
"Hyperledger Indy Node","Run load with a lot of requests not passing static validation (NACKs)",,Task,Medium,Complete,"2018-08-15 07:42:30","2018-08-15 07:42:30",3
"Hyperledger Indy Node","A node should be able to participate in BLS multi-signature only if it has a valid proof of possession","Once all Stewards send a NODE txn with proof of possession, we should allow a Node to participate in BLS multi-signature only if it has a BLS key with a proof of possession.    *Acceptance criteria*   * Check that there is a valid proof of possession when creating a registry of BLS keys (see `BlsKeyRegister`)   * Add tests that Nodes without PoP can not participate in BLS multi-signature.   * The code may be disabled for a time being (until all Stewards provide a PoP).",Task,Medium,Complete,"2018-08-14 10:09:49","2018-08-14 10:09:49",2
"Hyperledger Indy Node","As a Steward, I need to have a script that can generate Proof of possession for my BLS key, so that I can use this value in a NODE txn","Each Steward must send a NODE txn with Proof of Possession of his BLS keys. So, we need a script that can help in generating the transaction.    *Acceptance criteria*   * Create a script in Indy-node: `generate_bls_proof_of_posession`     ** the script need to be installed with Indy Node   ** Input for the script: no   ** Output:   *** BLS Public key in a form to be sent in NODE txn (base58-encoded)   *** BLS Proof of possession for the Public Key in a form to be sent in NODE txn (base58-encoded)",Story,Medium,Complete,"2018-08-14 09:52:35","2018-08-14 09:52:35",1
"Hyperledger Indy Node","We need to be able to specify different quotas for Client-to-node and Node-to-node stacks","As of now, we can specify LISTENER_QUOTA and REMOTE_QUOTA, but LISTENER_QUOTA is applied for both Node-to-Node and Client-to-Node stacks (REMOTE_QUOTA is only for client-to-client, that is old deprecated agents).    We need to be able to set different quotas for node and client stacks.         *Acceptance criteria*   * Add new params to Plenum's config: NODE_TO_NODE_STACK_QUOTA, CLIENT_TO_NODE_STACK_QUOTA.   * Pass the values of these params to Node and Client Stacks as values for Listener quota.   * For now, let's keep the default values (100/100) for both",Task,Medium,Complete,"2018-08-14 08:38:37","2018-08-14 08:38:37",2
"Hyperledger Indy Node","Do not use average when calculating total throughput/latency of backups","Currently we use a simple average when calculating the total throughput/latency of backupo instances to compare it with the master ones.    This may lead to problems of false positive View Changes when there is a spike on just one of a backups.    *Acceptance criteria*   * Define a better way to calculate the average for backups. Consider median which is more robust to spikes.    * Write unit tests checking, in particular, that spikes should not trigger false positives.   * Implement the changes",Task,High,Complete,"2018-08-13 08:57:49","2018-08-13 08:57:49",3
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.6.70","*Version Information*   indy-node 1.6.70   indy-plenum 1.6.49   indy-anoncreds 1.0.11   sovrin 1.1.13    *Major Fixes*   INDY-1473 - Several nodes (less than f) get ahead the rest ones under load   INDY-1539 - Pool has stopped to write txns   INDY-1497 - Re-send messages to disconnected remotes   INDY-1478 - Pool stopped writing under 20txns/sec load   INDY-1519 - 1.3.62 -> 1.5.67 forced upgrade without one node in schedule was failed   INDY-1502 - tmp.log must have unique name   INDY-1199 - A node need to hook up to a lower viewChange   INDY-1470 - One of the nodes laged behind others after forced view changes   INDY-1544 - View Change should not be triggered by re-sending Primary disconnected if Primary is not disconnected anymore    *Changes and Additions*   INDY-1491 - As a Trustee running POOL_UPGRADE txn, I need to specify any package depending on indy-node, so that the package with the dependencies get upgraded   INDY-1555 - Monitor needs to be reset after the view change   INDY-1545 - GC by Checkpoints should not be triggered during View Change   INDY-1542 - Validator Info must show committed and uncommitted roots for all states   INDY-1475 - Explore timing and execution time   INDY-1493 - Memory leaks profiling   INDY-1531 - Bind connection socket to NODE_IP   INDY-1496 - Enable TRACK_CONNECTED_CLIENTS_NUM option   INDY-1378 - We should update revocation registry delta value during REG_ENTRY_REVOC writing   INDY-1480 - Support latest SDK in Indy Plenum and Node   INDY-1468 - Latency measurements in monitor should be windowed   INDY-1528 - Trust anchor permission not needed for ledger writes    *Known Issues*  INDY-1517 - Docker pool can't be built because of new python3-indy-crypto in sdk repo  (i) The problem described in INDY-1517 will be fixed in the next release of indy-node. Workaround for this problem is to add python3-indy-crypto=0.4.1 to the list of packages to be installed.   INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66   (!) Note that INDY-1447 was fixed in indy-node 1.5.68, but it still presents in indy-node 1.3.62 and 1.4.66 code. So, *some of the nodes may not to be upgraded during simultaneous pool-upgrade*. If this problem will appear, stewards should perform manual upgrade of indy-node in accordance with this instruction: [https://docs.google.com/document/d/1vUvbioL5OsmZMSkwRcu0p0jdttJO5VS8K3GhDLdNaoI]   (!) To reduce the risk of reproducing INDY-1447, it is *recommended to use old CLI for pool upgrade*.    (!) *Pool upgrade from indy-node 1.3.62 to indy-node 1.6.70 should be performed simultaneously for all nodes due to txn format changes.*   (!) *There must be sovrin package upgrade to 1.1.13 version after indy-node package upgrade. You need to specify package=sovrin in pool-upgrade command to do this.*   (!) *All indy-cli pools should be recreated with actual genesis files.*   (i) *For more details about txn format changes see INDY-1421.*",Task,Medium,Complete,"2018-08-11 14:31:08","2018-08-11 14:31:08",1
"Hyperledger Indy Node","Explore memory leaks when a primary on one of backup instances is stopped","There is a high chance of memory leaks if one of the (non-master) primaries is stopped.    The corresponding instance will not be able to order, and requests will not be cleared till the next view change.    *Acceptance criteria*:    Perform the following tests and report results about memory consumption with memory checks enabled (in particular, the checks for memory usage of our internal queues and structures).   * Disable view change. Stop a primary on 1 backup instance. Run load of 10 writes per second.   * Disable view change. Stop a primary on multiple backup instances. Run load of 10 writes per second.",Task,Medium,Complete,"2018-08-10 11:33:40","2018-08-10 11:33:40",3
"Hyperledger Indy Node","Do not create strings for log messages we are not going to display","Most log messages have an input as a message created using .format call.    It means that we create new strings even if the log message will not be displayed for the current log level. This may affect performance a lot.    Please use a standard logger's way to create parametrized log messages which will create new strings only if the log message will be displayed.",Task,Medium,New,"2018-08-10 08:26:35","2018-08-10 08:26:35",3
"Hyperledger Indy Node","ZStack quotas should take into account size of received messages",,Task,Medium,Complete,"2018-08-09 16:44:47","2018-08-09 16:44:47",3
"Hyperledger Indy Node","As a developer I need a simple tool to show graphical representation of some common metrics","We need several aligned graphs:    1. Throughput  - client_stack_messages_processed_per_sec  - master_ordered_requests_count_per_sec    2. Latency  - avg_master_monitor_avg_latency  - avg_monitor_avg_latency    3. Queues  - avg_node_stack_messages_processed  - avg_client_stack_messages_processed    4. Looper (Y log scale)  - AVG_NODE_PROD_TIME  - AVG_SERVICE_REPLICAS_TIME  - AVG_SERVICE_NODE_MSGS_TIME  - AVG_SERVICE_CLIENT_MSGS_TIME  - AVG_SERVICE_ACTIONS_TIME  - AVG_SERVICE_VIEW_CHANGER_TIME  ",Story,Medium,Complete,"2018-08-09 15:55:56","2018-08-09 15:55:56",3
"Hyperledger Indy Node","Emulate non-smooth load","We need to check that the pool works as expected when the load is not smooth, but with spikes, and there are periods of time when there are no write load.    *Acceptance criteria*    Provide test results for the following cases:   * Do a load of 10 writes per sec and 100 reads per sec for 10 minutes (all txn except revocation), then stop the load for 10 mins. Repeat multiple times.   * Permanent read of 100 txns per sec. Do a load of 10 writes per sec for 10 minutes (all txns except revocation), then stop the load. Repeat multiple times.    Monitor how stable we are, and how often view change happens.",Task,Medium,Complete,"2018-08-09 10:21:42","2018-08-09 10:21:42",5
"Hyperledger Indy Node","Investigate why Looper and serviceActions take so long","One run of a looper may last for quite a long time (>5 secs, sometimes even 1000 secs) during the load. Also a lot of time is spent in `serviceAction` part.    *Acceptance criteria*   * Check why looper may take so long time   * Check why `serviceAction` part takes so long   * Provide recommendations how to fix it if possible",Task,Medium,Complete,"2018-08-09 09:57:49","2018-08-09 09:57:49",3
"Hyperledger Indy Node","Improve throughput calculation to reduce a chance of false positive View Changes","During analysis of INDY-1556 and INDY-1562, we discovered that there can be cases where View Change may happen unexpectedly.    One of the problems is that we calculate the throughput based on requests number, while the trend is more for 3PC batches count. If we have 3PC batches with a big difference in requests count, we may see false positives because of spikes in the trend.    As of now, we use Exponential moving average for throughput calculation with constant window and constant alpha.             *Acceptance criteria:*   * Write tests that there is no view change detected if we got the same amount of requests on all instances, but with a different distribution of ordering time:   ** All instances ordered equally with some medium throughput; one instance didn't order for a some period, and then ordered all stashed requests in 1 huge 3PC batch   ** All instances ordered equally with some medium throughput; one instance didn't order for a some period, and then ordered all stashed requests in multiple huge 3PC batches   ** All instances didn't order for a long period of time, and then a small load started, and some backups started ordering earlier than master   ** All instances didn't order for a long period of time, and then a huge load started, and some backups started ordering earlier than master   ** All instances ordered equally with some medium throughput; and then all instances started ordering with a heavier load, but some non-master did it a bit earlier   ** All instances ordered equally with some medium throughput; and then load stopped, leading to no more requests ordered   ** Other test cases     * Provide a theory for modification of the current algorithm.   Some ideas:   ** Fall back to initial-window mode if the current throughput value is almost zero.   ** Do not trigger view change immediately, but wait for a sequence of trigger events (for example, backups noticed that master throughput degraded 5 times in a row).   ** Use dynamic window and/or dynamic alpha for Exponential moving average   *** consider the window doesn't contain zero intervals and extended till we have some non-zero values   ** Use different moving average algorithm (not exponential) with a proper window   *** for example, not exponential, but quadratic trend at the beginning of the window   *** explore moving average with momentum   *** consider digital filter synthesis (IIR or FIR) to obtain needed response and avoid ringing   ** Include information about 3PC batch size into algorithm   ** use moving average for 3PC batches, not requests. Consider a separate metric for requests   *** beware that using moving average just for 3PC batches will allow malicious primary to drop requests unnoticed   * Implement the modifications     ",Task,High,Complete,"2018-08-09 09:54:43","2018-08-09 09:54:43",5
"Hyperledger Indy Node","Discard any client requests during view change","As of now, we don't process any client requests during view change (just not take them from the client's ZMQ queue while view change is in progress).    That may potentially lead to memory leaks. Most probably the client will get timeout for the request received during the view change, so it should not be a big issue to discard it.    *Acceptance criteria:*   * Process client queue during the view change (`serviceClientMsgs` in node.py), but discard all requests (send NACK) with a message Discarding client request since View Change is in progress.   * Write tests",Task,Medium,Complete,"2018-08-09 09:50:43","2018-08-09 09:50:43",3
"Hyperledger Indy Node","POA: Trust anchor writes preserve owner","*Story*  As a user of an Indy network that does not require fees for writes, I want my writes to the global ledger to be sponsored by a trust anchor who prevents spam, but I want to retain future control of my writes so that I can modify my writes or work with a different trust anchor in the future.    *Acceptance Criteria*  * Create a plan for effectively using trust anchors to sponsor writes to the ledger  * Get architectural review of the plan  * Create the necessary issues    Goals:  * A trust anchor can write a cred def that is owned by a third party  * The owner can modify that cred def  * A trust anchor can write a schema def that is owned by a third party  * The owner can modify that schema def",Story,High,Complete,"2018-08-08 19:28:51","2018-08-08 19:28:51",3
"Hyperledger Indy Node","Monitor needs to be reset after the view change","As of now, monitor is reset when view change starts, so there can be more ViewChanges because of degradation, as ViewChange may take up to 5 mins, and degradation may be detected for 4 mins.         *Acceptance criteria:*   * Add tests that no degradation can be detected during the view change   * Reset monitor when view change ends (add unit tests for this)   * Do not send InstanceChange because of degradation during view change (add unit tests for this)",Task,Medium,Complete,"2018-08-06 15:14:26","2018-08-06 15:14:26",3
"Hyperledger Indy Node","Need to enhance write permissions for Revocation transactions","As of now, there are no write permissions for REVOC_REG_DEF and REVOC_REG_ENTRY txns.    https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py     *Accepatance criteria:*    Add tests and enhance permissions so that   * if ANYONE_CAN_WRITE=True   ** REVOC_REG_DEF:   *** Anyone can create new REVOC_REG_DEF   *** Only owners can edit existing REVOC_REG_DEF   ** REVOC_REG_ENTRY:   *** Only the owner of the corresponding REVOC_REG_DEF can create new REVOC_REG_ENTRY   *** Only owners can edit existing REVOC_REG_ENTRY   * if ANYONE_CAN_WRITE=False   ** REVOC_REG_DEF:   *** Only Trustee/Steward/TrustAnchor can create new REVOC_REG_DEF   *** Only owners can edit existing REVOC_REG_DEF   ** REVOC_REG_ENTRY:   *** Only the owner of the corresponding REVOC_REG_DEF can create new REVOC_REG_ENTRY   *** Only owners can edit existing REVOC_REG_ENTRY    Integration tests makes sense to do in a similar way as for NYM: https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py     ",Task,Medium,Complete,"2018-08-06 15:08:06","2018-08-06 15:08:06",3
"Hyperledger Indy Node","Research how pool ledger size affect performance","1. Write 10.000 pool transactions  2. Run load testing with setup that similar for load with known metrics",Task,Medium,New,"2018-08-03 13:15:22","2018-08-03 13:15:22",2
"Hyperledger Indy Node","Change default configs for better performance and stability","As followed from INDY-1475, we need to change some default values for better stability and performance.    In particular:   * Max3PCBatchSize = 1000   * VIEW_CHANGE_TIMEOUT = 420   * MSG_LEN_LIMIT = 128 * 1024   * ZMQ_CLIENT_QUEUE_SIZE = 100",Task,Medium,Complete,"2018-08-03 10:28:43","2018-08-03 10:28:43",2
"Hyperledger Indy Node","If the node is crashed with some uncommitted state persisted, it must recover its state and continue ordering","If a node was crashed during ordering, there will be some uncommitted state persisted in the state.         *Acceptance criteria*  Write a test checking that a node can continue ordering in this case.  Do a fix if needed.         *Note*    - Do it with NYM txns and in Indy-Node since there it's suspected that IdrCache may not be able to recover properly.         Take a look",Task,Medium,Complete,"2018-08-02 13:27:40","2018-08-02 13:27:40",5
"Hyperledger Indy Node","Investigate MemoryLeak issues with Revocation transactions","As was found in INDY-1493, we do have some memory leak issues with Revocation transactions.    Need to investigate what is the cause.",Task,Medium,Complete,"2018-08-02 09:31:43","2018-08-02 09:31:43",5
"Hyperledger Indy Node","GC by Checkpoints should not be triggered during View Change","Checkpoints are processed during VC, so it may lead to garbage collection to be triggered, and PrePrepares cleared. As a result, the node will not be able to order till last prepared certificate, and the pool may end up with some nodes ahead",Task,Medium,Complete,"2018-08-01 13:30:02","2018-08-01 13:30:02",3
"Hyperledger Indy Node","View Change should not be triggered by re-sending Primary disconnected if Primary is not disconnected anymore","If a Primary was disconnected (probably for a short amount of time), we send INSTANCE_CHANGE.  This INSTANCE_CHANGE will be re-send every 60 seconds, even if Primary is already connected.",Task,Medium,Complete,"2018-08-01 10:23:14","2018-08-01 10:23:14",3
"Hyperledger Indy Node","Node metrics should include additional info","*Acceptance criteria*   We need to output the following additional statistics:   * Number of messages for each type   ** separately input and output   ** separately master and backup   * Monitor's statistics (latency, throughput) as used by view change   * Number of requests for master instance   * Validation and applying to state time",Task,Medium,Complete,"2018-07-31 10:21:39","2018-07-31 10:21:39",5
"Hyperledger Indy Node","Validator Info must show committed and uncommitted roots for all states",,Task,Medium,Complete,"2018-07-31 09:44:16","2018-07-31 09:44:16",1
"Hyperledger Indy Node","Bind connection socket to NODE_IP","As a part of 2 NICs support we need to forward outgoing connection through NIC for node-to-node communication. Now the default NIC is used for that. To make it deterministic we need to bind connection socket to corresponding NIC explicitly.",Task,High,Complete,"2018-07-27 17:12:37","2018-07-27 17:12:37",2
"Hyperledger Indy Node","Trust anchor permission not needed for ledger writes","*Story*   As a trustee of a network that has implemented a payment system, I want to use that payment system to manage writes (in particular credential definitions and schemas) to the ledger instead of requiring the use of trust anchors.    *Acceptance Criteria*   * Create a configuration setting: writes-require-trust-anchor, default to True   * When the configuration is false, all users have permission to write to the ledger.   * When the configuration is true, the Trust Anchor role is required to write.   * The permission should affect all write transactions.  * Permissions should continue to enforce that only owners of existing transactions can edit them (no change to this behavior).  * The Trust Anchor role should continue to exist in case it is needed in the future.    *Notes*   * Payment of fees during a write is enforced by plugins, and not part of the core ledger.   * The work to make this configuration flexible across all permissions is being tracked in INDY-1527.   * We assume that all nodes in a network set this property consistently. If n-f Stewards set this flag to not require TrustAnchor role, then it will not be required the same as with other consensus driven configuration.  * We decided against only removing the need for the Trust Anchor role for credential definitions and schema definitions, but still requiring it for writing nyms.  ** Requiring a Trust Anchor in order to writing a nym transaction would help us to ensure that best practices are being used--personal data is not being written to the ledger (GDPR compliance).  ** We decided that it is premature to enforce best practices regarding nyms early in the life of the ledger. We should wait to see our recommendations proved in practice before adopting inflexible policies.  ** We decided that we should wait until users of Indy have created formal policies for on-boarding Trust Anchors.  ",Story,Medium,Complete,"2018-07-26 16:42:26","2018-07-26 16:42:26",3
"Hyperledger Indy Node","Ledger permissions are configurable","*Story*  As a trustee responsible for an Indy Node network, I want to be able to update ledger permissions so that I can implement my governance rules specific to my network.    *Acceptance Criteria*  The ledger permissions available to each role should be configurable without code changes.    *Notes*  * The list of ledger roles and the list of permissions do not need to be configurable, but the mapping between them should be configurable.  * The current ledger roles and permissions should remain the same.",Story,Medium,Complete,"2018-07-26 16:13:12","2018-07-26 16:13:12",2
"Hyperledger Indy Node","How-to Add new node to existing pool","Adding new node to existing pool is not covered by documentation yet.  Current unique knowledge source is acceptance scenario",Task,Medium,Complete,"2018-07-25 12:54:00","2018-07-25 12:54:00",1
"Hyperledger Indy Node","Intermittent failure: test_restart_to_same_view_with_killed_primary",https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/1376/artifact/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-3027.txt,Task,Medium,Complete,"2018-07-23 14:10:54","2018-07-23 14:10:54",1
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.5.68","*Version Information*   indy-node 1.5.68   indy-plenum 1.5.48   indy-anoncreds 1.0.11   sovrin 1.1.12    *Major Fixes*   INDY-1471 - Logs appears in old CLI   INDY-1461 - Numerous blacklists under high load   INDY-1460 - Pool stopped writing after 1114k txns (different view_no)   INDY-1464 - AttributeError: 'NoneType' object has no attribute 'request' during load   INDY-1406 - validator-info reading empty file   INDY-1443 - validator-info -v --json does not produce valid JSON   INDY-1459 - First Pre-Prepare message has incorrect state trie root right after view_change (on master replica)   INDY-1455 - Pool cannot order transactions because Node set incorrect watermarks after it's restart   INDY-1454 - Pool has stopped working due to several incomplete view changes   INDY-1427 - Node crashes on _remove_stashed_checkpoints   INDY-1360 - Out of memory during non-completed viewChange process (under load)   INDY-1422 - Part of nodes continued ordering txns after `incorrect state trie` under load   INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66  INDY-1519 - 1.3.62 -> 1.5.67 forced upgrade without one node in schedule was failed    *Changes and Additions*   INDY-1431 - Implement periodic restart of client stack to allow new clients to connect   INDY-1467 - Get rid of peersWithoutRemotes   INDY-1462 - High Watermark on backup may be reset to 300   INDY-1494 - Allow optional field in node-to-node and client-to-node   INDY-1463 - Catchup during view change may last forever under the load   INDY-1458 - Propagate Primary mode should not be set for already started view change   INDY-1450 - Catchup needs to be finished during high load   INDY-1416 - Include reviewed logging strings in Indy   INDY-1483 - Benchmark performance impact of recorder tool   INDY-1311 - Decrease amount of logging with INFO level   INDY-1435 - Throughput measurements in monitor should be windowed   INDY-1386 - Limit the number of requested PROPAGATES in MessageRequests   INDY-1453 - Do not process any client requests during view change   INDY-1452 - A node must send LEDGER_STATUS with correct last ordered 3PC after catch-up   INDY-1385 - Fix calculation of prepared certificates during View Change   INDY-1404 - Catchup should not be interrupted by external events    *Known Issues*  INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66  (!) Note that INDY-1447 was fixed in indy-node 1.5.68, but it still presents in indy-node 1.3.62 and 1.4.66 code. So, *some of the nodes may not to be upgraded during simultaneous pool-upgrade*. If this problem will appear, stewards should perform manual upgrade of indy-node in accordance with this instruction: [https://docs.google.com/document/d/1vUvbioL5OsmZMSkwRcu0p0jdttJO5VS8K3GhDLdNaoI]  (!) To reduce the risk of reproducing INDY-1447, it is *recommended to use old CLI for pool upgrade*.    (!) *Pool upgrade from indy-node 1.3.62 should be performed simultaneously for all nodes due to txn format changes.*  (!) *All indy-cli pools should be recreated with actual genesis files.*  (i) *For more details about txn format changes see INDY-1421.*",Task,Medium,Complete,"2018-07-20 15:11:45","2018-07-20 15:11:45",1
"Hyperledger Indy Node","Explore load test behavior in case of read requests and lots of connections","The tests with fresh versions of libindy, with new timeouts, re/connections, shows lots of timeout errors. We need to figure it out what can be improved on node side",Task,Medium,Complete,"2018-07-20 14:10:08","2018-07-20 14:10:08",3
"Hyperledger Indy Node","tmp.log must have unique name","If indy-node.service restarts when logs rotate, archive hasn't created.  In case that node restarts again - new tmp file rewrite previous",Task,Medium,Complete,"2018-07-20 13:26:01","2018-07-20 13:26:01",1
"Hyperledger Indy Node","Re-send messages to disconnected remotes","If the remote (node) is not connected, we're trying to send it, but if re-connection will be needed (which means that the socket is re-created), the sent message will be lost.    This may be a cause of issues like the one found in the scope if INDY-1404 and INDY-1450 (when initial catchup can not be finished since some messages are not received).",Task,Medium,Complete,"2018-07-19 15:18:28","2018-07-19 15:18:28",3
"Hyperledger Indy Node","Enable TRACK_CONNECTED_CLIENTS_NUM option","An ability to restart the listener socket to avoid too many open connections was implemented and basically verified in the scope of INDY-1431.    This is disabled by default (TRACK_CONNECTED_CLIENTS_NUM_ENABLED flag).    We need to enable the flag and continue testing.",Task,Medium,Complete,"2018-07-19 13:34:49","2018-07-19 13:34:49",3
"Hyperledger Indy Node","Allow optional field in node-to-node and client-to-node","As of now the validation schema for all messages is strict and doesn't allow new (optional) fields to be included at the end.    As a long-term we should consider using protocolVersion, but for now it should be enough just to allow new (unknown) fields, so that old nodes don't break.",Task,Medium,Complete,"2018-07-18 11:36:15","2018-07-18 11:36:15",2
"Hyperledger Indy Node","Memory leaks profiling","_indy-node.service_ execution increases memory consumption, that leads to OOM node restart, that also increase other problems probability    *Acceptance Criteria*  * memory leak points have been found  * any problems found will be logged in JIRA as separate issues for independent prioritization  * load test shouldn't indicate stable memory consumption increase",Task,Medium,Complete,"2018-07-18 10:58:56","2018-07-18 10:58:56",5
"Hyperledger Indy Node","Create 1.6 Release",,Task,Medium,Complete,"2018-07-17 16:04:01","2018-07-17 16:04:01",3
"Hyperledger Indy Node","As a Trustee running POOL_UPGRADE txn, I need to specify any package depending on indy-node, so that the package with the dependencies get upgraded","As of now POOL_UPGRADE txn upgrades IndyNode only with the dependencies.   We need to be able to use the txn to upgrade the products based on IndyNode.    See a PoA in [https://docs.google.com/document/d/1_sVPtltsTZqP1ZK_wimrJuY8-Emc89TwYT51m0OZxF4/edit#heading=h.z1u0dzf62loi:]   * Extend the POOL_UPGRADE txn with the (optional) package name (assume indy-node if not present)   * get the list of dependencies for the specified package (apt-cache can be used on Ubuntu)     * If it sees indy-node there, it get the list of dependencies for indy-node as well (use existing logic).     * As an output, it creates a list of the packages with dependencies to be installed in the correct order.  For example,  [indy-plenum=1.4.4, indy-node=1.4.1, plugin1=1.0.1, plugin2=1.0.2, specified_package=2.0.0]",Story,Medium,Complete,"2018-07-17 15:12:45","2018-07-17 15:12:45",5
"Hyperledger Indy Node","Intermittent failure: test_restart_groups_6_of_7_wp_tm",https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/1325/artifact/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-1627.txt,Task,Medium,Complete,"2018-07-17 10:11:30","2018-07-17 10:11:30",1
"Hyperledger Indy Node","Intermittent failure: test_new_node_catchup_plugin_ledger",https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/1320/artifact/test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-1572.txt,Task,Medium,Complete,"2018-07-17 10:10:26","2018-07-17 10:10:26",1
"Hyperledger Indy Node","Intermittent failure: test_multiple_view_change_retries_by_timeouts ",https://ci.evernym.com/job/Indy-Plenum/job/indy-plenum-verify-x86_64/1881/artifact/test-result-plenum-1.ubuntu-03.txt,Task,Medium,Complete,"2018-07-17 10:04:51","2018-07-17 10:04:51",1
"Hyperledger Indy Node","Benchmark performance impact of recorder tool","*Story*  As a developer fixing issues with Indy Node, I want to understand the performance impact of running the recording tool so that I can deploy the recorder in situations where it will be helpful and not prevent my addressing issues.    *Acceptance Criteria*  * Reproduce the test in INDY-1343 with the recorder running, and compare the results with what we saw without the recorder running.",Story,Medium,Complete,"2018-07-16 14:08:14","2018-07-16 14:08:14",3
"Hyperledger Indy Node","Provide feedback on recorder / replayer","2 developers should complete the following tasks:  * Setup the recorder  * Do a recording  * Replay the recording  * Provide feedback  * Replay a recording provided by someone else    This effort will be limited to 2 days per developer, after which we will provide any feedback we have if we didn't get the recorder working.",Task,Medium,Complete,"2018-07-16 14:03:07","2018-07-16 14:03:07",3
"Hyperledger Indy Node","Do not calculate CRED_DEF's Keys in load script on every request","Creation of new CRED_DEF's keys may take a couple of seconds, which breaks the statistics for the load script.    We can create CRED_DEF's keys just once, and use the same keys when sending CRED_DEF txns.",Task,Medium,Complete,"2018-07-16 13:16:52","2018-07-16 13:16:52",2
"Hyperledger Indy Node","Support latest SDK in Indy Plenum and Node","There are some breaking changes in the latest SDK related to Wallet API.    We need to support the changes in plenum and node tests to use the latest Master version of SDK.",Task,Medium,Complete,"2018-07-16 13:14:49","2018-07-16 13:14:49",1
"Hyperledger Indy Node","Explore timing and execution time","In order to find out proper values for ZMQ queue sizes and a number of messages we process by each run of the looper (DEFAULT_LISTENER_QUOTA, DEFAULT_SENDER_QUOTA, ZMQ_CLIENT_QUEUE_SIZE, ZMQ_NODE_QUEUE_SIZE), we need to measure the following:   * How many messages we process in one run in node-to-node and client-to-node stacks   ** under the load and not   ** average, minimum and maximum values, distribution graph)   * How much time each run of the looper takes (from one call to network stack to another)   * Size of 3PC batches   * Size of messages   * How many node-to-node messages we have for 1 3PC Batch and 1 request   * How many transport Batches we create and their size (on both KB and number of messages)    For each value we need to measure:  * System not under load  * System with a steady-state of significant load (see INDY-1343)  We need to know the average, minimum and maximum values, distribution graph.  This baseline testing should only follow our recommended production configuration.    This baseline testing does not include:  * Edge cases  * Forced view changes  * Combinations of the various configuration values    *Acceptance criteria*   * Hooks in the code to perform the measurement   * A spreadsheet with info above   * A summary document (about 1 page) of findings and recommendations for network node resourcing that can be provided to the Sovrin Foundation   * Identified improvements should be raised as separate issues",Task,Medium,Complete,"2018-07-13 16:41:33","2018-07-13 16:41:33",5
"Hyperledger Indy Node","Create 1.5 Release",,Task,Medium,Complete,"2018-07-13 15:04:30","2018-07-13 15:04:30",2
"Hyperledger Indy Node","Latency measurements in monitor should be windowed","Regular view changes increase master latency (but not backup latencies?) according to current latency measurements and it causes to additional view changes due to master latency degradation so latency measurements also should be windowed the same way as throughput measurements in INDY-1435.",Task,Medium,Complete,"2018-07-11 16:19:55","2018-07-11 16:19:55",3
"Hyperledger Indy Node","Get rid of peersWithoutRemotes","The peersWithoutRemotes ZStack class member is a set of idents got from received messages (used in DEALER-ROUTER ZMQ pattern to distinguish clients during sending replies).    There are two purposes of peersWithoutRemotes usage:    1) it is an old mechanism to tell connected clients about changed pool membership;  2) to interrupt sending of answer to client in case of propagated request (in this case ZMQ does not have corresponding ident in inner routing table).    Regarding the first option: new SDK client does not use such mechanism.   Regarding the second option: it is absolutely safe to send message to unknown ident using ZMQ. If ident is not known then such message is silently dropped by ZMQ, reference:    [http://zguide.zeromq.org/page%3aall#ROUTER-Error-Handling]    So there is no ZMQ crashes or memory leaks here, it is up to ZMQ to drop such messages, we don't need to control this on our application level.    Usage of peersWithoutRemotes is not just useless, but rather dangerous especially taking into account new SDK client behaviour of keeping connections. Now each new ident is added to peersWithoutRemotes but never removed. Moreover, we can not detect disconnection of particular ident even having disconnected event as we can not match fds and idents, ZMQ does not provide such mechanism. So it is always memory leak. Since now new SDK client implements pretty fast rotation of idents that makes described memory leak even faster.    That's why we should get reed of peersWithoutRemotes usage.",Task,High,Complete,"2018-07-11 14:38:30","2018-07-11 14:38:30",2
"Hyperledger Indy Node","Catchup during view change may last forever under the load","If one of the node is still doing catchup during view change while other nodes in the pool finished view change and already order transactions, the catchup on the slow node may last forever, so that the node will not finish the view change.",Task,Medium,Complete,"2018-07-10 17:16:52","2018-07-10 17:16:52",3
"Hyperledger Indy Node","High Watermark on backup may be reset to 300","High watermark (H) can be reset to 300 after primary propagation for a joined node,  It leads to unavailability to restore last ordered on backup if ppSeqNo of other nodes in the pool is more than 300.  Also backup starts stashing messages which may lead to out of memory.",Task,Medium,Complete,"2018-07-10 12:33:06","2018-07-10 12:33:06",3
"Hyperledger Indy Node","Propagate Primary mode should not be set for already started view change","If there is a view change in progress from viewNo=X to viewNo > X+1, that is consisting of multiple rounds, then no nodes should start it in propagate primary mode (on processing future view change done messages). Otherwise these nodes will not send ViewChangeDone, and will have incorrect quorum of VCD (f+1 instead of n-f).",Task,Medium,Complete,"2018-07-06 09:48:20","2018-07-06 09:48:20",2
"Hyperledger Indy Node","Do not process any client requests during view change","Do not process any client requests during catch-up, just keep them in zstack internal queue.    If they reach the limit (5000), they will be silently dropped (we may consider sending some graceful Reject in future).",Task,Medium,Complete,"2018-07-05 11:31:06","2018-07-05 11:31:06",3
"Hyperledger Indy Node","A node must send LEDGER_STATUS with correct last ordered 3PC after catch-up","There is a structure (IntervalTree) in node that can map seqNo in ledger to 3PC key.    This is in-memory structure, so it's empty after node restart, and updated only during common ordering and execution of 3PC batches.    We need to have an entry for the current ledger size in this map after catchup too, as this is used in LEDGER_STATUSes sent to others, and LEDGER_STATUSes are used for initial calcualtion of `last_ordered-3PC`.",Task,Medium,Complete,"2018-07-05 11:23:18","2018-07-05 11:23:18",2
"Hyperledger Indy Node","Catchup needs to be finished during high load","As was found in INDY-1404, if a lot of catchup-related messages (LEDGER_STATUS, CONSISTENCY_PROOFS) are lost, the node doesn't try to start catchup again.   The messages can be lost due to high load (as in the test), and limited size of node-to-node's message queue.    The recent fixes regarding catchup and fixes done in the scope of INDY-1404 made the problem more visible, since we require that the previous round of catchup is finished before starting the next round.         *Acceptance criteria:*   * The following tests need to be written:   ** Skip sending of Message Responses to LEDGER STATUS request once for domain ledger to a node doing catchup; make sure that the node eventually finishes catchup   ** Skip sending of CONSISTENCY_PROOFs for domain ledger once to a node doing catchup; make sure that the node eventually finishes catchup   * A Node must finish catchup regardless of the load in the pool   * Catchup can be finished with some delay (in a timeout),",Task,Medium,Complete,"2018-07-04 10:06:09","2018-07-04 10:06:09",5
"Hyperledger Indy Node","Throughput measurements in monitor should be windowed","Current thoughput measurement in monitor is done as soon as there are a few batches ordered. It turned out that with large enough batches (and max batch size was significantly increased recently due to [INDY-1334|https://jira.hyperledger.org/browse/INDY-1334]) this behavior makes it's possible to enter perptual view change (details are in [INDY-1429|https://jira.hyperledger.org/browse/INDY-1429]).    Things to do:  1) Extract measurement code in monitor into unit testable class and cover it with enough tests, including ones showing problems with current throughput measurement  2) Implement windowed throughput measurements  3) Re-run load tests from [INDY-1429|https://jira.hyperledger.org/browse/INDY-1429] to see if problem is really gone",Task,High,Complete,"2018-06-28 11:44:00","2018-06-28 11:44:00",5
"Hyperledger Indy Node","Evaluate network performance with more nodes","*Background*    We currently test with a pool of 25 nodes, but we are on track to recruit more than that number of stewards. Additional nodes provides more confidence that the network cannot be undermined. We need to discover how the pool will behave with a larger number of stewards so that we can appropriately schedule the on-boarding of additional stewards.     We recognize that there are problems with view change that will not be resolved until late 2018 (See INDY-1376), so we need to know how the current implementation behaves.    *Acceptance Criteria*    Test consensus with the specified numbers of nodes. There may be three tests: 50 nodes, 40 nodes, and 30 nodes. If the test fails, test again with fewer nodes. If network performance is acceptable, then end the test and report back that we are ready for further testing at that level.   * Set up the pool and load it with traffic for 30 minutes   * Evaluate the quality of the network   ** Is there a consistent view of the master?   ** Is performance reasonable?   ** Are there lots of failed transactions?   * Trigger a view change, continue load for 10 minutes, and re-evaluate the quality of the network.    Report on the performance of the network at each tested level.",Task,High,New,"2018-06-27 14:39:28","2018-06-27 14:39:28",3
"Hyperledger Indy Node","Implement periodic restart of client stack to allow new clients to connect","All research was done in scope of [INDY-1417|https://jira.hyperledger.org/browse/INDY-1417], please check [research result and PoA|https://jira.hyperledger.org/browse/INDY-1417?focusedCommentId=46595&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-46595].    As a client stack restart option I prefer a combination of timeout and connections limit to avoid unnecessary restart and infinite restarts.    Note that as a result of stack restart we may have 2X sockets during about one minute, it should be considered in our file descriptors limits calculations.",Task,High,Complete,"2018-06-27 12:55:50","2018-06-27 12:55:50",5
"Hyperledger Indy Node","Load testing of Indy Node 1.4","*Acceptance Criteria*  * Run the load tests scripts against Indy Node 1.4  * Log bugs that are found",Task,Medium,Complete,"2018-06-22 15:40:35","2018-06-22 15:40:35",5
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.4.66","*Version Information*   indy-node 1.4.66   indy-plenum 1.4.45   indy-anoncreds 1.0.11   sovrin 1.1.11    *Major Fixes*   INDY-1410 - One of the nodes stopped writing after 44287 txns with errors in status   INDY-1365 - Pool stopped accepting transactions on 5731 txns (1 sec delays, no logging)   INDY-1315 - Pool stopped writing after ~300,000 txns from 5 clients   INDY-1351 - STN not accepting transactions with only one node down   INDY-1260 - Pool stops taking txns at ~178k txns written in ledger   INDY-1327 - `ReqIdrToTxn` does not store information about the ledger   INDY-1341 - Simple Timeout fixes of the current View Change protocol   INDY-1379 - Migration fails in case of upgrade to version with new transactions format   INDY-1318 - --network parameter of read_ledger doesn't work   INDY-1310 - The /var/log/indy/validator-info.log is inappropriately owned by root   INDY-1298 - Fix the issues found in the current logic of catch-up   INDY-1363 - GetValidatorInfo should have correct validation for signature and permissions   INDY-1316 - Unhandled exception during node working   INDY-1219 - validator-info and read_ledger give inconsistent responses in node on provisional   INDY-1259 - Pool stops taking txns at 3000 writing connections    *Changes and Additions*   INDY-810 - Review and replace 'assert' with exceptions in indy-plenum where needed   INDY-1245 - Tune RocksDB options for the best performance   INDY-1392 - As a developer, I need to have migration guide from Indy-node 1.3 to 1.4   INDY-1370 - Сhange key in requests map and field reqIdr in Pre Prepare and Ordered   INDY-1400 - Investigate issues found during load testing of 25-nodes pool with increased timeouts for catchups and viewchange   INDY-1332 - Support binding on separate NICs for Client-to-Node and Node-to-Node communication   INDY-1329 - Add short checkpoints stabilization without matching digests   INDY-1323 - Add indy-crypto package to hold list   INDY-1297 - Remove ledger status based catch-up trigger together with wrong catch-up workflow   INDY-1243 - Read-ledger without storage copy in case of RocksDB (RocksDB read-only mode support)   INDY-971 - Apply state machine to Catchup code   INDY-1124 - Refactor common Request structure   INDY-1123 - Refactor common transactions structure   INDY-1319 - Support new libindy with changed txn format   INDY-1334 - Explore config parameters to find the best performance/stability settings   INDY-1175 - Extend of Validator Info tool to provide more information about the current state of the pool   INDY-1184 - A Steward needs to be able to get validator-info from all nodes   INDY-1279 - Modify existing load scripts for a better load testing   INDY-1244 - Migration from LevelDB to RocksDB   INDY-1173 - A Trustee needs to be able to restart the pool in critical situations   INDY-1275 - Move log compression into separate process    *Known Issues*   INDY-1415 - Incorrect read_ledger info with seq_no parameter    (!) *Pool upgrade should be performed simultaneously for all nodes due to txn format changes.*  (!) *All indy-cli pools should be recreated with actual genesis files.*  (!) *Added and promoted nodes should be restarted (`systemctl restart indy-node`) after adding/promoting.*    *List of breaking changes for migration from indy-node 1.3 to 1.4:*   [https://github.com/hyperledger/indy-node/blob/master/docs/1.3_to_1.4_migration_guide.md]    *IndyNode 1.4 and LibIndy 1.5 compatibility:*    _General_    By default LibIndy 1.5 will be compatible with IndyNode 1.3 (current stable), and not 1.4 (the new one).   LibIndy 1.5 can become compatible with IndyNode 1.4 if `indy_set_protocol_version(2)` is called during app initialization.    _Guideline for teams and apps_    Applications can freely update to LibIndy 1.5 and still use stable Node 1.3   If an app wants to work with the latest master or Stable Node 1.4, then they need to   support breaking changes (there are not so many, mostly a new reply for write txns as txn format is changed, see 1.3_to_1.4_migration_guide.md)   call `indy_set_protocol_version(2)` during app initialization    *CLI Upgrading:*    Old CLI (`indy`):  - upgrade from 1.3 to 1.4 version  - delete `~.ind-cli/networks/<network_name>/data` folder  - replace both old genesis files by new ones (from 1.4 node or from sovrin repo)    New CLI (`indy-cli`):  - upgrade from 1.4 to 1.5 version  - recreate indy-cli pool using 1.4 pool genesis file (from 1.4 node or from sovrin repo)    Use https://github.com/hyperledger/indy-sdk/blob/b4a2bb82087e2eafe5e55bddb20a3069e5fb7d0b/cli/README.md#old-python-based-cli-migration to export dids from your old CLI wallet to the new one (new indy-cli).",Task,Medium,Complete,"2018-06-19 10:33:24","2018-06-19 10:33:24",2
"Hyperledger Indy Node","txnId should be part of transaction","We need to have `txnId` as part of `txnMetadata` for every txn (as described inhttps://github.com/hyperledger/indy-node/blob/master/docs/requests.md).    Migration script should also be updated.",Task,Medium,Complete,"2018-06-19 09:08:14","2018-06-19 09:08:14",3
"Hyperledger Indy Node","Ensure new client connections can be accepted on a periodic basis","*Story*  As an administrator of a node on an Indy network, I want an automatic process for new client connections to be accepted by the network.    *Acceptance Criteria*  Implement the solution designed in INDY-1418.",Story,Medium,Complete,"2018-06-18 15:44:06","2018-06-18 15:44:06",8
"Hyperledger Indy Node","POA: Ensure new client connections can be accepted on a periodic basis","*Story*  As an administrator of a node on an Indy network, I want an automatic process for new client connections to be accepted by the network.    *Acceptance Criteria*  Design a process by which new client connections can be accepted even when there are a lot of connections on the existing external NIC. The process must not require manual intervention.    The process may happen on a periodic basis.    *Notes*  Potential approaches:  * Force ZeroMQ to close connections on a timeout  * An external process such as a firewall monitors the length of open connections and kills them on a timeout  * * When the connection buffer becomes full, clear the connection buffer and force legitimate clients to reconnect  ",Story,Medium,Complete,"2018-06-18 15:42:49","2018-06-18 15:42:49",5
"Hyperledger Indy Node","Include reviewed logging strings in Indy","*Story*    As an administrator of an Indy node, I want log messages to be clear and well written.    *Acceptance Criteria*    Log messages that have been reviewed by Product Management and the Operations Team should be incorporated into the product.    Log messages and levels in the product should match the Revised Message and Revised Level in the spreadsheet. If the revised level is None then the log message is unnecessary. Blank revisions means that no changes are needed.    If there is a Revised Message but the Current Message is empty, then the log messages is new and should be added to the product.",Story,Medium,Complete,"2018-06-18 15:09:52","2018-06-18 15:09:52",3
"Hyperledger Indy Node","Review and replace 'assert' with exceptions in indy-node where needed","Currently production builds use optimization for bytecode (INDY-211) when running [node|https://github.com/<USER>indy-node/blob/master/build-scripts/ubuntu-1604/postinst_node#L54] and [node_control_tool|https://github.com/<USER>indy-node/blob/master/build-scripts/ubuntu-1604/postinst_node#L78].    This silently [removes|https://docs.python.org/3.5/tutorial/modules.html#compiled-python-files] all assert statements from the code. I think all asserts (in both indy-plenum and indy-node) should be reviewed and fixed (e.g. replaced with custom/built-in exeptions) where needed.",Task,Medium,New,"2018-06-18 08:05:06","2018-06-18 08:05:06",3
"Hyperledger Indy Node","Batch containing some already executed requests should be applied correctly","In [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350] and [INDY-1400|https://jira.hyperledger.org/browse/INDY-1400] it was found that next big problem with view change is that it can interrupt catchup process, leaving partially catched up state which leads to problems with applying batches that contain some transactions that were received through catch up.    Proper fix looks like enabling correct handling of this case. Also, this is a prerequisite for implementing PBFT view change: [INDY-1335|https://jira.hyperledger.org/browse/INDY-1335]",Task,Medium,Complete,"2018-06-09 12:26:29","2018-06-09 12:26:29",3
"Hyperledger Indy Node","Catchup should not be interrupted by external events","In [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350] and [INDY-1400|https://jira.hyperledger.org/browse/INDY-1400] it was found that next big problem with view change is that it can interrupt catchup process, leaving partially catched up state which leads to problems with applying batches that contain some transactions that were received through catch up.    Simplest possible fix looks like disabling ability to interrupt catch up by external events.",Task,Medium,Complete,"2018-06-09 12:21:26","2018-06-09 12:21:26",5
"Hyperledger Indy Node","Applying txns from catchup reply in case several nodes sent replies with overlapping seqNo intervals ","If node receives catchup replies from different nodes with overlapping seqNo intervals there is a possibility that one of replies will not be validated properly and node will be blacklisted.  It could happen because node applying txns from sorted list containing merged txns from all CRs based on first CR that includes starting seqNo. If starting seqNo is not the first seqNo of the CR the node will apply more txns than CR contains and proofs will not match",Task,Medium,Complete,"2018-06-08 13:54:30","2018-06-08 13:54:30",3
"Hyperledger Indy Node","Investigate issues found during load testing of 25-nodes pool with increased timeouts for catchups and viewchange","We should investigate and fix issues found (incorrect state trie) in scope of INDY-1350 testing.",Task,Medium,Complete,"2018-06-07 10:17:21","2018-06-07 10:17:21",5
"Hyperledger Indy Node","As I Steward or Node runner, I need to have instructions on how to whitelist IPs on node-to-node NIC","*Acceptance criteria*   * The doc needs to be in indy-node/docs folder   * It should contains the rules for iptables in Ubuntu 16.04 on AWS as a reference   * 1 NIC is public and used for Client communications, and 1 NIC is whitelisted to only talk to other consensus nodes   * System administrators are expected to use the list provided by the Sovrin Foundation (it is assumed that they already have the list).   * Documentation should clarify that adding a new node to the pool require these manual steps to keep the list updated   * Target audience consists of system administrators with experience on Linux systems    Excluded:  * We will not be automatically manipulating the firewall rules    See INDY-1394 for additional details.",Story,Medium,New,"2018-06-07 09:20:25","2018-06-07 09:20:25",3
"Hyperledger Indy Node","As I Steward or Node runner, I need to have instructions on how to configure separate NICs","*Acceptance criteria*   * The doc need to be in indy-node/doc folder   * It should link to instructions for configuring a second NIC for AWS, Azure, and DigitalOcean   * It should contain instructions for configuring Ubuntu 16.04 on AWS   ** Linking to a well maintained external official document for this topic is acceptable   * The Indy Node server should be configured to bind to two NICs: one for client communications and one for consensus    See INDY-1394 for additional details.",Story,Medium,New,"2018-06-07 09:18:36","2018-06-07 09:18:36",3
"Hyperledger Indy Node","As I Steward or Indy-Node runner, I need to have a doc with best practises on how to configure firewall rules so that the pool can survive during DDoS attacks","*Acceptance criteria*   * The doc needs to be in indy-node/docs folder   * The doc needs to contain at least the following guidelines for working with IPTables on AWS to:   ** Limit open file descriptors   *** [https://github.com/hyperledger/indy-node/blob/master/docs/setup-iptables.md]   *** [https://github.com/hyperledger/indy-node/blob/master/scripts/setup_iptables]   ** Use separate NICs   *** with a link of how-to docs for AWS, Azure, DigitalOcean   *** These external documents do not need to be tested   ** Node's interface must listen on whitelisted IPs only (other Nodes' IPs)   *** with a link to how-to configure such firewall rules  * The rules in the document should be tested to ensure that they work   ** Testing only needs to be done in an AWS environment   * Target audience is a Linux system administrator with experience on Ubuntu Linux.   * This story is for combining INDY-1396 and INDY-1397",Story,Medium,New,"2018-06-07 09:07:04","2018-06-07 09:07:04",2
"Hyperledger Indy Node","Indy-node 1.4 should reject requests from old unsupported clients gracefully","As Indy Node 1.4 release contains breaking changes, it can work with the new clients supporting this changes only.    *Acceptance criteria:*   * Add PROTOCOL_VERSION=2   * Add PROTOCOL_VERSION to `LedgerStatus` to handle catch ups from old clients   * If there is LEDGER_STATUS or Request with a protocol version !=2, then discard with the following message: make sure that the latest LibIndy is used and `indy_set_protocol_version(2)` is called.   * Wait until LibIndy supporting `indy_set_protocol_version(2)` is issued and use this LibIndy in integration tests   * Call `indy_set_protocol_version(2)` before opening a pool in integration tests   * If there is a Ledger Status with a protocol version < 2 (that is the one came from old SDK), send back LedgerStatus to force it finish catch-up, so that next reqeusts to the ledger will fail gracefully with an error message to update SDK.",Task,Medium,Complete,"2018-06-07 08:21:20","2018-06-07 08:21:20",3
"Hyperledger Indy Node","As a developer, I need to have migration guide from Indy-node 1.3 to 1.4 ","As upcoming Indy-node 1.4 release contains breaking changes, we need to write a migration with all the changes.",Task,Medium,Complete,"2018-06-07 08:15:56","2018-06-07 08:15:56",1
"Hyperledger Indy Node","Support Proof of Possession for BLS keys","Details are contained here:    [https://docs.google.com/document/d/14wcYSn9XXX6-0M5le1icT6N-funiB7K9cActzYfILto/edit]    *Acceptance Criteria:*  - Use the improved indy-crypto library from IS-750  - Make sure that we can extend NODE txn in a non-breaking way  - Extend NODE txn with a (optional) field to specify Proof of possession for BLS key  - Validate the Proof of possession for BLS once a NODE txn is received  - Blacklist the Node if verification failed  - Make sure that we verify the proof after the node is started      For future: Make the proof required, not optional, so every node should have the proof if it has a BLS key.     ",Task,High,Complete,"2018-06-05 17:44:30","2018-06-05 17:44:30",5
"Hyperledger Indy Node","Prove stability under a DOS of an Indy network","Before encouraging people to use the Sovrin network for live loads, we need to prove that network stability will be preserved even if availability to the network is denied due to very high loads.    *Acceptance Criteria*   Perform a test of an Indy network that has the following attributes:   * The ledger is pre-loaded with at least 1 million transactions   * Pool size at least matches the number of network nodes initially expected in the Sovrin network (currently 25 nodes).  * While maintaining a base line of legitimate read and write transactions, flood the pool with sufficient new read and write requests so that the external connection buffer is full and requests are denied for 10 minutes.  ** These requests should be in a similar mix to INDY-1343 (90% reads / 10% writes)  ** The requests should come from at least 10K concurrent connections  ** Throughput sufficient to cause the pool to delay in responding  * Stop flooding the pool and allow the network to catch up.  * Report on:  ** the level of traffic necessary to cause the network to deny new connections  ** any inconsistencies in the state of the pool after the network has caught up  ** the time necessary for the network to catch up and begin processing new transactions    Any problems found will be logged in JIRA as separate issues for independent prioritization.    As part of this issue, it is recognized that it will be necessary to create a load testing tool sufficient to perform the test.",Task,High,Complete,"2018-06-04 22:21:02","2018-06-04 22:21:02",8
"Hyperledger Indy Node","Change default config settings for a better stability","There are recommendations for a better stability found in the scope of INDY-1334.      *Acceptance criteria:*   * Change default config according to this settings   * Make sure that all the tests pass",Task,Medium,Complete,"2018-06-04 10:17:47","2018-06-04 10:17:47",3
"Hyperledger Indy Node","Limit the number of requested PROPAGATES in MessageRequests",,Task,Medium,Complete,"2018-06-01 10:21:39","2018-06-01 10:21:39",5
"Hyperledger Indy Node","Fix calculation of prepared certificates during View Change","As of now, prepared certificate is calculated based on received COMMITs which means that 3PC batch may be not actually prepared (have quorum of PREPARE) on the Replica.",Task,Medium,Complete,"2018-06-01 09:36:13","2018-06-01 09:36:13",2
"Hyperledger Indy Node","Simplify current view change process","Different investigations showed that while increasing view change timeouts did help, it seems that next problem is with catchup that goes on during view change.     View change should be simplifed:  - catchup should not start at the view change start  - no catchup should happen during view change for any reason  - at the end of view change uncommited state should be cleared  - catchup should start after the end of view change (it's still to be determined whether explicit start is needed, or nodes will be smart enough to start it themselves when needed)  ",Task,Medium,Complete,"2018-05-31 18:28:15","2018-05-31 18:28:15",8
"Hyperledger Indy Node","We should update revocation registry delta value during REG_ENTRY_REVOC writing","We should update revocation registry delta during REG_ENTRY_REVOC writing by sending registry revocation reading txn (that will be implemented in scope of INDY-1356) because if writing txn is failed the next txns to write are rejected due to incorrect revocation registry delta.",Task,Medium,Complete,"2018-05-30 16:53:51","2018-05-30 16:53:51",3
"Hyperledger Indy Node","As a developer I need to run log processor in parallel on all test nodes in order to get processing results faster","Main problem with processing logs is that it's very long CPU-bound (and sometimes memory-bound) task. Also, downloading logs from pool to process them locally takes a lot of time.    If we could run log processor on large 25-nodes test pool directly we:  - effectively get 50 cores and 200 Gb of RAM to process logs, which is much more than any desktop have  - can get results before downloading all logs, which takes time    In order to do this we need:  - remove dependency on matplotlib from log processor  - create ansible playbook to upload log processor and its config to pool, run it in pool and then download results",Story,Medium,Complete,"2018-05-30 10:50:47","2018-05-30 10:50:47",2
"Hyperledger Indy Node","As an Issuer, I need to be able to create multiple ClaimDefs for the same Schema, DID and Signature Type","As of now only one CLAIM_DEF can be created for the same Schema, DID (creator) and Signature Type.    We need to introduce a `tag` field which can be used to distinguish CLAIM_DEFs for the same Schema/DID/Type.    See the proposed format here: [https://github.com/hyperledger/indy-node/blob/master/docs/requests-new.md#claim_def]    See the design for anoncreds txns (including CLAIM_DEF) here: https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md",Story,Medium,Complete,"2018-05-30 09:24:01","2018-05-30 09:24:01",3
"Hyperledger Indy Node","Сhange key in requests map and field reqIdr in Pre Prepare and Ordered","Change key in requests map from (request id, DID) to digest owing to which will change field reqIdr in Pre Prepare and Ordered. Because if 2 clients with same DID send request with same request_id. Request map uses this value as a key. In this case we will have collision of requests and different request data on different nodes.",Task,Medium,Complete,"2018-05-28 09:20:09","2018-05-28 09:20:09",5
"Hyperledger Indy Node","Create 1.4 Release","*Acceptance criteria*  * Create a new RC 1.4 containing all the tasks from Release 1.4  * Make acceptance testing of the new RC, in particular RocksDB, new txn format, migrations.",Task,Medium,Complete,"2018-05-25 16:39:21","2018-05-25 16:39:21",5
"Hyperledger Indy Node","[QA] Perform load testing after catch-up changes and stability fixes","After implementation of INDY-1297 and INDY-1236 need to perform load testing and compare results with  the previous ones: https://docs.google.com/spreadsheets/d/1DTjDsLSysFBiKU-9z4-IzunJk4wEy44hE_PGZYxnN_8/edit#gid=1813415708",Task,High,Complete,"2018-05-24 16:25:13","2018-05-24 16:25:13",5
"Hyperledger Indy Node","GetValidatorInfo should have correct validation for signature and permissions","Right now Node require signature, but doesn't check role/permission for GET_VALIDATOR_INFO  I see 2 possible options here  1) Node doesn't require any signature  2) Node requires signature and check role/permission of the sender",Task,Medium,Complete,"2018-05-23 16:55:12","2018-05-23 16:55:12",2
"Hyperledger Indy Node","journalctl is unavailable for indy user","In _validator-info -v_ some data are unavailable because of _indy_ user permissions      ",Task,Medium,Complete,"2018-05-23 14:30:21","2018-05-23 14:30:21",3
"Hyperledger Indy Node","As a QA I want post-install automation tests to be run automatically in CI/CD pipeline","Things to do:  1. Install pool of 25 nodes *for each master/stable build of indy-node package* in CI/CD infrastructure.  2. Run post-install tests (https://github.com/hyperledger/indy-post-install-automation) against this pool in scope of CI/CD pipeline.  3. Send pytest results produced by this tests to QA team via mail.    [~<USER> [~<USER> Can we estimate effort for this task and plan it to some sprint?",Story,Medium,Complete,"2018-05-23 11:06:42","2018-05-23 11:06:42",5
"Hyperledger Indy Node","Support Payment API txns in load script","We need to support Payment API txns (provided by libindy and its wrappers) in load script.",Task,High,Complete,"2018-05-23 10:13:31","2018-05-23 10:13:31",5
"Hyperledger Indy Node","Implement mixture mode in load script or by cron","We need to implement mixture mode in load script or by cron to run mix of txns with predefined % of each txn type easily.",Task,High,Complete,"2018-05-23 10:08:16","2018-05-23 10:08:16",2
"Hyperledger Indy Node","Support all reading Indy txns in load script","1. DID reading.  2. Credential schema reading.  3. Credential definition reading.  4. Registry definition revocation reading.  5. Registry revocation reading. (!) - not listed in initial ticket  6. Registry revocation delta reading.  7. Attrib reading.",Task,High,Complete,"2018-05-23 10:08:11","2018-05-23 10:08:11",5
"Hyperledger Indy Node","Support all missing writing Indy txns in load script","1. Credential definition writing.  2. Registry definition revocation.  3. Registry entry revocation. (!) - not update  ",Task,High,Complete,"2018-05-23 10:04:11","2018-05-23 10:04:11",3
"Hyperledger Indy Node","As a Steward, I need to be able to configure iptables (or other firewall) so that client connections can be closed on timeouts","ZMQ (and hence Indy-Node) can not drop client connections from the server side if the client doesn’t close it on her side.  # https://jira.hyperledger.org/browse/INDY-1087  # https://jira.hyperledger.org/browse/INDY-1252  # https://github.com/zeromq/libzmq/issues/2877  So, If there are a lot of open connections from clients to pool, then the pool will not be able to accept any new clients/connections.    We need to define firewall rules (use iptables as a reference) to somehow deal with open connections problem.  A possible problem is that Stewards can use any firewalls. But we should at least give some recommendations and get them aware of the issue.    *Acceptance criteria*  # Instructions for Stewards  # Rules for iptables (or other firewall).  ",Task,Medium,Complete,"2018-05-22 12:02:41","2018-05-22 12:02:41",5
"Hyperledger Indy Node","Perform load testing of 25-nodes pool with increased timeouts for catchups and viewchange with enabled periodic view change","During analysis of logs retrieved from reproduction of failing case from [INDY-1259|https://jira.hyperledger.org/browse/INDY-1259] with increased timeouts for catchup and viewchange it was found that pool went through view change successfully (but view change took maximum timeout set at that time - 30 minutes). Now we need:  1) determine minimal timeout for which probability of ending up in incorrect state is low enough  2) try to catch any other possible bugs in view change that have low probability of appearance",Task,Medium,Complete,"2018-05-18 17:20:13","2018-05-18 17:20:13",3
"Hyperledger Indy Node","Add configurable option to force periodic view change in pool","This is needed to increase probability of view change during load test in order to increase probability of appearance of related bugs",Task,Medium,Complete,"2018-05-18 17:17:06","2018-05-18 17:17:06",1
"Hyperledger Indy Node","Validate new txn format","Perform validation of a new txn format done in the scope of INDY-1123 and INDY-1319.   Make sure that there is no regression.  ",Task,Medium,Complete,"2018-05-18 15:15:47","2018-05-18 15:15:47",3
"Hyperledger Indy Node","Implement hybrid legacy/PBFT viewchanger","Add minimal version of PBFT view change protocol to propagate upper bound on messages that should be ordered before entering new view - this should fix main theoretical issue with current view change approach as identified in [INDY-1296|https://jira.hyperledger.org/browse/INDY-1296].     While proper fix looks like implementing [INDY-1340|https://jira.hyperledger.org/browse/INDY-1340] it could turn out quite time consuming given current state of codebase.",Task,Medium,Complete,"2018-05-18 14:00:01","2018-05-18 14:00:01",8
"Hyperledger Indy Node","Integrate refactored legacy viewchanger into current codebase","This task is for integration of work done in [INDY-1344|https://jira.hyperledger.org/browse/INDY-1344] into current codebase.",Task,Medium,Complete,"2018-05-18 13:55:37","2018-05-18 13:55:37",5
"Hyperledger Indy Node","Implement legacy viewchanger using design for PBFT view change","Current implementation of view change is quite tangled with other codebase. This leads to following problems:  - it cannot be made pluggable as is  - it cannot be unit tested  - it's hard to modify    Reimplementing legacy view change using TDD practices and adhering to new design will solve all above mentioned problems.     This ticket is just for implementation of core logic (implementing mocks for Network, Orderer and Checkpointer as needed). Work done on mocks can be shared with [INDY-1338|https://jira.hyperledger.org/browse/INDY-1338].",Task,Medium,Complete,"2018-05-18 13:47:18","2018-05-18 13:47:18",5
"Hyperledger Indy Node","Prove production stability of an Indy network","Before encouraging people to use the Sovrin network for live loads, we need to prove that it will be stable under conditions similar to production use.    *Acceptance Criteria*   Perform a test of an Indy network that has the following attributes:   * The ledger is pre-loaded with 1 million transactions   * Pool size at least matches the number of network nodes initially expected in the Sovrin network (currently 25 nodes).   * 1K concurrent clients   * Over a 3 hour period induce a sustained throughput of 10 write transactions per second and 100 read transactions per second on average.   * Write load is a mixture of:   ** writing credentials schema (5%),   ** writing credential definition (5%)   ** revoke registry definition (5%)   ** revoke registry update (5%)   ** write DID to ledger (20%)   ** write payment to ledger (45%)   ** write attrib to ledger (15%)   * Read load is a mixture of:   ** read DID from ledger (45%)   ** read credential schema (10%)   ** read credential definition (10%)   ** read revoke registry definition (10%)   ** read revoke registry delta (10%)   ** read attrib from ledger (10%)   ** read payment balance from ledger (5%)   * Write response time should be less that 5 seconds (would also like a report of the average).   * Read response time should be less than 1 second (would also like a report of the average).    Any problems found will be logged in JIRA as separate issues for independent prioritization.    As part of this issue, it is recognized that it will be necessary to create a load testing tool sufficient to perform the test.",Task,High,Complete,"2018-05-17 14:28:45","2018-05-17 14:28:45",8
"Hyperledger Indy Node","Simple Timeout fixes of the current View Change protocol","There are problems in existing View Change protocol (see INDY-1296 and INDY-1303).  Although the correct way to deal with it is to implement PBFT View Change, we can try to have some fixes which can improve stability of the View Change.    Possible fix (with options):     # do not process any PrePrepares and Prepares above prepared_certificate.     # [optional]: send COMMITs to all nodes ot make sure they order till their prepared certificates.     # get rid of the logic on exiting on multiple rounds of catch-up without new txns caught-up.     # [optional]: we may have some positive timeout for ordering till  last_prepared     # we may continue doing catch-ups all the time, or just do one catch-up at the end, or don't do them at all for simplicity.",Task,Medium,Complete,"2018-05-17 10:43:13","2018-05-17 10:43:13",5
"Hyperledger Indy Node","Implementation: Make PBFT view change working","This includes making sure that current integration tests pass.    *Acceptance criteria:*   * Everything to make View change working should be done   * Basic integration and simulation tests (INDY-2149) with a new View Change protocol need to pass   * Enabling and replacing of the old view change protocol by the new one needs to be done in INDY-2223  ",Task,Medium,Complete,"2018-05-16 17:05:29","2018-05-16 17:05:29",5
"Hyperledger Indy Node","Implement network, executor, orderer and checkpointer as adaptors for existing codebase",,Task,Medium,Complete,"2018-05-16 17:03:37","2018-05-16 17:03:37",5
"Hyperledger Indy Node"," Define Interfaces needed for View Change Service ","Start PBFT View Change Service Implementation. Define Interfaces needed for View Change Service, such as ConsensusDataProvider and NetworkService.    *Acceptance Criteria:*   * Interfaces for services needed to implement View Change          ",Task,Medium,Complete,"2018-05-16 17:02:22","2018-05-16 17:02:22",3
"Hyperledger Indy Node","Modify WriteReqManager to meet Executor interface needs",,Task,Medium,Complete,"2018-05-16 16:56:30","2018-05-16 16:56:30",1
"Hyperledger Indy Node","Stop resetting ppSeqNo (and relying on this) in new view","PBFT View Change assumes that ppSeqNo is not reset in new views.   Plenum starts ppSeqNo from 1 on each new view.    We need to have Plenum not resetting ppSeqNo to have correct checkpoints after view change    Acceptance criteria:   * Keep (viewno, ppseqno) as 3PC key, but do not reset ppseqno on new views.   * Compare based on ppSeqNo only   ** check all Validators   * do not clear checkpoints after view change   * do not reset stable checkpoint (in `reset_checkpoints`)",Task,Medium,Complete,"2018-05-16 16:55:41","2018-05-16 16:55:41",5
"Hyperledger Indy Node","Enable full ordering of batches from last view that have been already ordered, make execution on replicas that executed them no-op","*Acceptance criteria*   # feature implemented   # all tests pass   # new tests to prove that this works   # make sure that we correctly process requests with the same did+reqId from clients (return already ordered txn)",Task,Medium,Complete,"2018-05-16 16:54:37","2018-05-16 16:54:37",5
"Hyperledger Indy Node","Explore config parameters to find the best performance/stability settings","Parameters to explore and its default values:    *3PC batching:*   Max3PCBatchSize = 100 : we send a batch if we reach this amount of 3PC messages (PROPAGATE/PRE-PREPARE/PREPARE/COMMIT) in it   Max3PCBatchWait = .001 : we send a batch if we reach this timeout from previous batch sent *and* we have at least one 3PC message in it    *Performance statistics:*   DELTA = 0.4 : sensitivity of node to master performance degradation, *increasing/reducing of it leads to increasing/reducing number of view changes respectively*   LAMBDA = 60 : master latency marker (seconds), *increase it to reduce number of view changes*   OMEGA = 5 : master/backup latency difference (seconds), *increase it to reduce number of view changes*    *ZMQ message quotes:*  ZMQ_INTERNAL_QUEUE_SIZE = 10000 : number of messages (any) that we keep in ZMQ queue, all above this number will be discarded         *List of config parameters:*  https://docs.google.com/document/d/1tqpHNdAhgLY0hftIY-tIxp2TkmXNWrMyHv_LzT8czkE/edit#heading=h.4pqcps2qiqrj",Task,High,Complete,"2018-05-16 13:15:39","2018-05-16 13:15:39",3
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.3.62","*Version Information*  indy-node 1.3.62  indy-plenum 1.2.42  indy-anoncreds 1.0.11  sovrin 1.1.10    *Major Fixes*  INDY-1256 - STN lost consensus  INDY-1284 - Unable to use read_ledger tool with the parameter to  INDY-1330 - Upgrade from 1.2.223 (1.3.55 stable analogue) to 1.3.410 (rocksdb) doesn't work    *Changes and Additions*  Support for supervisord is added (https://github.com/hyperledger/indy-node/pull/588)  Indy-node dependencies are fixed  ",Task,Medium,Complete,"2018-05-16 10:35:54","2018-05-16 10:35:54",1
"Hyperledger Indy Node","Support binding on separate NICs for Client-to-Node and Node-to-Node communication","Now indy-plenum's and indy-node's listener sockets are always bound on 0.0.0.0, it is hard-coded, so that we listen all interfaces. We need to bind listener sockets on addresses specified for node stack and client stack for complete support of separate NICs.  Also we need to be able to set different max queue sizes for node-to-node and client-to-node communications.",Task,High,Complete,"2018-05-15 16:17:52","2018-05-15 16:17:52",3
"Hyperledger Indy Node","Hot-fix of indy-node","We need to create and test a hot-fix release of indy-node with the following fixes:  * INDY-1256 (Issues with Primary restart)  * INDY-1284 (Issues with read_ledger)  * Changes for supervisord support: https://github.com/hyperledger/indy-node/pull/588    Acceptance criteria:  * Cherry-pick the fixes to stable and create a new RC  * Acceptance testing for the RC    ",Task,Medium,Complete,"2018-05-15 15:37:53","2018-05-15 15:37:53",3
"Hyperledger Indy Node","Add short checkpoints stabilization without matching digests","Now a short checkpoint which is created after catch-up cannot be stabilized because it has a not even lower bound and does not have an aggregated digest and so cannot be matched with checkpoint messages from other replicas in the protocol instance.    The next checkpoint can be stabilized. However, the threshold for stashed generations of checkpoint messages from other replicas is 2. So, if a replica lags even for one 3PC-batch at the end of the next checkpoint after the short one, there is a risk that it will gather the quorum of checkpoint messages from other replicas before it completes its own checkpoint and thus an undesired catch-up will be triggered.    To avoid such undesired triggering of catch-up, we would stabilize short checkpoints using only their upper bound for matching with checkpoint messages from other replicas and without matching digests. Such the logic of short checkpoints stabilization must be added in scope of this ticket.",Task,Medium,Complete,"2018-05-14 16:53:21","2018-05-14 16:53:21",3
"Hyperledger Indy Node","`ReqIdrToTxn` does not store information about the ledger","Currently {{ReqIdrToTxn}} stores a map of {{sha256(identifier||req_id) -> txn seq_no}}. It should store which ledger the seq no belongs to, this can be done by changing the map to {{sha256(identifier||req_id) -> ledger_id<delimiter>txn seq_no}}    It also makes sense to take into account requests' digest, not only reqId.    We also need a migration script for seqNoDb.",Task,Medium,Complete,"2018-05-14 06:06:14","2018-05-14 06:06:14",5
"Hyperledger Indy Node","Add indy-crypto package to hold list","We should add indy-crypto package to hold list in our code the same way as the other indy packages to avoid its upgrade.",Task,High,Complete,"2018-05-11 17:31:03","2018-05-11 17:31:03",2
"Hyperledger Indy Node","Create basic framework for indy-agent in python","Set up basic framework of indy-agent in python.",Task,Medium,Complete,"2018-05-11 00:06:29","2018-05-11 00:06:29",3
"Hyperledger Indy Node","Support new libindy with refactored txn format","The txn format was re-factored in the scope of INDY-1123 (`txn-refactoring` branches in plenum and node).  We need to make sure that all tests pass with a new libindy supporting the new format:    https://github.com/hyperledger/indy-plenum/tree/txn-refactoring  https://github.com/hyperledger/indy-node/tree/txn-refactroing  ",Task,Medium,Complete,"2018-05-10 09:49:36","2018-05-10 09:49:36",5
"Hyperledger Indy Node","Decrease amount of logging with INFO level","During the tests each node from the pool of 25 nodes generates about 70 Gb logs per 3 days.   100 Mb log files are rotated each 8 minutes.    Acceptance criteria:   * Changes of messages and log levels will be done in the scope of INDY-1416   * Reduce message sizes by other means:   ** get rid of class and method name   ** use one letter for message level   ** consider removing of prefixes for Replicas",Task,High,Complete,"2018-05-03 13:39:26","2018-05-03 13:39:26",3
"Hyperledger Indy Node","Improve ansible playbooks for testing pools","I've created simple ansible playbooks for test pool setup, configuration, log retrieval and cleanup: https://github.com/hyperledger/indy-node/tree/master/scripts/ansible  They are primitive, but do their job of simple tasks automation (at least for me). Now in order to become more useful these playbooks should be improved in several ways (from most to least important in my opinion):  - pool configuration options should be in separate file  - playbooks should be refactored into a number of reusable roles and tasks  - playbooks should be documented (especially after refactoring)  - playbooks should be capable of installing indy node on clean system (now they assume that instances have multiple things configured)  - test harness should be configured so further development of playbooks and roles can be done through TDD",Task,Medium,New,"2018-04-29 10:32:47","2018-04-29 10:32:47",5
"Hyperledger Indy Node","Implement more tests for view change","Based on the test plan from INDY-1303, we need to write more tests for view change.  The tests can be used to  * prove that the old protocol doesn't work  * prove that fixes of the old protocol work  * prove that the new implementation (PBFT) works     *Output*  New tests (integration, functional, load).  ",Task,Medium,Complete,"2018-04-27 08:09:40","2018-04-27 08:09:40",8
"Hyperledger Indy Node","Test Plan for View Change protocol","After analysis done in the scope of INDY-1261,  we need to come with a test plan. The plan will help us to test both existing view change as well as the new PBFT approach.    The tests can be used to  * prove that the old protocol doesn't work  * prove that fixes of the old protocol work  * prove that the new implementation (PBFT) works     *Output*  Test plan    ",Task,Medium,Complete,"2018-04-26 15:42:27","2018-04-26 15:42:27",5
"Hyperledger Indy Node","Remove ledger status based catch-up trigger together with wrong catch-up workflow","The workflow of catch-up being initiated by this trigger is wrong: a catch-up initiated in such the way is started from the ledger which the received newer ledger status belongs to - so not all ledgers may eventually catch up.    Also it seems that this catch-up trigger is redundant because we have another catch-up trigger - the checkpoint-based one - for the case if a ledger lags behind from the ledgers on other nodes.    In scope of this task we should remove ledger status based catch-up trigger and eliminate the related wrong catch-up workflow.",Task,Medium,Complete,"2018-04-25 16:57:39","2018-04-25 16:57:39",5
"Hyperledger Indy Node","Analyse the current View Change protocol issues","We are facing some issues with the current View Change protocol (see INDY-1259 for example).  One of the options is to implement PBFT View Change protocol.     But in order to fully understand the issues with the existing protocol and have more confidence in the new implementation, we need to perform deep analysis of the existing protocol.    *The output for this task*      Result of analysis and a list of issues.    For future tasks:  * test plan with test cases we need to cover  * implementation of these tests (a separate ticket)  * fixes    ",Task,Medium,Complete,"2018-04-25 16:44:12","2018-04-25 16:44:12",8
"Hyperledger Indy Node","Make sure that we have predictable dependencies version in plenum/node","We need to make sure that setup.py explicitly sets version for each dependency following the semver.  We've recently faced the issue with base58 which is fixed in the scope of INDY-1287.",Task,Medium,Complete,"2018-04-24 13:39:57","2018-04-24 13:39:57",3
"Hyperledger Indy Node","[Design] ViewChange protocol must be as defined in PBFT","We found some issues with existing View Change.  We need to implement it in a proper way as defined in PBFT: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/thesis-mcastro.pdf.    See details in https://jira.hyperledger.org/browse/INDY-1080    Please take the following requirements into consideration:  * We need to take into account RBFT and 3PC batching specific  * Have a look at the Fabric's PBFT experience and Sawtooth's RBFT one: INDY-1290  * The new View Change must be a pluggable strategy   * State machine needs to be used  * PBFT viewchange is more conservative. So this could be more succeptible to follow-the-leader DDOS attacks. We need to take this into account  * We need to have a suite of aggressive real-world simulation tests where we hammer the protocol such that we can quickly be confident we've improved without deploying.  * We need to prove that the new implementation is better by executing the tests written in the scope of INDY-1296",Story,Medium,Complete,"2018-04-23 14:45:41","2018-04-23 14:45:41",5
"Hyperledger Indy Node","Support read-only state in read_ledger script with RocksDB",,Task,Medium,Complete,"2018-04-23 14:31:08","2018-04-23 14:31:08",2
"Hyperledger Indy Node","As a Network Maintainer, I need to be able to check whether write works for every txn type without writing garbage to the ledger","*Story*   As an engineer responsible for the stability of an Indy network, I need to understand if the pool is in consensus even when there are no active transactions being written so that I can diagnose problems before they network is under and active transactions are disrupted.    *Acceptance Criteria*   * Add optional parameter for every txn: `test`   * Only Maintainer role can send txns with `test=True`   * The rules for validation and ordering of txns are the same as for non-test   * Execution of test txns doesn't write anything to the ledger (that is it's validated but not applied to uncommitted state)    *DESIGN TODO*   As an addition response for this txn can include   - send time and start processing time - to detect if pool under high load;   - how long it took to process - to be able to notice pool degradation;   - what nodes took part in consensus - determine node's availability, performance;  <---- an Item for SDK   etc    Combine with INDY-933?",Task,Medium,Complete,"2018-04-18 13:25:06","2018-04-18 13:25:06",8
"Hyperledger Indy Node","Modify existing load scripts for a better load testing","The [current scripts|https://github.com/hyperledger/indy-node/tree/master/scripts/performance] that we got look over-complicated and don't provide all necessary features that we need.   # We have multiple scripts that all do almost the same. Why can't we have just one script with parameters.   # We need to support at least the following parameters:     - type of requests (including random)   - delay between requests   - flag whether to wait for reply before sending the next request   - number of messages per thread   - number of threads",Task,High,Complete,"2018-04-18 10:15:43","2018-04-18 10:15:43",5
"Hyperledger Indy Node","As a Steward, I need to receive notifications about important events on the Node","We already have some ways and plugins for notification (email notifications, SQS, etc.)    Currently we have a feature to notify Stewards about Suspicious Spikes (by email): INDY-1251.    We need to send more notifications to Stewards:   * View Change start   * View change end   * Catchup start   * Catchup end   * Blacklisting a Node   * Blacklisting a Client   * Suspicious Primary   * Performance degradation (InstanceChange)   * Upgrade start/end   * other severe errors and warnings",Story,Medium,Complete,"2018-04-18 08:42:56","2018-04-18 08:42:56",5
"Hyperledger Indy Node","Move log compression into separate process","Currently log compression during rotation is performed in same process as request processing which can lead to 2-second pauses in processing during rotation (for 100 Mb log size threshold). Also if we switch to xz compression (which provides another 5-10 fold compression over currently used gzip) these pauses will be increased to 15-20 seconds. Moving compression into other process will eliminate these problems.",Task,Medium,Complete,"2018-04-16 14:27:13","2018-04-16 14:27:13",1
"Hyperledger Indy Node","If a requested transaction is not posted due to lack of consensus, issue an error in the logs","*Story*  As an administrator of a deployment of Indy Node, I want failed transactions due to lack of consensus to produce a log message alerting me to the fact the transaction failed so that I can troubleshoot the situation.    *Acceptance Criteria*  * If a node receives a request to write a transaction, and does not achieve consensus within the timeout period, then issue a log message with the level ERROR.  * The timeout period should be configurable, and default to 60 seconds.  * The log message should say:        *Notes*  * The work on [INDY-465|https://jira.hyperledger.org/browse/INDY-465] demonstrated that some transactions don't fail due to an error on the node, but due to some other problem with the network. The only symptom the node sees is that the transaction is not recorded to the ledger.  * We could send a keepalive to sender, but there are concerns that would be exploitable for denial of service.",Story,High,Complete,"2018-04-16 14:04:00","2018-04-16 14:04:00",2
"Hyperledger Indy Node","Date in log name doesn't accord to the day when the log was rotated","*Steps to Reproduce:*  1. Setup the pool and look at logs after several weeks:      *Actual Results:*  1. The same date (Monday of this week) is specified in log name during the week.  2. It's unclear when the file NodeX.log.2018-04-09.gz (without numeric index) was created. Looks like it is created daily, but on some nodes was rewritten:             *Expected Results:*  Dates in logs names should accord to the date when the log was rotated,  *or*  Dates should not be specified at all.     ",Task,Medium,Complete,"2018-04-16 12:03:19","2018-04-16 12:03:19",2
"Hyperledger Indy Node","Investigate and fix errors in post-install automation scripts","Build Info:  ?    Actual Results:    anoncreds_prover_get_claims_for_proof_req_works_with_empty_req_attrs_test  1 : Create wallet :: Passed  2 : Open wallet :: Passed  3 : Create 'issuer_did' :: Passed  4 : Create 'prover_did' :: Passed  5 : Create master secret :: Passed  6 : Create and store claim definition :: Passed  7 : Create claim request :: Passed  8 : Create claim :: Passed  9 : Store claims into wallet :: Failed  Traceback: 'int' object has no attribute 'encode'    anoncreds_prover_get_claims_for_proof_req_works_with_empty_req_predicate_test  1 : Create wallet :: Passed  2 : Open wallet :: Passed  3 : Create 'issuer_did' :: Passed  4 : Create 'prover_did' :: Passed  5 : Create master secret :: Passed  6 : Create and store claim definition :: Passed  7 : Create claim request :: Passed  8 : Create claim :: Passed  9 : Store claims into wallet :: Failed  Traceback: 'int' object has no attribute 'encode'    anoncreds_prover_get_claims_for_proof_req_works_with_req_predicate_and_req_attrs_test  1 : Create wallet :: Passed  2 : Open wallet :: Passed  3 : Create 'issuer_did' :: Passed  4 : Create 'prover_did' :: Passed  5 : Create master secret :: Passed  6 : Create and store claim definition :: Passed  7 : Create claim request :: Passed  8 : Create claim :: Passed  9 : Store claims into wallet :: Failed  Traceback: 'int' object has no attribute 'encode'    anoncreds_prover_get_claims_returns_all_claims_with_empty_filter_json_test  1 : Create wallet :: Passed  2 : Open wallet :: Passed  3 : Create 'issuer_did' :: Passed  4 : Create 'prover_did' :: Passed  5 : Create master secret :: Passed  6 : Create and store claim definition :: Passed  7 : Create claim request :: Passed  8 : Create claim :: Passed  9 : Create other claim :: Passed  10 : Store claims into wallet :: Failed  Traceback: 'int' object has no attribute 'encode'    ledger_build_claim_request_test  1 : Prepare pool and wallet :: Passed  2 : Create DIDs :: Passed  3 : build claim request :: Passed  4 : Verify json claim request is correct. :: Failed  Traceback: Failed. Json response is incorrect.    ledger_build_get_attrib_request_test  1 : Prepare pool and wallet :: Passed  2 : Create DIDs :: Passed  3 : Create DIDs :: Passed  4 : send attrib request :: Passed  5 : build get attrib request :: Passed  6 : Verify json get attrib request is correct. :: Failed  Traceback: Failed. Json response is incorrect. ",Task,Medium,Complete,"2018-04-13 11:52:30","2018-04-13 11:52:30",5
"Hyperledger Indy Node","Export logging strings in Indy","*Story*    As a Product Manager of Indy Node, I need a list of all the log messages so that I can review them holistically.    *Acceptance Criteria*    Produce a spreadsheet containing all the logging strings from Indy. It will have the columns:  * Current Message  * Log Level    We will add columns for Revised Message, Revised Level.    A member of the development team will review the messages and include proposed revisions.  * Reviewer can suggest a Revised Message or Revised Level, can mark level as None if the log message is unnecessary, or leave Revised Message and Revised Level blank if no changes are needed.  * If there is a Revised Message but the Current Message is empty, then the log messages is new and should be added to the product.    The spreadsheet should be delivered to Product Management and a representative of the Operations Team to review the strings.",Story,Medium,Complete,"2018-04-12 14:48:01","2018-04-12 14:48:01",5
"Hyperledger Indy Node","Default log level should contain more data to allow analize pool state","Need to show for INFO loglevel:  * received node message from   * connections changed from  * stash/stashing  * discard/discarding  * ordering COMMIT  * set last ordered  * set watermarks as  * Remote ricFlair is not connected - message will not be sent immediately.If this problem does not resolve itself",Task,Highest,Complete,"2018-04-11 13:20:08","2018-04-11 13:20:08",3
"Hyperledger Indy Node","Load testing with random delays","The QA team needs to operate a test network for 72 hours under conditions where iptables has been configured on nodes to drop packets in a way that creates periods of patchy network brownouts, and has seen no surprising loss of consensus, data corruption, crashes, stalling of catchup, or dysfunctional view change. (This experiment does NOT need to demonstrate sustained performance of 10 trans per sec; it is okay if performance is degraded during brownouts. It also might be okay to see some loss of consensus that is NOT surprising, if a brownout is very severe--as long as consensus is restored after the brownout.)",Task,High,Complete,"2018-03-29 11:38:24","2018-03-29 11:38:24",5
"Hyperledger Indy Node","Do we keep connection from clients alive if there is no requests from the client","We need to disable keep-alive and check whether ZMQ keeps connection to clients if clients don't send anything.    Do we have some settings in ZMQ to drop IDLE connections?",Task,Medium,Complete,"2018-03-29 11:27:24","2018-03-29 11:27:24",2
"Hyperledger Indy Node","Stewards must notice within 15 minutes that there was a sudden spike in traffic","Stewards must notice within 15 minutes that there was a sudden spike in traffic, and contacted Sovrin support.         We already have a functionality to detect Spikes (see `SpikeEventsEnabled`) and be able to send emails.    We need to   * make sure that it works properly   * make sure that it notifies Stewards properly   * have detailed instructions how to configure it for Stewards   * make sure that this is configured on all Nodes in the Live Pool.",Story,Medium,Complete,"2018-03-29 10:15:54","2018-03-29 10:15:54",5
"Hyperledger Indy Node","Separate NICs must be used for Client-to-Node and Node-to-Node communication","We do have at least one requirement related to DDoS that we really need to address, which is that we need to be able to bind listeners in indy-node to a specific NIC. (I am told that, although we can declare in config that we’re binding to a specific NIC, logs reveal that we always bind to all NICs. This may be inaccurate, but if it is, we should teach people how to do it right, because apparently nobody is.) The reason I feel like this is urgent is that a major DDoS mitigation strategy for MGL was the requirement that all validators should have 2 NICs--one dedicated to consensus with other validators, and one dedicated to clients. If we have bad clients doing DDoS, but we have two NICs, then we shouldn’t be able to defeat consensus on the other NIC.        We need to make sure that we support using two separate NICs and provide detailed instructions on how it can be achieved.    We should explore working capacity of 2 NIC node configuration before we will configure persistent pool this way because now both node and client IPs bind to 0.0.0.0.    - Will node work with 2 NIC and different IPs for node and client in pool ledger?  - Will pool work with 1 / f+1 / n-f / n nodes configured this way?",Story,Medium,Complete,"2018-03-28 10:26:40","2018-03-28 10:26:40",3
"Hyperledger Indy Node","[Design] Fix memory leaks when primary on backup instance is disconnected","When pool finished view change procedure and new primary for master replica is elected, each node will start selection primary on other backup replicas. Backup replica can choose disconnected node as primary, so transaction ordering will be blocked and this replica will stash all PROPAGATE request until the next view change.         *Acceptance criteria*   * Possible options for fixes   * Proposed option, plan of attack   * Create necessary tasks",Task,High,Complete,"2018-03-27 14:46:17","2018-03-27 14:46:17",3
"Hyperledger Indy Node","Investigate DID Doc Support w/in INDY","The Node needs to support the standard for DID Doc.    *Acceptance Criteria*  * Investigate the DID Doc standard.  * Create a Plan of Attack (POA) for bringing our implementation inline with the standard.  * Create the relevant issues.",Task,Medium,New,"2018-03-27 14:37:44","2018-03-27 14:37:44",5
"Hyperledger Indy Node","Tune RocksDB options for the best performance","Now RocksDB is used with default parameters. It would be nice to tune it for the best performance according to indy-node specificity.",Task,Medium,Complete,"2018-03-27 11:49:02","2018-03-27 11:49:02",5
"Hyperledger Indy Node","Migration from LevelDB to RocksDB","Corresponding migration scripts should be implemented since we plan to use RocksDB as a default key-value storage.",Task,Medium,Complete,"2018-03-27 11:43:29","2018-03-27 11:43:29",2
"Hyperledger Indy Node","Dev team needs to be able to get logs from live pool without communication with Stewards","The implementation based on the Design from INDY-1177",Story,Medium,Complete,"2018-03-27 10:03:57","2018-03-27 10:03:57",8
"Hyperledger Indy Node","Clarify current permissible actions with DID's roles","Main assumptions:  1) only the person that did the operation can add them back  2) Trustee can perform any operation no matter who remove the role    Findings:  [Current state|https://docs.google.com/spreadsheets/d/1gG99pc3Zf9j15JE64GqF7o9HVbGujdqmwUuPmYnF0sQ/edit#gid=0]",Task,Medium,Complete,"2018-03-26 16:00:18","2018-03-26 16:00:18",2
"Hyperledger Indy Node","Simultaneous connections load testing","We need to test:  - how many simultaneous connections can the pool handle without crash?  - are we able to reach and keep ~5000 simultaneous connections for a long time?  - exploratory testing of CLIENT_CONNECTIONS_LIMIT setting.    Specific test cases will be added in this ticket after discussion.",Task,High,Complete,"2018-03-23 09:19:48","2018-03-23 09:19:48",5
"Hyperledger Indy Node","Deprecation note for old GSG","We need to start the process of deprecation of the old GSG by providing necessary warnings in documentation in indy-node.",Task,Medium,Complete,"2018-03-16 13:51:05","2018-03-16 13:51:05",2
"Hyperledger Indy Node","indy-running-locally script from indy-node confuses Community","Many people in Community tries using indy-running-locally script from indy-node to start the pool.    But this script is not intended way to start the pool.    We need to find out how to help Community to avoid confusion.    As an option, we can    1) remove all references to indy-running-locally from main documentation (README)    2) Have a bold warning in this script that it should not be used and Docker/Vagrant are appropriate options.    3) Mention Docker-based solution from libindy repo to start the pool locally in README.",Task,Medium,Complete,"2018-03-16 13:49:49","2018-03-16 13:49:49",1
"Hyperledger Indy Node","Old (Python) CLI needs to have a deprecation label","We need to have a deprecation message when we start old CLI. It should say that the new libindy-based CLI needs to be used.",Task,Medium,Complete,"2018-03-16 13:46:00","2018-03-16 13:46:00",1
"Hyperledger Indy Node","Need to set enableStdOutLogging = False in /etc/indy/indy_config.py during installation of indy-node","Need to set enableStdOutLogging = False in /etc/indy/indy_config.py during installation of indy-node to don't duplicate node logs to syslog.",Task,Medium,Complete,"2018-03-12 17:29:39","2018-03-12 17:29:39",1
"Hyperledger Indy Node","[QA] Find out Stability Boundaries (number of clients and nodes for stable work)","The goals are    1) To find out the 'stability boundaries', that is how many treads (clients) and nodes in the pool we may have for a correct and stable work and when we start facing stability issues.    2) Measure performance based on the number of clients and nodes in the pool.         *Things to be tested:*   1. Set up the large pool.   2. Run load test with 1 thread writing.   3. Reset the pool.   4. Run load test with greater amount of threads.   5. Repeat steps 3-4 until problems like INDY-1180 will reproduce.",Task,High,Complete,"2018-03-12 12:35:43","2018-03-12 12:35:43",5
"Hyperledger Indy Node","Explore indy-cli `ledger custom` command","Explore indy-cli `ledger custom` command:  - non-UTF/special characters (/)  - fields' size constraints (/)  - invalid json structure (/)",Task,Medium,Complete,"2018-03-12 09:13:26","2018-03-12 09:13:26",3
"Hyperledger Indy Node","Raise to main node_request and pool_transactions fixtures and helper functions","Raise to main node_request and pool_transactions fixtures and helper functions.",Task,Medium,Complete,"2018-03-06 09:00:09","2018-03-06 09:00:09",5
"Hyperledger Indy Node","[QA] Load and performance testing","From conversation with [~<USER>:    I had a discussion last week with Nathan and <USER>around scale and performance. When you can get back to scale testing we need the following questions answered:  1 - We need to find our failure point. What I mean by that is how many transactions can we write to the ledger before it just stops taking transactions due to its size. We don't have to write transactions as fast as we can we just need to know we stop functioning if the ledger hits a certain size.  2 - How much can you front load through the genesis file? This is what was tried before by having a domain_transactions_genesis file with 10,000 NYMs in it. If we can front load the pool by loading up the genesis file it would make it faster to get a large ledger (in theory)  3 - Can we get to 10 million transactions on the ledger? (We have logigear working on some scripts that will help in this area. I will get more details to you on those scripts early next week.",Task,High,Complete,"2018-03-05 11:21:41","2018-03-05 11:21:41",5
"Hyperledger Indy Node","Use RocksDB as a key-value storage","Taken into account our agreement about refactoring with breaking changes, I believe we have a good time for migration from LevelDB to RocksDB because of the following:   # Migration to RocksDB requires migration (modification) of the ledger. We are going to migrate the ledger in any case (hopefully the last time) for refactoring, so we can combine it with RocksDB support, so no more migrations are needed in future.   # I thing migration to RocksDB should not take much time, since Dmitry did a PoC, and everything worked. We just need to merge the old branch [rocksdb-integration|https://github.com/hyperledger/indy-plenum/tree/rocksdb-integration].   # We have new resources for this work.   # It will fix bugs like INDY-1182, since it's possible to access RocksDB from other process in read only mode (it's not possible for Leveldb, that's why read_ledger script has to create a tmp copy of the ledger which is ugly, will not work for big ledgers, and has some problems for NFS mounted systems).   # RocksDB has snapshots support, which can help us in more efficient ledger catch-up.   # RocksDB works well on Windows    What needs to be done:   * Implement RocksDB implementation of key-value storage   * Integrate RocksDB into CI and CD   * Create migration script from leveldb to rocksdb   * Adapt `read_ledger` script to work with RocksDB and avoid creation of the ledger's copy",Story,Medium,Complete,"2018-03-02 08:10:55","2018-03-02 08:10:55",5
"Hyperledger Indy Node","[QA] Design traceability matrix for indy project","We need to map all *requirements and features* that we have with all *test cases* that we have (integration, system, acceptance; manual, automated).",Task,Medium,Complete,"2018-03-01 11:11:09","2018-03-01 11:11:09",5
"Hyperledger Indy Node","[TechDoc] We need to have more diagrams to understad the system from the technical point of view","We have some technical overview and documentation:   * [https://github.com/hyperledger/indy-plenum/tree/master/docs]   * [https://github.com/hyperledger/indy-plenum/blob/master/docs/main.md]   * [https://github.com/hyperledger/indy-node/tree/master/docs]    Most of the docs there are just markdown documents describing the concepts and workflows in text.    It would be great to also have some diagrams (sequence, relationship, use cases, etc.).    Examples of diagrams that we may have:   * Request ordering by a Node and replicas (validation, 3PC batch creation, 3PC protocol, BLS multi-sigs), see [https://github.com/hyperledger/indy-plenum/blob/master/docs/request_handling.md]   ** write requests (txns)   ** read requests (queries)   * View Change   * CatchUp, see https://github.com/hyperledger/indy-plenum/blob/master/docs/catchup.md   * Checkpoints   * Monitoring   * State Proofs and BLS multi-sigs                    ",Epic,Medium,Complete,"2018-02-28 15:47:53","2018-02-28 15:47:53",5
"Hyperledger Indy Node","Each next 3PC batch should have a timestamp not less than the previous one","We need to make sure that txns timestamps is a monotonically increasing sequence, so that each new transaction (3PC batch) has a timestamp greater than the previous one.    It can be done as part of dynamic validation of a timestamp proposed by the Primary.",Story,Medium,Complete,"2018-02-27 15:22:11","2018-02-27 15:22:11",2
"Hyperledger Indy Node","A node need to hook up to a lower viewChange","*Step to reproduce:*   # make some viewChanges   # demote node1 with current viewNo X   # Simultaneously stop indy-node on all nodes (exclude demoted node)   # Simultaneously start indy-node on all nodes (exclude demoted node)   # Promote node1    *Actual results:*   node1 cannot link with any viewChange from pool, until it reach #(X+1) (more than current viewChange on node1).    *Expected results:*   Re-promoted node accept viewNo from other nodes      *Acceptance Criteria:*  Create a plan of attack, and raise appropriate stories and epics that can be scheduled.",Task,Medium,Complete,"2018-02-27 14:00:15","2018-02-27 14:00:15",3
"Hyperledger Indy Node","Request view_change_done messages","If node send view_change_done messages after catchup completed and did not get quorum then it should send message request for not received view_change_done messages.",Task,Medium,Complete,"2018-02-26 11:30:22","2018-02-26 11:30:22",3
"Hyperledger Indy Node","As a developer, I need to be able to track the path of each request","It's quite hard to read the log files now.    We need a tool to track each request: what stages it passed, what was the status, etc.",Story,Medium,Complete,"2018-02-22 13:46:32","2018-02-22 13:46:32",3
"Hyperledger Indy Node","A developer needs to be able to distinguish logs of each replica","As of now we have just 1 big log file for base Node messages, plus all replicas.    It's better to split the log into the following files:    1) The main Node log for information related to all replicas    2) Each replica individual log files.",Story,Medium,Complete,"2018-02-22 11:08:15","2018-02-22 11:08:15",5
"Hyperledger Indy Node","Make sure that all tests pass on Hyperledger Jenkins","We have a number of failing tests (both plenum and node) on Hyperledger Jenkins.    We need to make sure that all the tests pass in order to fully migrate to Hyperledger Jenkins.",Task,Medium,Complete,"2018-02-22 11:06:06","2018-02-22 11:06:06",5
"Hyperledger Indy Node","A Steward needs to be able to get validator-info from all nodes","To be able to monitor pool state we should implement new request - read validator_info from all nodes. It should not require consensus, should be sent to all connected nodes",Task,Medium,Complete,"2018-02-21 16:00:17","2018-02-21 16:00:17",3
"Hyperledger Indy Node","[Design] Dev team needs to be able to get logs from live pool without communication with Stewards","Once INDY-1176 is done, we should be able to get Node Log files without communication with Stewards.    Let's design and implement how we can do it easily without sophisticated Sovrin Dashboard and UI.",Story,Medium,Complete,"2018-02-14 12:51:09","2018-02-14 12:51:09",5
"Hyperledger Indy Node","Node logs must have no sensentive information","Node logs need to be shared and public.    So, we need to make sure that they don't have any sensitive information (private keys, seeds, etc.)",Story,Medium,Complete,"2018-02-14 12:43:29","2018-02-14 12:43:29",2
"Hyperledger Indy Node","Extend of Validator Info tool to provide more information about the current state of the pool","Extend Validator Info tool to provide more information according to the Design created in the scope of INDY-1174.",Task,Medium,Complete,"2018-02-14 12:41:32","2018-02-14 12:41:32",5
"Hyperledger Indy Node","Design extension of Validator Info tool to provide more information about the current state of the pool","We need to think how Validator Info can be extended to provide more info about the health of the pool:   * Whether we have consensus or not   * Who is a Primary on each Instance   * Whether some nodes are blacklisted   * Whether some nodes are considered as Suspicious   * What are uncommitted root hashes   * etc.",Task,Medium,Complete,"2018-02-14 12:40:06","2018-02-14 12:40:06",3
"Hyperledger Indy Node","A Trustee needs to be able to restart the pool in critical situations","A Trustee needs to be able to send a Request (not transaction) to restart the whole pool at the given time.",Story,Medium,Complete,"2018-02-14 12:36:27","2018-02-14 12:36:27",3
"Hyperledger Indy Node","Sdk integarion of one tests undone in node_request","Sdk integarion for `test_different_ledger_request_interleave.py` test in plenum/test/node_request:     ",Task,Medium,Complete,"2018-02-14 12:30:22","2018-02-14 12:30:22",2
"Hyperledger Indy Node","Integrate pool_transactions with libindy","Integrate pool_transactions with libindy. ",Task,Medium,Complete,"2018-02-14 12:26:40","2018-02-14 12:26:40",5
"Hyperledger Indy Node","Move to master libindy","Need to move to master libindy for plenum. At least for tests which use libindy client.",Task,Medium,Complete,"2018-02-14 12:23:00","2018-02-14 12:23:00",1
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.3.55","*Version Information*   indy-node 1.3.55   indy-anoncreds 1.0.11   indy-plenum 1.2.34   sovrin 1.1.8    *Upgrade to this version should be performed simultaneously for all nodes (with `force=True`).*    *Major Fixes*   INDY-799 - Transactions missing from config ledger after upgrade   INDY-960 - Node is broken after load_test.py run   INDY-911 - Pool stopped taking transactions after sending 1,000 simultaneous transactions   INDY-986 - Pool stops working: Node services stop with 1,000 simultaneous clients doing GET_NYM reads   INDY-948 - Node is broken after adding it to pool   INDY-1048 - generate_indy_pool_transactions can be run only by indy user   INDY-1035 - Do not allow update of existing Schema   INDY-1018 - Pool is unable to write txns after two nodes adding   INDY-1083 - It should not be possible to override CLAIM_DEF for existing schema-did pair   INDY-1077 - The huge amount of calls and a lot of execution time in kv_store.py   INDY-1029 - One of added nodes doesn't catch up   INDY-1025 - Pool stopped working and lost consensus while new node was performing a catch-up   INDY-1054 - View Change on large pools of 19 or more nodes can cause pool to stop functioning   INDY-1034 - View change issue stopped pool from accepting new transactions   INDY-1076, INDY-1079 - Unable to send transactions in STN   INDY-1061 - Replica.lastPrePrepareSeqNo may not be reset on view change   INDY-897 - Unable to send an upgrade transaction without including demoted nodes   INDY-1069 - Nym request to STN results in inconsistent responses   INDY-959 - Validator node is re-promoted during view change   INDY-1078 - False cancel message during upgrade   INDY-1045 - Transactions added to nodes in STN during system reboot   INDY-1033 - Problems with nodes demotion during load test   INDY-995 - Node monitoring tool (email plugin) doesn't work   INDY-1074 - ATTRIB transaction with ENC and HASH doesn't work   INDY-1151 - When returning N-F nodes to the pool, View change does not occur if Primary node is stopped   INDY-1166 - Unable to recover write consensus at n-f after f+1 descent   INDY-1183 - Newly upgraded STN fails to accept transactions (pool has been broken after upgrade because of one not upgraded node)  INDY-1190 - Unable to submit upgrade transaction to STN    *Changes and Additions*   INDY-900, INDY-901 - Add indy-sdk test dependency to plenum and use indy-sdk for plenum tests   INDY-962 - Publish docker images to dockerhub   INDY-480 - Simplify view change code   INDY-878 - We need to re-factor config.py to reflect file folder re-factoring for Incubation   INDY-628 - Abstract Observers Support   INDY-1055 - Move scripts from sovrin-environment to one of Indy repo   INDY-1064 - Get rid of Sovrin dependency in environment scripts   INDY-1062 - Some mistakes and broken links in Getting Started with Indy   INDY-1060 - Some mistakes and broken links in Setting Up a Test Indy Network in VMs   INDY-1087 - Add iptables rules to limit the number of clients connections   INDY-1088 - Knowledge transfer on Indy build processes   INDY-837 - Incubation: Move CI part of pipelines to Hyperledger infrastructure   INDY-582 - As a User, I should revoke a connection by rotating my new key to nothing   INDY-1022 - Anyone needs to have access to up-to-date Technical overview of plenum and indy    *Known Issues*   INDY-1163 - Pool has lost consensus after primary demotion (with 4 nodes setup only)   INDY-1179 - Ambiguous behavior after node demotion   INDY-1180 - One of the nodes does not respond to libindy after several running load test  INDY-1197 - Pool does not work after not simultaneous manual pool upgrade  INDY-1198 - Pool stopped working if primary node was not included to schedule in upgrade transaction    *Node promoting is not recommended for 1.3.55 version according to known issues because backup protocol instances may work incorrectly until next view change.*",Task,Medium,Complete,"2018-02-12 17:12:56","2018-02-12 17:12:56",1
"Hyperledger Indy Node","[Revocation] Support getting state root by timestamp","[Timestamp Support in State|https://github.com/hyperledger/indy-node/blob/b790ab1155128e7e8ba05ac6d0d66fb5bf57bac9/design/anoncreds.md#timestamp-support-in-state]",Task,Medium,Complete,"2018-02-02 11:37:19","2018-02-02 11:37:19",5
"Hyperledger Indy Node","Support GET_REVOC_REG request","[GET_REVOC_REG|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#get_revoc_reg]  [GET_REVOC_REG_DELTA|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#get_revoc_reg_delta]",Task,Medium,Complete,"2018-02-02 11:36:17","2018-02-02 11:36:17",3
"Hyperledger Indy Node","Support REVOC_REG_ENTRY txn",[REVOC_REG_ENTRY|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#revoc_reg_entry],Task,Medium,Complete,"2018-02-02 11:34:09","2018-02-02 11:34:09",3
"Hyperledger Indy Node","Support GET_REVOC_REG_DEF request","[REVOC_REG_DEF txn|https://github.com/hyperledger/indy-node/blob/b790ab1155128e7e8ba05ac6d0d66fb5bf57bac9/design/anoncreds.md#revoc_reg_def]",Task,Medium,Complete,"2018-02-02 11:33:58","2018-02-02 11:33:58",2
"Hyperledger Indy Node","Support REVOC_REG_DEF txn","[REVOC_REG_DEF txn|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#revoc_reg_def]",Task,Medium,Complete,"2018-02-02 11:33:31","2018-02-02 11:33:31",3
"Hyperledger Indy Node","Optimize multiple agents per build usage by indy CI on Hyperledger Jenkins","Our pipelines for indy core have several stages each processed on Jenkins agents which are one-time on-demand OpenStack minions on Hyperledger Jenkins.    As long as new agent launching is quite slow operation need to:   * investigate how it actually happens on Hyperledger Jenkins for our case   * optimize    Optimization options:   * optimize pipelines according to Hyperledger Jenkins specific   * ask Hyperledger Jenkins admins to make number of hyp-x agents always available   * ask Hyperledger Jenkins admins to allow reuse of launched agents",Task,Medium,Complete,"2018-02-02 09:16:09","2018-02-02 09:16:09",5
"Hyperledger Indy Node","Add indy docker base images for CI into Hyperledger Jenkins agents","We need to optimize docker build routine by adding our base images for CI into local docker cache inside OpenStack VMs (minions) that are used as on-demand one time agents on Hyperledger Jenkins server",Task,Medium,Complete,"2018-02-02 09:08:19","2018-02-02 09:08:19",3
"Hyperledger Indy Node","Refactor NYM txn and request","[NYM req|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#nym]    [GET_NYM req|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#get_nym]    [NYM txn|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/transactions.md#nym]",Task,Medium,New,"2018-02-01 18:23:14","2018-02-01 18:23:14",3
"Hyperledger Indy Node","Refactor common Reply structure","[Common Reply format|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#common-reply-structure]",Task,Medium,New,"2018-02-01 18:22:48","2018-02-01 18:22:48",8
"Hyperledger Indy Node","Refactor common Request structure","[Common Request format|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#common-request-structure]",Task,Medium,Complete,"2018-02-01 18:22:33","2018-02-01 18:22:33",5
"Hyperledger Indy Node","Refactor common transactions structure","[Common Txn Structure|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/transactions.md#common-structure]",Task,Medium,Complete,"2018-02-01 18:22:21","2018-02-01 18:22:21",5
"Hyperledger Indy Node","How does pool deal with random delays in node-to-node messages","We need to explore delayed node-to-node messages handling because it can help to find more bugs (like it was found with view change delayed messages).",Task,Low,Complete,"2018-01-31 14:40:52","2018-01-31 14:40:52",3
"Hyperledger Indy Node","[REFACTOR] Refactoring of Request-Reply json structure","We need to perform re-factoring as described in INDY-886.",Task,Medium,Complete,"2018-01-30 14:49:17","2018-01-30 14:49:17",8
"Hyperledger Indy Node","Ensure that new load script, based on sdk, sends requests and wait for a responce in background",,Task,Medium,Complete,"2018-01-30 13:08:47","2018-01-30 13:08:47",3
"Hyperledger Indy Node","Test current View Change protocol","We need to have more intensive testing of the current view change protocol:   * that view is changed on performance degradation;   * that view is changed on disconnections of primary;   * that view change works properly with random delays;   * that view change works properly when some nodes in the pool are down;   * that view change happens when primary behaves maliciously (sends incorrect data for example);   * we need to test it also in a quite large and distributed pool    We need to design more scenarios and test them.",Story,Medium,Complete,"2018-01-17 14:31:31","2018-01-17 14:31:31",5
"Hyperledger Indy Node","Knowledge transfer on Indy build processes","To provide for steward support, we need to disseminate understanding of the Indy node build processes more widely.     Please provide time and documentation to educate [~<USER> and others as directed on the processes used to build the various parts of indy.",Task,High,Complete,"2018-01-11 21:38:44","2018-01-11 21:38:44",2
"Hyperledger Indy Node","Add iptables rules to limit the number of clients connections","The investigation done in scope of ticket [INDY-570|https://jira.hyperledger.org/browse/INDY-570] showed that there is no way to limit the number of clients connections using ZMQ API. So we need external firewall (iptables) to do it.    Corresponding iptables rule may be added manually by steward or automatically by install script. The questions here is what max number of sumultaneous connections should be specified? Just to remind: the main problem of non-limited number of clients connections is situation when we can not open some file as the limit of opened file descriptors is reached. The main point here is that we always should have ability to open files that are necessary for node functionality. So I propose the following solution:     1. calculate approximate number of file descriptors needed to open local files, DBs etc. (F)     2. calculate approximate number of file descriptors needed for communication with other nodes (N)     3. define some window, i.e. some number of spare file descriptors as two steps above calculate file descriptors approximately (W)     4. now we can calculate max number of clients connections (X): X = LimitNOFILE - (F + N + W)",Task,High,Complete,"2018-01-11 09:45:43","2018-01-11 09:45:43",3
"Hyperledger Indy Node","[Design] Design a better approach for View Change","We use a simple Catch-up based approach for View Change (Option 4 in [View Change Protocol|https://docs.google.com/document/d/1oftK70I00wPGXFvFqiA2tmpwU7-kIjgNNieYkZcFkic/edit#]).  Have a design for better approach.    - Send a PR with the proposed Design to the repo  - take into account desired re-factoring in the scope of INDY-970 and INDY-971",Task,Medium,Complete,"2018-01-09 09:31:14","2018-01-09 09:31:14",5
"Hyperledger Indy Node","Get rid of Sovrin dependency in environment scripts","get rid of all Sovrin dependencies after moving sovrin-environment scripts to Indy.",Task,Medium,Complete,"2017-12-20 15:04:36","2017-12-20 15:04:36",3
"Hyperledger Indy Node","[Refactor] Get rid of Registry-based pools","As of now, we have two ways to initialize the Pool: transaction-based (Pool ledger txns) and Registry-based (explicit).  Registry-based approach is not used anywhere except tests.    We need to get rid of Registry-based approach:  - simplify abstractions  - make all tests to use txnPoolNodeSet.",Task,Medium,Complete,"2017-12-19 15:52:57","2017-12-19 15:52:57",5
"Hyperledger Indy Node","[Refactor] Get rid of RAET code","We can remove all RAET-specific code",Task,Medium,Complete,"2017-12-19 15:49:33","2017-12-19 15:49:33",2
"Hyperledger Indy Node","Move scripts from sovrin-environment to one of Indy repo","We need to move Docker, Vagrant and other scirpts from sovrin-environment to one of Hyperledger Indy's team (need to decide to which one).  Update the documentation correspondingly.  ",Task,Medium,Complete,"2017-12-19 15:38:31","2017-12-19 15:38:31",2
"Hyperledger Indy Node"," Multi-node read/write ledger operations using global network","As a follow-up of INDY-974 and INDY-977,  we need to analyse multi-node read/write ledger operations using global network.  We need to investigate influence of global routing and AWS infrastructure, comparing throughput degradation with isolated pool.",Task,Medium,New,"2017-12-18 14:01:08","2017-12-18 14:01:08",5
"Hyperledger Indy Node"," Multi-node read/write ledger operations using isolated local network","As a follow-up of INDY-974 and INDY-977,  we need to investigate multi-node read/write ledger operations using isolated local network.    We can investigate functionality of isolated pool with different number of nodes without influence of global routing and check implementation of requests processing and RBFT influence.",Task,Medium,New,"2017-12-18 13:53:42","2017-12-18 13:53:42",5
"Hyperledger Indy Node","Single-node read/write ledger operations with hard disk (including levelDB)","As a follow-up of INDY-974 and INDY-977,  we need to perform single-node read/write ledger operations with hard disk (including levelDB).  At this stage we can analyse levelDB settings and hard disk usage, try to find the ways to minimise disk I/O operations (use caches, I/O batching etc.).",Task,Medium,New,"2017-12-18 13:41:35","2017-12-18 13:41:35",5
"Hyperledger Indy Node"," Single-node read/write ledger operations without hard disk (in-memory)","As a follow-up of INDY-974 and INDY-977,  we need to perform single-node read/write ledger operations without hard disk (in-memory).  At this stage we can determine problems in implementation of tree algorithms by analysis of decreasing of number of read/write operations per second caused by growing number of written records.",Task,Medium,New,"2017-12-18 13:40:20","2017-12-18 13:40:20",5
"Hyperledger Indy Node","Static code analysis to find the cause of performance issues","As a follow-up of INDY-974 and INDY-977,  we need to perform static code analysis to find out a possible reason of issues with performance.  We need to figure out some obvious implementation inaccuracy like unnecessary iteration over all records in ledgers and so on.",Task,Medium,New,"2017-12-18 13:38:39","2017-12-18 13:38:39",5
"Hyperledger Indy Node","Create Entity Relationship Diagram of current ledger objects including primary and secondary keys used to link each object together.","Create a diagram showing all the data objects that exist on the ledger, and are especially important for the identity protocol to use and reference the objects on the ledger (schema, claim definition, revocation registry, nym+attrs/did document, claim request, claim, proof request and proofs) so that we can demonstrate that the object retrieval calls on the ledger are properly formed and match the main use cases in libindy and elsewhere.",Task,Medium,Complete,"2017-12-14 17:02:45","2017-12-14 17:02:45",2
"Hyperledger Indy Node","Do not allow update of existing Schema","As Schema is identified by (NAME, VERSION, ORIGIN), but `schema_seq_no` is one of identifiers of CLAIM_DEF, we need to disallow any updates of existing Schemas.  If one needs to make changes in a Schema, he needs to create a new one with a new Version (or with a new name).",Task,Medium,Complete,"2017-12-14 10:12:24","2017-12-14 10:12:24",1
"Hyperledger Indy Node","Anyone needs access to a public Indy overview from the product point of view ","- We need to have a documentation describing Indy project from the Product point of view  - It should point to the main goals of Indy, use cases and requirements.   - It should mention what differs Indy from other Blockchains  - We need to have this doc public on GitHub  (in plenum and node repos).",Story,Medium,Complete,"2017-12-08 09:58:03","2017-12-08 09:58:03",5
"Hyperledger Indy Node","Anyone needs to have access to up-to-date Technical overview of plenum and indy","- We need to have enough technical documentation available in our Github repos   - We need to describe how Plenum works, what are key concepts and features, why it was created that way, what are the differences from other blockchains  - We need to have at least one Markdown document in plenum/node repos (doc folder) describing all this  - We need to have a link to this doc from README.",Story,Medium,Complete,"2017-12-08 09:53:53","2017-12-08 09:53:53",5
"Hyperledger Indy Node","Change fixture for 3pc and appropriative tests for node_request","Need to change fixtures like propageted1, preprepared1, prepared1, etc, which used for 3pc states testing.    list of tests:   plenum/test/node_request/test_commit/test_num_commit_with_2_of_6_faulty.py   plenum/test/node_request/test_commit/test_num_commit_with_one_fault.py   plenum/test/node_request/test_commit/test_num_of_commit_with_f_plus_one_faults.py   plenum/test/node_request/test_commit/test_num_of_commit_with_zero_faulty_node.py   plenum/test/node_request/test_commit/test_num_of_sufficient_commit.py    plenum/test/node_request/test_order/test_request_ordering_2.py    plenum/test/node_request/test_pre_prepare/test_non_primary_sends_a_pre_prepare.py   plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_f_plus_one_faults.py   plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_one_fault.py   plenum/test/node_request/test_pre_prepare/test_num_of_preprepare_with_zero_faulty_node.py   plenum/test/node_request/test_pre_prepare/test_num_of_sufficient_preprepare.py   plenum/test/node_request/test_pre_prepare/test_primary_sends_preprepare_of_high_num.py   plenum/test/node_request/test_prepare/*    plenum/test/node_request/test_propagate/test_num_of_propagate_with_f_plus_one_faulty_nodes.py   plenum/test/node_request/test_propagate/test_num_of_propagate_with_one_fault.py   plenum/test/node_request/test_propagate/test_num_of_propagate_with_zero_faulty_node.py   plenum/test/node_request/test_propagate/test_num_of_sufficient_propagate.py    plenum/test/node_request/node_request_helper.py   plenum/test/node_request/test_already_processed_request.py   plenum/test/node_request/test_different_ledger_request_interleave.py   plenum/test/node_request/test_quorum_disconnected.py   plenum/test/node_request/test_quorum_faulty.py   plenum/test/node_request/test_split_non_3pc_messages_on_batches.py",Task,Medium,Complete,"2017-12-06 12:16:46","2017-12-06 12:16:46",8
"Hyperledger Indy Node","We need to reduce the read load on Validators Pool","See https://docs.google.com/document/d/1HcVp0V1RvgHanW_et84ttiga-eWkOaNyzhBzFwsWBIg/edit#heading=h.gzcglndoi3x9",Story,Medium,New,"2017-12-06 09:56:06","2017-12-06 09:56:06",8
"Hyperledger Indy Node","clean up Getting Started Guide","A developer who encounters Indy should find the Getting Started Guide prominently linked from the readme.md in the repo of all indy-* repos, as well as on any wikis for the repos. All these links should point to the same place.    Links that are the master branch of the code should point to the version of the Guide that's on either master or stable, but never to any other branches. Links on the stable branch should always point to the Guide on the stable branch, never to master.    All old/stale versions of the Guide (in Google Docs, in PDFs on sovrin.org, in github.com/hyperledger/archive) should be marked as deprecated and/or updated to point to more recent content, or deleted/hidden so people can't be confused.    Testing of the Getting Started Guide should evaluate all branches of the flow described in the Guide (e.g., both Docker and Vagrant), and should begin by browsing to the github repo and following a link from the readme.md, to guarantee that QA folks are experiencing the guide the same way a developer would find it. Specifically, the QA workflow should not begin by browsing to a bookmarked URL that might be different from the one external developers would find on their own.    If a particular variant of the Guide isn't working (e.g., Docker doesn't work), the Guide should not be considered functional until the bug is fixed or the variant of the workflow is removed from the Guide.",Task,High,Complete,"2017-12-06 00:42:45","2017-12-06 00:42:45",2
"Hyperledger Indy Node","Change test_valid_message_request to using sdk","Need to change test to using indy sdk.  For now we cannot get invalid request from node.    Test location:  plenum/test/node_request/message_request/test_valid_message_request.py",Task,Medium,Complete,"2017-12-05 13:19:57","2017-12-05 13:19:57",1
"Hyperledger Indy Node","Cannot move plenum/test/instances tests to sdk",,Task,Medium,Complete,"2017-12-04 12:00:19","2017-12-04 12:00:19",2
"Hyperledger Indy Node","Yum repository to serve CentOS packages","Once CentOS CI testing and packaging is done we need to publish the packages to somewhere. Seems, it's not a problem to set up it on ubuntu machine as long as [creatrepo|http://manpages.ubuntu.com/manpages/xenial/man8/createrepo.8.html] is available. Thus, we can create yum repository on the same machine as apt one (repo.sovrin.org)",Task,Medium,Complete,"2017-12-04 08:29:08","2017-12-04 08:29:08",3
"Hyperledger Indy Node","Split private Jenkins shared repos into public and private","Currently CD routine for both building and publishing is placed in private [jenkins-shared|https://github.com/evernym/jenkins-shared] and [sovrin-packaging|https://github.com/evernym/sovrin-packaging] repos. And it makes really hard to contribute for developers from outside the Evernym organization.    The solution could be to split mentioned private repos to private and public parts:   * Public part will get the code for building. It also could define common CD pipeline steps sequence, implement them as a templates and provide an API to pass things specific for different environment (like credentials references, callback functions for delivering logic etc.). Also someday we can move there CI logic to resolve copy-paste madness between Jenkinsfile.ci in Indy core github repos.   * private will keep all things specific for Evernym company (like credentials references, publishing: packaging repos endpoints, specific logic of debian/centos... management). It will configure the public part    Also once we make public repo we should be able to verify PRs for it. Thus, we need testing pipeline on Jenkins as well.",Task,Medium,Complete,"2017-12-04 07:39:40","2017-12-04 07:39:40",5
"Hyperledger Indy Node","Change logging tests","Need to change tests for logging rejected requests.  testLoggingTxnStateForInvalidRequest  testLoggingTxnStateWhenCommitFails    Test location:    indy-plenum/plenum/test/logging/test_logging_txn_state.py",Task,Medium,Complete,"2017-12-01 14:06:58","2017-12-01 14:06:58",2
"Hyperledger Indy Node","Changing  test_batch_rejection to use sdk","Need to change tests:  testViewChangeAfterBatchRejected  testMoreBatchesWillBeSentAfterViewChange.    Test location:  indy-plenum/plenum/test/batching_3pc/test_batch_rejection.py",Task,Medium,Complete,"2017-12-01 13:40:02","2017-12-01 13:40:02",1
"Hyperledger Indy Node","Cannot move bls tests to sdk","connect  to pool with invalid bls key in one node does not work",Task,Medium,Complete,"2017-12-01 09:32:28","2017-12-01 09:32:28",2
"Hyperledger Indy Node","Change testRequestDynamicValidation test to using indy-sdk","Need to change test testRequestDynamicValidation to using indy-sdk.  For now, indy-sdk raises only general IndyError exception while getting REJECTED message.  Therefore, when corresponded functionality would be added to indy-sdk, this test should be changed.    Test location:  indy-plenum/plenum/test/batching_3pc/test_basic_batching.py",Task,Medium,Complete,"2017-11-30 09:50:56","2017-11-30 09:50:56",1
"Hyperledger Indy Node","[Refactor] Replace replica.py with new Actors","replica.py contains lots of different logic for processing of all 3PC messages, Checkpoints, parts of view change, etc.  Break the monolith.    Integrate all Request related actors together and remove replica.py (INDY-495, INDY-978, INDY-979, INDY-980)    See https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r    ",Task,Medium,Complete,"2017-11-29 13:08:20","2017-11-29 13:08:20",8
"Hyperledger Indy Node","[Refactor] Create Checkpoint Actor","replica.py contains lots of different logic for processing of all 3PC messages, Checkpoints, parts of view change, etc.  Break the monolith.    In particular, create Checkpoint Actor and apply state machine to it  See https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r    The code may live separately. Full integration and replacement of Replica can be done in another task.",Task,Medium,Complete,"2017-11-29 13:06:25","2017-11-29 13:06:25",5
"Hyperledger Indy Node","[Refactor] Apply state machine to 3PC Actor","replica.py contains lots of different logic for processing of all 3PC messages, Checkpoints, parts of view change, etc.  Break the monolith.    In particular, create 3PC Actor and apply state machine to it  See https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r    The code may live separately. Full integration and replacement of Replica can be done in another task.",Task,Medium,Complete,"2017-11-29 13:04:57","2017-11-29 13:04:57",8
"Hyperledger Indy Node","[Refactor] Apply state machine to Requests","replica.py contains lots of different logic for processing of all 3PC messages, Checkpoints, parts of view change, etc.   Break the monolith.    In particular, apply state machine to Requests   See [https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r]    The code may live separately. Full integration and replacement of Replica can be done in another task.",Task,Medium,"To Develop","2017-11-29 13:04:18","2017-11-29 13:04:18",8
"Hyperledger Indy Node","Documentation changes to How To documents due to rebrand and file / folder changes","We need to update many of the How To documents with the new release. We have rebranded sovrin to indy and changed several of the files and folder names or locations.    A google document in the How To directory contains a list of the documents and the changes necessary.    .Doc Changes for Indy Rebrand and Folder Migration  https://docs.google.com/document/d/1NCBC7o3a-G902gv7ltpQBs9ywzvC3HF58379m7TRiFM/edit#",Task,Medium,Complete,"2017-11-28 16:46:08","2017-11-28 16:46:08",3
"Hyperledger Indy Node","[Refactor] [Design] Apply state machine to Catchup code","See https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#    Design State Machine refactoring for Catch-up.  Analyse existing issues with catch-up and whether refactoring can fix them.",Task,Medium,Complete,"2017-11-28 15:49:42","2017-11-28 15:49:42",8
"Hyperledger Indy Node","[Refactor] Apply state machine to View Change code","See https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r",Task,Medium,Complete,"2017-11-28 15:48:59","2017-11-28 15:48:59",8
"Hyperledger Indy Node","Enhancements to validator-info","The validator-info diagnostic script is an excellent addition to the Indy suite.  Here are a few additional features that will enhance its usefulness:   # In the verbose human-readable output, indicate the time (which must include the timezone) in ISO 8601 format.  For example: Current time:    2017-12-15T15:53:00+05:00  # In addition to the DID and verkey, report the BLS public key   # Indicate the nodes that are the current primaries.  In the verbose human-readable output, this can be indicated by the primary number in parenthesis after the name of the nodes that are primaries, in this fashion:         For the json output, this could be done using a hash instead of a list for the nodes as shown, or by another means:  ",Story,Medium,Complete,"2017-11-27 21:36:43","2017-11-27 21:36:43",2
"Hyperledger Indy Node","DOC: Request for release notes on Indy-node 1.2.50","*Version Information*  indy-node 1.2.49  indy-anoncreds 1.0.11  indy-plenum 1.2.29  sovrin 1.1.7    *Major Fixes*  INDY-759 - validator maintains pace with network, exactly 12 transactions behind  INDY-895 - Wrote a migration that eliminates the rudimentary difference between ledgers  INDY-231 - Fixed a bug with a wrong upgrade time  INDY-157 - Fixed a bug with failed node restart after canceled pool upgrade  INDY-801 - Sovrin logs are insufficient for failed upgrade  INDY-917 - Node gets wrong upgrade_log entries after restart and runs the wrong upgrade  INDY-231 - Upgrade scheduled to future date happened in current date on part of nodes   INDY-701 - More earlier pool_upgrade was not happened when there were scheduled upgrade to future date.  INDY-932 - Validator runs instance change continually (issue found on live pool)  INDY-909 - New nodes added to existing pool are unable to participate in consensus after the upgrade  INDY-541 - Node logs repeat message NodeRequestSuspiciousSpike suspicious spike has been noticed  INDY-941 - Unable to catch up agent if a validator is down  INDY-907 - Enhance catch up mechanism to let it work with large ledgers  INDY-849 - Pool can't be restored after losing consensus if at least one view change was happened  INDY-941 - Unable to catch up agent if a validator is down  INDY-958 - Pool is unable to write NYMs after BLS keys enabling  INDY-953 - The last pool node is failed to upgrade during pool upgrade  INDY-954 - State Proof creating is fixed  INDY-949 - State Proof verifying is fixed    *Changes and Additions*  INDY-670 - Signed State implementation  INDY-790 - State Proofs implementation  INDY-829 - Remove all non-Indy branding from indy-plenum repo  INDY-855 - Remove all non-Indy branding from indy-anoncreds repo  INDY-830 - Remove all non-Indy branding from indy-node repo  INDY-877 - Backward compatibility of nodes with state proofs support with old clients  INDY-880 - Supported rebranding in sovrin package  INDY-891 - Supported rebranding in Docker and Vagrant environments of sovrin-environments  INDY-831 - Support of multiple pool networks by Indy Node  INDY-832 - Support of multiple pool networks by Indy Client (CLI)  INDY-833 - Proper file folder paths for system service  INDY-927 - Client needs to be able to send read requests to one Node only  INDY-928 - Client needs to be able to make sure that we have the latest State Proof     *Known Issues*  INDY-960 - Node is broken after load_test.py run    *Additional Info*  Mapping of all file/folder changes is here: https://docs.google.com/spreadsheets/d/1A84H8knCtn8rrTirzxta8XC1jpHBjvQiqrxquTv6bpc/edit#gid=0",Story,Medium,Complete,"2017-11-24 10:02:53","2017-11-24 10:02:53",1
"Hyperledger Indy Node","Publish docker images to dockerhub","Need:   * create an automated process of pushing indy's docker images to Hyperledger's dockerhub account   * push [baseimages|https://github.com/hyperledger/indy-node/tree/master/docker-files/baseimage] to optimize docker build routine for the ones that base on them (e.g. images for ci testing)",Task,Medium,Complete,"2017-11-24 07:36:50","2017-11-24 07:36:50",3
"Hyperledger Indy Node","[devops] We need to be able to work with docker images for running tests in Hyperledger infrustructure","- Our tests are run in Docker.  - Our current Jenkins uses persistent Agents, so there is no need to re-create Docker images  - Hyperledger Jenkins uses Agents on demand, so re-creating images may slow down tests a lot.    We need to find a solution for this (upload docker images to Dockerhub for example?)",Task,Medium,Complete,"2017-11-09 09:12:29","2017-11-09 09:12:29",5
"Hyperledger Indy Node","[Doc] Community needs to be able to follow guidelines we propose","- Need to analyse recent stories and add required guidelines   - Make the guidelines public to share with the Community    Examples of guidelines:  - file folder guideline;  - keys guideline",Story,Medium,Complete,"2017-11-07 15:31:05","2017-11-07 15:31:05",3
"Hyperledger Indy Node","There should always be fresh enough signature of a state","As of now, multi-sigs are created on every Request. But if there are no Requests in a pool, then the state and multi-signatures may become outdated.    So, a Primary needs to initiate re-signing of the same state periodically to have up-to-date state (for the latest pool ledger).   It will help to solve two problems:   - Have up-to-date multi-signature (with the latest timestamp)   - Have multi-signature for the latest pool ledger (the client can use the latest pool ledger in most of the cases)    All states (for all ledgers) need to be re-signed periodically.         *Acceptance criteria*   * Implement periodic BLS re-signing of all states (for all ledgers). It can be done via existing PrePrepare msg, or via a new message (decide when creating a PoA).   * Re-sign period needs to be configurable (config file for now). Default is 1 min.   * It should be possible to disable/enable the feature in config.  * An INFO level log message is issued which states Freshness was updated through consensus.",Story,Medium,Complete,"2017-10-27 15:26:04","2017-10-27 15:26:04",5
"Hyperledger Indy Node","Client needs to be able to make sure that we have the latest State Proof ","As of now, Results with state proofs contain the time when transaction was written to the Ledger.  It may be enough for some of the cases, but in general we also need to make sure that a Node returned a State Proof for the latest state. It's especially critical if we get reply from one node only, and this node can be malicious.    *Example:*  - There is a NYM with seqNo 10 and time X. Then the key was rotated in a NYM with seqNo 20 and time Y.  - We send GET_NYM to a node willing to get the latest key.   - But the Node is malicious, and returns result as it was at the time X and seqNo 10. A client doesn't know if this is the latest state or not.    *Possible solution:*  - we need to sign not only domain and pool state roots as of now, but also the timestamp of this state (and possibly seqNo as well).  - so, we create multi-signature over `state_root_hash+txn_root_hash+pool_state_root_hash+state_ts+ledger_id`  - include domain_state_ts and domain_state_last_seqNo into State Proof with each Reply  - With this approach, once client receives a Reply, he validates the multi-signature (against returned  `state_root_hash+txn_root_hash+pool_state_root_hash+state_ts+ledger_id`). So, a client can be sure that the result is not older than specified timestamp.  The client can later decide if the transaction is fresh enough for him. If not, he can ask another Node (another Observer for example), or fall back to f+1 replies.    The proposed solution can not guarantee that the result is really the latest one (but actually we can not guarantee it in general in BFT system, especially once we introduce Observer Nodes). But the provided information should be enough for the client to decide of the returned result is fresh enough for him.  ",Story,High,Complete,"2017-10-27 12:31:39","2017-10-27 12:31:39",3
"Hyperledger Indy Node","Client needs to be able to send read requests to one Node only","With State proofs implemented, we can reduce load on live pool dramatically, since we can send read request to one Node only.  We need to make it possible for existing client/CLI.    Think about how to do it best. Possible options: send reqs to Nodes in round robin or randomly (if there is state proof returned).",Story,High,Complete,"2017-10-26 09:37:04","2017-10-26 09:37:04",5
"Hyperledger Indy Node","Implement client migration for rebranding and multiple pool networks support upgrade","When CLI is being started for the first time (per user) after the rebranding upgrade the migration mechanism must check whether the current user's home directory contains {{.sovrin}} subdirectory. If the user's home directory contain {{.sovrin}} subdirectory, then the migration mechanism must offer the user to migrate this data. If the user agrees then the application data must be migrated from {{.sovrin}} subdirectory to {{.indy}} subdirectory and the former must be kept as a backup.    See comments to INDY-830 for details on migration for client.    *UPD:*  In scope of this task a client migration for the indy-node version that supports multiple pool networks (see INDY-832, INDY-833 for details) must also be implemented. This client migration may be combined with the client migration for the rebranded indy-node version (because both rebranding and multiple pool networks support will be introduced simultaneously in the upcoming stable indy-node version).",Task,High,Complete,"2017-10-24 17:49:23","2017-10-24 17:49:23",2
"Hyperledger Indy Node","analyze work required to do multifaceted state machine refactors","State machine thinking (not necessarily a formal state machine pattern) could probably help us implement with greater confidence in various contexts. We don't have the freedom to do a comprehensive overhaul as one big effort, but if we could identify granular subtasks and incremental improvements, we could begin them quickly.    This task is to analyze how state machine thinking could help us in at least the following 3 areas: consensus, view change, and catchup. Each of those could be different state machines.    What we'd like to do in this task is study/think about the state machines that would be appropriate to these problems, and draw a picture (UML state machine) or create some other sort of diagram that describes how we want them to work. We can then generate new tickets/subtasks to implement some or all of the work implied by those artifacts.",Task,Medium,Complete,"2017-10-23 22:02:36","2017-10-23 22:02:36",5
"Hyperledger Indy Node","Hot fix 1.1.43 Release Notes","{color:#205081}*Hot Fix*{color}    *Version Information*  indy-plenum=1.1.27   indy-anoncreds=1.0.10  indy-node=1.1.43  sovrin=1.1.6    *Bug Fixes  *    The following changes have been made in scope of the hotfix for INDY-895/INDY-869:  - Added a migration script which eliminates redundant fields with `null` values from legacy transactions in the domain ledger.  - Added a constraint on `version` field of `POOL_UPGRADE` transaction that denies values lower than the current installed version.  - Added prevention of upgrade to a lower version to `Upgrader` class.  - Fixed a bug in `Upgrader` class in search for a `POOL_UPGRADE cancel` transaction for the last `POOL_UPGRADE start` transaction.  - Added a test verifying prevention of upgrade to a lower version.  - Corrected existing tests according to introduced prevention of upgrade to a lower version.  ",Task,Medium,Complete,"2017-10-20 15:34:29","2017-10-20 15:34:29",1
"Hyperledger Indy Node","Fix documentation: init_indy_node now must be called several time for each desired network","For the moment init_indy_node script should be called only once, right after node installed.    After new file structure will be implemented the init_indy_node script must be called once for each network. So every document that mentions init_indy_node script run, should now be changed to something like this   * open /etc/indy/indy_config.py   * change NETWORK_NAME parameter to desired network   * run init_indy_node as before    NOTE: now these steps must be done for each network     ",Task,Medium,Complete,"2017-10-19 13:13:10","2017-10-19 13:13:10",1
"Hyperledger Indy Node","Remove python CLI and client code","Once indy-sdk is used for tests, and we have a CLI in indy-sdk, remove all client and CLI code from plenum and indy-node.",Task,Medium,Complete,"2017-10-10 09:45:30","2017-10-10 09:45:30",3
"Hyperledger Indy Node","Use indy-sdk for integration tests in indy-node","Use indy-sdk for sending requests to the pool in indy-node",Task,High,Complete,"2017-10-10 09:44:12","2017-10-10 09:44:12",5
"Hyperledger Indy Node","Use indy-sdk for integration tests in plenum","Use indy-sdk to send requests to the pool (instead if current python client) in plenum tests.  Do it for as much tests as possible. If there are some tests where we can not do it because of, for example, a missing feature/specific in libindy, then please create separate tickets for this.",Task,Medium,Complete,"2017-10-10 09:43:07","2017-10-10 09:43:07",13
"Hyperledger Indy Node","Add indy-sdk test dependency to plenum","We need to be able to use indy-sdk for tests.  We should take a version of indy-sdk and add it as a *test* dependency. ",Task,Medium,Complete,"2017-10-10 09:35:57","2017-10-10 09:35:57",3
"Hyperledger Indy Node","Node monitoring manual for stewards","Get all stewards running a plug in that can notify themselves of upgrades, etc. Sample SNS plug in. PART A - Need to craft information on how to monitor your node. Communicate the raw content regarding plug ins to Misty, Do this in Jira, and assign to Misty.",Task,Medium,Complete,"2017-10-03 21:16:58","2017-10-03 21:16:58",1
"Hyperledger Indy Node","Make state proof processing logic independent of txn type","As of now, a client needs to create key and value (how they are stored in State Trie) from a Replies DATA depending on the transaction type.  It makes the client code (indy-sdk in particular) dependent on the current structure of the State Trie.  One of the options on how we can get rid of it is sending KEY together with the DATA in each Reply. But we must be sure that malicious Node didn't replaced the KEY with something wrong. So, the KEY should be a part of VALUE in State Trie, that is signed by multi-sig.  ",Task,Medium,Complete,"2017-10-03 09:24:33","2017-10-03 09:24:33",3
"Hyperledger Indy Node","[REFACTOR] Design refactoring of Request-Reply and tranasactions structure","As of now, Request-Reply jsons in client-to-node communication doesn't separate properly transaction data (body) and metadata.  We need to re-factor it (see details in https://docs.google.com/document/d/1Y2e_J2sWii2f6V6aS4g8Z6bq0g9Tqr-VMKbJWx86mRI/edit#)",Task,Medium,Complete,"2017-10-03 09:13:45","2017-10-03 09:13:45",8
"Hyperledger Indy Node","Incubation: Remove all non-Indy branding from documentation beyond GitHub repositories","Non-Indy branding and names was removed from documentation in {{indy-plenum}}, {{indy-anoncreds}} and {{indy-node}} repositories in scope of INDY-829, INDY-855 and INDY-830. However there are other resources with Indy documentation (e.g. Google Drive). In scope of this ticket remove non-Indy branding and names from documentation in such repositories.    For the purposes of this ticket, the scope is all Google Doc references that are made in {{hyperledger/indy*}} repositories. It also includes links found in the Indy wiki. Any additional documents that need changes should be submitted as separate tickets.",Story,Medium,Complete,"2017-10-02 11:43:18","2017-10-02 11:43:18",2
"Hyperledger Indy Node","Provide smooth possibility to enable BLS for live pool","In order to use BLS multi-sigs and state proofs, a node needs to generate BLS keys and send NODE txn with BLS key specified.    We have the following options on how live pool can have BLS signatures supported.  1. Manual  - each Steward runs 'init_bls_keys' script  - each Steward send NODE txn with BLS key specified there    2. Semi-automated 1  - each Steward runs 'init_bls_keys' script  - we gather public BLS keys from each Steward, and edit genesis txn file and current ledgers in migration script.    3. Semi-automated 2  - we provide a script for each Steward (which needs to be executed manually) which does the following:  -- runs 'init_bls_keys'  -- send NODE txn with BLS keys  ",Task,Medium,Complete,"2017-09-29 18:36:00","2017-09-29 18:36:00",2
"Hyperledger Indy Node","[CI] Support new repo for indy-crypto in our CI/CD","A separate repo will be used for libindy-crypto and python3-indy-crypto (see IS-360).    We need to support it in indy-node and indy-plenum:  - use correct repo when running tests (ci's Dockerfile)  - copy libindy-crypto and python3-indy-crypto debs of the required version from repo X (created in IS-360) to master/stable when creating master/stable releases of indy-node (similar to how we copy plenum for latest to master)  - update Docs to point to a new repo for libindy-crypto",Task,High,Complete,"2017-09-29 10:45:59","2017-09-29 10:45:59",8
"Hyperledger Indy Node","[Incubation] Support indy branding in sovrin","https://github.com/sovrin-foundation/sovrin/blob/master/build-scripts/ubuntu-1604/postinst contains code which relies on /home/sovrin/.sovrin.  It must be changed since we use indy user now.",Task,High,Complete,"2017-09-28 11:32:48","2017-09-28 11:32:48",1
"Hyperledger Indy Node","[Refactor] We need to re-factor config.py to reflect file folder re-factoring for Incubation","There is file folder paths re-factoring in the scope of INDY-833. It makes valid folder layout for output deb packages and provides required migration.   But it doesn't assume any deep re-factoring of the code base.    We need to perform the following re-factoring on the code level:  1) Separate a number of paths in the config responsible for outputs  2) These paths must be absolute  3) These paths must be configurable for different installations (default, CLI, tests, deb, rpm, Windows, etc.)  4) We need to re-factor the code to not use paths from config only during initialization  5) We need to re-factor the code to use proper paths  6) We need to get rid of BASEDIR (there is no BASEDIR anymore, all paths are absolute).  7) We need to make tests use special config and be consistent.  8) DEB packages should use proper system-dependent configs  9) Required migration must be provided    ",Story,High,Complete,"2017-09-28 11:24:18","2017-09-28 11:24:18",13
"Hyperledger Indy Node","Backward compatibility for clients with state proof support ","We may have old client (python indy-clients) be not 100% backward compatible with pool having state proofs support.  This is because old clients will check for f+1 equal replies from the nodes. Noes with state proof support will include proof into replies. As the proof's state root multi-sig may be different for the latest txn (this is because it becomes equal only with the next batch when Primary propagates )    So, we have the following Options:    Option1:  - do nothing, and require update of the client (we will deprecate the python CLI in any case soon, so probably it isn't worth efforts here?)    Option 2:  - make sure that we always have equal replies.   - It can be done by not calculating multi-sigs be individual nodes for the latest batch, and always use the multi-sig from the Primary only.  We need to send multi-sig for timeout in this case  - It will have a problem that we will not have a multi-sig for the ;ast batch immediately (only after some timeout)  - Also we will still have a problem if some nodes in the pool have state proofs support (that is they return proofs in replies), but some nodes don't have it (that is they don't return it). It means that we may not have equal f+1 replies.    Option3:  - A new client must specify its version with each of the requests.  - If there is no version, then we assume that the client doesn't support state proofs, and we don't return it with replies.  - This is a long-term solution for other possible backward-compatibilities problems.    I prefer either Option1 or Option3",Task,Medium,Complete,"2017-09-28 11:11:50","2017-09-28 11:11:50",5
"Hyperledger Indy Node","Support a new version of indy-crypto","- There are changes in indy-crypto build that need to be supported  - Also there is a new version of indy-crypto that uses another curve",Task,Medium,Complete,"2017-09-27 11:57:36","2017-09-27 11:57:36",1
"Hyperledger Indy Node","As a Trustee, I need to be able to config Multisig parameters via config ledger","- Support MULTI_SIG_PARAMS txn in config ledger  -- Elliptic curve to use  -- generators  -- What groups (G1, G2, GT) should we use by default?  -- What functions should we choose as H and F?",Story,Medium,New,"2017-09-18 12:15:54","2017-09-18 12:15:54",5
"Hyperledger Indy Node","Incubation: Split indy-anoncreds CD pipeline to CI and CD parts","Current CD pipeline is complex and contains closed parts:  - SovrinHelpers library that is setup as jar on Jenkins  - Secrets: Keys and accounts    Community suggests the following:  - Split CD pipeline to CI and CD parts  - CI part must be as simple as possible. Ideally, doesn't require specific Jenkins setup to be executed  - CD part can keep all current CD logic and secrets.",Story,High,Complete,"2017-09-15 13:53:34","2017-09-15 13:53:34",3
"Hyperledger Indy Node","Incubation: Remove all non-Indy branding from indy-anoncreds repo","We need to remove all non-Indy branding and names from indy-anoncreds repo:  * Package names  * Code primitive names  * Paths  * Documentation    Ideally, if full-text search for Sovrin and Evernym will return nothing.",Story,Highest,Complete,"2017-09-15 13:43:55","2017-09-15 13:43:55",3
"Hyperledger Indy Node","Incubation: Check that Indy Node, CLI and SDK can be used on one PC","It must be possible to easy setup and use Indy Node, Indy CLI and Indy SDK on one PC.",Story,Medium,Complete,"2017-09-12 15:10:20","2017-09-12 15:10:20",5
"Hyperledger Indy Node","Incubation: Move CI part of pipelines to Hyperledger infrastructure","CI parts of all pipelines must be moved to Hyperledger infrastructure.",Story,High,Complete,"2017-09-12 15:05:26","2017-09-12 15:05:26",5
"Hyperledger Indy Node","Incubation: Split indy-node CD pipeline to CI and CD parts","Current CD pipeline is complex and contains closed parts:  - SovrinHelpers library that is setup as jar on Jenkins  - Secrets: Keys and accounts    Community suggests the following:  - Split CD pipeline to CI and CD parts  - CI part must be as simple as possible. Ideally, doesn't require specific Jenkins setup to be executed  - CD part can keep all current CD logic and secrets.",Story,High,Complete,"2017-09-12 15:00:37","2017-09-12 15:00:37",3
"Hyperledger Indy Node","Incubation: Split indy-plenum CD pipeline to CI and CD parts","Current CD pipeline is complex and contains closed parts:  - SovrinHelpers library that is setup as jar on Jenkins  - Secrets: Keys and accounts    Community suggests the following:  - Split CD pipeline to CI and CD parts  - CI part must be as simple as possible. Ideally, doesn't require specific Jenkins setup to be executed  - CD part can keep all current CD logic and secrets.",Story,High,Complete,"2017-09-12 15:00:10","2017-09-12 15:00:10",3
"Hyperledger Indy Node","Incubation: Proper file folder paths for system service","We need to use proper file folder paths for system service: usr, var, etc, user-configuration vs system configuration, folders for network configuration so multiple networks can co-exist    The most urgent task here is likely the file and folder location refactor, we will want to reorganize how things are laid out as soon as we can, before we pick up more production use cases and grow the network.  I walked through this with Dan and Devin and have attached an outline of what we discussed (see attached image).  Notice the paths under /var, /etc and $HOME/.indy.  The variables at the top of the board could be used as potential substitutes.  We also talked about having some sort of environment variable(s) so that system users can overwrite the home folder location to something more suitable.  Notice that this was from a brainstorming session, there is nothing set in stone.  Please add your thoughts and suggestions.  Obviously some plan as to how we can migrate files from existing locations in the current network will be important.   I look forward to hearing what tickets you think will be needed.",Story,Highest,Complete,"2017-09-12 14:54:45","2017-09-12 14:54:45",8
"Hyperledger Indy Node","Incubation: Support of multiple pool networks by Indy Client (CLI)","It should be possible to choose which network to use from the same install of Indy CLI.    This is important for test networks, private network shards for regulatory reasons, development could use this, and it helps prove Indy can be used independently of Sovrin -- reduces open source fork risk.    It is related to path refactoring in INDY-833",Story,Highest,Complete,"2017-09-12 14:37:28","2017-09-12 14:37:28",8
"Hyperledger Indy Node","Incubation: Support of multiple pool networks by Indy Node","It should be possible to choose which network to use from the same install of Indy Node.    This is important for test networks, private network shards for regulatory reasons, development could use this, and it helps prove Indy can be used independently of Sovrin -- reduces open source fork risk.    It is related to path refactoring in INDY-833",Story,Highest,Complete,"2017-09-12 14:36:52","2017-09-12 14:36:52",3
"Hyperledger Indy Node","Incubation: Remove all non-Indy branding from indy-node repo","We need to remove all non-Indy branding and names from indy-node repo:  * Package names  * Code primitive names  * Paths  * Documentation    Ideally, if full-text search for Sovrin and Evernym will return nothing.",Story,Highest,Complete,"2017-09-12 14:16:36","2017-09-12 14:16:36",5
"Hyperledger Indy Node","Incubation: Remove all non-Indy branding from indy-plenum repo","We need to remove all non-Indy branding and names from indy-plenum repo:  * Package names  * Code primitive names  * Paths  * Documentation    Ideally, if full-text search for Sovrin and Evernym will return nothing.",Story,Highest,Complete,"2017-09-12 13:41:50","2017-09-12 13:41:50",5
"Hyperledger Indy Node","Review and replace 'assert' with exceptions in indy-plenum where needed","Currently production builds use optimization for bytecode (INDY-211) when running [node|https://github.com/<USER>indy-node/blob/master/build-scripts/ubuntu-1604/postinst_node#L54] and [node_control_tool|https://github.com/<USER>indy-node/blob/master/build-scripts/ubuntu-1604/postinst_node#L78].    This silently [removes|https://docs.python.org/3.5/tutorial/modules.html#compiled-python-files] all assert statements from the code. I think all asserts (in both indy-plenum and indy-node) should be reviewed and fixed (e.g. replaced with custom/built-in exeptions) where needed.",Task,Medium,Complete,"2017-09-08 09:50:26","2017-09-08 09:50:26",3
"Hyperledger Indy Node","As I Validator or Observer Node, I need to be able to present Signed State Proof for the client request","- Support generation of State Proofs creation for client requests  - https://github.com/hyperledger/indy-plenum/pull/364  - https://docs.google.com/document/d/1x94NtronycZUppYnO_847yJ06f33QT-L90xdL1ZbTiE/edit?ts=59a687f1#",Story,High,Complete,"2017-08-29 09:00:25","2017-08-29 09:00:25",8
"Hyperledger Indy Node","Need to limit transaction size independent of message size and by transaction type","INDY-25 was meant to limit transaction size. The implementation limits the message size and not the individual transaction size to 128k.    *Problem*  We need to be able to limit the transaction size by type of transaction and not by the size of the message. One large message could contain several hundred transactions. In the case of the load_test.py script when sending 400 transactions at once a single message is sent and then rejected due to message size.     *Use Case*  The best way for users to add large amounts of NYMs to the ledger will be in sending large batches of transactions within a single message. We should be able to break out the message and deal with each transaction individually.    Limiting the size by transaction type will keep transactions within a reasonable range without giving a blanket size to all transaction types. Adding a NYM transaction does not need to be large while adding a schema or CLAIM_DEF will require a large limit.    *Requirement*  Limit size of transactions by transaction type and not by message size.      ",Story,Medium,Complete,"2017-08-23 16:59:12","2017-08-23 16:59:12",3
"Hyperledger Indy Node","[Doc] Have up-to-date HOWTO and list of Architecture docs available from GitHub","Indy's GitGub page must have all necessary information for ramp up and get familiar with the project.  It can either has  * necessary docs in .md format in source code  * docs in Github project's wiki  * docs in Hyperledger's wiki  * reference to other docs (google docs?)      It includes:  * Instructions on how to work with the code (build, run tests, static code validation) - {color:red}needs to be created{color}  * CI/CD instructions - we have  [Releases and CD pipelines architecture and Guides|https://docs.google.com/document/d/1gKf3F_r7WamoIYS_-ts2nruB2VIuiIpjmzPIVNf2VK0/edit#heading=h.sgv0qe7trz1p], but it needs to be reviewed for sure, there were some changes from that time  * Architecture docs (or references to them) - please see the list below  * CLI help: [List of CLI commands|https://docs.google.com/spreadsheets/d/1wizYZUNXdDvvj6zu7XVVwW03bVTp2mOsZakbLyj7RLM/edit#gid=0]    Some architecture and design docs we may want to reference:  * [RBFT|https://pakupaku.me/plaublin/rbft/5000a297.pdf]  * [Batching transactions into blocks|https://docs.google.com/document/d/1qYKTttP40RF0pRKVQms6_3ll9j7Wfh2F8IzTBRfVqq4/edit#heading=h.raerzlqofz47]  * [DID/DDO Design|https://docs.google.com/document/d/1UGyoLlmv9OMBi-EStDrLFvY6URwlN4hSCfXghRbSgLo/edit#heading=h.4dabf3er5xg1]  * [ledger catchup|https://docs.google.com/document/d/1GaGoEa5pPPPajnnECGWVGy6VpSgbZgKFNZwRRx3uPNU/edit]  * [View Change Protocol|https://docs.google.com/document/d/1oftK70I00wPGXFvFqiA2tmpwU7-kIjgNNieYkZcFkic/edit#heading=h.426nyiht19nr]  * [HLD of python Sovrin client|https://docs.google.com/document/d/1OxwZdhA5SjKRJ03A4IondcRqi_DsPYgYIvFIMl09lWs/edit]  * [Important Node Log Messages|https://docs.google.com/document/d/1xEHNXn9-PiPP61BccWD0SOSDsw7LYxyOgY2dHXn0X8I/edit#heading=h.fyqxs8wc7yks]  * [Sovrin Ledger Transaction Types|https://docs.google.com/spreadsheets/d/1p7nt1e6iNRfdYG-SVQmhsB2-iZ1zhFgJ1ey-kIBguiE/edit#gid=657206024]  * [Sovrin State Proofs Technical Design|https://docs.google.com/document/d/1UTzvC6by1q_Ny5ob6JFz4-q3YU2gd9-qH5iAowhVbqE/edit#heading=h.4dabf3er5xg1]  * [Anoncreds in Sovrin|https://docs.google.com/document/d/1pqyvXCcGAWFm0mHWIh9PsJp0lIdIraUsntf_3HcD294/edit]  * [Releases and CD pipelines architecture and Guides|https://docs.google.com/document/d/1gKf3F_r7WamoIYS_-ts2nruB2VIuiIpjmzPIVNf2VK0/edit#heading=h.sgv0qe7trz1p]  * [Sovrin network roles and permissions|https://docs.google.com/document/d/1gKf3F_r7WamoIYS_-ts2nruB2VIuiIpjmzPIVNf2VK0/edit#heading=h.sgv0qe7trz1p]  * [FAQ (a bit outdated)|https://docs.google.com/document/d/1XPaLrnTi4gbTFqSeakpeC1O-6vhWaH0OWUJ--TUIEsQ/edit]  ",Task,Medium,Complete,"2017-08-23 09:27:35","2017-08-23 09:27:35",5
"Hyperledger Indy Node","[QA] Add load testing to acceptance tests","* Add load testing to acceptance tests (scenario in Acceptance Tests and Matrix folder).   * Add according row to Test Matrix - RC - Sovrin - INDY spreadsheet.",Task,Medium,Complete,"2017-08-22 14:15:38","2017-08-22 14:15:38",8
"Hyperledger Indy Node","[Refactor] Make all transactions, requests and replies consistent","As of now, we may return either serialized json (in GET_ATTR) or json object in Replies (GET_SCHEMA).  Let's make it similar and consistent    Also it would be great to have one very explicit place with format of all requests and schemas (we already have schemas for validation, so maybe just make it more explicit).",Task,Medium,"To Develop","2017-08-02 13:40:35","2017-08-02 13:40:35",5
"Hyperledger Indy Node","[Design] Support anoncreds revocation in Indy","Support all transaction types related to revocation - [Anoncreds Design|https://docs.google.com/document/d/1pqyvXCcGAWFm0mHWIh9PsJp0lIdIraUsntf_3HcD294/edit]  ",Story,Medium,Complete,"2017-08-02 10:27:25","2017-08-02 10:27:25",5
"Hyperledger Indy Node","Signed State","Multisig mechanism in consensus protocol.   This is on hold as we might do a different signature scheme called BLS as shown here [https://github.com/evernym/anoncreds-priv/blob/bls-sig/anoncreds/bls_experiment.py]. Please find the paper attached. [^BLS-BFT.pdf]. Here is a supporting doc [https://docs.google.com/document/d/1WRkqNqXXi1LoVxZu0C353uR0KRatoQuvrV_UZNDuNcc/edi|https://docs.google.com/document/d/1WRkqNqXXi1LoVxZu0C353uR0KRatoQuvrV_UZNDuNcc/edit]  h3.  ",Story,High,Complete,"2017-08-02 10:27:24","2017-08-02 10:27:24",13
"Hyperledger Indy Node","Full Design for Observers","This design should be a Full Design complete with vertical slices, such that we'll start with simple static validator pool, but then move to a dynamic validator pool that has dynamic promotion and demotion based on downed nodes and poor performance and reputation.We don't have to fully design all aspects of future concerns like node reputation, but we include a good idea of how those concerns will work together.",Story,Medium,Complete,"2017-08-02 10:27:20","2017-08-02 10:27:20",5
"Hyperledger Indy Node","Abstract Observers Support",https://docs.google.com/document/d/1HcVp0V1RvgHanW_et84ttiga-eWkOaNyzhBzFwsWBIg/edit#,Story,Medium,Complete,"2017-08-02 10:27:20","2017-08-02 10:27:20",8
"Hyperledger Indy Node","Indy network should have a process for notification of upgrade error and resolution",,Story,Medium,Complete,"2017-08-02 10:27:16","2017-08-02 10:27:16",5
"Hyperledger Indy Node","As a User, I should revoke a connection by rotating my new key to nothing",,Story,Medium,Complete,"2017-08-02 10:27:15","2017-08-02 10:27:15",1
"Hyperledger Indy Node","ClientZStack should have a provision to disconnect a client","For multipart messages where the addr of the remote socket is not known (listener in ClientZStack), the remote cannot be disconnected, the consequence is that the node cannot forcefully disconnect the client, need to find some way to either send a FIN flag or something else. This mailing list thread and its replies might help, https://lists.zeromq.org/pipermail/zeromq-dev/2016-August/030774.html. Some links from zyre codebase https://github.com/zeromq/zyre/blob/055219523325cef4e87941e07abc39718d7350e4/src/zyre_node.c#L555 and https://github.com/zeromq/zyre/blob/055219523325cef4e87941e07abc39718d7350e4/src/zyre_node.c#L948. Once this is done, unskip testClientRetryRequestWhenAckNotReceived and ensure it works",Task,Highest,Complete,"2017-08-02 10:27:14","2017-08-02 10:27:14",5
"Hyperledger Indy Node","Update test deps specifications","     For now test deps declared in setup.py in 'tests_require' section but it's a setuptools's thing and pip doesn't support it. Thus we have to 'manually' add these packages during test env configuring instead of just 'pip install ...'    As a better approach we can use setuptools's [extras_require|https://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies] field which is [supported by pip|https://pip.pypa.io/en/latest/reference/pip_install/#examples].    In such case we will get the following advantages:   - we can use e.g. 'pip install .[test]' or 'pip install plenum[test]'.   - don't need to waste time for exploring test_require manually when building new pipeline in Jenkins (cause we need these packages for testing but 'pip install .' won't install it)    POA:   * update and clean up (where it necessary) setup.py files in all repos:       * update Jenkins scripts to use 'pip install .[test]' and remove unnecessary installation routine",Task,Medium,Complete,"2017-08-02 10:27:09","2017-08-02 10:27:09",1
"Hyperledger Indy Node","The Indy Network should have size limits for Schemas, Issuer Keys and Revocation registries","See rows 11,12 and 13 of https://docs.google.com/a/evernym.com/spreadsheets/d/1pf7XgCaKiiVMUJfsuwQ8TBrOvgSW0QNKhIDz9XfylBg/edit?usp=sharing",Story,Medium,Complete,"2017-08-02 10:27:08","2017-08-02 10:27:08",3
"Hyperledger Indy Node","[Refactor] Create decorator for 'lazy' fields","There are number of properties which do some initialization on first call and then return that cached value. To do it they use protected fields which are always in scope, but do nothing useful except of storing cached value.    We need decorator for this.  There is *functools.lru_cache* decorator which does memorization. If it is created with *maxsize=1* it does exactly what we need. But it is too resource consuming.     ",Story,Medium,Complete,"2017-08-01 16:25:02","2017-08-01 16:25:02",2
"Hyperledger Indy Node","Add system tests (with docker pool)","Our current tests use in memory pool of test nodes (running in one process). They are integration, but not system.  Create real system tests, that will run against a pool of real nodes (probably in a docker).  Integrate them into CI.",Task,Medium,Complete,"2017-08-01 13:33:17","2017-08-01 13:33:17",3
"Hyperledger Indy Node","[Refactor] Apply state machine to RequestManager Actor","replica.py contains lots of different logic for processing of all 3PC messages, Checkpoints, parts of view change, etc.  Break the monolith.    In particular, create Request Manager Actor and apply state machine to it  See https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r    The code may live separately. Full integration and replacement of Replica can be done in another task.",Task,Medium,Complete,"2017-08-01 13:26:00","2017-08-01 13:26:00",8
"Hyperledger Indy Node","[Refactor] Apply State machine to Node Initialization Code","node.py is a huge class dealing with lots of aspects. Re-factor it.    In particular, separate out Node Initialization:  https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r",Task,Medium,Complete,"2017-08-01 13:25:39","2017-08-01 13:25:39",8
"Hyperledger Indy Node","Simplify view change code","* Use proper abstractions for view change  * Split catch-up and view change (they should not be related)  * Separate all view change logic in a special class",Task,Medium,Complete,"2017-08-01 13:20:58","2017-08-01 13:20:58",5
"Hyperledger Indy Node","Have an explicit state machine in the code","We have states (in particular, node's states). But it's not obvious/explicit how the system goes from one state to another.  Think about State machine and State Design Pattern.  Think about existing state or state machine frameworks that can help.",Task,Medium,Complete,"2017-08-01 13:20:38","2017-08-01 13:20:38",13
"Hyperledger Indy Node","Validator: Quick System Status","We probably need to show the Sovrin Version on OS Banner(one that is shown when you ssh into validator) just like Ubuntu shows, to quickly make it clear what version of Sovrin is installed and running at the moment. The only way to know this at the moment is by running this command:  {{apt-get -s install sovrin}}    We also need some command (bash / python script) that gives a quick status of the validator  Like: Validator running time for this many hours, All Ledgers and there synced up Status, Upcoming Upgrade schedule etc.    This will make the life of stewards easy, they do not need to know the internals of using sovrin cli or even having one for knowing such trivial status.",Task,Low,"To Develop","2017-08-01 12:16:34","2017-08-01 12:16:34",3
"Hyperledger Indy Node","Log failure to post transaction","In the case that a transaction is sent to the validator pool via CLI or otherwise, and is unable to be posted to the ledger, all nodes in the pool should log the failure to post as an ERROR, including an explanation of why the ledger was unable to post the transaction.  For example, if consensus was not achieved, list the nodes that were able to participate, and which were not.    This posting may occur after a short timeout period, if necessary.",Story,Medium,Complete,"2017-07-27 21:13:33","2017-07-27 21:13:33",3
"Hyperledger Indy Node","nodeRequestSpike triggers far too often","We have a simple anomaly detector in Sovrin. It fires on several events, one of which is nodeRequestSpike. This event doesn't handle the transition from idle well enough; it is constantly firing. I suggest that we make it less sensitive as throughput nears zero, and more sensitive when throughput is significant enough that we can reason about changes being large.",Task,Medium,Complete,"2017-07-25 15:00:33","2017-07-25 15:00:33",2
"Hyperledger Indy Node","roll back default where zmq high water mark = 0","This is a follow-up to INDY-366. In that ticket, we changed the default FIFO queue in ZMQ to be unlimited. This makes certain problems less likely, but it also masks problems for a lot longer. I would be more comfortable that we were truly robust if:   # The high water mark were finite (e.g., 100,000).   # We had a way to detect that we are nearing the high water mark, and that we would report a graceful error that makes this condition easy to diagnose.   # We implemented logic in STP to discard old messages rather than new ones. (Desirable but not necessarily required.)   # If we exceed the high water mark, we exit the daemon process gracefully rather than crashing due to an out-of-memory condition.    This ticket is to track the work in items 1-4. Possibly 3 could be omitted.",Task,Medium,Complete,"2017-07-18 21:11:06","2017-07-18 21:11:06",5
"Hyperledger Indy Node","Event oriented architecture using asyncio","During work on INDY-358 it was found that actually we use asyncio without any event oriented patterns and it leads to some valuable problems and unnecessary  CPU usage.         I have created the doc that describes that in more details and proposes a set of changes to fix that:    https://docs.google.com/document/d/1m0pLAMKWRVdAxZN4ardDKGh_xV0J-0SBP-LCDOeN0M8/edit#heading=h.qapj7lclfhub",Story,Medium,New,"2017-07-13 14:50:38","2017-07-13 14:50:38",8
"Hyperledger Indy Node","improve error message for wrong verkey length","Currently, when our input validation reports an error about an incorrect length of a verkey, it uses awkward and confusing wording (though its content is correct).    Steps to Validate:  1. Add new identifier.  2. Login as Trustee.  3. Send NYM to added identifier with FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF verkey.    Actual Results:  Error: client request invalid: InvalidClientRequest('validation error: b58 decoded value length 33 should be one of 32).         Preferred wording:    Error: client request invalid: InvalidClientRequest('validation error: b58 decoded value length = 33, but should be 32.    Note that I am asking that we be smart enough that if the acceptable values is only one item long (should be one of 32), we should change the verbiage to be simply should be 32.",Task,Medium,Complete,"2017-06-27 21:23:16","2017-06-27 21:23:16",1
"Hyperledger Indy Node","Stability: Reading bad data out of the ledger","In creating a ledger and reading it in we add in bad data to see how it is read out: ",Task,Medium,Complete,"2017-06-19 23:29:48","2017-06-19 23:29:48",3
"Hyperledger Indy Node","need early validation of genesis transactions","INDY-219 showed that we could have an invalid genesis transactions file (in that case, one that used the symbolic identifier TRUSTEE instead of its numeric equivalent, 0) and not notice for a while.    I want it to be the case that an invalid genesis transactions file causes an immediate failure with a clear error message that identifies the problematic line. We may have already done this as part of our input validation effort; if not, we need to add additional logic.",Task,Medium,Complete,"2017-06-19 18:52:40","2017-06-19 18:52:40",3
"Hyperledger Indy Node","Make i/o representations of keys consistent","When dealing with keys of various types, which are really just large numbers, There are various ways to represent them to humans.  Sovrin seems to use them all - hexadecimal, base58, x85, etc.    These should be standardized, and a single representation should be used throughout Sovrin.    Example:  recently a note in the Alpha network was not communicating properly with other validators. The log message was this:    2017-06-13 15:00:14,025 | DEBUG | zstack.py ( 683) | handlePingPong | ev1 got ping from b'\{8pT}}%5v=qw0m>HUfdvJG<{/[zLO*P[s:$HKDZv'   2017-06-13 15:00:14,025 | DEBUG | zstack.py ( 788) | sendPingPong | ev1 will be sending in batch   2017-06-13 15:00:14,026 | WARNING | batched.py ( 103) | flushOutBoxes | ev1 rid b'\{8pT}}%5v=qw0m>HUfdvJG<{/[zLO*P[s:$HKDZv' has been removed   2017-06-13 15:00:14,026 | DEBUG | message_processor.py ( 29) | discard | ev1 discarding message deque([b'po']) because rid b'\{8pT}}%5v=qw0m>HUfdvJG<{/[zLO*P[s:$HKDZv' no longer available    This was difficult to debug since b'\{8pT}}%5v=qw0m>HUfdvJG<{/[zLO*P[s:$HKDZv'  does not correspond to any recognizable entry in a transaction file.          *Acceptance criteria:*   * init_indy_node must output a public key and verification key in base58   * check that there are not other public places where we output non-base58",Story,Low,Complete,"2017-06-13 17:52:03","2017-06-13 17:52:03",3
"Hyperledger Indy Node","Implement 'hash' and 'enc' fields support for ATTRIB txn in client","     The 'NotImplementedError' exceptions:    [https://github.com/sovrin-foundation/sovrin-client/blob/fa01e3eabcfcad88c54bfe51158c3de77659b600/sovrin_client/client/wallet/attribute.py#L74]         The tests which skipped by this issue:   * testSendAttribSucceedsForHexHash   * testSendAttribFailsForBase58Hash   * testSendAttribFailsForBase64Hash   * testSendAttribSucceedsForHexEnc   * testSendAttribFailsForBase58Enc   * testSendAttribFailsForBase64Enc   * testSendAttribHasInvalidSyntaxIfRawAndHashPassedAtSameTime   * testSendAttribHasInvalidSyntaxIfRawAndEncPassedAtSameTime   * testSendAttribHasInvalidSyntaxIfRawHashAndEncPassedAtSameTime   * testSendAttribHasInvalidSyntaxIfUnknownParameterIsPassed",Story,Medium,Complete,"2017-05-31 15:07:26","2017-05-31 15:07:26",3
"Hyperledger Indy Node","CLONE - ClientZStack should have a provision to disconnect a client","For multipart messages where the addr of the remote socket is not known (listener in ClientZStack), the remote cannot be disconnected, the consequence is that the node cannot forcefully disconnect the client, need to find some way to either send a FIN flag or something else. This mailing list thread and its replies might help, https://lists.zeromq.org/pipermail/zeromq-dev/2016-August/030774.html. Some links from zyre codebase https://github.com/zeromq/zyre/blob/055219523325cef4e87941e07abc39718d7350e4/src/zyre_node.c#L555 and https://github.com/zeromq/zyre/blob/055219523325cef4e87941e07abc39718d7350e4/src/zyre_node.c#L948. Once this is done, unskip testClientRetryRequestWhenAckNotReceived and ensure it works",Task,Highest,Complete,"2017-05-12 20:12:00","2017-05-12 20:12:00",3
